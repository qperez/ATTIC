label,summmarydescription
0,"RangeQuery and RangeFilter should use collation to check for range inclusionSee [this java-user discussion|http://www.nabble.com/lucene-farsi-problem-td16977096.html] of problems caused by Unicode code-point comparison, instead of collation, in RangeQuery.

RangeQuery could take in a Locale via a setter, which could be used with a java.text.Collator and/or CollationKey's, to handle ranges for languages which have alphabet orderings different from those in Unicode."
1,"NodeReferences are lost when deleting and setting the same reference in the same save() cycleI've written the following snippet to illustrate the issue :

        Node root = session.getRootNode();
        
        Node a = root.addNode(""a"");
        Node b = root.addNode(""b"");
        b.addMixin(""mix:referenceable"");
        
        a.setProperty(""p"", b);
        
        root.save();
        
        System.out.println(b.getReferences().getSize());     // --> correctly returns 1
        
        a.setProperty(""p"", (Node) null);
        a.setProperty(""p"", b);
        
        root.save();
        
        System.out.println(b.getReferences().getSize());    // --> returns 0 !

When the ChangeLog is processed, added references are processed before deleted ones, so the persisted NodeReferences is finally wrong.

I've set the priority of this issue to critical, because the persisted references count is corrupted.

A simple workaround is to first remove the property, then save, then add the property again, but it not satisfying.
"
0,"Missing class JNDIDatabaseJournalWe're dealing to set up a clustered repository and run into some issues and missing features stated to be fixed in the upcoming v1.4. But while scanning the sources of the v1.4-rc1, i still can't find the class

  JNDIDatabaseJournal (org.apache.jackrabbit.core.journal.JNDIDatabaseJournal)

as a silbing to the classes 

  JNDIDatabaseFileSystem (org.apache.jackrabbit.core.fs.db.JNDIDatabaseFileSystem)

and 

  JNDIDatabasePersistenceManager (org.apache.jackrabbit.core.persistence.db.JNDIDatabasePersistenceManager)

The missing one you'll need to configure all parts of a repository handeled in an abstract way by (e.g one common) JNDI database resource. From the shortness and simplicity of the source code of the other ones, i think adding this missing feature takes just about an hour.

Thank you for support"
0,"bulk postings should be codec privateIn LUCENE-2723, a lot of work was done to speed up Lucene's bulk postings read API.

There were some upsides:
* you could specify things like 'i dont care about frequency data up front'.
  This made things like multitermquery->filter and other consumers that don't
  care about freqs faster. But this is unrelated to 'bulkness' and we have a
  separate patch now for this on LUCENE-2929.
* the buffersize for standardcodec was increased to 128, increasing performance
  for TermQueries, but this was unrelated too.

But there were serious downsides/nocommits:
* the API was hairy because it tried to be 'one-size-fits-all'. This made consumer code crazy.
* the API could not really be specialized to your codec: e.g. could never take advantage that e.g. docs and freqs are aligned.
* the API forced codecs to implement delta encoding for things like documents and positions. 
  But this is totally up to the codec how it wants to encode! Some codecs might not use delta encoding.
* using such an API for positions was only theoretical, it would have been super complicated and I doubt ever
  performant or maintainable.
* there was a regression with advance(), probably because the api forced you to do both a linear scan thru
  the remaining buffer, then refill...

I think a cleaner approach is to let codecs do whatever they want to implement the DISI
contract. This lets codecs have the freedom to implement whatever compression/buffering they want
for the best performance, and keeps consumers simple. If a codec uses delta encoding, or if it wants
to defer this to the last possible minute or do it at decode time, thats its own business. Maybe a codec
doesn't want to do any buffering at all.
"
0,"Order of stored Fields not maintainedAs noted in these threads...

http://www.nabble.com/Order-of-fields-returned-by-Document.getFields%28%29-to21034652.html
http://www.nabble.com/Order-of-fields-within-a-Document-in-Lucene-2.4%2B-to24210597.html

somewhere prior to Lucene 2.4.1 a change was introduced that prevents the Stored fields of a Document from being returned in same order that they were originally added in.  This can cause serious performance problems for people attempting to use LoadFirstFieldSelector or a custom FieldSelector with the LOAD_AND_BREAK, or the SIZE_AND_BREAK options (since the fields don't come back in the order they expect)

Speculation in the email threads is that the origin of this bug is code introduced by LUCENE-1301 -- but the purpose of that issue was refactoring, so if it really is the cause of the change this would seem to be a bug, and not a side affect of a conscious implementation change.

Someone who understands indexing internals should investigate this.  At a minimum, if it's decided that this is not actual a bug, then prior to resolving this bug the wiki docs and some of the FIeldSelector javadocs should be updated to make it clear what order Fields will be returned in.

"
1,"Bundle persistence name index not case-sensitive in MySQL and MS SQLAs reported by Martijn Hendriks on the dev mailing list (see http://www.nabble.com/Bundle-persistence-managers---db-collation-tf3571522.html), the NAME column of the NAMES table in the bundle persistence manager needs to be case-sensitive."
1,"Preemtive Auth fails whithout credentialsThe preemtive authorization causes a HttpException to be thrown in teh
Authenticator if no credentials were provided at all. This case should be
handled quietly. A test case should be added."
1,"System view export truncates carriage returnIf a string contains a carriage return (\r), this character was truncated on some platforms. "
1,"VersionManagerImplRestore internalRestoreFrozen method has identity versus equals bugIn method protected void internalRestoreFrozen(NodeStateEx state,
                                         InternalFrozenNode freeze,
                                         VersionSelector vsel,
                                         Set<InternalVersion> restored,
                                         boolean removeExisting,
                                         boolean copy)
in the VersionManagerImplRestore class line 557 the code performs an == instead of calling the NodeId.equals() method.  We ran into problems with the code that executes below this (trying to restore a folder node throws an ItemExistsException since same sibling not allowed on folder nodes)"
1,"In JCAConnectionRequestInfo, equals() and hashCode() implementations are inconsistentJCAConnectionRequestInfo behaves differently in its equals() and hashCode() methods. The former is aware about SimpleCredentials structure, so two instances of JCAConnectionRequestInfo were supplied SimpleCredentials instances with same userID, password and attributes, they are considered equal.
But JCAConnectionRequestInfo.hashCode() just delegates to SimpleCredentials.hashCode() which is same as Object's method. This breaks session pooling."
1,"consistency check fails with derbypm if bundle size exceeds 32kdue to a 'problem' in derby DERBY-1486 interleaved reads on a bundle that is larger than about 32k results in an error:
  ERROR XJ073: The data in this BLOB or CLOB is no longer available.  
               The BLOB or CLOBs transaction may be committed, or its 
               connection is closed.

this issue was already addressed in JCR-1039 but not fixed for the consistency check."
0,"Should SegmentTermPositionVector be public?I'm wondering why SegmentTermPositionVector is public. It implements the public
interface TermPositionVector. Should we remove ""public""?"
0,"Improve FilteredQuery to shortcut on wrapped MatchAllDocsQuerySince the rewrite of Lucene trunk to delegate all Filter logic to FilteredQuery, by simply wrapping in IndexSearcher.wrapFilter(), we can do more short circuits and improve query execution. A common use case it to pass MatchAllDocsQuery as query to IndexSearcher and a filter. For the underlying hit collection this is stupid and slow, as MatchAllDocsQuery simply increments the docID and checks acceptDocs. If the filter is sparse, this is a big waste. This patch changes FilteredQuery.rewrite() to short circuit and return ConstantScoreQuery, if the query is MatchAllDocs."
1,"Cluster Journal directory should be created automaticallyIf the cluster journal directory does not exist when starting the cluster, an exception is thrown: ERROR org.apache.jackrabbit.core.RepositoryImpl - failed to start Repository: Directory specified does either not exist or is not a directory: ...

As far as I know, this is not consistent with how all other components of Jackrabbit work. I think the directory should be created automatically if it does not exist:

new File(...).mkdirs();

I know you could argue this is not a bug, but in my view it is an important usability issue."
1,Use of Multi-Args URI Causes URI-Rewriting to improperly unescape charactersSee: http://www.nabble.com/unable-to-encode-reserved-characters-using-java.net.URI-multi-arg-constructors-td14954679.html for information from the httpclient-dev thread.  The basic idea is that URI's multi-arg constructors break things.
0,"optimizations for bufferedindexinputalong the same lines as LUCENE-2816:
* the readVInt/readVLong/readShort/readInt/readLong are not optimal here since they defer to readByte. for example this means checking the buffer's bounds per-byte in readVint instead of per-vint.
* its an easy win to speed this up, even for the vint case: its essentially always faster, the only slower case is 1024 single-byte vints in a row, in this case we would do a single extra bounds check (1025 instead of 1024)
"
0,"Convert NumericUtils and NumericTokenStream to use BytesRef instead of Strings/char[]After LUCENE-2302, we should use TermToBytesRefAttribute to index using NumericTokenStream. This also should convert the whole NumericUtils to use BytesRef when converting numerics."
1,"NodeTypeDef depends on supertype orderingCurrently the NodeTypeDef.setSupertypes() method simply sets the given QName array as the supertype QName array of the defined node type, thus preserving whatever ordering a node type parser or ultimately a node type definition file uses. This causes problems for example in the equals() method that uses the order-sensitive Arrays.equals() method to check for equality of the supertype QName arrays. The current implementation does therefore not consider the node type definitions ""A > B, C"" and ""A > C, B"" as equal even though they really should be so considered.

The same problem affects also child node and property definitions. The proper fix for this issue would probably be to use Sets to store and handle this information."
0,"REFERENCE properties produce duplicate strings in memoryWhen reference property is loaded from PM, Serializer.deserialize(NodeReferences, InputStream) is called, which calls PropertyId.valueOf(String), which in turn calls NameFactoryImpl.create(String) which finally splits a full property name to namespace and local name. Namespace is internalized, but local name is not (comments say that this is done to avoid perm space overfilling).
So, in the end, a new String instance is created for local name. This leads to considerable memory waste when repository has a lot of nodes with REFERENCE properties.
It seems that local name part could be internalized here too because in the most repositories it's not allowed to create properties with arbitrary names, so the danger of perm space exhaust does not seem to be an argument.

As for ways to resolve this, maybe a new NameFactory implementation could be created which would be used for properties only (and, possibly, mainly in the PropertyId.valueOf(String)) which would extend an existing NameFactoryImpl overriding its create(String) method.

What do you think about all this?"
0,"Missing support for some ""general"" relations in QueryTreeDump and xpath.QueryFormatThe two classes lack support for some of the ""general"" relations in XPath.
"
0,"benchmark pkg: allow TrecContentSource not to change the docnameTrecContentSource currently appends 'iteration number' to the docname field.
Example: if the original docname is DOC0001 then it will be indexed as DOC0001_0

this presents a problem for relevance testing, because when judging results, the expected docname will never be present.
This patch adds an option to disable this behavior, defaulting to the existing behavior (which is to append the iteration number).
"
0,"Remove Unnecessary NULL check in FindSegmentsFile - cleanupFindSegmentsFile accesses the member ""directory"" in line 579 while performing a null check in 592. The null check is unnecessary as if directory is null line 579 would throw a NPE.
I removed the null check and made the member ""directory"" final. In addition I added a null check in the constructor as If the value is null we should catch it asap. 

"
0,"Thread starvation problems in some testsIn some of the tests, a time limit is set and the tests have a ""while (inTime)"" loop. If creation of thread under heavy load is too slow, the tasks are not done. Most tests are only useful, if the task is at least done once (most would even fail).

This thread changes the loops to be do...while, so the task is run at least one time."
1,Scorer.skipTo() does not initialize hitsSome of the custom Scorer implementations in Jackrabbit do not initialize the internal hits BitSet if skipTo() is called before next().
0,"TopDocsCollector should have bounded generic <T extends ScoreDoc>TopDocsCollector was changed to be TopDocsCollector<T>. However it has methods which specifically assume the PQ stores ScoreDoc. Therefore, if someone extends it and defines a type which is not ScoreDoc, things will break.

We shouldn't put <T> on TopDocsCollector at all, but rather change its ctor to *protected TopDocsCollector(PriorityQueue<? extends ScoreDoc> pq)*. TopDocsCollector should handle ScoreDoc types. If we do this, we'll need to change FieldValueHitQueue's Entry to extend ScoreDoc as well."
0,NodeCanAddMixinTest.testCheckedIn() has wrong option checkNodeCanAddMixinTest.testCheckedIn() checks for locking option instead of versioning option.
0,"JackrabbitParser and tika 0.7 parserHi,

I was trying to implement a custom parser and found the following problem.
Since tika 0.7 it is possible to implement your custom parser and specify it into a service provider configuration file (META-INF/services/org.apache.tika.parser.Parser). In this way there would be no need to maintain a custom tika-config.xml file if you'd like to implement a custom parser.

The problem that I had was in the JackrabbitParser because I wasn't able to instantiate the AutoDetectParser with the default constructor is will be instantiated using the default TikaConfig constructor.
Basically from tika 0.7, the TikaConfig.getTikaConfig() is instantiating the TikaConfig using the default constructor instead of accessing the tika-config.xml file from withing the package, and reads the service provider configuration files and populate the parsers map.

What I'm proposing is to change the JackrabbitParser to instantiate the AutoDetectParser using the default constructor, in this way the using tika version >= 0.7 we could easily implement our own parsers and there won't be a reason to maintain the tika-config.xml, also a sort of ""backward"" compatibility would be maintained because using the AutoDetectParser default constructor the TikaConfig is instantiated using TikaConfig.getTikaConfig() wich for tika versions < 0.7 calls the TikaConfig(InputStream) constructor whcih reads the configuration directly from the package.

Basically the JackrabbitParser should look like this:

    public JackrabbitParser() {
            	parser = new AutoDetectParser();
    }
 
Thanks,
Dan"
1,"new QueryParser fails to set AUTO REWRITE for multi-term queriesThe old QueryParser defaults to constant score rewrite for Prefix,Fuzzy,Wildcard,TermRangeQuery, but the new one seems not to."
0,"Skip deployment of jackrabbit-standaloneThe jackrabbit-standalone jar currently can't be deployed to the repository.apache.org server probably because of its size. I'm not sure if there are any good use cases where you'd want to use the standalone jar as a Maven dependency, so having it on Maven central doesn't seem that important. I'd like to make this explicit by configuring the deploy plugin to skip deploying the standalone jar."
0,"Code cleanups for Java 1.5 and more.I can't resist giving code a good cleansing when I start hacking.  Here's some simple things:
- Use character constants instead of string contstants
- Use java 1.5 style for loops
- Use StringBuilder where appropriate
- Fix javadocs
- switch somestring.equals("""") to .length() == 0
-  simplify some boolean expressions
- eliminate redundant initializers
- fix some html nits
- remove final keyword from static methods
"
0,Make all classes that have a close() methods instanceof Closeable (Java 1.5)This should be simple.
0,"Can't put non-index files (e.g. CVS, SVN directories) in a Lucene index directoryLucene won't tolerate foreign files in its index directories.  This makes it impossible to keep an index in a CVS or Subversion repository.

For instance, this exception appears when creating a RAMDirectory from a java.io.File that contains a subdirectory called "".svn"".

java.io.FileNotFoundException: /home/local/ejj/ic/.caches/.search/.index/.svn
(Is a directory)
        at java.io.RandomAccessFile.open(Native Method)
        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:212)
        at
org.apache.lucene.store.FSIndexInput$Descriptor.<init>(FSDirectory.java:425)
        at org.apache.lucene.store.FSIndexInput.<init>(FSDirectory.java:434)
        at org.apache.lucene.store.FSDirectory.openInput(FSDirectory.java:324)
        at org.apache.lucene.store.RAMDirectory.<init>(RAMDirectory.java:61)
        at org.apache.lucene.store.RAMDirectory.<init>(RAMDirectory.java:86)"
0,"All Tokenizer implementations should have constructors that take AttributeSource and AttributeFactoryI have a TokenStream implementation that joins together multiple sub TokenStreams (i then do additional filtering on top of this, so i can't just have the indexer do the merging)

in 2.4, this worked fine.
once one sub stream was exhausted, i just started using the next stream 

however, in 2.9, this is very difficult, and requires copying Term buffers for every token being aggregated

however, if all the sub TokenStreams share the same AttributeSource, and my ""concat"" TokenStream shares the same AttributeSource, this goes back to being very simple (and very efficient)


So for example, i would like to see the following constructor added to StandardTokenizer:
{code}
  public StandardTokenizer(AttributeSource source, Reader input, boolean replaceInvalidAcronym) {
    super(source);
    ...
  }
{code}

would likewise want similar constructors added to all Tokenizer sub classes provided by lucene
"
0,"it is impossible to use a custom dictionary for SmartChineseAnalyzerit is not possible to use a custom dictionary, even though there is a lot of code and javadocs to allow this.

This is because the custom dictionary is only loaded if it cannot load the built-in one (which is of course, in the jar file and should load)
{code}
public synchronized static WordDictionary getInstance() {
    if (singleInstance == null) {
      singleInstance = new WordDictionary(); // load from jar file
      try {
        singleInstance.load();
      } catch (IOException e) { // loading from jar file must fail before it checks the AnalyzerProfile (where this can be configured)
        String wordDictRoot = AnalyzerProfile.ANALYSIS_DATA_DIR;
        singleInstance.load(wordDictRoot);
      } catch (ClassNotFoundException e) {
        throw new RuntimeException(e);
      }
    }
    return singleInstance;
  }
{code}

I think we should either correct this, document this, or disable custom dictionary support..."
0,"The terms index divisor in IW should be set via IWC not via getReaderThe getReader call gives a false sense of security... since if deletions have already been applied (and IW is pooling) the readers have already been loaded with a divisor of 1.

Better to set the divisor up front in IWC."
0,"rev. 169301: wrong directory name in build.xmlbuild.xml mentions non-existing directory contrib/WordNet/ which should read
contrib/wordnet in line 418.

below the result of svn diff against the corrected and working version of build.xml

--- build.xml   (revision 169301)
+++ build.xml   (working copy)
@@ -415,7 +415,7 @@
         <!-- TODO: find a dynamic way to do include multiple source roots -->
         <packageset dir=""src/java""/>
         <packageset dir=""contrib/analyzers/src/java""/>
-        <packageset dir=""contrib/WordNet/src/java""/>
+        <packageset dir=""contrib/wordnet/src/java""/>
         <packageset dir=""contrib/highlighter/src/java""/>
         <packageset dir=""contrib/similarity/src/java""/>
         <packageset dir=""contrib/spellchecker/src/java""/>"
0,Move NodeTypeStorage to spi-commons and provide default implementationThis facilitates the implementation of NodeTypes methods of RepositoryService.
1,"Transient Repository cannot be used more than once when configured with DataSourcesThe TransientRepository cannot be used more than once when the repository is configured with the DataSources construct. This has been verified with both Oracle and Derby configurations. Once the TransientRepository closes for the first time, the ConnectionFactory class sets a boolean value named closed to 'true'.  Thereafter, any use of the ConnectionFactory throws a runtime exception.

The following stacktrace is thrown on the second attempt to utilize the repository:

2011-01-25 08:12:14 DatabaseFileSystem [ERROR] failed to initialize file system
java.lang.IllegalStateException: this factory has already been closed
	at org.apache.jackrabbit.core.util.db.ConnectionFactory.sanityCheck(ConnectionFactory.java:213)
	at org.apache.jackrabbit.core.util.db.ConnectionFactory.getDataBaseType(ConnectionFactory.java:134)
	at org.apache.jackrabbit.core.fs.db.DbFileSystem.getDataSource(DbFileSystem.java:228)
	at org.apache.jackrabbit.core.fs.db.DatabaseFileSystem.init(DatabaseFileSystem.java:190)
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$6.getFileSystem(RepositoryConfigurationParser.java:1057)
	at org.apache.jackrabbit.core.config.RepositoryConfig.getFileSystem(RepositoryConfig.java:892)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:284)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:602)
	at org.apache.jackrabbit.core.TransientRepository$1.getRepository(TransientRepository.java:179)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:279)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:375)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:123)
	...
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:115)
2011-01-25 08:12:14 RepositoryImpl [ERROR] failed to start Repository: File system initialization failure.
javax.jcr.RepositoryException: File system initialization failure.
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$6.getFileSystem(RepositoryConfigurationParser.java:1060)
	at org.apache.jackrabbit.core.config.RepositoryConfig.getFileSystem(RepositoryConfig.java:892)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:284)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:602)
	at org.apache.jackrabbit.core.TransientRepository$1.getRepository(TransientRepository.java:179)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:279)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:375)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:123)
	at TransientRepositoryTest.addNodeToRepository(TransientRepositoryTest.java:32)
	at TransientRepositoryTest.shouldNotFailWhenUsingTransientRepositoryTwice(TransientRepositoryTest.java:26)
	...
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:115)
Caused by: org.apache.jackrabbit.core.fs.FileSystemException: failed to initialize file system
	at org.apache.jackrabbit.core.fs.db.DatabaseFileSystem.init(DatabaseFileSystem.java:210)
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$6.getFileSystem(RepositoryConfigurationParser.java:1057)
	... 42 more
Caused by: java.lang.IllegalStateException: this factory has already been closed
	at org.apache.jackrabbit.core.util.db.ConnectionFactory.sanityCheck(ConnectionFactory.java:213)
	at org.apache.jackrabbit.core.util.db.ConnectionFactory.getDataBaseType(ConnectionFactory.java:134)
	at org.apache.jackrabbit.core.fs.db.DbFileSystem.getDataSource(DbFileSystem.java:228)
	at org.apache.jackrabbit.core.fs.db.DatabaseFileSystem.init(DatabaseFileSystem.java:190)
	... 43 more
2011-01-25 08:12:14 RepositoryImpl [ERROR] Error while closing Version Manager.
java.lang.NullPointerException
	at org.apache.jackrabbit.core.RepositoryImpl.doShutdown(RepositoryImpl.java:1117)
	at org.apache.jackrabbit.core.RepositoryImpl.shutdown(RepositoryImpl.java:1063)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:388)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:602)
	at org.apache.jackrabbit.core.TransientRepository$1.getRepository(TransientRepository.java:179)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:279)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:375)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:123)
	at TransientRepositoryTest.addNodeToRepository(TransientRepositoryTest.java:32)
	at TransientRepositoryTest.shouldNotFailWhenUsingTransientRepositoryTwice(TransientRepositoryTest.java:26)
	...
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:115)
2011-01-25 08:12:14 RepositoryImpl [ERROR] In addition to startup fail, another unexpected problem occurred while shutting down the repository again.
java.lang.NullPointerException
	at org.apache.jackrabbit.core.RepositoryImpl.doShutdown(RepositoryImpl.java:1136)
	at org.apache.jackrabbit.core.RepositoryImpl.shutdown(RepositoryImpl.java:1063)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:388)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:602)
	at org.apache.jackrabbit.core.TransientRepository$1.getRepository(TransientRepository.java:179)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:279)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:375)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:123)
	at TransientRepositoryTest.addNodeToRepository(TransientRepositoryTest.java:32)
	at TransientRepositoryTest.shouldNotFailWhenUsingTransientRepositoryTwice(TransientRepositoryTest.java:26)
	...
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:115)

javax.jcr.RepositoryException: File system initialization failure.
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$6.getFileSystem(RepositoryConfigurationParser.java:1060)
	at org.apache.jackrabbit.core.config.RepositoryConfig.getFileSystem(RepositoryConfig.java:892)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:284)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:602)
	at org.apache.jackrabbit.core.TransientRepository$1.getRepository(TransientRepository.java:179)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:279)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:375)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:123)
	at TransientRepositoryTest.addNodeToRepository(TransientRepositoryTest.java:32)
	at TransientRepositoryTest.shouldNotFailWhenUsingTransientRepositoryTwice(TransientRepositoryTest.java:26)
	...
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:115)
Caused by: org.apache.jackrabbit.core.fs.FileSystemException: failed to initialize file system
	at org.apache.jackrabbit.core.fs.db.DatabaseFileSystem.init(DatabaseFileSystem.java:210)
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$6.getFileSystem(RepositoryConfigurationParser.java:1057)
	... 42 more
Caused by: java.lang.IllegalStateException: this factory has already been closed
	at org.apache.jackrabbit.core.util.db.ConnectionFactory.sanityCheck(ConnectionFactory.java:213)
	at org.apache.jackrabbit.core.util.db.ConnectionFactory.getDataBaseType(ConnectionFactory.java:134)
	at org.apache.jackrabbit.core.fs.db.DbFileSystem.getDataSource(DbFileSystem.java:228)
	at org.apache.jackrabbit.core.fs.db.DatabaseFileSystem.init(DatabaseFileSystem.java:190)
	... 43 more"
0,DatabasePersistenceManager & DatabaseFileSystem: try to gracefully recover from connection loss
0,"PropertyImpl.getNode() and NamePropertyTest use different exception than documented in the JCR API JavaDocThe Property.getNode() method's JavaDoc [1] lists 3 types of exceptions: ValueFormatException, ItemNotFoundException, and RepositoryException, and that ItemNotFoundException is to be thrown when the target node could not be found.  However, the NamePropertyTest.testGetProperty() method is checking for a PathNotFoundException rather than the documented ItemNotFoundException (see [2], line 189).  Jackrabbit's implementation in PropertyImpl (see [3] line 539) delegates to Session.getNode(absolutePath) or Property.getParent().getNode(relativePath), and these methods are documented as throwing PathNotFoundException (see [4] and [5]).

Therefore, the unit test and PropertyImpl.getNode() implementation appear to be in disagreement with the JCR 2.0 API JavaDoc.

[1] http://www.day.com/maven/javax.jcr/javadocs/jcr-2.0/javax/jcr/Property.html#getNode()
[2] http://svn.apache.org/viewvc/jackrabbit/trunk/jackrabbit-jcr-tests/src/main/java/org/apache/jackrabbit/test/api/NamePropertyTest.java?revision=772352&view=markup
[3] http://svn.apache.org/viewvc/jackrabbit/trunk/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/PropertyImpl.java?revision=948827&view=markup
[4] http://www.day.com/maven/javax.jcr/javadocs/jcr-2.0/javax/jcr/Session.html#getNode(java.lang.String)
[5] http://www.day.com/maven/javax.jcr/javadocs/jcr-2.0/javax/jcr/Node.html#getNode(java.lang.String)
"
1,"addIndexesNoOptimize intermittantly throws incorrect ""segment exists in external directory..."" exceptionSpinoff from here:

    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200809.mbox/%3Cba72f77f0809111418l29cf215dnd45bf679832d7d42%40mail.gmail.com%3E

Here's my response on that thread:

The bug only happens when you call addIndexesNoOptimize, and one
simple workaround would be to use SerialMergeScheduler.

I think this is already fixed in trunk (soonish to be 2.4) as a side
effect of https://issues.apache.org/jira/browse/LUCENE-1335.

In 2.3, merges that involve external segments (which are segments
folded in by addIndexesNoOptimize) are not supposed to run in a BG
thread.  This is to prevent addIndexesNoOptimize from returning until
after all external segments have been carried over (merged or copied)
into the index, so that if there is an exception (eg disk full),
addIndexesNoOptimize is able to rollback to the index to the starting
point.

The primary merge() method of CMS indeed does not BG any external
merges, but the bug is that when a BG merge finishes it then selects a
new merge to kick off and that selection is happy to pick an external
segment."
1,"DocValues merging is not associative, leading to different results depending upon how merges executerecently I cranked up TestDuelingCodecs to actually test docvalues (previously it wasn't testing it at all).

This test is simple, it indexes the same random content with 2 different indexwriters, it just allows them
to use different codecs with different indexwriterconfigs.

then it asserts the indexes are equal.

Sometimes, always on BYTES_FIXED_DEREF type, we end out with one reader that has a zero-filled byte[] for a doc,
but that same document in the other reader has no docvalues at all.
"
0,"remove MultiTermQuery get/inc/clear totalNumberOfTermsThis method is not correct if the index has more than one segment.
Its also not thread safe, and it means calling query.rewrite() modifies
the original query. 

All of these things add up to confusion, I think we should remove this 
from multitermquery, the only thing that ""uses"" it is the NRQ tests, which 
conditionalizes all the asserts anyway.
"
0,"Move Jackrabbit Query Parser from core to spi-commonsThe query parser can be used outside jackrabbit-core, for instances in other repository implementations based on JCR2SPI.

Proposal:

- move source and build infrastructure from o.a.j.core.query to o.a.j.spi.commons.query

- switch over jackrabbit.core to use spi-commons for query

- optimally, add specific test cases for the query tree generation. "
1,"FieldCache.getStringIndex should not throw exception if term count exceeds doc countSpinoff of LUCENE-2133/LUCENE-831.

Currently FieldCache cannot handle more than one value per field.
We may someday want to fix that... but until that day:

FieldCache.getStringIndex currently does a simplistic check to try to
catch when you've accidentally allowed more than one term per field,
by testing if the number of unique terms exceeds the number of
documents.

The problem is, this is not a perfect check, in that it allows false
negatives (you could have more than one term per field for some docs
and the check won't catch you).

Further, the exception thrown is the unchecked RuntimeException.

So this means... you could happily think all is good, until some day,
well into production, once you've updated enough docs, suddenly the
check will catch you and throw an unhandled exception, stopping all
searches [that need to sort by this string field] in their tracks.
It's not gracefully degrading.

I think we should simply remove the test, ie, if you have more terms
than docs then the terms simply overwrite one another.
"
1,"Cannot version the root nodeAfter making the root node versionable, the checkin method fails with the following exception. 

java.lang.ArrayIndexOutOfBoundsException: 0
    at org.apache.jackrabbit.core.version.persistence.PersistentNode.copyFrom(PersistentNode.java:589)
    at org.apache.jackrabbit.core.version.persistence.InternalFrozenNodeImpl.checkin(InternalFrozenNodeImpl.java:277)
    at org.apache.jackrabbit.core.version.persistence.InternalFrozenNodeImpl.checkin(InternalFrozenNodeImpl.java:307)
    at org.apache.jackrabbit.core.version.persistence.InternalFrozenNodeImpl.checkin(InternalFrozenNodeImpl.java:307)
    at org.apache.jackrabbit.core.version.persistence.InternalVersionHistoryImpl.checkin(InternalVersionHistoryImpl.java:354)
    at org.apache.jackrabbit.core.version.persistence.NativePVM.checkin(NativePVM.java:506)
    at org.apache.jackrabbit.core.version.VersionManagerImpl.checkin(VersionManagerImpl.java:212)
    at org.apache.jackrabbit.core.NodeImpl.checkin(NodeImpl.java:2184)
    at com.gtnet.jcr.VersionedNodeTest.testVersionRootNode(VersionedNodeTest.java:218)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at com.intellij.rt.execution.junit2.JUnitStarter.main(JUnitStarter.java:31)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:78)
"
0,"Make CachingTokenFilter fasterThe LinkedList used by CachingTokenFilter is accessed using the get() method. Direct access on a LinkedList is slow and an Iterator should be used instead. For more than a handful of tokens, the difference in speed grows exponentially."
0,"Change default value of SearchIndex extractorPoolSizeThe current default value for the extractorPoolSize is 0, which means it is disabled by default. I think we should change that default because it is a useful feature and people should not have to dig through documentation to make use of it.

The new default should be computed based on the available processors. I suggest we use: 2 * Runtime.availableProcessors()"
0,"Document the temporary free space requirements of IndexWriter methodsJust opening an issue to track fixes to javadocs around Directory
space usage of optimize(), addIndexes(*), addDocument.

This came out of a recent thread on the users list around unexpectedly
high temporary disk usage during optimize():

  http://www.gossamer-threads.com/lists/lucene/java-user/43475

"
0,"BundleFsPersistenceManager: remove deprecated settingsSome settings of the BundleFsPersistenceManager are not used internally and should be removed:

blobFSInitialCache, blobFSMaximumCache, itemFSBlockSize, itemFSInitialCache, itemFSMaximumCache"
1,"LockMethod.getResponseAsLockDiscovery() fails when status==201 RFC 4918 Section 9.10.6 specifies that 201 is a valid response code for LOCK: ""201 (Created) - The LOCK request was to an unmapped URL, the request succeeded and resulted in the creation of a new resource, and the value of the DAV:lockdiscovery property is included in the response body.""

However, LockMethod.getResponseAsLockDiscovery() would fail in that scenario. 
    org.apache.jackrabbit.webdav.DavException: Created
	 at org.apache.jackrabbit.webdav.client.methods.DavMethodBase.getResponseException(DavMethodBase.java:164)
	 at org.apache.jackrabbit.webdav.client.methods.LockMethod.getResponseAsLockDiscovery(LockMethod.java:119)


The reason is in LockMethod:175 
      return statusCode == DavServletResponse.SC_OK;

Should be: 
      return statusCode == DavServletResponse.SC_OK
             || statusCode ==DavServletResponse.SC_CREATED;"
0,"contrib/benchmark files need eol-style setThe following files in contrib/benchmark don't have eol-style set to native, so when they are checked out, they don't get converted.

./build.xml:                    
./CHANGES.txt:                                                             
./conf/sample.alg:                                                                                
./conf/standard.alg:                                                                           
./conf/sloppy-phrase.alg:                                                                                 
./conf/deletes.alg:                                                                                         
./conf/micro-standard.alg:                                                                   
./conf/compound-penalty.alg:                                                                          
"
0,"Index creates many folders when re-indexingWhen the repository is re-indexed the search index creates a lot of directories, which are finally cleaned up. If the repository contains a lot of content the number of directories that are created can be quite high (thousands of directories).

The re-indexing process should clean up unused index folders right away and not wait until the changes are committed."
0,Upgrade to Tika 1.0Tika 1.0 was released today and has many improvements over the earlier 0.10 release. We should upgrade.
0,"Remove all interning of field names from flex APIIn previous versions of Lucene, interning of fields was important to minimize string comparison cost when iterating TermEnums, to detect changes in field name. As we separated field names from terms in flex, no query compares field names anymore, so the whole performance problematic interning can be removed. I will start with doing this, but we need to carefully review some places e.g. in preflex codec.

Maybe before this issue we should remove the Term class completely. :-) Robert?"
0,"Create jackrabbit-api(.jar) and the respective jackrabbit-rmi extensionscurrently some of the management functions not covered in jcr, like notetype management and workspace creation, are not exposed via any specific api and therfor not accessible via rmi.

create a jackrabbit api and the respective rmi extension."
0,Make contrib analyzers finalThe analyzers in contrib/analyzers should all be marked final. None of the Analyzers should ever be subclassed - users should build their own analyzers if a different combination of filters and Tokenizers is desired.
0,"Supplementary Character Handling in CharTokenizerCharTokenizer is an abstract base class for all Tokenizers operating on a character level. Yet, those tokenizers still use char primitives instead of int codepoints. CharTokenizer should operate on codepoints and preserve bw compatibility. "
0,Parsing mixed inclusive/exclusive range queriesThe current query parser doesn't handle parsing a range query (i.e. ConstantScoreRangeQuery) with mixed inclusive/exclusive bounds.
0,"change sort order to binary orderSince flexible indexing, terms are now represented as byte[], but for backwards compatibility reasons, they are not sorted as byte[], but instead as if they were char[].

I think its time to look at sorting terms as byte[]... this would yield the following improvements:
* terms are more opaque by default, they are byte[] and sort as byte[]. I think this would make lucene friendlier to customizations.
* numerics and collation are then free to use their own encoding (full byte) rather than avoiding the use of certain bits to remain compatible with char[] sort order.
* automaton gets simpler because as in LUCENE-2265, it uses byte[] too, and has special hacks because terms are sorted as char[]
"
0,"Jcr-Server: DavException doesn't allow to specify an exception causeWhile DavException extends Exception it does not allow to specify a exception cause in the constructor.
Adding a separate constructor taking status code plus a Throwable would provide the possibility to specify the original cause."
0,"[PATCH] cleaner API for Field.TextCurrently there are four methods named Field.Text(). As those methods have 
the same name and a very similar method signature, everyone will think these 
are just convenience methods that do the same thing. But they behave 
differently: the one that takes a Reader doesn't store the data, the one that 
takes a String does. I know that this is documented, but it's still not a nice 
API. Methods that behave differently should have diffent names. The attached 
patch deprecates two of the old methods and adds two new ones named 
Field.StoredText(). I think this is much easier to understand from the 
programmer's point-of-view and will help avoid bugs."
1,"HttpConnection isOpen flag concurrency problemThe HttpConnection.java class contains an isOpen boolean used to track the state
of the connection (opened or closed).  The problem is that in the
closeSocketAndStreams(), the flag is only flipped at the end of the
unsynchronized method (after resources have been released) which causes a
concurrency issue in flushRequestOutputStream() where the flag is checked first
and the the outputStream is accessed.

I'm providing a patch for this problem."
0,"Remove/deprecate Tokenizer's default ctorI was working on a new Tokenizer... and I accidentally forgot to call super(input) (and super.reset(input) from my reset method)... which then meant my correctOffset() calls were silently a no-op; this is very trappy.

Fortunately the awesome BaseTokenStreamTestCase caught this (I hit failures because the offsets were not in fact being corrected).

One minimal thing we can do (but it sounds like from Robert there may be reasons why we can't) is add {{assert input != null}} in Tokenizer.correctOffset:

{noformat}
Index: lucene/core/src/java/org/apache/lucene/analysis/Tokenizer.java
===================================================================
--- lucene/core/src/java/org/apache/lucene/analysis/Tokenizer.java	(revision 1242316)
+++ lucene/core/src/java/org/apache/lucene/analysis/Tokenizer.java	(working copy)
@@ -82,6 +82,7 @@
    * @see CharStream#correctOffset
    */
   protected final int correctOffset(int currentOff) {
+    assert input != null: ""subclass failed to call super(Reader) or super.reset(Reader)"";
     return (input instanceof CharStream) ? ((CharStream) input).correctOffset(currentOff) : currentOff;
   }
{noformat}

But best would be to remove the default ctor that leaves input null..."
0,"UserImporter should use User.changePasswordthe UserImporter lists a limitation that the password value is expected to be hashed already as it writes the
value as it was retrieved from the xml-import.

Instead it could make use of User#changePassword that (in the implementation present with JR) creates a 
pw-hash if the password is found to be plain text."
0,"Make TokenStream Reuse Mandatory for AnalyzersIn LUCENE-2309 it became clear that we'd benefit a lot from Analyzer having to return reusable TokenStreams.  This is a big chunk of work, but its time to bite the bullet.

I plan to attack this in the following way:

- Collapse the logic of ReusableAnalyzerBase into Analyzer
- Add a ReuseStrategy abstraction to Analyzer which controls whether the TokenStreamComponents are reused globally (as they are today) or per-field.
- Convert all Analyzers over to using TokenStreamComponents.  I've already seen that some of the TokenStreams created in tests need some work to be reusable (even if they aren't reused).
- Remove Analyzer.reusableTokenStream and convert everything over to using .tokenStream (which will now be returning reusable TokenStreams)."
0,"Minor performance improvement to IdleConnectionHandlerThe attached patch does the following changes to IdleConnectionHandler
 - as it iterator over a map of connections, using a LinkedHashMap is a faster
 - rather than using an iterator over the keyset and subsequently getting the values, an iterator over the entry set is used instead for efficiency (at least according to FindBugs)

Note that the patch contains other changes to make variables final where possible. This was done automatically by Eclipse, and can be removed if desired. However I see no harm in them, other than they affect more of the code than intended by the patch."
1,"XML import using MacOS X WebDAV client does not workwhen trying to import a xml file via a webdav mount this does not work.

this is mainly because the client first tries to create a 0-sized file, which fails with the xml importer. after the file is created, it will lock it and put the xml body. a second problem might be the ""dot-underscore"" files mac tries to create. "
1,"New LaxRedirectStrategy class should probably call the super method first.LaxRedirectStrategy extends the defaulRedirect class but does not call the super method as one would expect.

Just adding a patch to make sure it gets called."
0,"Improve InfoStream class in trunk to be more consistent with logging-frameworks like slf4j/log4j/commons-loggingFollowup on a [thread by Shai Erea on java-dev@lao|http://lucene.472066.n3.nabble.com/IndexWriter-infoStream-is-final-td3537485.html]: I already discussed with Robert about that, that there is one thing missing. Currently the IW only checks if the infoStream!=null and then passes the message to the method, and that *may* ignore it. For your requirement it is the case that this is enabled or disabled dynamically. Unfortunately if the construction of the message is heavy, then this wastes resources.

I would like to add another method to this class: abstract boolean isEnabled() that can also be implemented. I would then replace all null checks in IW by this method. The default config in IW would be changed to use a NoOutputInfoStream that returns false here and ignores the message.

A simple logger wrapper for e.g. log4j / slf4j then could look like (ignoring component, could be enabled):

{code:java}
Loger log = YourLoggingFramework.getLogger(IndexWriter.class);

public void message(String component, String message) {
  log.debug(component + "": "" + message);
}

public boolean isEnabled(String component) {
  return log.isDebugEnabled();
}
{code}

Using this you could enable/disable logging live by e.g. the log4j management console of your app server by enabling/disabling IndexWriter.class logging.

The changes are really simple:
- PrintStreamInfoStream returns true, always, mabye make it dynamically enable/disable to allow Shai's request
- infoStream.getDefault() is never null and can never be set to null. Instead the default is a singleton NoOutputInfoStream that returns false of isEnabled(component).
- All null checks on infoStream should be replaced by infoStream.isEanbled(component), this is possible as always != null. There are no slowdowns by this - it's like Collections.emptyList() instead stupid null checks."
1,"If indexwriter hits a non-ioexception from indexExists it leaks a write.lockthe rest of IW's ctor is careful about this.

IndexReader.indexExists catches any IOException and returns false, but the problem
occurs if some other exception (in my test, UnsupportedOperationException, but you
can imagine others are possible), when trying to e.g. read in the segments file.

I think we just need to move the IR.exists stuff inside the try / finally"
1,"WorkspaceImporter throws exceptionsessio.getWorkspace().getImportContentHandler() throws 
java.lang.UnsupportedOperationException: Workspace-Import of protected nodes: Not yet implement.

suggest to issue warning instead of throwing."
0,"refactor HttpClientConnection and HttpProxyConnectionInstead of trying to define a full abstraction for client connections, let's define only a minimal interface in HttpCore with only those methods actually needed in the core. In particular, the core does not need to open connections (since HTTPCORE-11), and it does not care whether a connection is direct or through a proxy. An abstraction for client connections can be defined in HttpConn.

(original description:)
As discussed on the mailing list, separating the responsibility for establishing connections from the connection objects could improve the design and help with proxy support.
"
1,"JNDI Referencable IssuesI'm questioning the use of Referencable in the BindableResource and BindableResourceFactory classes for the JNDI lookup process. Reason for this is because Referencable needs the Addrs to be in the EXACT order in order for it to be considered the same. (see http://java.sun.com/j2se/1.4.2/docs/api/javax/naming/Reference.html#equals(java.lang.Object) )

In order for me to get the JNDI reference to be found correctly I had to change the BindableResource.getReference method to swap the order the StringReferences were added to match up what was being passed in by glassfish. This seems EXTREMELY fragile to me as I don't know what order, say JBoss, would pass the StringRefences in in the Reference object for the Factory method.

Also, another problem is that getReference is binding the class name to BindableRepository class implementation and not javax.jcr.Repository. This again causes them not to match if you follow the example on the wiki on setting up the JNDI reference and use javax.jcr.Repository as the type. This can either be fixed by changing the JNDI reference to use the BindableRepository class or the change the BindableRepository class to set that to the Repository interface. Not sure which would be considered 'better'

I have a patch that fixes the first issue (at least for glassfish), but not the second. Again, this seems like a really 'breakable' setup right now and not sure what would be better to make sure this is avoided."
0,"[PATCH] simplify conversion of strings to primitives by using parseXXX, not valueOf(xxx).xxxValue()Code converts strings to primitives using a two step process, eg

Boolean.valueOf(myString).booleanValue();

can be simplified to 

Boolean.parseBoolean(myString);

true of Float, Double, Int etc. 

In some cases, this avoids allocating temporary boxed objects

patch fixes this."
1,"fix assertions/checks that use File.length() to use getFilePointer()This came up on this thread ""Getting RuntimeException: after flush: fdx size mismatch while Indexing"" 
(http://www.lucidimagination.com/search/document/a8db01a220f0a126)

In trunk, a side effect of the codec refactoring is that these assertions were pushed into codecs as finish() before close().
they check getFilePointer() instead in this computation, which checks that lucene did its part (instead of falsely tripping if directory metadata is stale).

I think we should fix these checks/asserts on 3.x too
"
1,"Database connection leak with DBCP, MySQL, and ObserversWhen using DBCP and MySQL with an observer that modifies the content repository, we are seeing abandoned connections in our connection pool."
0,Improve docs for deployment Models 1 and 2 on Tomcat 5.5.x and provide an example webapp.New users would find a small webapp and associated documentation that walks them through the process of setting up a model 1 or model 2 (or both) deployment scheme.
0,"Avoid string concatenation in AbstractBundlePersistenceManagerThe following line:

        log.debug(""stored bundle "" + bundle.getId());

should be changed to:

        log.debug(""stored bundle {}"", bundle.getId());
"
1,"ShingleFilter skips over trie-shingles if outputUnigram is set to falseSpinoff from http://lucene.markmail.org/message/uq4xdjk26yduvnpa

{quote}
I noticed that if I set outputUnigrams to false it gives me the same output for
maxShingleSize=2 and maxShingleSize=3.

please divide divide this this sentence

when i set maxShingleSize to 4 output is:

please divide please divide this sentence divide this this sentence

I was expecting the output as follows with maxShingleSize=3 and
outputUnigrams=false :

please divide this divide this sentence 
{quote}


"
1,"Redirect to a relative URL failsRequest the url 
http://commerce1.cera.net/discount-pcbooks/catalog/categories.asp?
search_str=0782128092

On a browser the redirect works, while with HttpClient it doesn't."
1,"The deprecated constructor of BooleanClause does not set new stateNick Burch reported this on lucene-user. 
 
Patch will follow. 
 
Regards, 
Paul Elschot"
0,Upgrade all default socket factories to use SO_REUSEADDR parameterSee HTTPCORE-209
0,"Add generics to DocumentsWriterDeleteQueue.NodeDocumentsWriterDeleteQueue.Note should be generic as the subclasses hold different types of items. This generification is a little bit tricks, but the generics policeman can't wait to fix this *g*."
1,"BoostingTermQuery's explanation should be marked as Match even if the payload part negated or zero'ed itSince BTQ multiplies the payload on the score it might return a negative score.
The explanation should be marked as ""Match"" otherwise it is not added to container explanations,
See also in LUCENE-1302."
0,Replace IndexReader.getFieldNames with IndexReader.getFieldInfos
0,"Node Type Management subproject : Default namespace should be emtpyWhen creating node types matching to the class descriptors,  the default namespace should be empty instead of  'ocm'."
0,"queryparser makes all CJK queries phrase queries regardless of analyzerThe queryparser automatically makes *ALL* CJK, Thai, Lao, Myanmar, Tibetan, ... queries into phrase queries, even though you didn't ask for one, and there isn't a way to turn this off.

This completely breaks lucene for these languages, as it treats all queries like 'grep'.

Example: if you query for f:abcd with standardanalyzer, where a,b,c,d are chinese characters, you get a phrasequery of ""a b c d"". if you use cjk analyzer, its no better, its a phrasequery of  ""ab bc cd"", and if you use smartchinese analyzer, you get a phrasequery like ""ab cd"". But the user didn't ask for one, and they cannot turn it off.

The reason is that the code to form phrase queries is not internationally appropriate and assumes whitespace tokenization. If more than one token comes out of whitespace delimited text, its automatically a phrase query no matter what.

The proposed patch fixes the core queryparser (with all backwards compat kept) to only form phrase queries when the double quote operator is used. 

Implementing subclasses can always extend the QP and auto-generate whatever kind of queries they want that might completely break search for languages they don't care about, but core general-purpose QPs should be language independent.
"
0,Cookie docs are outdated.The cookie docs do not reflect the latest code changes.
0,"Cache jcr name to QName mappingsCurrently jcr names are always parsed and resolved into QName instances. Introducing a cache would increase performance and also save memory because well known and often used jcr names would always return the same QName instance from cache.

Testing with common read operations shows a performance improvement of about 25%.
The test involved the following methods on Node interface:

- getProperty()
- getProperties()
- getName()
- getPath()
- isLocked()
- isNodeType()
- getPrimaryNodeType()
- hasNodes()
- getNodes()

Attached proposed implementation of a QNameResolver.

Please comment."
1,"Query index not in sync with workspaceAfter some time the search index is not in sync anymore with the data in the workspace and returns uuids which have no corresponding Node in the workspace. This results in a NodeIterator which throws an ItemNotFoundException on nextNode().

Instructions how to reproduce this error are not yet available.

Possible areas for further investigation are:
- NodeType registry which maps the node types into the workspace with the use of virtual item states
- versioning?
- atomicity of indexing?"
1,"Fix small perf issues with String/TermOrdValComparatorUncovered some silliness when working on LUCENE-2504, eg we are doing unnecessary binarySearch on a single-segment reader."
1,"SpanRegexQuery and SpanNearQuery is not working with MultiSearcherMultiSearcher is using:
queries[i] = searchables[i].rewrite(original);
to rewrite query and then use combine to combine them.

But SpanRegexQuery's rewrite is different from others.
After you call it on the same query, it always return the same rewritten queries.

As a result, only search on the first IndexSearcher work. All others are using the first IndexSearcher's rewrite queries.
So many terms are missing and return unexpected result.

Billow"
0,"Pass potent SR to IRWarmer.warm(), and also call warm() for new segmentsCurrently warm() receives a SegmentReader without terms index and docstores.
It would be arguably more useful for the app to receive a fully loaded reader, so it can actually fire up some caches. If the warmer is undefined on IW, we probably leave things as they are.

It is also arguably more concise and clear to call warm() on all newly created segments, so there is a single point of warming readers in NRT context, and every subreader coming from getReader is guaranteed to be warmed up -> you don't have to introduce even more mess in your code by rechecking it.

"
0,"Move NoDeletionPolicy from benchmark to coreAs the subject says, but I'll also make it a singleton + add some unit tests, as well as some documentation. I'll post a patch hopefully today."
0,"Add LuSql project to ""Apache Lucene - Contributions"" wiki pageAdd [LuSql|http://lab.cisti-icist.nrc-cnrc.gc.ca/cistilabswiki/index.php/LuSql] to the Apache Lucene - Contributions page [http://lucene.apache.org/java/2_9_0/contributions.html]
I am the author of LuSql. I can supply any text needed. 

Perhaps a new heading is needed to capture Database/JDBC oriented Lucene tools (there are others out there)?"
1,"/contrib/orm-persistence/ OJBPersistenceManagerOJBPersistenceManager seems to have the following problems

1. OJBPersistenceBroker inherites from AbstractPersistenceBroker. There's no 
need of using a non transactional implementation as the feature is available in 
jdbc. 

2. A single broker is used and It's not thread-safe. This is not a problem now 
because it inherits from AbstractPersistenceManager, and the store(ChangeLog ) 
method is synchronized.

3. The broker is never closed so it leaves an open connection.

4. There's no pooling with only one broker.

5 Each write method (e.g. store(NodeState state)) starts its own transaction 
but the transaction should start and end in store(ChangeLog log).

6. It never rollbacks, even when an item in the changelog can't be persisted.

7. The mysql example create MyISAM tables which don't support transactions. 
Innodb tables would be more appropriate.

8. jdbc to java type mapping is wrong for 
class: org.apache.jackrabbit.core.state.orm.ORMBlobValue
field: size
Changed from INTEGER to BIGINT

9. When a Blob value is loaded a ArrayStoreException is thrown because in 
load(PropertyId id) BlobFileValues are added to internalValueList instead of 
InternalValue instances.

10. in store(NodeReferences). When storing a NodeReferences which have some (but not all) the references deleted the OJB persistence Manager doesn't delete any one.

Some of this problems are present in the Hibernate implementation."
0,Backport JCR-1197: Node.restore() may throw InvalidItemStateExceptionBackport issue JCR-1197 (Node.restore() may throw InvalidItemStateException) to 1.3 branch for 1.3.4 (separate issue to avoid re-opening JCR-1197 which was already released with 1.4).
0,"Provide names for constants in QueryConstantsFor debugging, logging, and user interaction purposes QueryConstants should include descriptive names for the constants it provides."
0,"Typo in log outputjackrabbit/src/java/org/apache/jackrabbit/core/fs/local/LocalFileSystem.java:
133c133
<         log.info(""LocaaFileSystem initialized on "" + root.getPath());
---
>         log.info(""LocalFileSystem initialized on "" + root.getPath());
"
0,DEFAULT spelled DEFALT in MoreLikeThis.javaDEFAULT is spelled DEFALT in contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThis.java
0,"Upgrade to latest SLF4J and LogbackWhile fixing JCR-2836 I ran into LBCLASSIC-183 [1] that's fixed in a recent Logback release. To get this and other fixes we should upgrade Logback and SLF4J in Jackrabbit 2.3.

[1] http://jira.qos.ch/browse/LBCLASSIC-183"
0,"[PATCH] png, apng, mng text extractorText extractor for tEXt chunk for png, apng and mng formats"
1,"Mixin type lossWhen using a bundle persistence manager, the mixin type information may be corrupted in the lucene index, causing queries like '//element(*, my:mixin)' to fail.



The problem is that the 'jcr:mixinTypes' may be stored in the bundle. Here is how this could happen :


First step: Create a node and add a mixin 'A'.

Everything's fine. The query '//element(*, 'A')' works.


Second step : Select the node and add a second mixin 'B'.

When the second mixin is added, the AbstractBundlePersistenceManager#load(PropertyId) is called to get the current mixins for the node. This method will store the PropertyState for 'jcr:mixinTypes' in the bundle (containing only the mixin 'A'). Then the NodeImpl#setMixinTypesProperty() will set the PropertyState for 'jcr:mixinTypes' in the node state (containing the mixins 'A' and 'B').
When the session is saved, the ChangeLog in AbstractBundlePersistenceManager#store() contains a modification for the 'jcr:mixinTypes' but it's being ignored, leaving the bundle with only mixin 'A'. The NodeIndexer looks into the node state to get the mixin types and indexes the node correctly. The queries '//element(*, 'A')' and '//element(*, 'B')' work.


Thrid step : Select the node and update a property. 

When the session is saved, the NodeIndexer asks again for the 'jcr:mixinTypes' property, by calling the AbstractBundlePersistenceManager#load(PropertyId) to load it. The bundle contains this property and returns only mixin 'A' (as it was stored in the second step), causing the index to use only mixin 'A'. The query '//element(*, 'A')' still works but '//element(*, 'B') doesn't work anymore.



A simple solution to this would be to not store the PropertyState for the 'jcr:mixinTypes' (and 'jcr:uuid' and 'jcr:primaryType', as the class description states) in the bundle when the PropertyState is loaded. It would fix the issue but not the contents on existing repositories. One way to allow the repositories to fix themselves is to not read or write these 3 properties in the BundleBinding#readBundle and BundleBinding#writeBundle methods, but I'm not sure wether or not it would have a performance impact.


"
0,"Demo HTML parser gives incorrect summaries when title is repeated as a headingIf you have an html document where the title is repeated as a heading at the top of the document, the HTMLParser will return the title as the summary, ignoring everything else that was added to the summary. Instead, it should keep the rest of the summary and chop off the title part at the beginning (essentially the opposite). I don't see any benefit to repeating the title in the summary for any case.

In HTMLParser.jj's getSummary():

    String sum = summary.toString().trim();
    String tit = getTitle();
    if (sum.startsWith(tit) || sum.equals(""""))
      return tit;
    else
      return sum;

change it to: (* denotes a line that has changed)

    String sum = summary.toString().trim();
    String tit = getTitle();
*    if (sum.startsWith(tit))             // don't repeat title in summary
*      return sum.substring(tit.length()).trim();
    else
      return sum;
"
1,Lucene queries are not properly rewrittenSome of the jackrabbit internal lucene queries are not properly rewritten and may lead to UnsupportedOperationException when terms are extracted from the lucene query.
1,"Deleting docs of all returned Hits during search causes ArrayIndexOutOfBoundsExceptionFor background user discussion:
http://www.nabble.com/document-deletion-problem-to14414351.html

{code}
Hits h = m_indexSearcher.search(q); // Returns 11475 documents 
for(int i = 0; i < h.length(); i++) 
{ 
  int doc = h.id(i); 
  m_indexSearcher.getIndexReader().deleteDocument(doc);  <-- causes ArrayIndexOutOfBoundsException when i = 6400
} 
{code}
"
0,"simple improvements to testsSimon had requested some docs on what all our test options do, so lets clean it up and doc it.

i propose:
# change all vars to be tests.xxx (e.g. tests.threadspercpu, tests.multiplier, ...)
# ensure all 6 build systems (lucene, solr, each solr contrib) respect these.
# add a simple wiki page listing what these do."
1,"SSLSocketFactory.createSSLContext does not process trust storeorg.apache.http.conn.ssl.SSLSocketFactory.createSSLContext() does not process a provided trust store.
Only the default (cacerts) is processed. An additional provided trust store is ignored.
Adding the ""trusted"" certificate to the keystore, the peer is authenticated.

Eventually
        tmfactory.init(keystore);
needs to be
        tmfactory.init(truststore);

"
0,"Improve name resolutionAs discussed in JCR-685, the current CachingNamespaceResolver class contains excessive synchronization causing monitor contention that reduces performance.

In JCR-685 there's a proposed patch that replaces synchronization with a read-write lock that would allow concurrent read access to the name cache."
0,"Add insertWithOverflow to PriorityQueueThis feature proposes to add an insertWithOverflow to PriorityQueue so that callers can reuse the objects that are being dropped off the queue. Also, it changes heap to protected for easier extensibility of PQ"
1,"TestStressNRT failures (reproducible)Build server logs. Reproduces on at least two machines.

{noformat}
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestStressNRT -Dtestmethod=test -Dtests.seed=69468941c1bbf693:19e66d58475da929:69e9d2f81769b6d0 -Dargs=""-Dfile.encoding=UTF-8""
    [junit] NOTE: test params are: codec=Lucene3x, sim=RandomSimilarityProvider(queryNorm=true,coord=false): {}, locale=ro, timezone=Etc/GMT+1
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestStressNRT]
    [junit] NOTE: Linux 3.0.0-16-generic amd64/Sun Microsystems Inc. 1.6.0_27 (64-bit)/cpus=2,threads=1,free=74960064,total=135987200
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: test(org.apache.lucene.index.TestStressNRT):	Caused an ERROR
    [junit] MockDirectoryWrapper: cannot close: there are still open files: {_ng.cfs=8}
    [junit] java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_ng.cfs=8}
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:555)
    [junit] 	at org.apache.lucene.index.TestStressNRT.test(TestStressNRT.java:385)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$SubclassSetupTeardownRule$1.evaluate(LuceneTestCase.java:743)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$InternalSetupTeardownRule$1.evaluate(LuceneTestCase.java:639)
    [junit] 	at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$TestResultInterceptorRule$1.evaluate(LuceneTestCase.java:538)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$RememberThreadRule$1.evaluate(LuceneTestCase.java:600)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:164)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit] 	at org.apache.lucene.util.StoreClassNameRule$1.evaluate(StoreClassNameRule.java:21)
    [junit] 	at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)
    [junit] Caused by: java.lang.RuntimeException: unclosed IndexInput: _ng.cfs
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.addFileHandle(MockDirectoryWrapper.java:479)
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper$1.openSlice(MockDirectoryWrapper.java:777)
    [junit] 	at org.apache.lucene.store.CompoundFileDirectory.openInput(CompoundFileDirectory.java:221)
    [junit] 	at org.apache.lucene.codecs.lucene3x.TermInfosReader.<init>(TermInfosReader.java:112)
    [junit] 	at org.apache.lucene.codecs.lucene3x.Lucene3xFields.<init>(Lucene3xFields.java:84)
    [junit] 	at org.apache.lucene.codecs.lucene3x.PreFlexRWPostingsFormat$1.<init>(PreFlexRWPostingsFormat.java:51)
    [junit] 	at org.apache.lucene.codecs.lucene3x.PreFlexRWPostingsFormat.fieldsProducer(PreFlexRWPostingsFormat.java:51)
    [junit] 	at org.apache.lucene.index.SegmentCoreReaders.<init>(SegmentCoreReaders.java:108)
    [junit] 	at org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:51)
    [junit] 	at org.apache.lucene.index.IndexWriter$ReadersAndLiveDocs.getMergeReader(IndexWriter.java:521)
    [junit] 	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3587)
    [junit] 	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3257)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:382)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:451)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.TestStressNRT FAILED
{noformat}"
1,"ConcurrentModificationException during registration of nodetypesDuring the registration of a set of nodetypes this exception may be encountered:

java.util.ConcurrentModificationException
        at org.apache.commons.collections.map.AbstractReferenceMap$ReferenceEntrySetIterator.checkMod(AbstractReferenceMap.java:761)
        at org.apache.commons.collections.map.AbstractReferenceMap$ReferenceEntrySetIterator.hasNext(AbstractReferenceMap.java:735)
        at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.notifyRegistered(NodeTypeRegistry.java:1750)
        at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.registerNodeTypes(NodeTypeRegistry.java:223)

It seems that the copying of the listeners triggered this exception:

    private void notifyRegistered(QName ntName) {
        // copy listeners to array to avoid ConcurrentModificationException
        NodeTypeRegistryListener[] la =
                new NodeTypeRegistryListener[listeners.size()];
        Iterator iter = listeners.values().iterator();
        int cnt = 0;
1750:   while (iter.hasNext()) {
            la[cnt++] = (NodeTypeRegistryListener) iter.next();
        }
        for (int i = 0; i < la.length; i++) {
            if (la[i] != null) {
                la[i].nodeTypeRegistered(ntName);
            }
        }
    }

The methods ""notifyReRegistered"" and ""notifyUnregistered"" will probably suffer from the same problem.

Reproduction of this exception may be tricky; it only occurred once in our application. It is probably a race condition: another thread might access the listeners during the copy. It may be helpful to use a debugger and set a breakpoint in the middle of the copy giving other threads the opportunity to access the listeners...

We think that a possible solution is the following:

    /**
     * Notify the listeners that a node type <code>ntName</code> has been registered.
     */
    private void notifyRegistered(QName ntName) {
        // copy listeners to array to avoid ConcurrentModificationException
    	NodeTypeRegistryListener[] la;
    	synchronized (listeners) {
            la = (NodeTypeRegistryListener[]) listeners.values().toArray(new NodeTypeRegistryListener[listeners.size()]);
		}

        for (int i = 0; i < la.length; i++) {
            if (la[i] != null) {
                la[i].nodeTypeRegistered(ntName);
            }
        }
    }



"
0,"Allow servlet filters to specify custom session providersIn order to integrate the Jackrabbit davex server functionality with their custom authentication logic, the Sling project currently needs to embed and subclass the davex servlet classes. It would be cleaner if such tight coupling wasn't needed.

One way to achieve something like that would be to allow external components to provide a custom SessionProvider instance as an extra request attribute. This way for example a servlet filter that implements such custom authentication logic could easily make its functionality available to the standard davex servlet in Jackrabbit."
1,"SearchWithSortTask ignores sorting by DocDuring my work in LUCENE-3912, I found the following code:

{code}
if (field.equals(""doc"")) {
    sortField0 = SortField.FIELD_DOC;
} if (field.equals(""score"")) {
    sortField0 = SortField.FIELD_SCORE;
} ...
{code}

This means the setting of SortField.FIELD_DOC is ignored.  While I don't know much about this code, this seems like a valid setting and obviously just a bug."
1,"TestIndexWriter.testBackgroundOptimize fails with too many open filesRecreate with this line:

ant test -Dtestcase=TestIndexWriter -Dtestmethod=testBackgroundOptimize -Dtests.seed=-3981504507637360146:51354004663342240

Might be related to LUCENE-2873 ?"
1,"AbstractJournal doesn't create deep paths for revision filesAbstractJournal throws when trying to create the revision file if the directory the revision file is in doesn't already exist. When initializing a repository during its startup, the create fails is you use a revision param like <param name=""revision"" value=""${rep.home}/repository/revision"" /> because the repository directory hasn't been created yet. Attached is a repository.xml that demonstrates. It uses Oracle for FS and PMs."
1,"DocumentViewExportVisitor class incorrectly handles XML escaping for element namesThe method private static String escapeName(String name) should have the following test:

 if ((i == 0) ? XMLChar.isNCNameStart(ch) : XMLChar.isNCName(ch)) {

changed into

 if ((i == 0) ? !XMLChar.isNCNameStart(ch) :! XMLChar.isNCName(ch)) {

in order to properly escape the text (those two methods in XMLChar return true when the character is valid, not the other way around."
0,"DefaultLoginModule performs anonymous login in case of unsupported Credentials implementationIf Repository.login in called with an unsupported Credentials implementation the DefaultLoginModule#getCredentials returns null
and thus an anonymous login. The expected behavior from my point of view however was, that login with unsupported credentials 
would not be handled by the LoginModule and - if no other module is able to handle it -  login would consequently fails."
0,Remove DocumentWriterDocumentWriter has been replaced by DocumentsWriter (from LUCENE-843) so we need to remove it & fix the unit tests that directly use it...
0,Fixes a handful of misspellings/mistakes in changes.txtThere are a handful of misspellings/mistakes in changes.txt. This patch fixes them. Avoided the one or two British to English conversions <g>
0,"AuthorizableImpl#memberOf and #declaredMemberOf should return RangeIteratorit would be favorable if the iterator returned by Authorizable#memberOf and #declaredMemberOf
would return a RangeIterator in order to all the caller to determine the size without having to
iterate."
1,"SQL parser chokes on prefixes containing a ""-"" characterSQL parser chokes on prefixes containing a ""-"" character, such as in

  SELECT a-b:c FROM nt:resource

"
0,"add an interface for plugable dns clientsCurrently Httpclient implicitly uses InetAddress.getByName() for DNS resolution.
This has some drawbacks. One is that the DNS cache of Java per default caches entries forever.

So I'd like to be able to replace InetAddress.getByName() with another DNS client implementation.

"
1,"NullpointerException in SessionItemStateManagerI got the following exception which is not reproducible and occured during a large batch of write operations. Unfortunately I got no idea how this happened. May be someone has an idea?

[2006-11-27 21:43:53,065, WARN ] {} support.RemoteInvocationTraceInterceptor:80: Processing of RmiServiceExporter remote call resulted in fatal exception: com.subshell.sophora.content.server.IContentManager.importDocument
org.springframework.transaction.TransactionSystemException: Could not commit JCR transaction; nested exception is java.lang.NullPointerException Caused by: 
java.lang.NullPointerException
        at org.apache.jackrabbit.core.state.SessionItemStateManager.nodeModified(SessionItemStateManager.java:878)
        at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyNodeModified(StateChangeDispatcher.java:143)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.stateModified(LocalItemStateManager.java:426)
        at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyStateModified(StateChangeDispatcher.java:85)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.stateModified(SharedItemStateManager.java:388)
        at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyStateModified(StateChangeDispatcher.java:85)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.stateModified(SharedItemStateManager.java:388)
        at org.apache.jackrabbit.core.state.ItemState.notifyStateUpdated(ItemState.java:241)
        at org.apache.jackrabbit.core.state.ChangeLog.persisted(ChangeLog.java:271)
        at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:691)
        at org.apache.jackrabbit.core.state.XAItemStateManager.commit(XAItemStateManager.java:169)
        at org.apache.jackrabbit.core.version.XAVersionManager.commit(XAVersionManager.java:478)
        at org.apache.jackrabbit.core.TransactionContext.commit(TransactionContext.java:172)
        at org.apache.jackrabbit.core.XASessionImpl.commit(XASessionImpl.java:315)
        at org.springmodules.jcr.jackrabbit.support.JackRabbitUserTransaction.commit(JackRabbitUserTransaction.java:104)
        at org.springmodules.jcr.jackrabbit.LocalTransactionManager.doCommit(LocalTransactionManager.java:192)
        at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:540)
        at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:510)
        at org.springframework.transaction.interceptor.TransactionAspectSupport.commitTransactionAfterReturning(TransactionAspectSupport.java:310)
        at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:117)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:185)
        at org.springframework.aop.interceptor.ExposeInvocationInterceptor.invoke(ExposeInvocationInterceptor.java:89)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:185)
        at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:209)
        at $Proxy14.importDocument(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:318)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:203)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:162)
        at org.springframework.remoting.support.RemoteInvocationTraceInterceptor.invoke(RemoteInvocationTraceInterceptor.java:70)
        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:185)
        at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:209)
        at $Proxy15.importDocument(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor21.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at org.springframework.remoting.support.RemoteInvocation.invoke(RemoteInvocation.java:181)
        at org.springframework.remoting.support.DefaultRemoteInvocationExecutor.invoke(DefaultRemoteInvocationExecutor.java:38)
        at org.springframework.remoting.support.RemoteInvocationBasedExporter.invoke(RemoteInvocationBasedExporter.java:76)
        at org.springframework.remoting.rmi.RmiBasedExporter.invoke(RmiBasedExporter.java:72)
        at org.springframework.remoting.rmi.RmiInvocationWrapper.invoke(RmiInvocationWrapper.java:62)
        at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:294)
        at sun.rmi.transport.Transport$1.run(Transport.java:153)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:149)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:460)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:701)
        at java.lang.Thread.run(Thread.java:595)
"
0,"don't require an analyzer, if all fields are NOT_ANALYZEDThis seems wierd, if you analyze only NOT_ANALYZED fields, you must have an analyzer (null will not work)
because documentsinverter wants it for things like offsetGap"
0,"Test failure: org.apache.jackrabbit.test.TestAllSubsequent test runs fail unless doing a mvn clean first.

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running org.apache.jackrabbit.spi2jcr.spi.TestAll
Tests run: 50, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.965 sec
Running org.apache.jackrabbit.test.TestAll
Tests run: 1038, Failures: 11, Errors: 0, Skipped: 0, Time elapsed: 44.925 sec <<< FAILURE!
Running org.apache.jackrabbit.spi2jcr.jcr2spi.TestAll
Tests run: 394, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.416 sec

Results :

Failed tests:
  testOrderByAscending(org.apache.jackrabbit.test.api.query.SQLOrderByTest)
  testOrderByDescending(org.apache.jackrabbit.test.api.query.SQLOrderByTest)
  testOrderByDefault(org.apache.jackrabbit.test.api.query.SQLOrderByTest)
  testDocOrderIndexedNotation(org.apache.jackrabbit.test.api.query.XPathPosIndexTest)
  testDocOrderPositionFunction(org.apache.jackrabbit.test.api.query.XPathDocOrderTest)
  testDocOrderPositionIndex(org.apache.jackrabbit.test.api.query.XPathDocOrderTest)
  testDocOrderLastFunction(org.apache.jackrabbit.test.api.query.XPathDocOrderTest)
  testDocOrderFirstFunction(org.apache.jackrabbit.test.api.query.XPathDocOrderTest)
  testOrderByAscending(org.apache.jackrabbit.test.api.query.XPathOrderByTest)
  testOrderByDescending(org.apache.jackrabbit.test.api.query.XPathOrderByTest)
  testOrderBy(org.apache.jackrabbit.test.api.query.XPathOrderByTest)

"
1,"Preemptive Authorization parameter initialization incorrect, causes preemptive auth not to workPreemptive authorization is defeated by an incorrect initialization. Patch 
follows:
--- DefaultHttpParamsFactory.java       2005-10-10 19:09:10.000000000 -0700
+++ DefaultHttpParamsFactory.java.fixed 2005-10-17 17:00:10.259174920 -0700
@@ -118,9 +118,9 @@
         if (preemptiveDefault != null) {
             preemptiveDefault = preemptiveDefault.trim().toLowerCase();
             if (preemptiveDefault.equals(""true"")) {
-                params.setParameter
(HttpClientParams.PREEMPTIVE_AUTHENTICATION, ""on"");
+                params.setParameter
(HttpClientParams.PREEMPTIVE_AUTHENTICATION, Boolean.TRUE);
             } else if (preemptiveDefault.equals(""false"")) {
-                params.setParameter
(HttpClientParams.PREEMPTIVE_AUTHENTICATION, ""off"");
+                params.setParameter
(HttpClientParams.PREEMPTIVE_AUTHENTICATION, Boolean.FALSE);
             }
         }"
1,"remove IndexInput.copyBufthis looks really broken/dangerous as an instance variable.

what happens on clone() ?! copyBytes can instead make its own array inside the method.

its protected, so ill list in the 3.x backwards breaks section since its technically a backwards break."
0,change some access-level modifiers to allow for better subclassingSome of the methods in the core parts of jackrabbit are package protected and do not allow easy subclassing. suggest to make some of the methods 'protected'.
1,"Error while restoring OPV=Version childnodes (Restore of root version not allowed)when restoring a version of a node (by name) that has opv=version childnodes, the following error is thrown, if such a version does not exist in the child nodes versionhistory:

Error while restoring nodes: javax.jcr.version.VersionException: Restore of root version not allowed."
0,"Add unit test showing how to do a ""live backup"" of an indexThe question of how to backup an index comes up every so often on the
lists.  Backing up and index is also clearly an important fundamental
admin task that many applications need to do for fault tolerance.

In the past you were forced to stop & block all changes to your index,
perform the backup, and then resume changes.  But many applications
cannot afford a potentially long pause in their indexing.

With the addition of DeletionPolicy (LUCENE-710), it's now possible to
do a ""live backup"", which means backup your index in the background
without pausing ongoing changes to the index.  This
SnapshotDeletionPolicy just has to mark the chosen commit point as not
deletable, until the backup finishes.
"
1,"SSL connections cannot be established using the IP addressHttpClient 4.x introduced a regression in establishing SSL connections to remote peers. The AbstractVerifier class only checks for matches in CN and SubjectAlternative->DNSName. But, when an IP (instead of a hostname) is used, the check should be done on CN and SubjectAlternative->IPAddress."
0,"Sun hotspot compiler bug in 1.6.0_04/05 affects LuceneThis is not a Lucene bug.  It's an as-yet not fully characterized Sun
JRE bug, as best I can tell.  I'm opening this to gather all things we
know, and to work around it in Lucene if possible, and maybe open an
issue with Sun if we can reduce it to a compact test case.

It's hit at least 3 users:

  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200803.mbox/%3c8c4e68610803180438x39737565q9f97b4802ed774a5@mail.gmail.com%3e
  http://mail-archives.apache.org/mod_mbox/lucene-solr-user/200804.mbox/%3c4807654E.7050900@virginia.edu%3e
  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200805.mbox/%3c733777220805060156t7fdb8fectf0bc984fbfe48a22@mail.gmail.com%3e

It's specific to at least JRE 1.6.0_04 and 1.6.0_05, that affects
Lucene.  Whereas 1.6.0_03 works OK and it's unknown whether 1.6.0_06
shows it.

The bug affects bulk merging of stored fields.  When it strikes, the
segment produced by a merge is corrupt because its fdx file (stored
fields index file) is missing one document.  After iterating many
times with the first user that hit this, adding diagnostics &
assertions, its seems that a call to fieldsWriter.addDocument some
either fails to run entirely, or, fails to invoke its call to
indexStream.writeLong.  It's as if when hotspot compiles a method,
there's some sort of race condition in cutting over to the compiled
code whereby a single method call fails to be invoked (speculation).

Unfortunately, this corruption is silent when it occurs and only later
detected when a merge tries to merge the bad segment, or an
IndexReader tries to open it.  Here's a typical merge exception:

{code}
Exception in thread ""Thread-10"" 
org.apache.lucene.index.MergePolicy$MergeException: 
org.apache.lucene.index.CorruptIndexException:
    doc counts differ for segment _3gh: fieldsReader shows 15999 but segmentInfo shows 16000
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:271)
Caused by: org.apache.lucene.index.CorruptIndexException: doc counts differ for segment _3gh: fieldsReader shows 15999 but segmentInfo shows 16000
        at org.apache.lucene.index.SegmentReader.initialize(SegmentReader.java:313)
        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:262)
        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:221)
        at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3099)
        at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2834)
        at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:240)
{code}

and here's a typical exception hit when opening a searcher:

{code}
org.apache.lucene.index.CorruptIndexException: doc counts differ for segment _kk: fieldsReader shows 72670 but segmentInfo shows 72671
        at org.apache.lucene.index.SegmentReader.initialize(SegmentReader.java:313)
        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:262)
        at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:230)
        at org.apache.lucene.index.DirectoryIndexReader$1.doBody(DirectoryIndexReader.java:73)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:636)
        at org.apache.lucene.index.DirectoryIndexReader.open(DirectoryIndexReader.java:63)
        at org.apache.lucene.index.IndexReader.open(IndexReader.java:209)
        at org.apache.lucene.index.IndexReader.open(IndexReader.java:173)
        at org.apache.lucene.search.IndexSearcher.<init>(IndexSearcher.java:48)
{code}

Sometimes, adding -Xbatch (forces up front compilation) or -Xint
(disables compilation) to the java command line works around the
issue.

Here are some of the OS's we've seen the failure on:

{code}
SuSE 10.0
Linux phoebe 2.6.13-15-smp #1 SMP Tue Sep 13 14:56:15 UTC 2005 x86_64 
x86_64 x86_64 GNU/Linux 

SuSE 8.2
Linux phobos 2.4.20-64GB-SMP #1 SMP Mon Mar 17 17:56:03 UTC 2003 i686 
unknown unknown GNU/Linux 

Red Hat Enterprise Linux Server release 5.1 (Tikanga)
Linux lab8.betech.virginia.edu 2.6.18-53.1.14.el5 #1 SMP Tue Feb 19 
07:18:21 EST 2008 i686 i686 i386 GNU/Linux
{code}

I've already added assertions to Lucene to detect when this bug
strikes, but since assertions are not usually enabled, I plan to add a
real check to catch when this bug strikes *before* we commit the merge
to the index.  This way we can detect & quarantine the failure and
prevent corruption from entering the index.

"
1,"Destination URI should be normalizedWebdavRequestImpl.getHrefLocator tests if the URI passed as parameter starts with the context path, and passes the next segments to the locator factory.
 
There is a potential hole if the parameter contains "".."", because ""http://example.com/dav/../foo"" starts with the context path ""http://example.com/dav"" but represents to ""http://example.com/foo"". Currently, it is up to the locator factory to detect this situation, meaning that every locator factory should implement this check. Additionally, DavLocatorFactory.createResourceLocator cannot throw exceptions, hence it would not fail cleanly (RuntimeException causing a 500 INTERNAL SERVER ERROR response, when a 403 FORBIDDEN status code would have been apropriate)

Note that the Request-URI should have already been normalized by the servlet container, but in COPY/MOVE operations, the Destination-URI is not normalized.

Conformant clients MUST NOT use dot-segments (""."" or "".."")  [RFC 4918, Section 8.3] in Simple-Ref constructions such as the Destination header [RFC 4918, Section 10.3]), but the server should be able to detect this error.

Proposed change in WebdavRequestImpl:193 (in package org.apache.jackrabbit.webdav from webdav/java)
- ref = uri.getRawPath();
+ ref = uri.normalize().getRawPath();

(This causes /dav/../foo to be rejected because it doesn't start with the context path, and accepts dav/foo/../bar because it starts with the context path)"
0,"Add JCR-RMI documentation to the Jackrabbit web siteUse the org.apache.jackrabbit.rmi.{client,server} package javadocs as a base for a JCR-RMI document page on the Jackrabbit web site."
0,JSR 283: Locking
0,"Update StandardTokenizer and UAX29Tokenizer to Unicode 6.0.0Newly released Unicode 6.0.0 contains some character property changes from the previous release (5.2.0) that affect word segmentation (UAX#29), and JFlex 1.5.0-SNAPSHOT now supports Unicode 6.0.0, so Lucene's UAX#29-based tokenizers should be updated accordingly.

Note that the UAX#29 word break rules themselves did not change between Unicode versions 5.2.0 and 6.0.0."
1,SpanScorer does not respect ConstantScoreRangeQuery settingConstantScoreRangeQuery is actually on and can't be disabled when it should default to off with the option to turn it on.
0,"improve test coverage for Multi*It seems like an easy win that when the test calls newSearcher(), 
it should sometimes wrap the reader with a SlowMultiReaderWrapper.
"
1,"close() throws incorrect IllegalStateEx after IndexWriter hit an OOME when autoCommit is trueSpinoff from http://www.nabble.com/IllegalStateEx-thrown-when-calling-close-to20201825.html

When IndexWriter hits an OOME, it records this and then if close() is
called it calls rollback() instead.  This is a defensive measure, in
case the OOME corrupted the internal buffered state (added/deleted
docs).

But there's a bug: if you opened IndexWriter with autoCommit true,
close() then incorrectly throws an IllegalStatException.

This fix is simple: allow rollback to be called even if autoCommit is
true, internally during close.  (External calls to rollback with
autoCommmit true is still not allowed).
"
0,"AttributeSource holds strong reference to class instances and prevents unloading e.g. in Solr if webapplication reload and custom attributes in separate classloaders are used (e.g. in the Solr plugins classloader)When working on the dynmaic proxy classes using cglib/javaassist i recognized a problem in the caching code inside AttributeSource:
- AttributeSource has a static (!) cache map that holds implementation classes for attributes to be faster on creating new attributes (reflection cost)
- AttributeSource has a static (!) cache map that holds a list of all interfaces implemented by a specific AttributeImpl

Also:
- VirtualMethod in 3.1 hold a map of implementation distances keyed by subclasses of the deprecated API

Both have the problem that this strong reference is inside Lucene's classloader and so persists as long as lucene lives. The classes referenced can never be unloaded therefore, which would be fine if all live in the same classloader. As soon as the Attribute or implementation class or the subclass of the deprecated API are loaded by a different classloder (e.g. Lucene lives in bootclasspath of tomcat, but lucene-consumer with custom attributes lives in a webapp), they can never be unloaded, because a reference exists.

Libs like CGLIB or JavaAssist or JDK's reflect.Proxy have a similar cache for generated class files. They also manage this by a WeakHashMap. The cache will always work perfect and no class will be evicted without reason, as classes are only unloaded when the classloader goes and this will only happen on request (e.g. by Tomcat)."
0,"Use the new Jackrabbit parent POMNow that we have an official release of the new org.apache.jackrabbit:parent:2 POM, we should start using that as the parent of all Jackrabbit builds."
0,"Further improvements to contrib/benchmark for testing NRTSome small changes:

  * Allow specifying a priority for BG threads, after the ""&""
    character; priority increment is + or - int that's added to main
    thread's priority to set child thread's.  For my NRT tests I make
    the reopen thread +2, the indexing threads +1, and leave searching
    threads at their default.

  * Added test case

  * NearRealTimeReopenTask now reports @ the end the full array of
    msec of each reopen latency

  * Added optional breakout of counts by time steps.  If you set
    log.time.step.msec to eg 1000 then reported counts for serial task
    sequence is broken out by 1 second windows.  EG you can use this
    to measure slowdown over time.
"
0,"SnowballAnalyzer has a link to net.sf (a package that is empty and needs to be removed).need to remove net.sf and points to org.tartarus.snowball.ext. Doesn't work as a link though, so I'll also remove the @link to lose the javadoc error and broken link."
1,"Item.isSame() may return true for 2 nodes from different workspaces.the code in ItemImpl.isSame() only compares the item id, but not the source workspace."
1,"OracleFileSystem can't handle empty filesthe following exception is thrown when trying to access a 0-length file
in an OracleFileSystem:
java.sql.SQLException: ORA-22275: invalid LOB locator specified

issue reported on the users list, 
see http://www.nabble.com/problems-with-Oracle-tf2483987.html#a6926522

"
1,"FixedIntBlockIndexInput.Reader does not initialise 'pending' int arrayThe FixedIntBlockIndexInput.Reader.pending int array is not initialised. As a consequence, the FixedIntBlockIndexInput.Reader#next() method returns always 0.

A call to FixedIntBlockIndexInput.Reader#blockReader.readBlock() during the Reader initialisation may solve the issue (to be tested)."
0,"Deprecate StandardBenchmarker and ""old"" benchmarker code in favor of the Task based approachWe should deprecate the StandardBechmarker code that was the start of the benchmark contribution in favor of the much easier to use/extend byTask benchmark code"
1,".war distribution should be configurable, prompting you to setup JNDI with the Repository Home and Config locations.The Embedded Deployment Model documentation (http://jackrabbit.apache.org/doc/deploy/howto-model1.html) on the jackrabbit page describes how to package up a .war file so that you can use JNDI Resource settings to change the location of the repository home and the repository configuration xml file.

Unfortunately, the .war file that is provided as part of the Jackrabbit distribution doesn't behave like this. Instead, it has an inbuilt repository.xml file and settings in web.xml that act as defaults. These defaults are not useful and force a user to act like a developer and modify the files within the .war file.

The current situation is that we have a .war that's not going to be useful to anyone without modification. The repository.xml file that is contained within the .war makes the repository home to be the Tomcat/bin/repository directory. This is not a useful default. It's better to have no default setup and a clear error message that JNDI needs to be setup. It would be even better if the web application could recognise when the JNDI wasn't configured and could prompt the user with an instructional webpage, describing how to setup the required JNDI settings on Tomcat, JBoss etc.

----

The .war distribution for Jackrabbit ignores the JNDI settings that are described in the documentation. I am using this Tomcat config.xml snippet to configure Tomcat 5.5:

{{{
<?xml version='1.0' encoding='utf-8'?>
<Context displayName=""Ark"" docBase=""c:\dev\ark\jackrabbit-server-1.1.1.war"" path=""/ark"" 
         useNaming=""false"" workDir=""work\Catalina\localhost\ark"" unpackWAR=""false"">

<Resource name=""jcr/repository""
          auth=""Container""
          type=""javax.jcr.Repository""
          factory=""org.apache.jackrabbit.core.jndi.BindableRepositoryFactory""
          configFilePath=""c:/dev/ark/src/main/resources/repository.xml""
          repHomeDir=""c:/jackrabbitrepo""/>

</Context>
}}}

Jackrabbit loads fine. However, the logs show:

{{{
02.01.2007 10:33:00 *INFO * RepositoryStartupServlet: RepositoryStartupServlet initializing... (RepositoryStartupServlet.java, line 190)
02.01.2007 10:33:00 *INFO * RepositoryStartupServlet:   repository-home = C:\Program Files\Apache Software Foundation\Tomcat 5.5\bin\jackrabbit\repository (RepositoryStartupServlet.java, line 242)

...
...

02.01.2007 10:33:00 *INFO * LocalFileSystem: LocalFileSystem initialized at path C:\Program Files\Apache Software Foundation\Tomcat 5.5\bin\jackrabbit\repository\repository (LocalFileSystem.java, line 166)
}}}






----

My use case is that I want to use Jackrabbit to host a Maven 2 repository within my company. So, ideally I want to:
   * Download the Jackrabbit .war file and mount it on my Tomcat server as context ""/maven2"".
   * Configure Tomcat to use LDAP authentication and point it at my company's LDAP server. This is a standard J2EE feature, of course.
   * Create my own repository.xml file which points to my AccessManager implementation (which goes to my company's SingleSignOn service for authorization). My AccessManager implementation will be placed on the Tomcat shared classpath.
   * Set the repository home directory, where all the working files will be placed and the location of the repository.xml file. Ideally, this would be done in JNDI.

If I have to put together my own Jackrabbit .war file, I consider that I have my ""developer"" hat on when I only really want to have my ""Jackrabbit user"" hat on.
"
0,"QueryWrapperFilter should not do scoringThe purpose of QueryWrapperFilter is to simply filter to include the docIDs that match the query.

Its implementation is wasteful now because it computes scores for those matching docs even though the score is unused.  We could fix this by getting a Scorer and iterating through the docs without asking for the score:

{code}
Index: src/java/org/apache/lucene/search/QueryWrapperFilter.java
===================================================================
--- src/java/org/apache/lucene/search/QueryWrapperFilter.java	(revision 707060)
+++ src/java/org/apache/lucene/search/QueryWrapperFilter.java	(working copy)
@@ -62,11 +62,9 @@
   public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
     final OpenBitSet bits = new OpenBitSet(reader.maxDoc());
 
-    new IndexSearcher(reader).search(query, new HitCollector() {
-      public final void collect(int doc, float score) {
-        bits.set(doc);  // set bit for hit
-      }
-    });
+    final Scorer scorer = query.weight(new IndexSearcher(reader)).scorer(reader);
+    while(scorer.next())
+      bits.set(scorer.doc());
     return bits;
   }
{code}

Maybe I'm missing something, but this seams like a simple win?
"
0,"Add SearcherManager, to manage IndexSearcher usage across threads and reopensThis is a simple helper class I wrote for Lucene in Action 2nd ed.
I'd like to commit under Lucene (contrib/misc).

It simplifies using & reopening an IndexSearcher across multiple
threads, by using IndexReader's ref counts to know when it's safe
to close the reader.

In the process I also factored out a test base class for tests that
want to make lots of simultaneous indexing and searching threads, and
fixed TestNRTThreads (core), TestNRTManager (contrib/misc) and the new
TestSearcherManager (contrib/misc) to use this base class.
"
0,"Clean up old JIRA issues in component ""Analysis""A list of all JIRA issues in component ""Analysis"" that haven't been updated in 2007:

   *	 LUCENE-760  	 Spellchecker could/should use n-gram tokenizers instead of rolling its own n-gramming   
   *	LUCENE-677 	Italian Analyzer 
   *	LUCENE-571 	StandardTokenizer parses decimal number as <HOST> 
   *	LUCENE-566 	Esperanto Analyzer 
   *	LUCENE-559 	Turkish Analyzer for Lucene 
   *	LUCENE-494 	Analyzer for preventing overload of search service by queries with common terms in large indexes 
   *	LUCENE-424 	[PATCH] Submissiom form simple Romanian Analyzer 
   *	LUCENE-417 	StandardTokenizer has problems with comma-separated values 
   *	LUCENE-400 	NGramFilter -- construct n-grams from a TokenStream 
   *	LUCENE-396 	[PATCH] Add position increment back into StopFilter 
   *	LUCENE-387 	Contrib: Main memory based SynonymMap and SynonymTokenFilter 
   *	LUCENE-321 	[PATCH] Submissiom of my Tswana Analyzer 
   *	LUCENE-233 	[PATCH] analyzer refactoring based on CVS HEAD from 6/21/2004 
   *	LUCENE-210 	[PATCH] Never write an Analyzer again 
   *	LUCENE-205 	[PATCH] Patches for RussianAnalyzer 
   *	LUCENE-185 	[PATCH] Thai Analysis Enhancement 
   *	LUCENE-152 	[PATCH] KStem for Lucene 
   *	LUCENE-82 	[PATCH] HTMLParser: IOException: Pipe closed 

"
0,Allow using FST to hold terms data in DocValues.BYTES_*_SORTED
0,"Analyzer for preventing overload of search service by queries with common terms in large indexesAn analyzer used primarily at query time to wrap another analyzer and provide a layer of protection
which prevents very common words from being passed into queries. For very large indexes the cost
of reading TermDocs for a very common word can be  high. This analyzer was created after experience with
a 38 million doc index which had a term in around 50% of docs and was causing TermQueries for 
this term to take 2 seconds.

Use the various ""addStopWords"" methods in this class to automate the identification and addition of 
stop words found in an already existing index."
0,"remove contrib deprecationsthere aren't too many deprecations in contrib to remove for 3.0, but we should get rid of them."
1,"TCK: Test root path not escaped when used in XPath queriesA repository implementation might use a test root path that contains names that need _xXXXX_ escaping when used in XPath queries. Currently the TCK just uses the test path as-is when constructing queries. Even though this only affects few repositories (I've heard of one legacy connector to run into this problem), it would be good to add the proper escaping."
1,"Multithreading issue with versioningIn a multithreading environment with two or more threads accessing the same version history, inconsistent state may be encountered. Concretely, the first thread is currently checking in the node to which the version history is attached while the second thread walks this same version history by means of a ""self-built"" iterator, which just accesses the successors of each version to get the ""next"" to visit.

At a certain point the second point may encounter an ItemNotFoundException with a stack trace similar to this:

javax.jcr.ItemNotFoundException: c9bd405b-dff4-46ef-845c-d98e073e473a
        at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:354)
        at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:230)
        at org.apache.jackrabbit.core.SessionImpl.getNodeByUUID(SessionImpl.java:494)
        at org.apache.jackrabbit.core.version.VersionImpl.getSuccessors(VersionImpl.java:86)
        ....

It seems that the first thread has already filled the successor of the version, while the node is not yet accessible by the createItemInstance method.

This bug seems to not be enforcible, but it is easily reproducible."
0,"[PATCH] to remove synchronized code from TermVectorsReaderOtis,

here the latest and last patch to get rid of all synchronized code from
TermVectorsReader. It should include at least 3 files, TermVectorsReader.diff,
SegmentReader.diff and the new junit test case TestMultiThreadTermVectors.java.
The patch was generated against the current CVS version of TermVectorsReader and
SegmentReader. All lucene related junit tests pass fine.

best regards
Bernhard"
0,"Some tests assume that an implementation of javax.jcr.Item overrides equals()The following 3 tests (followed by the line number containing the bad assertion):

org.apache.jackrabbit.test.api.ReferencesTest.testReferenceTarget:135
org.apache.jackrabbit.test.api.ReferencesTest.testAlterReference:169
org.apache.jackrabbit.test.api.version.VersionHistoryTest:152

assume that an implementation of javax.jcr.Item overrides equals(), such that 

Assert.assertEquals(n1, n2) or 
java.util.Set.contains(n1) 

works for two ""equal"" nodes n1,n2 or for some node n1 that has been previously put into a set. However, there is no section in the specification that would mandate this. The tests above should therefore replace assertEquals() with one of the other mechanism that officially supported, such as javax.jcr.Node.isSame().

"
1,"NPE w/ AbstractPoolEntry.openjava.lang.NullPointerException
    at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:171)
    at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:119)
    at org.apache.http.impl.client.DefaultClientRequestDirector.execute(DefaultClientRequestDirector.java:309)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:501)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:456)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:422)
    at com.limegroup.gnutella.http.DefaultHttpExecutor.performRequest(DefaultHttpExecutor.java:97)
    at com.limegroup.gnutella.http.DefaultHttpExecutor.access$000(DefaultHttpExecutor.java:26)
    at com.limegroup.gnutella.http.DefaultHttpExecutor$MultiRequestor.run(DefaultHttpExecutor.java:135)
    at org.limewire.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1006)
    at org.limewire.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:549)
    at java.lang.Thread.run(Unknown Source)

Seeing a lot of these against Alpha4.  Also seeing still the occassional IllegalStateException of:

java.lang.IllegalStateException: Connection already open.
    at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:150)
    at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:119)
    at org.apache.http.impl.client.DefaultClientRequestDirector.execute(DefaultClientRequestDirector.java:309)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:501)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:456)
    at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:422)
    at com.limegroup.gnutella.http.DefaultHttpExecutor.performRequest(DefaultHttpExecutor.java:97)
    at com.limegroup.gnutella.http.DefaultHttpExecutor.access$000(DefaultHttpExecutor.java:26)
    at com.limegroup.gnutella.http.DefaultHttpExecutor$MultiRequestor.run(DefaultHttpExecutor.java:135)
    at org.limewire.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1006)
    at org.limewire.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:549)
    at java.lang.Thread.run(Unknown Source)
"
0,"No way to get the requestBody out of a PostMethod or use if extending classAttempting to extend the PostMethod class I discovered that I had no access to 
the requestBody because the member is declared private and there is no get 
method.

I was trying to override the setRequestContentLength() when I discovered the 
problem.

So my enhancement request specifically is:
1. add a get method to be able to get the requestBody (probably a get method to 
the parameters as well)
2. (optionally) make the requestBody and parameters members protected instead 
of private so extending the class is easier.

In case you were wondering, the reason for extending the class was to add the 
ability to set a timeout value on the httpconnection and also the ability to 
set the character encoding of the request body.  I don't know if these are 
worthy of an enhancement request but I require them for what I'm doing.  Nice 
work by the way.

Thank you."
0,Use java.util.UUIDReplace the use of org.apache.jackrabbit.uuid.UUID with the new java.util.UUID class introduced in Java 5.
0,"Make lazy loading proxy callback SerializableHello (probably Christophe :) ),

It would be nice to have the CGLib callbacks Serializable, because if they're not, the proxies are not either, even if the target is.

I've seen that you've made BeanLazyLoader Serializable. Wouldn't be better to have AbstractLazyLoader Serializable, so CollectionLazyLoader is too?

What I've done is :
*) declaring AbstractLazyLoader as Serializable
*) declaring non-Serializable resources such as Session as volatile
*) throwing an IllegalStateException in BeanLazyLoader.fetch and CollectionLazyLoader.fetch if the Session is null. This case can only happen if the proxy has been serialized without the volatile Session, and it doesn't make sense to lazy load the target then.

BTW, I realized I didn't clean up the beanClassDescriptor in BeanLazyLoader.cleanUp, so I corrected this.

I'll attach the modified classes.

Sincerely,

Stphane Landelle"
0,"Setting SSLSocket parametersIn HttpClient 4.0.3, it was easy to subclass SSLSocketFactory, and set SSLSocket options (e.g. setEnabledCipherSuites() or setSSLParameterse()) before the SSL handshake happened. This way it was possible to e.g. restrict cipher suites on per-HttpClient basis (instead of JVM-wide system properties).

In HttpClient 4.1.1, the design has changed quite a lot, and copy-pasting of several long methods is needed. 

Ideally, SSLSocketFactory should support applying SSLParameters to the socket. However, SSLParameters is Java 1.6, so if we want to keep compatibility with 1.5, that's out.

However, it'd be nice to at least have a method (e.g. ""protected SSLSocket prepareSSLSocket(SSLSocket s)"") that would get called immediately after a socket is retrieved from the socket factory. The default implementation could be just ""return s;"", but subclasses could do something like s.setEnabledCipherSuites() s.setSSLParameters()."
0,"RepositoryLock does not work on NFS sometimesThe RepositoryLock mechanism currently used in Jackrabbit uses FileLock. This doesn't work on some NFS file system. It looks like only NFS version 4 and newer supports locking. Older implementations may throw a IOException ""No locks available"", which means the NFS does not support byte-range locking.

I propose to add a second locking mechanism, and add a configuration option to use it. For example: <FileLocking class=""acme"" />. This second locking mechanism is a cooperative locking protocol that uses a background (watchdog) thread and only uses regular file operations.

"
1,"Merging between workspaces failsI have setup 2 workspaces in Jackrabbit.  I have a preview and a production
workspace.  These workspaces keep a tree of menu nodes that can have content
associated to those menus.  Each node is of type nt:unstructured and has
mixin types of versionable, lockable, and referenceable.

In our system you are only allowed to edit nodes in the preview workspace.
So what I do is when you go to edit a node we check it out, allow for edits,
then check it in.  This creates a new version on the node.  Then we merge
the node up to the production workspace.  All nodes in the production
workspace are always checked in and not locked.

When I go to do a merge I run into problems when I try to merge a node that
has children.  Lets say I have node A with children B and C.  These all have
the same node types as stated above.  I make a change to a property in Node
A in the preview workspace and now want to merge it into the production
workspace (where it exists already).  Here is the code that is run:

Node destNode = destSession.getNodeByUUID(getUUID());
NodeIterator ni = destNode.merge(""preview"", true);

Now this fails in the ItemImpl.internalRemove() method with a
VersionException of cannot remove a child of a checked-in node.  Here is the
trace for the error:
at org.apache.jackrabbit.core.ItemImpl.internalRemove(ItemImpl.java:848)
at org.apache.jackrabbit.core.NodeImpl.internalMerge(NodeImpl.java:3693)
at org.apache.jackrabbit.core.NodeImpl.internalMerge(NodeImpl.java:3587)
at org.apache.jackrabbit.core.NodeImpl.merge(NodeImpl.java:3003)

Now if I understand correctly when doing a merge the node that you are
trying to merge to needs to be older then the source node and the
destination node cannot be checked out (NodeImpl.doMergeTest() is where I
figured that out).  But then when I step through further in the merge in
NodeImpl it gets all the nodes of the src node and retrieves the same
children in the destination workspace and then tries to remove those
destination children but it can't remove those children b/c the parent node
(which is node A in the production workspace) is not checked out, but
according to the mergeTest it can't be checked out or the merge won't even
begin."
0,"Allow Junit4 tests in our environment.Now that we're dropping Java 1.4 compatibility for 3.0, we can incorporate Junit4 in testing. Junit3 and junit4 tests can coexist, so no tests should have to be rewritten. We should start this for the 3.1 release so we can get a clean 3.0 out smoothly.

It's probably worthwhile to convert a small set of tests as an exemplar.


"
0,"Making Term Vectors more accessibleOne of the big issues with term vector usage is that the information is loaded into parallel arrays as it is loaded, which are then often times manipulated again to use in the application (for instance, they are sorted by frequency).

Adding a callback mechanism that allows the vector loading to be handled by the application would make this a lot more efficient.

I propose to add to IndexReader:
abstract public void getTermFreqVector(int docNumber, String field, TermVectorMapper mapper) throws IOException;
and a similar one for the all fields version

Where TermVectorMapper is an interface with a single method:
void map(String term, int frequency, int offset, int position);

The TermVectorReader will be modified to just call the TermVectorMapper.  The existing getTermFreqVectors will be reimplemented to use an implementation of TermVectorMapper that creates the parallel arrays.  Additionally, some simple implementations that automatically sort vectors will also be created.

This is my first draft of this API and is subject to change.  I hope to have a patch soon.

See http://www.gossamer-threads.com/lists/lucene/java-user/48003?search_string=get%20the%20total%20term%20frequency;#48003 for related information."
1,"HttpConnection.isResponseAvailable() calls setSoTimeout() but does not catch IOExceptionHttpConnection.isResponseAvailable() can throw an IOException when setting the
soTimeout but should probably just return false in this case.

<http://marc.theaimsgroup.com/?t=106268485100002&r=1&w=2>"
0,"Maven build failure in textfilter contrib projectI've tried to build the textfilters contrib but get the following error:

Attempting to download jackrabbit-0.16.4.1-dev.jar.
WARNING: Failed to download jackrabbit-0.16.4.1-dev.jar.
The build cannot continue because of the following unsatisfied dependency:
jackrabbit-0.16.4.1-dev.jar

I tried changing the project.xml to look for jackrabbit-1.0-dev.jar instead, but it didn't work, not sure what maven expects here, but should be an easy fix for somebody in the know."
0,"Incorrect transitive snapshot dependenciesUsing ${version} in dependency declarations causes troubles since for snapshot dependencies the version variable apparently gets replaced by the exact timestamp of a deployed snapshot instead of the ""x.y-SNAPSHOT"" string. Typically the timestamps of different artifacts are not the same, causing broken dependencies."
0,"SpanOrQuery.java: simplification and testThe current SpanOrQuery.java has some unnessary attributes. After removing these, I found that there was no existing test for it, so I added some tests to TestSpans.java."
0,"Update SPI locking to match JCR 2.0jcr2spi currently uses the JSR 170 way to determine whether a given Session owns the lock by checking of the lock token is null.
with JSR 283 a new Lock method has been defined for this, while on the other hand the lock token is always null for session-scoped
locks.

In addition 283-locking allows to specify a timeout hint and hint about the owner info that should be displayed
for information purpose.

Proposed changes to SPI:

- extend org.apache.jackrabbit.spi.LockInfo to cover the new functionality added with JSR 283
- add an variant of RepositoryService.lock that allows to specify timeout and owner hint.

Proposed changes to JCR2SPI:
- change jcr2spi to make use of the new functionality and modify the test for session being lock holder.
  this mainly affects
  > LockOperation
  > LockManager impl
  > Lock impl"
1,"Bundle persistence managers node id key store/load is not symertric on MySql causing NoSuchItemState Exceptions It looks like the binary values read back from MySql where the UUID contains 0's is not the same as that generated from the UUID getRawBytes() call. As result, you can store a node with the UUID that has 0's but its never found when read back. This therefore causes corruption in random places when certain UUIDs are generated.

Test Case: 

I've attached 2 files. One causes node corruption when imported, the other does not.
The only difference is that I removed any 0 values from the problem UUID in the file that causes corruption.

As Stefan pointed out, I had manipulated the test case to use standard nt types when in fact I should have provided the following info (sorry Stefan) e.g. the test folder types are referencable hence the jcr:uuid allocation

[acme:Folder] > nt:folder, mix:referenceable

If I import causes_corruption.xml and then attempt to ""ls"" AclObjectIdentities then loadBundle() returns null for the UUID 

a55f3f6b-a909-4e8d-b65a-93002ced0920 which in bytes is [-91, 95, 63, 107, -87, 9, 78, -115, -74, 90, -109, 0, 44, -19, 9, 32]

If I import works.xml then ""ls"" works fine for the same node as I've manually changed the UUID to replace 0s with 1s in the last section.

a55f3f6b-a909-4e8d-b65a-93112ced1921 [-91, 95, 63, 107, -87, 9, 78, -115, -74, 90, -109, 17, 44, -19, 25, 33]


Testing shows this issue highlights a problem with the Bundle persistence manager and MySqls method of handling BINARY columns.
The solution looks to be to replace BINARY(16) with VARBINARY(16). Quoting from http://dev.mysql.com/doc/refman/5.0/en/binary-varbinary.html...
""If the value retrieved must be the same as the value specified for storage with no padding, it might be preferable to use VARBINARY or one of the BLOB data types instead.""
A review of our logs shows that all of the corruption we've seen has related to nodes with UUIDs including 0's.

* Shall I log a JIRA ticket for this?
* Anyone see any issues with this fix?


In the following example you can see I'm showing all bundles in the ""test1"" workspace.

mysql> select hex(node_id) from test1_bundle;
+----------------------------------+
| hex(node_id)                     |
+----------------------------------+
| 28126C3E36A0471D9CDC5AC423BAC9C5 |
| A55F3F6BA9094E8DB65A93002CED0920 |
| CAFEBABECAFEBABECAFEBABECAFEBABE |
| D638EACCDEB641FD8868804C8ECEFFFD |
| DEADBEEFCAFEBABECAFEBABECAFEBABE |
+----------------------------------+
5 rows in set (0.00 sec)

...but a select using the same UUID hex value returns no rows.

mysql>  select node_id from test1_bundle where 
mysql> unhex('A55F3F6BA9094E8DB65A93002CED0920') = node_id;
Empty set (0.00 sec)

I've then created a new ""test3"" workspace which I modified to use varbinary instead of binary with:

alter table test3_bundle modify NODE_ID varbinary(16); alter table test3_refs modify NODE_ID varbinary(16);

My import test case now no longer fails and the following query proves that query operations, after a store, return rows as expected.

mysql>  select node_id from test3_bundle where 
mysql> unhex('A55F3F6BA9094E8DB65A93002CED0920') = node_id;
+------------------+
| node_id          |
+--------Z ,--  |
+------------------+
1 row in set (0.00 sec)

mysql> desc test3_bundle;
ERROR 2006 (HY000): MySQL server has gone away No connection. Trying to reconnect...
Connection id:    7116
Current database: mmptest

+-------------+---------------+------+-----+---------+-------+
| Field       | Type          | Null | Key | Default | Extra |
+-------------+---------------+------+-----+---------+-------+
| NODE_ID     | varbinary(16) | YES  | UNI | NULL    |       |
| BUNDLE_DATA | longblob      | NO   |     |         |       |
+-------------+---------------+------+-----+---------+-------+
2 rows in set (0.00 sec)


mysql>  alter table test3_bundle modify NODE_ID varbinary(16);
Query OK, 2 rows affected (0.00 sec)
Records: 2  Duplicates: 0  Warnings: 0

"
0,"Make the Highlighter use SpanScorer by defaultI've always thought this made sense, but frankly, it took me a year to get the SpanScorer included with Lucene at all, so I was pretty much ready to move on after I it got in, rather than push for it as a default.

I think it makes sense as the default in Solr as well, and I mentioned that back when it was put in, but alas, its an option there as well.

The Highlighter package has no back compat req, but custom has been conservative - one reason I havn't pushed for this change before. Might be best to actually make the switch in 3? I could go either way - as is, I know a bunch of people use it, but I'm betting its the large minority. It has never been listed in a changes entry and its not in LIA 1, so you pretty much have to stumble upon it, and figure out what its for.

I'll point out again that its just as fast as the standard scorer for any clause of a query that is not position sensitive. Position sensitive query clauses will obviously be somewhat slower to highlight, but that is because they will be highlighted correctly rather than ignoring position."
0,"Make it possible to adjust MaxTotalConnections parameter dynamicalyMake it possible to adjust MaxTotalConnections parameter at run time. Document behaviour of MaxTotalConnections and MaxConnectionsPerRoute behaviour (latter cannot be changed for allocated pools)

Oleg"
1,"AttributeSource can have an invalid computed stateIf you work a tokenstream, consume it, then reuse it and add an attribute to it, the computed state is wrong.
thus for example, clearAttributes() will not actually clear the attribute added.

So in some situations, addAttribute is not actually clearing the computed state when it should.
"
0,"BooleanWeight should size the weights Vector correctlyThe weights field on BooleanWeight uses a Vector that will always be sized exactly the same as the outer class' clauses Vector, therefore can be sized correctly in the constructor. This is a trivial memory saving enhancement."
1,"Intermittent failure in TestFieldCacheTermsFilter.testMissingTermsRunning tests in while(1) I hit this:

{noformat}

NOTE: reproduce with: ant test -Dtestcase=TestFieldCacheTermsFilter -Dtestmethod=testMissingTerms -Dtests.seed=-1046382732738729184:5855929314778232889

1) testMissingTerms(org.apache.lucene.search.TestFieldCacheTermsFilter)
java.lang.AssertionError: Must match 1 expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:91)
	at org.junit.Assert.failNotEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:126)
	at org.junit.Assert.assertEquals(Assert.java:470)
	at org.apache.lucene.search.TestFieldCacheTermsFilter.testMissingTerms(TestFieldCacheTermsFilter.java:63)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1214)
	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1146)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.junit.runners.Suite.runChild(Suite.java:128)
	at org.junit.runners.Suite.runChild(Suite.java:24)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:157)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:136)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:117)
	at org.junit.runner.JUnitCore.runMain(JUnitCore.java:98)
	at org.junit.runner.JUnitCore.runMainAndExit(JUnitCore.java:53)
	at org.junit.runner.JUnitCore.main(JUnitCore.java:45)
{noformat}

Unfortunately the seed doesn't [consistently] repro for me..."
0,"Improve BufferedIndexInput.readBytes() performanceDuring a profiling session, I discovered that BufferedIndexInput.readBytes(),
the function which reads a bunch of bytes from an index, is very inefficient
in many cases. It is efficient for one or two bytes, and also efficient
for a very large number of bytes (e.g., when the norms are read all at once);
But for anything in between (e.g., 100 bytes), it is a performance disaster.
It can easily be improved, though, and below I include a patch to do that.

The basic problem in the existing code was that if you ask it to read 100
bytes, readBytes() simply calls readByte() 100 times in a loop, which means
we check byte after byte if the buffer has another character, instead of just
checking once how many bytes we have left, and copy them all at once.

My version, attached below, copies these 100 bytes if they are available at
bulk (using System.arraycopy), and if less than 100 are available, whatever
is available gets copied, and then the rest. (as before, when a very large
number of bytes is requested, it is read directly into the final buffer).

In my profiling, this fix caused amazing performance
improvement: previously, BufferedIndexInput.readBytes() took as much as 25%
of the run time, and after the fix, this was down to 1% of the run time! However, my scenario is *not* the typical Lucene code, but rather a version of Lucene with added payloads, and these payloads average at 100 bytes, where the original readBytes() did worst. I expect that my fix will have less of an impact on ""vanilla"" Lucene, but it still can have an impact because it is used for things like reading fields. (I am not aware of a standard Lucene benchmark, so I can't provide benchmarks on a more typical case).

In addition to the change to readBytes(), my attached patch also adds a new
unit test to BufferedIndexInput (which previously did not have a unit test).
This test simulates a ""file"" which contains a predictable series of bytes, and
then tries to read from it with readByte() and readButes() with various
sizes (many thousands of combinations are tried) and see that exactly the
expected bytes are read. This test is independent of my new readBytes()
inplementation, and can be used to check the old implementation as well.

By the way, it's interesting that BufferedIndexOutput.writeBytes was already efficient, and wasn't simply a loop of writeByte(). Only the reading code was inefficient. I wonder why this happened."
0,"PrefixQuery is missing the equals() methodThe PrefixQuery is inheriting the java.lang.Object's object default equals method. This makes it hard to have test working of PrefixFilter or any other task requiring equals to work proerply (insertion in Set, etc.). The equal method should be very similar, not to say identical except for class casting, to the equals() of TermQuery. "
0,"Make the Lucene jar an OSGi bundleIn order to use Lucene in an OSGi environment, some additional headers are needed in the manifest of the jar. As Lucene has no dependency, it is pretty straight forward and it ill be easy to maintain I think."
0,"factor CharTokenizer/CharacterUtils into analyzers moduleCurrently these analysis components are in the lucene core, but should really
be .util in the analyzers module.

Also, with MockTokenizer extending Tokenizer directly, we can add some additional
checks in the future to try to ensure our consumers are being good consumers (e.g. calling reset).

This is mentioned in http://wiki.apache.org/lucene-java/TestIdeas, I didn't implement it here yet,
this is just the factoring. I think we should try to do this before LUCENE-3040.
"
0,"Store all metadata in human-readable segments fileVarious index-reading components in Lucene need metadata in addition to data.
This metadata is presently stored in arbitrary binary headers and spread out
over several files.  We should move to concentrate it in a single file, and 
this file should be encoded using a human-readable, extensible, standardized 
data serialization language -- either XML or YAML.

* Making metadata human-readable makes debugging easier.  Centralizing it
  makes debugging easier still.  Developers benefit from being able to scan
  and locate relevant information quickly and with less debug printing.  Users
  get a new window through which to peer into the index structure.
* Since metadata is written to a separate file, there would no longer be a 
  need to seek back to the beginning of any data file to finish a header, 
  solving issue LUCENE-532.
* Special-case parsing code needed for extracting metadata supplied by 
  different index formats can be pared down.  If a value is no longer 
  necessary, it can just be ignored/discarded.
* Removing headers from the data files simplifies them and makes the file
  format easier to implement. 
* With headers removed, all or nearly all data structures can take the
  form of records stacked end to end, so that once a decoder has been
  selected, an iterator can read the file from top to tail.  To an extent,
  this allows us to separate our data-processing algorithms from our
  serialization algorithms, decoupling Lucene's code base from its file
  format.  For instance, instead of further subclassing TermDocs to deal with
  ""flexible indexing"" formats, we might replace it with a PostingList which
  returns a subclass of Posting.  The deserialization code would be wholly
  contained within the Posting subclass rather than spread out over several
  subclasses of TermDocs.
* YAML and XML are equally well suited for the task of storing metadata, 
  but in either case a complete parser would not be needed -- a small subset 
  of the language will do.  KinoSearch 0.20's custom-coded YAML parser 
  occupies about 600 lines of C -- not too bad, considering how miserable C's 
  string handling capabilities are. "
0,"IndexReader subclasses must implement flex APIsTo be fixed only on trunk...

I made IndexReader's base flex APIs abstract, fixed all core/contrib/solr places that subclassed IR and didn't already implement flex (including contrib/memory, contrib/instantiated), and remove all the classes for the back-compat layer that emulated flex APIs on top of pre-flex APIs."
1,"BooleanQuery.hashCode and equals ignore isCoordDisabledBooleanQuery.isCoordDisabled() is not considered by BooleanQuery's hashCode() or equals() methods ... this can cause serious badness to happen when caching BooleanQueries.

bug traces back to at least 1.9"
0,"Excessive Arrays.fill(0) in DocumentsWriter drastically slows down small docs (3.9X slowdown!)I've been doing some ""final"" performance testing of 2.3RC1 and
uncovered a fairly serious bug that adds a large fixed CPU cost when
documents have any term vector enabled fields.

The bug does not affect correctness, just performance.

Basically, for every document, we were calling Arrays.fill(0) on a
large (32 KB) byte array when in fact we only needed to zero a small
part of it.  This only happens if term vectors are turned on, and is
especially devastating for small documents."
1,"some valid email address characters not correctly recognizedthe EMAIL expression in StandardTokenizerImpl.jflex misses some unusual but valid characters in the left-hand-side of the email address. This causes an address to be broken into several tokens, for example:

somename+site@gmail.com gets broken into ""somename"" and ""site@gmail.com""
husband&wife@talktalk.net gets broken into ""husband"" and ""wife@talktalk.net""

These seem to be occurring more often. The first seems to be because of an anti-spam trick you can use with google (see: http://labnol.blogspot.com/2007/08/gmail-plus-smart-trick-to-find-block.html). I see the second in several domains but a disproportionate amount are from talktalk.net, so I expect it's a signup suggestion from the service.

Perhaps a fix would be to change line 102 of StandardTokenizerImpl.jflex from:
EMAIL      =  {ALPHANUM} (("".""|""-""|""_"") {ALPHANUM})* ""@"" {ALPHANUM} (("".""|""-"") {ALPHANUM})+

to 

EMAIL      =  {ALPHANUM} (("".""|""-""|""_""|""+""|""&"") {ALPHANUM})* ""@"" {ALPHANUM} (("".""|""-"") {ALPHANUM})+

I'm aware that the StandardTokenizer is meant to be more of a basic implementation rather than an implementation the full standard, but it is quite useful in places and hopefully this would improve it slightly."
0,"Contrib: ThaiAnalyzer to enable Thai full-text search in LuceneThai text don't have space between words. Usually, a dictionary-based algorithm is used to break string into words. For Lucene to be usable for Thai, an Analyzer that know how to break Thai words is needed.

I've implemented such Analyzer, ThaiAnalyzer, using ICU4j DictionaryBasedBreakIterator for word breaking. I'll upload the code later.

I'm normally a C++ programmer and very new to Java. Please review the code for any problem. One possible problem is that it requires ICU4j. I don't know whether this is OK."
1,SPI: Description of Path.isDescendantOf(Path) Description of Path.isDescendantOf lists different reasons for IllegalArgumentException and RepositoryException than isAncestorOf... this is obviously a mistake.
0,"Provide feedback mechanism to CredentialsProviderIf the remote server is using BASIC or NT authentication and you pass in 
invalid credentials you get stuck in an infinite for loop, repeatedly sending 
the same authentication request again and again to the server.  The for loop is 
in the executeMethod method of the HttpMethodDirector class.

Sample code:
=================================================================


import org.apache.commons.httpclient.Credentials;
import org.apache.commons.httpclient.NTCredentials;
import org.apache.commons.httpclient.UsernamePasswordCredentials;
import org.apache.commons.httpclient.HttpClient;
import org.apache.commons.httpclient.methods.GetMethod;
import org.apache.commons.httpclient.auth.*;

import java.io.IOException;
import java.io.BufferedInputStream;
import java.io.ByteArrayOutputStream;

/**
 * Created by IntelliJ IDEA.
 * User: dmartineau
 * Date: Nov 8, 2005
 * Time: 1:43:21 PM
 */
public class ShowProblem
{

    private String location;
    private String user;
    private String pass;
    private String domain;

    public ShowProblem(String location, String user, String pass, String domain)
    {
        this.location = location;
        this.user=user;
        this.pass=pass;
        this.domain=domain;

    }

    public int getFile()
    {
        int status = 500;
        HttpClient client = new HttpClient();
        client.getParams().setParameter(
            CredentialsProvider.PROVIDER, new CProvider(user,pass,domain));
        GetMethod httpget = new GetMethod(location);
        httpget.setDoAuthentication(true);

        try
        {
            // execute the GET
            status = client.executeMethod(httpget);
            if (status==200)
            {
                BufferedInputStream bin = new BufferedInputStream
(httpget.getResponseBodyAsStream());

                ByteArrayOutputStream bos = new ByteArrayOutputStream();
                int bytesRead = 0;
                byte[] buff = new byte[16384];

                while ( (bytesRead = bin.read(buff)) != -1) {
                    bos.write(buff, 0, bytesRead);
                }

                // display the results.
                System.out.println(new String(bos.toByteArray()));
            }
        }
        catch (Throwable t)
        {
            t.printStackTrace();
        }
        finally
        {
            // release any connection resources used by the method
            httpget.releaseConnection();
        }
        return status;

    }

    public static void main(String[] args)
    {
        ShowProblem showProblem = new ShowProblem(args[0],args[1],args[2],args
[3]);
        int response = showProblem.getFile();
        
    }



    class CProvider implements CredentialsProvider
    {
        private String user;
        private String password;
        private String domain;

        public CProvider(String user, String password, String domain)
        {
            super();
            this.user = user;
            this.password = password;
            this.domain = domain;
        }

        public Credentials getCredentials(final AuthScheme authscheme,final 
String host,int port,boolean proxy)
        throws CredentialsNotAvailableException
        {
            if (authscheme == null)
            {
                return null;
            }
            try
            {
                if (authscheme instanceof NTLMScheme)
                {
                    return new NTCredentials(user, password, host, domain);
                }
                else if (authscheme instanceof RFC2617Scheme)
                {
                    return new UsernamePasswordCredentials(user, password);
                }
                else
                {
                    throw new CredentialsNotAvailableException(""Unsupported 
authentication scheme: "" +
                        authscheme.getSchemeName());
                }
            }
            catch (IOException e)
            {
                throw new CredentialsNotAvailableException(e.getMessage(), e);
            }
        }

    }
}"
1,"NullPointerException on DelegatingObservationDispatcher cause by parameter null on call : createEventStateCollection(null)There is a NullPointerException when jackrabbit try to synchronise its indexes :
22 janv. 2009 09:53:56 INFO  [ClusterNode] - Processing revision: 4485
22 janv. 2009 09:53:56 ERROR [ClusterNode] - Unexpected error while syncing of journal: null
java.lang.NullPointerException
        at org.apache.jackrabbit.core.observation.DelegatingObservationDispatcher.createEventStateCollection(DelegatingObservationDispatcher.java:80)
        at org.apache.jackrabbit.core.version.VersionManagerImpl$DynamicESCFactory.createEventStateCollection(VersionManagerImpl.java:556)
        at org.apache.jackrabbit.core.version.VersionManagerImpl.externalUpdate(VersionManagerImpl.java:500)
        at org.apache.jackrabbit.core.cluster.ClusterNode.process(ClusterNode.java:853)
        at org.apache.jackrabbit.core.cluster.ChangeLogRecord.process(ChangeLogRecord.java:457)
        at org.apache.jackrabbit.core.cluster.ClusterNode.consume(ClusterNode.java:799)
        at org.apache.jackrabbit.core.journal.AbstractJournal.doSync(AbstractJournal.java:213)
        at org.apache.jackrabbit.core.journal.AbstractJournal.sync(AbstractJournal.java:188)
        at org.apache.jackrabbit.core.cluster.ClusterNode.sync(ClusterNode.java:315)
        at org.apache.jackrabbit.core.cluster.ClusterNode.run(ClusterNode.java:286)
        at java.lang.Thread.run(Thread.java:595)

In fact the method createEventStateCollection() of DelegatingObservationDispatcher is called by the VersionManagerImpl with session parameter as null...

DelegatingObservationDispatcher:

 public EventStateCollection createEventStateCollection(SessionImpl session,
                                                           Path pathPrefix) {
        String userData = null;
        try {
            userData = ((ObservationManagerImpl) session.getWorkspace().getObservationManager()).getUserData();
        } catch (RepositoryException e) {
            // should never happen because this
            // implementation supports observation
        }
        return new EventStateCollection(this, session, pathPrefix, userData);
    }

VersionManagerImpl$DynamicESCFactory :
 public EventStateCollection createEventStateCollection(SessionImpl source) {
            return obsMgr.createEventStateCollection(source, VERSION_STORAGE_PATH);
        }

VersionManagerImpl :
public void externalUpdate(ChangeLog changes, List events,
                               long timestamp, String userData)
            throws RepositoryException {
        EventStateCollection esc = getEscFactory().createEventStateCollection(null);
        esc.addAll(events);
        esc.setTimestamp(timestamp);
        esc.setUserData(userData);

        sharedStateMgr.externalUpdate(changes, esc);
    }"
1,"method.getURI()  returns escaped URIs but it shouldn'tHi guys,

Please, consider the following imaginary and simplified code:


URI u = new URI(""http://some.host.com/%41.html"", true);
HttpClient httpClient = new HttpClient();
GetMethod method = new GetMethod();
method.setURI(u);
URI u2 = method.getURI();

System.out.println(""1. "" + u);
System.out.println(""2. "" + new String(u.getRawURI()));
System.out.println(""3. "" + u.getURI());
System.out.println(""4. "" + u2);
System.out.println(""5. "" + new String(u2.getRawURI()));
System.out.println(""6. "" + u2.getURI());


The result that you'll get is:

1. http://some.host.com/%41.html
2. http://some.host.com/%41.html
3. http://some.host.com/A.html
4. http://some.host.com/%2541.html
5. http://some.host.com/%2541.html
6. http://some.host.com/%41.html


You can see that for lines 4, 5, and 6, the URI suddenly gets escaped (the 
percent sign gets converted to %25).

Why is that? Am I doing something wrong? Is this the desired behaviour? I would 
have expected to get the SAME URI back, without any escaping.

Besides, I have another question:

After executing a method -- httpClient.executeMethod(method) -- what will 
method.getURI() return? The URI *after* all redirections or the original URI? 
It seems I get the URI *after* the redirections, which is fine, but the 
documentation doesn't say that. It only explicitly says that the getPath() 
method has that behaviour.

Best regards and thanks,
Bisser"
1,"RepositoryImpl.acquireRepositoryLock() fails to detect that the file lock is already held by the current processwith java 1.4 and 1.5 on a *nix-based platform it is possible to (concurrently) instantiate 
more than one repository instance in the same jvm based on same/identical configurations.

this is a critical issue since it might lead to data corruption.

the issue only exists with java versions prior to 1.6 and *nix-based platforms (only verified
on mac os-x 10.4).

note that the issue does not exist when the file lock is held by another jvm.

 code snippet to reproduce the issue:

            Repository rep1 = new TransientRepository();
            Session s1 = rep1.login(new SimpleCredentials(""johndoe"", """".toCharArray()));
            Repository rep2 = new TransientRepository();
            Session s2 = rep2.login(new SimpleCredentials(""johndoe"", """".toCharArray()));


the root problem is the incorrect behavior of java.nio.channels.FileChannel#tryLock()
which is demonstrated by the following code snippet:

            try {
                FileLock fl1 = new FileOutputStream(""foo"").getChannel().tryLock();
                System.out.println(""1st lock: "" + fl1);
                FileLock fl2 = new FileOutputStream(""foo"").getChannel().tryLock();
                System.out.println(""2nd lock: "" + fl2);
            } catch (Throwable t) {
                t.printStackTrace();
            }

"
1,"HttpClient loops endlessly while trying to retrieve status lineWhen fed with the wrong URL, for example http://localhost:19/ (chargen port),
HttpClient will loop endlessly while attempting to read the status line.

This is caused by a bug in HttpMethodBase.readStatusLine(HttpState, HttpConnection)

(while loop without any exceptional abort condition).

wire log excerpt:

2003/11/10 12:33:04:085 CET [DEBUG] HttpMethodDirector - -Execute loop try 1
2003/11/10 12:33:04:312 CET [DEBUG] wire - ->> ""GET / HTTP/1.1[\r][\n]""
2003/11/10 12:33:04:351 CET [DEBUG] HttpMethodBase - -Adding Host request header
2003/11/10 12:33:04:532 CET [DEBUG] wire - ->> ""User-Agent: Jakarta
Commons-HttpClient[\r][\n]""
2003/11/10 12:33:04:554 CET [DEBUG] wire - ->> ""Host: localhost:19[\r][\n]""
2003/11/10 12:33:04:559 CET [DEBUG] wire - ->> ""[\r][\n]""
2003/11/10 12:33:04:639 CET [DEBUG] wire - -<<
""!""#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefgh[\r][\n]""
2003/11/10 12:33:04:669 CET [DEBUG] wire - -<<
""""#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghi[\r][\n]""
2003/11/10 12:33:04:673 CET [DEBUG] wire - -<<
""#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghij[\r][\n]""
2003/11/10 12:33:04:692 CET [DEBUG] wire - -<<
""$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijk[\r][\n]""
2003/11/10 12:33:04:698 CET [DEBUG] wire - -<<
""%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijkl[\r][\n]""
2003/11/10 12:33:04:703 CET [DEBUG] wire - -<<
""&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklm[\r][\n]""
<snip>"
0,"contrib/remote tests fail randomlyThe contrib/remote tests will fail randomly.

This is because they use this _TestUtil.getRandomSocketPort() which
simply generates a random number, but if this is already in use, it will fail.

Additionally there is duplicate RMI logic across all 3 test classes."
0,"Small performance enhancement for StandardAnalyzerThe class StandardAnalyzer has an inner class, SavedStreams, which is used internally for maintaining some state. This class doesn't use the implicit reference to the enclosing class, so it can be made static and reduce some memory requirements. A patch will be attached shortly."
0,"Observation: avoid running out of memoryJackrabbit uses an unbounded observation queue for event listeners (for asynchronous listeners, which are the default). If an observation listener is very slow, the observation queue gets larger and larger, and the JVM will eventually run out of memory.

I suggest to use a maximum queue size of 100'000 by default. Adding new events to the queue will block until the observation listeners removed an item. I'm not sure if we need a way to configure this option; probably a system property is enough as a start (we can still add a better way to configure this setting if it turns out somebody actually needs a different value).

A special case is observation listeners that themselves write to the repository and therefore cause new events. In this case, it doesn't make sense to block adding an event, because that would block the whole system. However a warning should be written to the log file."
0,"tests-local fails on 3 tests1)
testMultiSendCookieGet(org.apache.commons.httpclient.TestWebappCookie)junit.framework.AssertionFailedError:
<html>
<head><title>ReadCookieServlet: GET</title></head>
<body>
<p>This is a response to an HTTP GET request.</p>
<p><tt>Cookie:
$Version=1;simplecookie=value;$Path=/httpclienttest/cookie;$Domain=localhost</tt></p>
<tt>$Version=1</tt><br>
<tt>simplecookie=value</tt><br>
<tt>$Path=/httpclienttest/cookie</tt><br>
<tt>$Domain=localhost</tt><br>
</body>
</html>
    at
org.apache.commons.httpclient.TestWebappCookie.testMultiSendCookieGet(TestWebappCookie.java:348)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
2)
testDeleteCookieGet(org.apache.commons.httpclient.TestWebappCookie)junit.framework.AssertionFailedError:
<html>
<head><title>ReadCookieServlet: GET</title></head>
<body>
<p>This is a response to an HTTP GET request.</p>
<p><tt>Cookie:
$Version=1;simplecookie=value;$Path=/httpclienttest/cookie;$Domain=localhost</tt></p>
<tt>$Version=1</tt><br>
<tt>simplecookie=value</tt><br>
<tt>$Path=/httpclienttest/cookie</tt><br>
<tt>$Domain=localhost</tt><br>
</body>
</html>
    at
org.apache.commons.httpclient.TestWebappCookie.testDeleteCookieGet(TestWebappCookie.java:389)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
3)
testDeleteCookiePut(org.apache.commons.httpclient.TestWebappCookie)junit.framework.AssertionFailedError:
<html>
<head><title>ReadCookieServlet: PUT</title></head>
<body>
<p>This is a response to an HTTP PUT request.</p>
<p><tt>Cookie:
$Version=1;simplecookie=value;$Path=/httpclienttest/cookie;$Domain=localhost</tt></p>
<tt>$Version=1</tt><br>
<tt>simplecookie=value</tt><br>
<tt>$Path=/httpclienttest/cookie</tt><br>
<tt>$Domain=localhost</tt><br>
</body>
</html>
    at
org.apache.commons.httpclient.TestWebappCookie.testDeleteCookiePut(TestWebappCookie.java:464)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
FAILURES!!!
Tests run: 108,  Failures: 3,  Errors: 0
httpclient/build.xml [271] Java returned: -1
BUILD FAILED
Total time: 9 seconds"
0,IndexWriter has incomplete JavadocsA couple of getter methods in IndexWriter have no javadocs.
1,"Host request header does not contain portThe Host request header is always added with just the hostname used for the 
connection.  If the port is different than 80 it needs to be included as well, 
with a colon separating it from the hostname.  This problem is especially 
apparent when you use the httpclient to connect to tomcat 4 and then use 
HttpUtils to create a full URL representing the request.  HttpUtils pulls the 
host and port from the Host header.  When commons-httpclient is used HttpUtils 
never includes the port since it was never in the Host header."
0,"Cleanup MMapDirectory to use only one MMapIndexInput impl with mapping sized of powers of 2Robert and me discussed a little bit after Mike's investigations, that using SingleMMapIndexinput together with MultiMMapIndexInput leads to hotspot slowdowns sometimes.

We had the following ideas:
- MultiMMapIndexInput is almost as fast as SingleMMapIndexInput, as the switching between buffer boundaries is done in exception catch blocks. So normal code path is always the same like for Single*
- Only the seek method uses strange calculations (the modulo is totally bogus, it could be simply: int bufOffset = (int) (pos % maxBufSize); - very strange way of calculating modulo in the original code)
- Because of speed we suggest to no longer use arbitrary buffer sizes. We should pass only the power of 2 to the indexinput as size. All calculations in seek and anywhere else would be simple bit shifts and AND operations (the and masks for the modulo can be calculated in the ctor like NumericUtils does when calculating precisionSteps).
- the maximum buffer size will now be 2^30, not 2^31-1. But thats not an issue at all. In my opinion, a buffer size of 2^31-1 is stupid in all cases, as it will no longer fit page boundaries and mmapping gets harder for the O/S.

We will provide a patch with those cleanups."
1,"RMI published Repository using the jcr-rmi library gets lost over timeThe jcr-server/webapp project contains a servlet - RepositoryStartupServlet - which may be used in a web app to start a repository and optionally register the repository with JNDI and RMI. To register the repository with JNDI, the jcr-rmi library is used to create a Remote repository instance, which is registered with the RMI registry. Inside the RMI implementation mechanisms based on stub classes created by the RMI compiler are created to make the remote repository available remotely. This includes creating a object table to map remote references to local objects. This table stores references to the local object as weak references to support distributed garbage collection.

Over time, it may now be that this remote repository instance is actually collected and the object table cannot access it anymore thus preventing the repository from being accessed remotely. To prevent this from happening, the RepositoryStartupServlet must keep a strong reference to the remote repository and drop this reference when the servlet is destroyed and the repository unregistered.

*NOTE:* This is an issue to all long running applications which publish repository instances over RMI using the jcr-rmi library."
0,"When A URL is redirected, there is no easy way to encode the new url before HC tries to execute it/When you implement your custom RedirectHandler, there is no easy way to encode the new URL being redirected to.

A public method to access the location string prior to the URI generation would be useful."
0,"Change security configuration from 'simple' to the some reasonable defaultI'd like to change the security configuration from 'simple' to a default that enables ac-management and user management (and consequently doesn't skip the corresponding tests).

there is currently an issue with WEAKREFERENCEs (see issue JCR-2135) that prevents me from changing the config.
unless someone objects i would like to change the default config as soon as JCR-2135 is solved.

"
0,"Make MultiThreadedHttpConnectionManager defaults public statics.Could the defaults for MultiThreadedHttpConnectionManager be made public
constants? I would do it my self since I have karma as a contributer to [lang]
and [codec] but I do not want to step on anyones toes. ;-)

Patch attached."
0,"Add top-down version of BlockJoinQueryToday, BlockJoinQuery can join from child docIDs up to parent docIDs.
EG this works well for product (parent) + many SKUs (child) search.

But the reverse, which BJQ cannot do, is also useful in some cases.
EG say you index songs (child) within albums (parent), but you want to
search and present by song not album while involving some fields from
the album in the query.  In this case you want to wrap a parent query
(against album), joining down to the child document space.
"
0,Versioning operations should be done on the workspacecurrently all versioning operations modify the transient states of the items where the operation is executed although all operations are workspace operations.
0,"contrib/xml-query-parser, BoostingTermQuery supportI'm not 100% on this patch. 

BooleanTermQuery is a part of the spans family, but I generally use that class as a replacement for TermQuery.  Thus in the DTD I have stated that it can be a part of the root queries as well as a part of a span. 

However, SpanFooQueries xml elements are named <SpanFoo/> rather than <SpanFooQuery/>, I have however chosen to call it <BoostingTermQuery/>. It would be possible to set it up so it would be parsed as <SpanBoostingTerm/> when inside of a <SpanSomething>, but I just find that confusing.
"
0,"SPI: improve description of locking methods on RepositoryServicein detail:

1) getLockInfo

- intended behavior if no lock is present?
- intended behavior if locking is not supported?

2) lock

- currently InvalidItemStateException is listed. i don't think this make too much sense.

3) refreshLock

- intended behavior if locking is not supported?

4) unlock

- currently InvalidItemStateException is listed. i don't think this make too much sense."
1,"jcr:baseVersion is not updated when the base version is removed from the version history
        Session s1 = repo.login(new SimpleCredentials(""user1"", ""pwd1"".toCharArray()));
        Node root1 = s1.getRootNode() ;
        Node test1 = root1.addNode(""test"") ;
        test1.addMixin(""mix:versionable"");
        s1.save() ;
        System.out.println(test1.getProperty(""jcr:baseVersion"").getValue().getString()) ;
        test1.checkin() ;
        System.out.println(test1.getProperty(""jcr:baseVersion"").getValue().getString()) ;
        test1.getVersionHistory().removeVersion(""1.0"") ;
        // the base version wasn't updated :(
        System.out.println(test1.getProperty(""jcr:baseVersion"").getValue().getString()) ;
        // the next line throws ItemNotFoundException :(
        test1.getBaseVersion() ;

javax.jcr.ItemNotFoundException: c33bf049-c7e1-4b34-968a-63ff1b1113b0
	at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:498)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:349)
	at org.apache.jackrabbit.core.PropertyImpl.getNode(PropertyImpl.java:642)
	at org.apache.jackrabbit.core.NodeImpl.getBaseVersion(NodeImpl.java:2960)
	at org.apache.jackrabbit.core.RemoveVersionTest.main(RemoveVersionTest.java:56)


"
1,"cache does not honor must-revalidate or proxy-revalidate Cache-Control directiveshttp://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.4

There are a couple of missed requirements here regarding must-revalidate and proxy-revalidate (which applies only to shared caches).
1. When a cache entry with this directive is revalidated, it must be an end-to-end revalidation (meaning it must include 'max-age=0' on the request).
2. If the revalidation with the origin fails, the cache MUST NOT return a stale entry and MUST return a 504 response.
"
0,"Adding same IndexDocValuesField twice trips assertDoc values fields are single-valued by design, ie a given field name can only occur once in the document.

But if you accidentally add it more than once, you get an assert error, which is spooky because if you run w/o asserts maybe something eviler happens.

I think we should explicitly check for this and throw clear exc since user could easily do this by accident."
1,"NPE in RequestProxyAuthentication on AndroidGot a NPE backtrace in RequestProxyAuthentication.process(). 

HttpRoute route = conn.getRoute();
        if (route.isTunnelled()) {      <= line 88, NPE here
            return;
        }

There's no null check on the returned route although getRoute() can return null.
I guess it's not supposed to happen.

In the httpclient code, there's a few more calls to getRoute() without a null check on the returned route.


java.lang.NullPointerException
at com.bubblesoft.org.apache.http.client.protocol.RequestProxyAuthentication.process(SourceFile:88)
at com.bubblesoft.org.apache.http.protocol.ImmutableHttpProcessor.process(SourceFile:108)
at com.bubblesoft.org.apache.http.protocol.HttpRequestExecutor.preProcess(SourceFile:174)
at com.bubblesoft.org.apache.http.impl.client.DefaultRequestDirector.execute(SourceFile:457)
at com.bubblesoft.org.apache.http.impl.client.AbstractHttpClient.createHttpProcessor(SourceFile:821)
                                                                 execute
at com.bubblesoft.org.apache.http.impl.client.AbstractHttpClient.createHttpProcessor(SourceFile:755)
                                                                 execute
at com.bubblesoft.org.apache.http.impl.client.AbstractHttpClient.createHttpProcessor(SourceFile:733)
                                                                 execute
"
0,Promote the classloader component from contribThis is a dummy issue for a change I already made (revisions 529068 and 529137) to promote the classloader component from contrib. I'm including this here in the issue tracker to complete the release notes for Jackrabbit 1.3.
1,"VarDerefBytesImpl doc values prefix length may fall across two pagesThe VarDerefBytesImpl doc values encodes the unique byte[] with prefix (1 or 2 bytes) first, followed by bytes, so that it can use PagedBytes.fillSliceWithPrefix.

It does this itself rather than using PagedBytes.copyUsingLengthPrefix...

The problem is, it can write an invalid 2 byte prefix spanning two blocks (ie, last byte of block N and first byte of block N+1), which fillSliceWithPrefix won't decode correctly.

"
0,"Checksum Wrong for HttpComponent project pom v4.1 on centralAs evidenced on the log here: http://vmgump.apache.org/gump/public/httpcomponents/httpcomponents-core/gump_work/build_httpcomponents_httpcomponents-core.html

The checksum in central for httpcomponents-project-4.1.pom is incorrect in maven central.

---------------------------8<--------------------------------

Downloading: http://localhost:8192/maven2/org/apache/httpcomponents/project/4.1/project-4.1.pom

[WARNING] *** CHECKSUM FAILED - Checksum failed on download: local = 'b63ff67e6ffc1940041319e0e06d7c6b1d671fd2'; remote = '8edff11652ca51b9d110ebfb321daac24f031c07' - RETRYING
Downloading: http://localhost:8192/maven2/org/apache/httpcomponents/project/4.1/project-4.1.pom

[WARNING] *** CHECKSUM FAILED - Checksum failed on download: local = 'b63ff67e6ffc1940041319e0e06d7c6b1d671fd2'; remote = '8edff11652ca51b9d110ebfb321daac24f031c07' - IGNORING
This pom appears to be a dependency for httpcomponents 4.0.3

---------------------------8<--------------------------------

This checksum failure causes configurations that reject such artifacts (such as many maven proxy configurations) to result in build failures due to unsatisfied dependencies. 

"
1,"Clustering is broken due to duplicated CachingPersistenceManager interfaceThere are now two interfaces CachingPersistenceManager in the packages org.apache.jackrabbit.core.persistence.bundle and org.apache.jackrabbit.core.persistence.pool. A persistence manager that implements the ..bundle... interface doesn't receive the onExternalUpdate events that are required for clustering to work.

I will move this interface to the package org.apache.jackrabbit.core.persistence and remove the second implementation.

This change has no affect to backward compatibility, because anyway there were many breaking changes in the past (NodeId / UUID for example).
"
0,[API Doc] HttpClient tutorial updateBring the tutorial up to date with the latest best practices
0,"WriteLineDocTask improvementsMake WriteLineDocTask and LineDocSource more flexible/extendable:
* allow to emit lines also for empty docs (keep current behavior as default)
* allow more/less/other fields"
0,Reduce number of compiler warning by adding @Override and generics where appropriate Add @Override and generics where possible to reduce the number of warnings issued by the compiler.
1,"sysview export/import of multivalue properties seems not to workthe sysview export of multivalue properties does not differ from the export of singlevalue properties.

if a mv-property contains only 1 value, how can the import find the correct property-def?

currently, it throws: 
  javax.jcr.nodetype.ConstraintViolationException: no matching property definition found for <propertyname>"
0,"in advance(), don't try to skip if there is evidence it will failThere are TODO's about this in the code everywhere, and this was part of
Mike speeding up ExactPhraseScorer.

I think the codec should do this."
0,AbstractLockTest.testLockExpiration fails intermittently This seems to be a timing issue. I propose to wait a bit longer for the lock to expire. 
0,"Fine grained locking in SharedItemStateManagerThe SharedItemStateManager (SISM) currently uses a simple read-write lock to ensure data consistency. Store operations to the PersistenceManager (PM) are effectively serialized.

We should think about more sophisticated locking to allow concurrent writes on the PM.

One possible approach:

If a transaction is currently storing data in a PM a second transaction may check if the set of changes does not intersect with the first transaction. If that is the case it can safely store its data in the PM.

This fine grained locking must also be respected when reading from the SISM. A read request for an item that is currently being stored must be blocked until the store is finished."
1,"SO_TIMEOUT is not set on a request levelThe scenario is as follows: I'm doing two consecutive requests to the same host, using a multi-threaded (or thread safe) connection pool manager. The first invocation has a timeout of 10s and the second has a timeout of 30s. 

In version 3.1 of HttpClient all works well, but in 4.0 I get a timeout exception in the second request, after ~10 seconds, which means the first timeout is used.

Looking at the code, I see that in version 3.1, the HttpMethodDirector.executeWithRetry() method invokes a method named applyConnectionParams() that took care of setting the timeout taken from the request on the socket. 

But in version 4.0, the only place I see the timeout is set on the socket is when DefaultRequestDirector.execute(HttpHost, HttpRequest, HttpContext) opens a connection using the managedConn.open() method. Since the connection is reused between the requests, the second request uses a socket with a timeout of the first request.
"
0,Add a Lucene3x private SegmentInfosFormat implemenationwe still don't have a Lucene3x & preflex version of segment infos format. we need this before we release 4.0
1,"StaleItemStateException with distributed transactionsThere seams to be a serious bug in jackrabbit when used in distributed transactions. It does not occur with local transactions! And it seams to be related to JCR-566.

There are 2 scenarios where a StaleItemStateException occurs reproducible that causes transactions to fail. All my operations (implemented in a custom ServiceBean) such as setProperty() or deleteNode() run in separate transactions. The transactions are configured through Spring Annotations (@Transactional).

Scenario A (setProperty):
(1) multiple setProperty() with same property name on the same node (newly created or already existent)
=> With the 3. setProperty() (and sometimes also the 5.), a StaleItemStateException for the property state is raised when the transaction is commited. Following setProperty invocations will not fail!

Scenario B (deleteNode):
(1) iterate 10 times:
(1.1) create new node n and a subnode for n
(1.2) delete node n
=> Deletion of node n raises a StaleItemStateException for node n in iteration 1, 3 and (6 or 7), when the related transaction is commited. Following deletions of node n will also fail with a predictable pattern.

The Exception trace for scenario A (it's the same for scenario B, with one difference: StaleItemStateException is raised for the node and not for the property):

org.springframework.transaction.UnexpectedRollbackException: JTA transaction unexpectedly rolled back (maybe due to a timeout); nested exception is javax.transaction.RollbackException: Error during one-phase commit
	at org.springframework.transaction.jta.JtaTransactionManager.doCommit(JtaTransactionManager.java:1031)
	at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:709)
	at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:678)
	at org.springframework.transaction.interceptor.TransactionAspectSupport.commitTransactionAfterReturning(TransactionAspectSupport.java:321)
	at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:116)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)
	at $Proxy9.setNodeProperty(Unknown Source)
	at de.zeb.control.prototype.jrTxBug.test.TestJackrabbitTxBug.testTransactionBug001(TestJackrabbitTxBug.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.testng.internal.MethodHelper.invokeMethod(MethodHelper.java:580)
	at org.testng.internal.Invoker.invokeMethod(Invoker.java:478)
	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:607)
	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:874)
	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:125)
	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109)
	at org.testng.TestRunner.runWorkers(TestRunner.java:689)
	at org.testng.TestRunner.privateRun(TestRunner.java:566)
	at org.testng.TestRunner.run(TestRunner.java:466)
	at org.testng.SuiteRunner.runTest(SuiteRunner.java:301)
	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:296)
	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:276)
	at org.testng.SuiteRunner.run(SuiteRunner.java:191)
	at org.testng.TestNG.createAndRunSuiteRunners(TestNG.java:808)
	at org.testng.TestNG.runSuitesLocally(TestNG.java:776)
	at org.testng.TestNG.run(TestNG.java:701)
	at org.testng.remote.RemoteTestNG.run(RemoteTestNG.java:73)
	at org.testng.remote.RemoteTestNG.main(RemoteTestNG.java:124)
Caused by: javax.transaction.RollbackException: Error during one-phase commit
	at org.apache.geronimo.transaction.manager.TransactionImpl.commit(TransactionImpl.java:281)
	at org.apache.geronimo.transaction.manager.TransactionManagerImpl.commit(TransactionManagerImpl.java:143)
	at org.apache.geronimo.transaction.context.InheritableTransactionContext.complete(InheritableTransactionContext.java:196)
	at org.apache.geronimo.transaction.context.InheritableTransactionContext.commit(InheritableTransactionContext.java:146)
	at org.apache.geronimo.transaction.context.OnlineUserTransaction.commit(OnlineUserTransaction.java:80)
	at org.jencks.factory.UserTransactionFactoryBean$GeronimoUserTransaction.commit(UserTransactionFactoryBean.java:118)
	at org.springframework.transaction.jta.JtaTransactionManager.doCommit(JtaTransactionManager.java:1028)
	... 30 more
Caused by: javax.transaction.xa.XAException
	at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:155)
	at org.apache.jackrabbit.core.XASessionImpl.commit(XASessionImpl.java:337)
	at org.apache.jackrabbit.jca.TransactionBoundXAResource.commit(TransactionBoundXAResource.java:39)
	at org.apache.geronimo.transaction.manager.WrapperNamedXAResource.commit(WrapperNamedXAResource.java:47)
	at org.apache.geronimo.transaction.manager.TransactionImpl.commit(TransactionImpl.java:272)
	... 36 more
Caused by: org.apache.jackrabbit.core.TransactionException: Unable to prepare transaction.
	at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:150)
	at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:138)
	... 40 more
Caused by: org.apache.jackrabbit.core.state.StaleItemStateException: bef3c056-bc91-4195-a35c-aa184182b5ad/{}TEST_PROPERTY has been modified externally
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:620)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:843)
	at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:144)
	... 41 more


When debugging into jackrabbit you will see, that the cause of the StaleItemStateException is, that the local state und the overlayed state differ in the value of the 'modCount' attribute: modCount of local state is lower than modCount of overlayed state. Perhaps its a state caching problem...
	
I'm attaching a simple java application configured with maven and ready to run standalone. The JCA container of JBoss is therefore replaced with jencks in order to support distributed transactions. The configured repository uses the InMemPersistenceManager. Both scenarios are implemented in a TestNG - test, that catches the occuring TransactionExceptions and prints out the stacktrace. Therefore you will see the exceptions, but the tests will not fail."
0,"Update commons-io dependency from versiom 1.4 to 2.0.1 Jackrabbit may be used as JackRabbit-JCA. In many application servers we need to change the classloading policy to ""global"" or ""flat"". These classloading policies result in many conflicts between versions of own server implementation or version of applications. It would be diminished if Jackrabbit dependences are keep ""up-to-date"".

<dependency>
	<groupId>commons-io</groupId>
	<artifactId>commons-io</artifactId>
	<version>2.0.1</version>
</dependency>"
0,"Enable FileSystem unit testsThe FileSystem tests are implemented, but not actually run, because the TestAll class is missing.
Also, there is a bug in the tests that causes the tests to fail."
1,BitVector never skips fully populated bytes when writing ClearedDgapsWhen writing cleared DGaps in BitVector we compare a byte against 0xFF (255) yet the byte is casted into an int (-1) and the comparison will never succeed. We should mask the byte with 0xFF before comparing or compare against -1
1,"invalid groupid for tm-extractors in textfilters projectgroupid for tm-extractors should be ""org.textmining"" and not ""textmining"".
The dependency with the correct groupid is available on ibiblio"
0,"improve how MTQs interact with the terms dict cacheSome small improvements:

  * Adds a TermsEnum.cacheCurrentTerm ""hint"" (codec can make this a no-op)

  * Removes the FTE.useTermsCache

  * Changes MTQ's TermCollector API to accept the TermsEnum so collectors can eg call .docFreq directly

  * Adds expert ctor to TermQuery allowing you to pass in the docFreq"
0,"RAMDirectory.close() should have a comment about not releasing any resourcesI wrongly assumed that calling RAMDirectory.close() would free up the memory occupied by the RAMDirectory.
It might be helpful to add a javadoc comment that warns users that RAMDirectory.close() is a no-op, since it might be a common assumption that close() would release resources."
0,"Bad transitive dependencies in commons-httpclientAs reported in HTTPCLIENT-605, the commons-httpclient 3.0 dependency introduces junit as a transitive ""compile"" scope dependency. The library also uses commons-logging, which sidesteps Jackrabbit's use of slf4j for logging.

To avoid these issues we should locally override the junit dependency in commons-httpclient and replace the commons-logging dependency with jcl104-over-slf4j."
0,"Tests failing when run with tests.iter > 1TestMultiLevelSkipList and TestsFieldReader are falling if run with -Dtests.iter > 1 - not all values are reset though
I will attach a patch in a second."
0,"Command line access to remote repositoriesA few years ago Edgar Poce implemented a nice command line JCR access tool called jcr-commands. We haven't really been using it much and the code is currently parked in sandbox/inactive.

I'd like to resurrect this codebase and integrate it to jackrabbit-standalone to implement command line access to remote repositories. The idea would be to have an easy-to-use tool for simple testing and administration tasks.
"
1,"DocViewSaxEventGenerator may generate non-NS-wellformed XMLThe XML serialization code relies on the fact that all required prefix-to-uri mappings are known beforehand (actually, when serializing the root node). So there's an assumption that the permanent namespace registry will never change during serialization, which may be incorrect when another client adds namespace registrations while the XML export is in progress.

To fix this, ""addNamespacePrefixes"" should ensure that namespace declarations have been written for all prefixes used on the current node (node name + properties), potentially going back to the namespace resolver when needed.

(Should there be consensus for that change I'm happy to give it a try)"
1,"Proxy NTLM Authentication  Redirecting to different address fails saying Proxy Auth Required.The issue has been discussed in,
http://www.nabble.com/redirect-fails-when-NTLM-authentication-is-used-for-proxy-tt23867531.html

This was found in http client 3.1 release,  where NTLM proxy authentication is must and the server ask the redirect to a new url, in this case, when redirecting, the earlier proxy auth status is not cleared, so, it does not do proxy authentication for the new URL and hence fails.

Target Host Authenticaiton NTLM authentication - redirect also had problem and fixed as said,
http://issues.apache.org/jira/browse/HTTPCLIENT-211
Proxy Authentication - redirect has to be fixed, 

The wire logs for the release https://repository.apache.org/content/repositories/snapshots/org/apache/httpcomponents/httpclient/4.0-beta3-SNAPSHOT/
is given below,

[DEBUG] wire - >> ""GET http://verisign.com HTTP/1.1[EOL]""
[DEBUG] wire - >> ""Host: verisign.com[EOL]""
[DEBUG] wire - >> ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - >> ""User-Agent: Apache-HttpClient/UNAVAILABLE (java 1.5)[EOL]""
[DEBUG] wire - >> ""[EOL]""
[DEBUG] wire - << ""HTTP/1.1 407 Proxy Authentication Required ( The ISA Server requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )[EOL]""
[DEBUG] wire - << ""Via: 1.1 lab1[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: Negotiate[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: Kerberos[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: NTLM[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: Basic realm=""lab1.""[EOL]""
[DEBUG] wire - << ""Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Pragma: no-cache[EOL]""
[DEBUG] wire - << ""Cache-Control: no-cache[EOL]""
[DEBUG] wire - << ""Content-Type: text/html[EOL]""
[DEBUG] wire - << ""Content-Length: 4107  [EOL]""
[DEBUG] wire - << ""[EOL]""
[DEBUG] wire - << ""<!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 4.0 Transitional//EN"">[\r][\n]""
[DEBUG] wire - << ""<HTML><HEAD><TITLE>Error Message</TITLE>[\r][\n]""
[DEBUG] wire - << ""<META http-equiv=Content-Type content=""text/html; charset=UTF-8"">[\r][\n]""
[DEBUG] wire - << ""<STYLE id=L_default_1>A {[\r][\n]""
[DEBUG] wire - << ""[0x9]FONT-WEIGHT: bold; FONT-SIZE: 10pt; COLOR: #005a80; FONT-FAMILY: tahoma[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""A:hover {[\r][\n]""
[DEBUG] wire - << ""[0x9]FONT-WEIGHT: bold; FONT-SIZE: 10pt; COLOR: #0d3372; FONT-FAMILY: tahoma[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""TD {[\r][\n]""
[DEBUG] wire - << ""[0x9]FONT-SIZE: 8pt; FONT-FAMILY: tahoma[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""TD.titleBorder {[\r][\n]""
[DEBUG] wire - << ""[0x9]BORDER-RIGHT: #955319 1px solid; BORDER-TOP: #955319 1px solid; PADDING-LEFT: 8px; FONT-WEIGHT: bold; FONT-SIZE: 12pt; VERTICAL-ALIGN: middle; BORDER-LEFT: #955319 0px solid; COLOR: #955319; BORDER-BOTTOM: #955319 1px solid; FONT-FAMILY: tahoma; HEIGHT: 35px; BACKGROUND-COLOR: #d2b87a; TEXT-ALIGN: left[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""TD.titleBorder_x {[\r][\n]""
[DEBUG] wire - << ""[0x9]BORDER-RIGHT: #955319 0px solid; BORDER-TOP: #955319 1px solid; PADDING-LEFT: 8px; FONT-WEIGHT: bold; FONT-SIZE: 12pt; VERTICAL-ALIGN: middle; BORDER-LEFT: #955319 1px solid; COLOR: #978c79; BORDER-BOTTOM: #955319 1px solid; FONT-FAMILY: tahoma; HEIGHT: 35px; BACKGROUND-COLOR: #d2b87a; TEXT-ALIGN: left[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << "".TitleDescription {[\r][\n]""
[DEBUG] wire - << ""[0x9]FONT-WEIGHT: bold; FONT-SIZE: 12pt; COLOR: black; FONT-FAMILY: tahoma[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""SPAN.explain {[\r][\n]""
[DEBUG] wire - << ""[0x9]FONT-WEIGHT: normal; FONT-SIZE: 10pt; COLOR: #934225[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""SPAN.TryThings {[\r][\n]""
[DEBUG] wire - << ""[0x9]FONT-WEIGHT: normal; FONT-SIZE: 10pt; COLOR: #934225[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << "".TryList {[\r][\n]""
[DEBUG] wire - << ""[0x9]MARGIN-TOP: 5px; FONT-WEIGHT: normal; FONT-SIZE: 8pt; COLOR: black; FONT-FAMILY: tahoma[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << "".X {[\r][\n]""
[DEBUG] wire - << ""[0x9]BORDER-RIGHT: #955319 1px solid; BORDER-TOP: #955319 1px solid; FONT-WEIGHT: normal; FONT-SIZE: 12pt; BORDER-LEFT: #955319 1px solid; COLOR: #7b3807; BORDER-BOTTOM: #955319 1px solid; FONT-FAMILY: verdana; BACKGROUND-COLOR: #d1c2b4[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << "".adminList {[\r][\n]""
[DEBUG] wire - << ""[0x9]MARGIN-TOP: 2px[\r][\n]""
[DEBUG] wire - << ""}[\r][\n]""
[DEBUG] wire - << ""</STYLE>[\r][\n]""
[DEBUG] wire - << ""<META content=""MSHTML 6.00.2800.1170"" name=GENERATOR></HEAD>[\r][\n]""
[DEBUG] wire - << ""<BODY bgColor=#f3f3ed>[\r][\n]""
[DEBUG] wire - << ""<TABLE cellSpacing=0 cellPadding=0 width=""100%"">[\r][\n]""
[DEBUG] wire - << ""  <TBODY>[\r][\n]""
[DEBUG] wire - << ""  <TR>[\r][\n]""
[DEBUG] wire - << ""    <TD class=titleborder_x width=30>[\r][\n]""
[DEBUG] wire - << ""      <TABLE height=25 cellSpacing=2 cellPadding=0 width=25 bgColor=black>[\r][\n]""
[DEBUG] wire - << ""        <TBODY>[\r][\n]""
[DEBUG] wire - << ""        <TR>[\r][\n]""
[DEBUG] wire - << ""          <TD class=x vAlign=center alig""
[DEBUG] wire - << ""n=middle>X</TD>[\r][\n]""
[DEBUG] wire - << ""        </TR>[\r][\n]""
[DEBUG] wire - << ""        </TBODY>[\r][\n]""
[DEBUG] wire - << ""      </TABLE>[\r][\n]""
[DEBUG] wire - << ""    </TD>[\r][\n]""
[DEBUG] wire - << ""    <TD class=titleBorder id=L_default_2>Network Access Message:<SPAN class=TitleDescription> The page cannot be displayed</SPAN> </TD>[\r][\n]""
[DEBUG] wire - << ""  </TR>[\r][\n]""
[DEBUG] wire - << ""  </TBODY>[\r][\n]""
[DEBUG] wire - << ""</TABLE>[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - << ""<TABLE id=spacer>[\r][\n]""
[DEBUG] wire - << ""  <TBODY>[\r][\n]""
[DEBUG] wire - << ""  <TR>[\r][\n]""
[DEBUG] wire - << ""    <TD height=10></TD></TR></TBODY></TABLE>[\r][\n]""
[DEBUG] wire - << ""<TABLE width=400>[\r][\n]""
[DEBUG] wire - << ""  <TBODY>[\r][\n]""
[DEBUG] wire - << ""  <TR>[\r][\n]""
[DEBUG] wire - << ""    <TD noWrap width=25></TD>[\r][\n]""
[DEBUG] wire - << ""    <TD width=400><SPAN class=explain><ID id=L_default_3><B>Explanation:</B></ID></SPAN><ID id=L_default_4> There is a problem with the page you are trying to reach and it cannot be displayed. </ID><BR><BR>[\r][\n]""
[DEBUG] wire - << ""    <B><SPAN class=tryThings><ID id=L_default_5><B>Try the following:</B></ID></SPAN></B> [\r][\n]""
[DEBUG] wire - << ""      <UL class=TryList>[\r][\n]""
[DEBUG] wire - << ""        <LI id=L_default_6><B>Refresh page:</B> Search for the page again by clicking the Refresh button. The timeout may have occurred due to Internet congestion.[\r][\n]""
[DEBUG] wire - << ""<LI id=L_default_7><B>Check spelling:</B> Check that you typed the Web page address correctly. The address may have been mistyped.[\r][\n]""
[DEBUG] wire - << ""<LI id=L_default_8><B>Access from a link:</B> If there is a link to the page you are looking for, try accessing the page from that link.[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - << ""      </UL>[\r][\n]""
[DEBUG] wire - << ""<ID id=L_default_9>If you are still not able to view the requested page, try contacting your administrator or Helpdesk.</ID> <BR><BR>[\r][\n]""
[DEBUG] wire - << ""    </TD>[\r][\n]""
[DEBUG] wire - << ""  </TR>[\r][\n]""
[DEBUG] wire - << ""  </TBODY>[\r][\n]""
[DEBUG] wire - << ""</TABLE>[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - << ""<TABLE id=spacer><TBODY><TR><TD height=15></TD></TR></TBODY></TABLE>[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - << ""<TABLE width=400>[\r][\n]""
[DEBUG] wire - << ""  <TBODY>[\r][\n]""
[DEBUG] wire - << ""  <TR>[\r][\n]""
[DEBUG] wire - << ""    <TD noWrap width=25></TD>[\r][\n]""
[DEBUG] wire - << ""    <TD width=400 id=L_default_10><B>Technical Information (for support personnel)</B> [\r][\n]""
[DEBUG] wire - << ""      <UL class=adminList>[\r][\n]""
[DEBUG] wire - << ""        <LI id=L_default_11>Error Code: 407 Proxy Authentication Required. The ISA Server requires authorization to fulfill the request. Access to the Web Proxy filter is denied. (12209)[\r][\n]""
[DEBUG] wire - << ""<LI id=L_default_12>IP Address: x.x.x.x[\r][\n]""
[DEBUG] wire - << ""<LI id=L_default_13>Date: 6/29/2009 11:15:15 AM [GMT][\r][\n]""
[DEBUG] wire - << ""<LI id=L_default_14>Server: lab1[\r][\n]""
[DEBUG] wire - << ""<LI id=L_default_15>Source: proxy[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - << ""      </UL>[\r][\n]""
[DEBUG] wire - << ""    </TD>[\r][\n]""
[DEBUG] wire - << ""  </TR>[\r][\n]""
[DEBUG] wire - << ""  </TBODY>[\r][\n]""
[DEBUG] wire - << ""</TABLE>[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - << ""</BODY>[\r][\n]""
[DEBUG] wire - << ""</HTML>[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] wire - >> ""GET http://verisign.com HTTP/1.1[EOL]""
[DEBUG] wire - >> ""Host: verisign.com[EOL]""
[DEBUG] wire - >> ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - >> ""User-Agent: Apache-HttpClient/UNAVAILABLE (java 1.5)[EOL]""
[DEBUG] wire - >> ""Proxy-Authorization: NTLM TlRMTVNTUAABAAAAATIAAAgACAAgAAAADgAOACgAAABNWURPTUFJTkpDSUZTMjMwXzg2Xzkx[EOL]""
[DEBUG] wire - >> ""[EOL]""
[DEBUG] wire - << ""HTTP/1.1 407 Proxy Authentication Required ( Access is denied.  )[EOL]""
[DEBUG] wire - << ""Via: 1.1 lab1[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: NTLM TlRMTVNTUAACAAAAAAAAADgAAAABAgACqbXrIWnZ3i4AAAAAAAAAAAAAAAA4AAAABQLODgAAAA8=[EOL]""
[DEBUG] wire - << ""Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Pragma: no-cache[EOL]""
[DEBUG] wire - << ""Cache-Control: no-cache[EOL]""
[DEBUG] wire - << ""Content-Type: text/html[EOL]""
[DEBUG] wire - << ""Content-Length: 0     [EOL]""
[DEBUG] wire - << ""[EOL]""
[DEBUG] wire - >> ""GET http://verisign.com HTTP/1.1[EOL]""
[DEBUG] wire - >> ""Host: verisign.com[EOL]""
[DEBUG] wire - >> ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - >> ""User-Agent: Apache-HttpClient/UNAVAILABLE (java 1.5)[EOL]""
[DEBUG] wire - >> ""Proxy-Authorization: NTLM TlRMTVNTUAADAAAAGAAYAEAAAAAwADAAWAAAABAAEACIAAAAGgAaAJgAAAAcABwAsgAAAAAAAAAAAAAAAQIAAAXLpW40q7jqh7E6FgFnJqy9529ANaSLqfTiwjyF2BrUP9F8ObYOyYsBAQAAAAAAACDgxRg9+skBRt4mUOFFCs0AAAAAAAAAAE0AWQBEAE8ATQBBAEkATgBBAGQAbQBpAG4AaQBzAHQAcgBhAHQAbwByAEoAQwBJAEYAUwAyADMAMABfADgANgBfADkAMQA=[EOL]""
[DEBUG] wire - >> ""[EOL]""
[DEBUG] wire - << ""HTTP/1.1 301 Unknown reason[EOL]""
[DEBUG] wire - << ""Via: 1.1 lab1[EOL]""
[DEBUG] wire - << ""Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Content-length: 0[EOL]""
[DEBUG] wire - << ""Date: Mon, 29 Jun 2009 11:16:50 GMT[EOL]""
[DEBUG] wire - << ""Location: http://www.verisign.com/[EOL]""
[DEBUG] wire - << ""Content-type: text/html[EOL]""
[DEBUG] wire - << ""Server: Netscape-Enterprise/4.1[EOL]""
[DEBUG] wire - << ""[EOL]""
[ERROR] RequestProxyAuthentication - Proxy authentication error: Unexpected state: MSG_TYPE3_GENERATED
[DEBUG] wire - >> ""GET http://www.verisign.com/ HTTP/1.1[EOL]""
[DEBUG] wire - >> ""Host: www.verisign.com[EOL]""
[DEBUG] wire - >> ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - >> ""User-Agent: Apache-HttpClient/UNAVAILABLE (java 1.5)[EOL]""
[DEBUG] wire - >> ""[EOL]""
[DEBUG] wire - << ""HTTP/1.1 407 Proxy Authentication Required ( The ISA Server requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )[EOL]""
[DEBUG] wire - << ""Via: 1.1 lab1[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: Negotiate[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: Kerberos[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: NTLM[EOL]""
[DEBUG] wire - << ""Proxy-Authenticate: Basic realm=""lab1.""[EOL]""
[DEBUG] wire - << ""Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Proxy-Connection: Keep-Alive[EOL]""
[DEBUG] wire - << ""Pragma: no-cache[EOL]""
[DEBUG] wire - << ""Cache-Control: no-cache[EOL]""
[DEBUG] wire - << ""Content-Type: text/html[EOL]""
[DEBUG] wire - << ""Content-Length: 4107  [EOL]""
[DEBUG] wire - << ""[EOL]""
----------------------------------------
HTTP/1.1 407 Proxy Authentication Required ( The ISA Server requires authorization to fulfill the request. Access to the Web Proxy filter is denied.  )

Thanks,
Raj





"
1,"Impossible comparison in NodeTypeImplorg.apache.jackrabbit.jcr2spi.nodetype.NodeTypeImpl does

    public boolean isNodeType(Name nodeTypeName) {
        return getName().equals(nodeTypeName) ||  ent.includesNodeType(nodeTypeName);
    }


as getName() is a string and nodeTypeName is a Name this will always be false. Perhaps you meant

    public boolean isNodeType(Name nodeTypeName) {
        return getName().equals(nodeTypeName.getLocalName()) ||  ent.includesNodeType(nodeTypeName);
    }

"
0,Introduce QNodeTypeDefinition cache per userIdOnce read from the server a QNodeTypeDefinition can be cached and shared across SessionImpl with the same userId.
1,"FNFE hit when creating an empty index and infoStream is onShai just reported this on the dev list.  Simple test:
{code}
Directory dir = new RAMDirectory();
IndexWriter writer = new IndexWriter(dir, new SimpleAnalyzer(), MaxFieldLength.UNLIMITED);
writer.setInfoStream(System.out);
writer.addDocument(new Document());
writer.commit();
writer.close();
{code}

hits this:

{code}
Exception in thread ""main"" java.io.FileNotFoundException: _0.prx
    at org.apache.lucene.store.RAMDirectory.fileLength(RAMDirectory.java:149)
    at org.apache.lucene.index.DocumentsWriter.segmentSize(DocumentsWriter.java:1150)
    at org.apache.lucene.index.DocumentsWriter.flush(DocumentsWriter.java:587)
    at org.apache.lucene.index.IndexWriter.doFlushInternal(IndexWriter.java:3572)
    at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:3483)
    at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:3474)
    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1940)
    at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1894)
{code}

Turns out it's just silly -- this is actually an issue I've already fixed on the flex (LUCENE-1458) branch.  DocumentsWriter has its own method to enumerate the flushed files and compute their size, but really it shouldn't do that -- it should use SegmentInfo's method, instead."
1,Aggregate include ignored if no primaryType setIf the include element of an aggregate definition does not have a primaryType attribute then the include is never matched.
1,"Proxy authentication error: Unexpected state: MSG_TYPE3_GENERATED when using NTLM authenticationTrying to connect to a website that requires basic authentication through a proxy that requires NTLM authentication.

Proxy authentication fails with ""Proxy authentication error: Unexpected state: MSG_TYPE3_GENERATED"".

Full wire log attached.  Code to replicate problem follows:

    private void execute() throws HttpException, IOException {
    	
    	URL targetUrl = new URL(TARGET_URL);
    	
        DefaultHttpClient httpclient = new DefaultHttpClient();

        HttpHost targetHost = new HttpHost(targetUrl.getHost()); 
        HttpHost proxyHost = new HttpHost(PROXY_HOST, PROXY_PORT); 
        
        httpclient.getParams().setParameter(ConnRoutePNames.DEFAULT_PROXY, 
        		proxyHost);

        CredentialsProvider credProvider = httpclient.getCredentialsProvider();
        
        Credentials proxyCredentials = new NTCredentials(PROXY_USER, 
        		PROXY_PASSWORD, PROXY_MACHINE, PROXY_DOMAIN);
        AuthScope proxyAuthScope = new AuthScope(proxyHost.getHostName(),
        		proxyHost.getPort());
        
        credProvider.setCredentials(proxyAuthScope, proxyCredentials);
        
        Credentials targetCredentials = new UsernamePasswordCredentials(
        		TARGET_USER, TARGET_PASSWORD);
        AuthScope targetAuthScope = new AuthScope(targetHost.getHostName(),
        		targetHost.getPort());
        
        credProvider.setCredentials(targetAuthScope, targetCredentials);
      
        HttpGet httpget = new HttpGet(targetUrl.getPath());

        HttpResponse response = httpclient.execute(targetHost, httpget);
        
        System.out.println(""response = "" + response);
        
       
    }
"
0,"RepositoryConfig instance can not be reused once it has been passed to RepositoryImpl constructorRepositoryConfig and other *Config classes maintain state apart from parsed configuration information;
specifically they instantiate FileSystem implementations based on their configurations. this makes it
for the config consumers very hard to control the lifecycle of such FileSystem instances as they need
to close the file systems on repository shutdown.

the following code illustrates the issue:

RepositoryConfig repConf = RepositoryConfig.create(configFile, repHomeDir);
RepositoryImpl rep = RepositoryImpl.create(repConf);
// ...
rep.shutdown();

rep = RepositoryImpl.create(repConf);   
// ==> repConfig (et al) contains references to FileSystem objects 
// that have been closed by previous rep.shutdown() call

"
0,NodeImpl.checkin() calls save() three timesThe version related properties on a versionable node that is checked in are saved individually. There is no need to save them individually because such a node must not have pending changes and save() can be called safely on the node itself.
0,"Add method to remove mappings from NamespaceMappingo.a.j.spi.commons.namespace.NamespaceMapping has currently no means to remove a mapping. I suggest to add a method
public String removeMapping(String uri) "
0,"Node.addNode() does not scale with increasing contentWith increasing repository content (and versions), the time to create new nodes increases. For example with around 6500 nodes and 33500 properties, it takes around 3 seconds (!) to just add one single node !

When attaching to the application with a Debugger and delibaretly suspending the VM, this stack trace is displayed all the times :

   [ changing internals of access List iterator ]
   PersistentNodeState(NodeState).getChildNodeEntries(String) line: 362
   PersistentNode.getName() line: 84
   PersistentVersionManager.getVersion(String) line: 278
   VersionManager.getVersion(String) line: 304
   VersionItemStateProvider.getNodeState(NodeId) line: 124
   VersionItemStateProvider.hasPropertyState(PropertyId) line: 154
   VersionItemStateProvider.hasItemState(ItemId) line: 174
   SessionItemStateManager.getItemState(ItemId) line: 246
   ItemManager.createItemInstance(ItemId) line: 563
   ItemManager.getItem(ItemId) line: 332
   NodeImpl.getProperty(QName) line: 876
   NodeImpl.hasProperty(QName) line: 893
   NodeImpl.safeIsCheckedOut() line: 2515
   NodeImpl.internalAddChildNode(QName, NodeTypeImpl, String) line: 527
   NodeImpl.internalAddNode(String, NodeTypeImpl, String) line: 475
   NodeImpl.internalAddNode(String, NodeTypeImpl) line: 436
   NodeImpl.addNode(String, String) line: 1145
   ...

It seems, that virtual item state providers are asked for whatever property is looked for and this in return calls into the version handler, which loops over some child entries (currently around 1100 entries) to find one single entry with a given UUID.

Besides the latter not being optimal and certainly not scaling, the former has its problems in its own right."
0,"NodeTypeRegistry could auto-subtype from nt:basewhen tying to register a (primary) nodetype that does not extend from nt:base the following error is
thrown:

""all primary node types except nt:base itself must be (directly or indirectly) derived from nt:base""

since the registry is able to detect this error, it would be easy to auto-subtype all nodetypes from nt:base. imo it's pointless to explzitely add the nt:base to every supperclass set. as an analogy, you don't need to 'extend from java.lang.Object' explicitely - the compiler does that automatically for your."
0,"extend security config -> repository-1.5.dtdalong with issue #JCR-1171 i'd like to extend the configuration. 

this requires a new version (repository-1.5.dtd) of the repository dtd to be present in jackrabbit-site/src/site/resources/dtd in order to have the new versions of repository.xml work properly.

see attached diff that shows the difference to the most recent repository-1.4.dtd

if nobody objects i would put the proposed repository-1.5.dtd to the site.
once this is done i can properly adjust the repository.xml files (uncommenting the DOCTYPE tag) and start committing the new security functionality.

angela"
0,"Replace commons-logging dependency with SLF4JThe poi dependency in jackrabbit-text-extractors brings in a transitive dependency to commons-logging. Since we use SLF4J for all logging, we should exclude the commons-logging dependency and replace it with jcl104-over-slf4j."
0,"Log creation impairs performanceRunning JProfiler on a program that uses HttpClient with a ThreadSafeClientConnManager, revealed that 5% of the time was spent constructing Log instances in class ClientParamsStack.

Oleg did some further investigation and found that DefaultRequestDirector also has the same problem.

A simple solution would be to make the Log a static member variable, and do this on all classes for consistency.  However this might not be the best solution for interoperating with some frameworks (see http://wiki.apache.org/jakarta-commons/Logging/StaticLog)

Another solution would be to simply remove the Log from the affected classes, although they are presumably there for a reason...
"
0,"Callback for intercepting merging segments in IndexWriterFor things like merging field caches or bitsets, it's useful to
know which segments were merged to create a new segment.

"
0,"Change AttributeSource API to use genericsThe AttributeSource API will be easier to use with JDK 1.5 generics.

Uwe, if you started working on a patch for this already feel free to assign this to you."
1,"spi2dav: EventFilters not respectedi have the impression that the event filter passed to the event subscription in spi2dav is not (or not properly) respected.

marcel, is there a specific reason that you always pass the static SubscriptionInfo constant (no node type filter, noLocal false) to the SubscribeMethod
in spi2dav/RepositoryServiceImpl#createSubscription ?

i guess this is the reason for the failure of
  testNodeType(org.apache.jackrabbit.test.api.observation.AddEventListenerTest)
  testNoLocalTrue(org.apache.jackrabbit.test.api.observation.AddEventListenerTest)
"
0,"ItemInfoBuilder should not include PropertyInfos in ChildInfosWhen building a NodeInfo instance using the ItemInfoBuilder, getChildInfos() returns entries for the PopertyInfos. This is wrong: getChildInfos should only include entries for the child nodes. "
1,"FileDataStore garbage collection can throw a NullPointerException if there is I/O problemThe FileDataStore can throw a NPE when doing garbage collection, if there is file I/O problem (for example an access rights problem). The reason is that it doesn't check if File.list / listFiles returns null. Stack trace:

Exception in thread ""Thread-461"" java.lang.NullPointerException
	at org.apache.jackrabbit.core.data.FileDataStore.deleteOlderRecursive(FileDataStore.java:334)
	at org.apache.jackrabbit.core.data.FileDataStore.deleteOlderRecursive(FileDataStore.java:328)
	at org.apache.jackrabbit.core.data.FileDataStore.deleteOlderRecursive(FileDataStore.java:328)
	at org.apache.jackrabbit.core.data.FileDataStore.deleteOlderRecursive(FileDataStore.java:328)

"
0,"MultiReader.norm() takes up too much memory: norms byte[] should be made into an ObjectMultiReader.norms() is very inefficient: it has to construct a byte array that's as long as all the documents in every segment.  This doubles the memory requirement for scoring MultiReaders vs. Segment Readers.  Although this is cached, it's still a baseline of memory that is unnecessary.

The problem is that the Normalization Factors are passed around as a byte[].  If it were instead replaced with an Object, you could perform a whole host of optimizations
a.  When reading, you wouldn't have to construct a ""fakeNorms"" array of all 1.0fs.  You could instead return a singleton object that would just return 1.0f.
b.  MultiReader could use an object that could delegate to NormFactors of the subreaders
c.  You could write an implementation that could use mmap to access the norm factors.  Or if the index isn't long lived, you could use an implementation that reads directly from the disk.

The patch provided here replaces the use of byte[] with a new abstract class called NormFactors.  
NormFactors has two methods on it
    public abstract byte getByte(int doc) throws IOException;  // Returns the byte[doc]
    public float getFactor(int doc) throws IOException;            // Calls Similarity.decodeNorm(getByte(doc))

There are four implementations of this abstract class
1.  NormFactors.EmptyNormFactors - This replaces the fakeNorms with a singleton that only returns 1.0
2.  NormFactors.ByteNormFactors - Converts a byte[] to a NormFactors for backwards compatibility in constructors.
3.  MultiNormFactors - Multiplexes the NormFactors in MultiReader to prevent the need to construct the gigantic norms array.
4.  SegmentReader.Norm - Same class, but now extends NormFactors to provide the same access.

In addition, Many of the Query and Scorer classes were changes to pass around NormFactors instead of byte[], and to call getFactor() instead of using the byte[].  I have kept around IndexReader.norms(String) for backwards compatibiltiy, but marked it as deprecated.  I believe that the use of ByteNormFactors in IndexReader.getNormFactors() will keep backward compatibility with other IndexReader implementations, but I don't know how to test that.
"
0,"Flush volatile index when size limit is reachedCurrently the volatile index is committed when minMergeDocs is reached. This is inconvenient because it does not take the size of nodes into account account. When lots of small nodes are added the volatile index should be committed less frequently. Similarly when nodes with lots of properties are indexed the volatile index should be committed more frequently.

Instead the size of the volatile index in bytes should trigger a disk write."
0,"Define clear semantics for Directory.fileLengthOn this thread: http://mail-archives.apache.org/mod_mbox/lucene-java-dev/201003.mbox/%3C126142c1003121525v24499625u1589bbef4c0792e7@mail.gmail.com%3E it was mentioned that Directory's fileLength behavior is not consistent between Directory implementations if the given file name does not exist. FSDirectory returns a 0 length while RAMDirectory throws FNFE.

The problem is that the semantics of fileLength() are not defined. As proposed in the thread, we'll define the following semantics:

* Returns the length of the file denoted by <code>name</code> if the file exists. The return value may be anything between 0 and Long.MAX_VALUE.
* Throws FileNotFoundException if the file does not exist. Note that you can call dir.fileExists(name) if you are not sure whether the file exists or not.

For backwards we'll create a new method w/ clear semantics. Something like:

{code}
/**
 * @deprecated the method will become abstract when #fileLength(name) has been removed.
 */
public long getFileLength(String name) throws IOException {
  long len = fileLength(name);
  if (len == 0 && !fileExists(name)) {
    throw new FileNotFoundException(name);
  }
  return len;
}
{code}

The first line just calls the current impl. If it throws exception for a non-existing file, we're ok. The second line verifies whether a 0 length is for an existing file or not and throws an exception appropriately."
0,Add memory based bundle store
0,"Change all FilteredTermsEnum impls into TermsEnum decoratorsCurrently, FilteredTermsEnum has two ctors:
* FilteredTermsEnum(IndexReader reader, String field)
* FilteredTermsEnum(TermsEnum tenum)

But most of our concrete implementations (e.g. TermsRangeEnum) use the IndexReader+field ctor

In my opinion we should remove this ctor, and switch over all FilteredTermsEnum implementations to just take a TermsEnum.

Advantages:
* This simplifies FilteredTermsEnum and its subclasses, where they are more decorator-like (perhaps in the future we could compose them)
* Removes silly checks such as if (tenum == null) in every next()
* Allows for consumers to pass in enumerators however they want: e.g. its their responsibility if they want to use MultiFields or not, it shouldnt be buried in FilteredTermsEnum.

I created a quick patch (all core+contrib+solr tests pass), but i think this opens up more possibilities for refactoring improvements that haven't yet been done in the patch: we should explore these too.
"
0,"Line-separator differences cause PredefinedNodeTypeTest to fail on different operating systems.In testPredefinedNodeType(), the test reads in a test file from the file system and then performs a string comparison, which may fail due to line-separator differences:

    private void testPredefinedNodeType(String name)
            throws NotExecutableException {
        try {
            StringBuffer spec = new StringBuffer();
            String resource =
                ""org/apache/jackrabbit/test/api/nodetype/spec/""
                + name.replace(':', '-') + "".txt"";
            Reader reader = new InputStreamReader(
                    getClass().getClassLoader().getResourceAsStream(resource));
            for (int ch = reader.read(); ch != -1; ch = reader.read()) {
                spec.append((char) ch);
            }

            NodeType type = manager.getNodeType(name);

            assertEquals(
                    ""Predefined node type "" + name,
                    spec.toString(),
                    getNodeTypeSpec(type));
...

The above works when the file being read in has line-separators that match the operating system the test is being run on.  However, if there is a mismatch, the string comparison will fail.

The fix is to replace line-separators in both strings being compared:

Helper method to replace line separators

    /** Standardize line separators around ""\n"". */
    public String replaceLineSeparators(String stringValue) {
        // Replace ""\r\n"" (Windows format) with ""\n"" (Unix format) 
        stringValue = stringValue.replaceAll(""\r\n"", ""\n"");
        // Replace ""\r"" (Mac format) with ""\n"" (Unix format)
        stringValue = stringValue.replaceAll(""\r"", ""\n"");
        
        return stringValue;
    }
    
Updated test method:

    private void testPredefinedNodeType(String name)
            throws NotExecutableException {
        try {
            StringBuffer spec = new StringBuffer();
            String resource =
                ""org/apache/jackrabbit/test/api/nodetype/spec/""
                + name.replace(':', '-') + "".txt"";
            Reader reader = new InputStreamReader(
                    getClass().getClassLoader().getResourceAsStream(resource));
            for (int ch = reader.read(); ch != -1; ch = reader.read()) {
                spec.append((char) ch);
            }

            NodeType type = manager.getNodeType(name);
            
            String nodeTypeSpecValue = replaceLineSeparators(getNodeTypeSpec(type));
            String specValue = replaceLineSeparators(spec.toString());
            
            assertEquals(
                    ""Predefined node type "" + name,
                    specValue,
                    nodeTypeSpecValue);
..."
0,"unicode escapes in files generated by JJTreeMaven build fails on windows machines if sources are located in a directory starting with a 'u'. This is because files created by JJTree (javacc) put filename and path in a comment of the generated files, like this:

/*@bgen(jjtree) Generated By:JJTree: Do not edit this line. D:\usr\projects\workspace\jackrabbit\target\generated-src\main\org\apache\jackrabbit\core\query\sql\JCRSQL.jj */

The \u in interpreted as an escape characted and so you get a 
""BUILD FAILED ... Invalid escape character""
"
0,"Improper deprecation of Locked classThe Locked class in the jcr-commons package has been deprecated with 1.4 and moved to the spi-commons.
However as this is a common class which does not depend on the spi, it should rather stay in jcr-commons.
The dependencies to spi can simply be removed again."
0,"Position based TermVectorMapperAs part of the new TermVectorMapper approach to TermVectors, the ensuing patch loads term vectors and stores the term info by position.  This should let people directly index into a term vector given a position.  Actually, it does it through Maps, b/c the array based bookkeeping is a pain given the way positions are stored.  

The map looks like:
Map<String,   Map<Integer, TVPositionInfo>>

where the String is the field name, the integer is the position, and TVPositionInfo is a storage mechanism for the terms and offsets that occur at a position.  It _should_ handle multiple terms per position (which is always my downfall! )

I have not tested performance of this approach.
"
1,"jcr2spi: Item.isSame may return wrong result if any ancestor is invalidatedjulian detected an issue with jcr2spi that was previously shadowed due to heavy reloading of items upon save.
with the most recent changes however reloading of items is postponed until the next access. this will cause the following test to fail:

        Node n = testRootNode.addNode(""aFile"", ""nt:file"");
        n = n.addNode(""jcr:content"", ""nt:resource"");
        n.setProperty(""jcr:lastModified"", Calendar.getInstance());
        n.setProperty(""jcr:mimeType"", ""text/plain"");
        Property jcrData = n.setProperty(""jcr:data"", ""abc"", PropertyType.BINARY);
        testRootNode.save();

        // access same property through different session
        Session otherSession = helper.getReadOnlySession();
        try {
            Property otherProperty = (Property) otherSession.getItem(jcrData.getPath());
            assertTrue(jcrData.isSame(otherProperty));
        } finally {
            otherSession.logout();
        }

while 
     
       assertTrue(n.isSame(otherSession.getItem(n.getPath()));

would be successful.

the reason: the jcrData property is not reloaded and it's parent is still _invalidated_. consequently the property isn't aware of it's id having changed due to the fact that nt:resource is a node type extending from mix:referenceable.

possible fixes:

1) mark all items _invalid_ after save 
    instead of setting status non-protected/autocreated properties to EXISTING.
    -> forcing jcrData to be reloaded before isSame can be called.
    -> drawback: much more round trip(s) to the server just to make sure the id is up to date.

2) change Item#isSame to make sure the workspaceId is up to date (walking up the
     hierarchy and force reloading of the first invalidated ancestor).
     -> drawback: if referenceable nodes are rare or missing at all, this causes some
          extra round trips.
 
3) change Item.isSame to compare the 'workspacePath' instead of the 'workspaceId'.
     -> drawback: upon persisted move of a referenceable node Item#isSame will return false


after taking a closer look at the code and at some additional tests i would opt for 2).
"
0,"DefaultItemStateProvider contains grow-only cacheThe DefaultItemStateProvider class contains a private HashMap ""items"" which contains references to ItemState objects. The bad thing about this cache is, that it only grows, but is not being managed to forget about ""unused"" items.

Example: A repository which is filled with 9350 nodes and 52813 properties grows this items map to 1'667'557 (!) entries. In this concrete case, the VM all13ates 213MB to the heap of which 57MB is referenced by the DefaultItemStateProvider.items map."
0,"nuke/clean up AtomicReader.hasNormsimplementations already have to return fieldInfos() [which can tell you this], and normValues() [which can also tell you this].

So if we want to keep it, I think it should just have a final implementation and not be required for FilterReaders, etc.

Or we can just nuke it... do we really need 3 ways to do the same thing?"
0,cutover oal.index.* tests to use a random IWC to tease out bugs
1,"TestNRTThreads hangs in nightly 3.x buildsMaybe we have a problem, maybe its a bug in the test.

But its strange that lately the 3.x nightlies have been hanging here."
1,"PerFieldCodecWrapper.loadTermsIndex concurrency problemSelckin's while(1) testing on RT branch hit another error:
{noformat}
    [junit] Testsuite: org.apache.lucene.TestExternalCodecs
    [junit] Testcase: testPerFieldCodec(org.apache.lucene.TestExternalCodecs):	Caused an ERROR
    [junit] (null)
    [junit] java.lang.NullPointerException
    [junit] 	at org.apache.lucene.index.PerFieldCodecWrapper$FieldsReader.loadTermsIndex(PerFieldCodecWrapper.java:202)
    [junit] 	at org.apache.lucene.index.SegmentReader.loadTermsIndex(SegmentReader.java:1005)
    [junit] 	at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:652)
    [junit] 	at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:609)
    [junit] 	at org.apache.lucene.index.BufferedDeletesStream.applyDeletes(BufferedDeletesStream.java:276)
    [junit] 	at org.apache.lucene.index.IndexWriter.applyAllDeletes(IndexWriter.java:2660)
    [junit] 	at org.apache.lucene.index.IndexWriter.maybeApplyDeletes(IndexWriter.java:2651)
    [junit] 	at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:381)
    [junit] 	at org.apache.lucene.index.IndexReader.open(IndexReader.java:316)
    [junit] 	at org.apache.lucene.TestExternalCodecs.testPerFieldCodec(TestExternalCodecs.java:541)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1246)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1175)
    [junit] 
    [junit] 
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.909 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestExternalCodecs -Dtestmethod=testPerFieldCodec -Dtests.seed=-7296204858082494534:5010909751437000758
    [junit] WARNING: test method: 'testPerFieldCodec' left thread running: merge thread: _i(4.0):Cv130 _m(4.0):Cv30 _n(4.0):cv10 into _o
    [junit] RESOURCE LEAK: test method: 'testPerFieldCodec' left 1 thread(s) running
    [junit] NOTE: test params are: codec=PreFlex, locale=zh_TW, timezone=America/Santo_Domingo
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDemo, TestExternalCodecs]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=2,free=104153512,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.TestExternalCodecs FAILED
    [junit] Exception in thread ""Lucene Merge Thread #5"" org.apache.lucene.util.ThreadInterruptedException: java.lang.InterruptedException: sleep interrupted
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:505)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:472)
    [junit] Caused by: java.lang.InterruptedException: sleep interrupted
    [junit] 	at java.lang.Thread.sleep(Native Method)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:503)
    [junit] 	... 1 more
{noformat}

I suspect this is also a trunk issue, but I can't reproduce it yet.

I think this is happening because the codecs HashMap is changing (via another thread), while .loadTermsIndex is called."
0,"Hardening of NativeFSLockNativeFSLock create a test lock file which its name might collide w/ another JVM that is running. Very unlikely, but still it happened a couple of times already, since the tests were parallelized. This may result in a false exception thrown from release(), when the lock file's delete() is called and returns false, because the file does not exist (deleted by another JVM already). In addition, release() should give a second attempt to delete() if it fails, since the file may be held temporarily by another process (like AntiVirus) before it fails. The proposed changes are:

1) Use ManagementFactory.getRuntimeMXBean().getName() as part of the test lock name (should include the process Id)
2) In release(), if delete() fails, check if the file indeed exists. If it is, let's attempt a re-delete() few ms later.
3) If (3) still fails, throw an exception. Alternatively, we can attempt a deleteOnExit.

I'll post a patch later today."
0,"CompoundFileReader's openInput produces streams that may do an extra buffer copySpinoff of LUCENE-888.

The class for reading from a compound file (CompoundFileReader) has a
primary stream which is a BufferedIndexInput when that stream is from
an FSDirectory (which is the norm).  That is one layer of buffering.

Then, when its openInput is called, a CSIndexInput is created which
also subclasses from BufferedIndexInput.  That's a second layer of
buffering.

When a consumer actually uses that CSIndexInput to read, and a call to
readByte or readBytes runs out of what's in the first buffer, it will
go to refill its buffer.  But that refill calls the first
BufferedIndexInput which in turn may refill its buffer (a double
copy) by reading the underlying stream.

Not sure how to fix it yet but we should change things to not do the
extra buffer copy.
"
1,"DbDataStore connection does not always reconnectIf a DbDataStore connection is closed due to an error all subsequent addRecord calls will fail with 'connection has been closed and autoReconnect == false'
 after getRecord is called and the connection is reconnected addRecord will succeed.

the connection should be validated before setting autoReconnect = false or on retrieval from the pool."
1,"intermittant exceptions in TestConcurrentMergeScheduler
The TestConcurrentMergeScheduler throws intermittant exceptions that
do not result in a test failure.

The exception happens in the ""testNoWaitClose()"" test, which repeated
tests closing an IndexWriter with ""false"", meaning abort any
still-running merges.  When a merge is aborted it can hit various
exceptions because the files it is reading and/or writing have been
deleted, so we ignore these exceptions.

The bug was just that we were failing to properly check whether the
running merge was actually aborted because of a scoping issue of the
""merge"" variable in ConcurrentMergeScheduler.  So the exceptions are
actually ""harmless"".  Thanks to Ning for spotting it!

"
1,"SpellChecker min score is increased by timeThe minimum score, an instance variable, is modified in a search. That is wrong, since it makes it 1. thread unsafe and 2. not working. 

Lucky enought it is only used from the one and same method call, so I simply compied the instance variable to a local method variable.

        float min = this.min; 
"
0,"Error in FSDirectory if java.io.tmpdir incorrectly specifiedA user of the JAMWiki project (http://jamwiki.org/) reported an error with the following stack trace:

SEVERE: Unable to create search instance /usr/share/tomcat5/webapps/jamwiki-0.3.4-beta7/test/base/search/indexen
java.io.IOException: Cannot create directory: /temp
        at org.apache.lucene.store.FSDirectory.init(FSDirectory.java:171)
        at org.apache.lucene.store.FSDirectory.getDirectory(FSDirectory.java:141)
        at org.apache.lucene.store.FSDirectory.getDirectory(FSDirectory.java:117)
        at org.jamwiki.search.LuceneSearchEngine.getSearchIndexPath(LuceneSearchEngine.java:318)

The culprit is that the java.io.tmpdir property was incorrectly specified on the user's system.  Lucene could easily handle this issue by modifying the FSDirectory.init() method.  Currently the code uses the index directory if java.io.tmpdir and org.apache.lucene.lockDir are unspecified, but it could use that directory if those values are unspecified OR if they are invalid.  Doing so would make Lucene a bit more robust without breaking any existing installations.
"
1,"Problem with redirect on HEAD when (bad, naughty) server returns body contentI've been testing/using HttpClient 2.0a3 with Resin 2.1.9. I've found that when
using a HEAD request on a JSP, Resin returns the body content along with the
headers.

In this case, something in the HttpClient breaks. Looking at the httpclient
logs, it looks like:

1) HttpClient does a HEAD against the original URL
2) Resin returns valid status line and headers
3) HttpClient parses the headers and recognizes the redirect header
4) HttpClient does a HEAD against the new URL (from the Location header)
5) HttpMethodBase calls readStatusLine, which (eventually) calles readRawLine in
HttpConnection (which reads from the internal inputStream)
6) readRawLine returns the first line in the body from the original HEAD request
in (1).

It looks like the original body content (in response to the first HEAD) is being
buffered somewhere, but I can't figure out where.

I know that this is invalid behavior on the server's part, but I would like to
be able to recover from it.



---- redir_test.jsp ----
<?xml version=""1.0""?>
<% 
  response.setStatus(response.SC_MOVED_TEMPORARILY);
  response.setHeader(""Location"", ""redirect_pass.xml"");
%>
<some>
  <dummy>
    <data attr=""yea, well""/>
  </dummy>
</some>"
1,"NTLM Authentication FailsNTLM Authentication requires multiple request/responses for the authentication
to succeed.  Since HttpMethodBase is now using just the host, port and realm to
identify whether or not authentication has been attempted the second pass for
NTLM authentication is never performed."
1,Deleted documents are visible across reopened MSRs
1,"NoSuchItemStateException on checkin after removeVersion in XA EnvironmentAfter removing a version, a checkin on the same node in a different transaction (with a different session) fails.
The NoSuchItemStateException refer to the uuid of the previously removed version. 
I'll attach a test demonstrating the problem. "
0,"Sort and SortField does not have equals() and hashCode()During developing for my project panFMP I had the following issue:
I have a cache for queries (like Solr has, too)  for query results. This cache also uses the Sort/SortField as key into the cache. The problem is, because Sort/SortField does not implement equals() and hashCode(), you cannot store them as cache keys. To workaround, currently I use Sort.toString() as cache key, but this is not so nice.

In corelation with issue LUCENE-1478, I could fix this there in one patch together with the other improvements."
1,"BindableRepositoryFactory doesn't handle repository shutdownThe BindableRepositoryFactory class keeps a cached reference to a repository even after the repository has been shut down.

This causes the following code snippet to fail with an IllegalStateException:

        Hashtable environment = new Hashtable();
        environment.put(
                Context.INITIAL_CONTEXT_FACTORY,
                DummyInitialContextFactory.class.getName());
        environment.put(Context.PROVIDER_URL, ""http://jackrabbit.apache.org/"");
        Context context = new InitialContext(environment);

        JackrabbitRepository repository;
        String xml = ""src/test/repository/repository.xml"";
        String dir = ""target/repository"";
        String key = ""repository"";

        // Create first repository
        RegistryHelper.registerRepository(context, key, xml, dir, true);
        repository = (JackrabbitRepository) context.lookup(key);
        repository.login().logout();
        repository.shutdown();

        // Create second repository with the same configuration
        RegistryHelper.registerRepository(context, key, xml, dir, true);
        repository = (JackrabbitRepository) context.lookup(key);
        repository.login().logout(); // throws an IllegalStateException!
        repository.shutdown();
"
0,"test case (TCK) maintenance for JCR 2.0Umbrella issue for changes/additions to JUnit test cases, setup and config."
1,"Internal error in WorkspaceItemStateFactory#createDeepNodeState When WorkspaceItemStateFactory#createDeepNodeState receives the current entry as argument for anyParent, it throws RepositoryException with the message ""Internal error while getting deep itemState"". This is incorrect (probably a leftover from JCR-1797) since any entry is valid as argument for anyParent. "
0,"Add User#changePassword(String newPw, String oldPw)... where the oldPw must match in order to have the password of the user successfully changed.

while this could be done by applications with quite some effort, the implementation can easily achieve this
as the functionality required is already present."
0,"Missing jackrabbit-rmi-service.xml from jackrabbit-jcr-rmi-1.2.1.jarThe file jackrabbit-rmi-service.xml is missing from the jackrabbit-jcr-rmi-1.2.1.jar.

The cause of the issue appears that the directory structure of the jackrabbit-jcr-rmi sub-project doesn't match the Maven 2 standard.  

To fix: src/resources should be moved to src/main/resources."
0,"Text extractor classes are obsolete in webText extractor classes are obsolete in http://jackrabbit.apache.org/doc/components/index-filters.html

""org.apache.jackrabbit.core.query"" are actually ""org.apache.jackrabbit.extractor""

Plain text extractor continue being ""org.apache.jackrabbit.core.query.lucene.TextPlainTextFilter""? "
0,"Improve reusability of AbstractRepositoryService and AbstractReadableRepositoryServiceMuch of the functionality in AbstractReadableRepositoryService is not specific to reading but rather applies to any implementation (node types, name spaces, descriptors). I suggest to pull this functionality up from AbstractReadableRepositoryService to AbstractRepositoryService"
0,"Make Token.DEFAULT_TYPE publicMake Token.DEFAULT_TYPE public so that TokenFilters using the reusable Token model have a way of setting the type back to the default.

No patch necessary.  I will commit soon."
1,"SQL Parser fails with SQL 92 timestamp formatThe SQL query parser fails with an exception if the SQL 92 timestamp format is used.

E.g:
... WHERE my:date > TIMESTAMP '1976-01-01 00:00:00.000+01:00'

does not work, but the following will succeed using ISO8601:

... WHERE my:date > TIMESTAMP '1976-01-01T00:00:00.000+01:00'"
1,"Automatic type conversion no longer worksString values are no longer converted to binary when required. Example:

Node n = testRootNode.addNode(""testConvert"", ""nt:file"");
Node content = n.addNode(""jcr:content"", ""nt:resource"");
content.setProperty(""jcr:lastModified"", Calendar.getInstance());
content.setProperty(""jcr:mimeType"", ""text/html"");
content.setProperty(""jcr:data"", ""Hello"");
n.getSession().save();

This used to work in a previous 2.0 build, but now throws:

javax.jcr.nodetype.ConstraintViolationException: no matching property definition found for {http://www.jcp.org/jcr/1.0}data
at org.apache.jackrabbit.core.nodetype.EffectiveNodeType.getApplicablePropertyDef(EffectiveNodeType.java:782)
at org.apache.jackrabbit.core.NodeImpl.getApplicablePropertyDefinition(NodeImpl.java:747)
at org.apache.jackrabbit.core.ItemManager.getDefinition(ItemManager.java:241)
at org.apache.jackrabbit.core.ItemData.getDefinition(ItemData.java:101)
at org.apache.jackrabbit.core.ItemManager.canRead(ItemManager.java:409)
at org.apache.jackrabbit.core.ItemManager.getItemData(ItemManager.java:383)
at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:316)
at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:595)
at org.apache.jackrabbit.core.NodeImpl.removeChildProperty(NodeImpl.java:554)
at org.apache.jackrabbit.core.NodeImpl.removeChildProperty(NodeImpl.java:534)
at org.apache.jackrabbit.core.NodeImpl.setProperty(NodeImpl.java:2303)
at org.apache.jackrabbit.core.nodetype.ConvertDataTypeTest.testStringToBinary(ConvertDataTypeTest.java:36)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)"
0,"Move ocm documentation to jackrabbit-siteThe OCM documentation from jackrabbit-ocm/xdocs should be moved to jackrabbit-site.

Also all old references to Graffito should be replaced with Jackrabbit."
1,"IndexWriter.addIndexes(IndexReader[] readers) doesn't correctly handle exception success flag.After this bit of code in addIndexes(IndexReader[] readers)

 try {
        flush(true, false, true);
        optimize();					  // start with zero or 1 seg
        success = true;
      } finally {
        // Take care to release the write lock if we hit an
        // exception before starting the transaction
        if (!success)
          releaseWrite();
      }

The success flag should be reset to ""false"" because it's used again in another try/catch/finally block.  

TestIndexWriter.testAddIndexOnDiskFull() sometimes will hit this bug; but it's infrequent.


"
1,"importXML prepending line feeds to tag valuesImporting using Session.importXML(...) results in new line characters being inserted at the beginning of tag
values:

<?xml version=""1.0"" encoding=""UTF-8""?>
<Policy xmlns=""urn:oasis:names:tc:xacml:1.0:policy""  xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" PolicyId=""test:policy-one"" RuleCombiningAlgId=""urn:oasis:names:tc:xacml:1.0:rule-combining-algorithm:ordered-permit-overrides"">
  <Description>policy-description</Description>
  <Target>
...

Becomes

/test/policies/Policy/jcr:primaryType=nt:unstructured
/test/policies/Policy/PolicyId=test:policy-one
/test/policies/Policy/RuleCombiningAlgId=urn:oasis:names:tc:xacml:1.0:rule-combining-algorithm:ordered-permit-overrides
/test/policies/Policy/Description/jcr:primaryType=nt:unstructured
/test/policies/Policy/Description/jcr:xmltext/jcr:primaryType=nt:unstructured
/test/policies/Policy/Description/jcr:xmltext/jcr:xmlcharacters=
policy-description
/test/policies/Policy/Target/jcr:primaryType=nt:unstructured

(in other cases, many LFs are inserted)

FULL EXAMPLE XML FILE:

<?xml version=""1.0"" encoding=""UTF-8""?>
<Policy xmlns=""urn:oasis:names:tc:xacml:1.0:policy"" 
  xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" 
  PolicyId=""test:policy-one"" 
  RuleCombiningAlgId=""urn:oasis:names:tc:xacml:1.0:rule-combining-algorithm:ordered-permit-overrides"">
  <Description>policy-description</Description>
  <Target>
    <Resources>
      <Resource>
        <ResourceMatch 
          MatchId=""urn:oasis:names:tc:xacml:1.0:function:string-equal"">
          <AttributeValue DataType=""http://www.w3.org/2001/XMLSchema#string"">test/12345-resource-67890</AttributeValue>
          <ResourceAttributeDesignator 
            DataType=""http://www.w3.org/2001/XMLSchema#string"" 
            AttributeId=""urn:oasis:names:tc:xacml:1.0:resource:resource-id""/>
        </ResourceMatch>
      </Resource>
    </Resources>
    <Actions>
      <AnyAction/>
    </Actions>
  </Target>
  <Rule RuleId=""PermitRule"" Effect=""Permit"">
    <Target>
      <Subjects>
        <Subject>
          <SubjectMatch 
            MatchId=""urn:oasis:names:tc:xacml:1.0:function:string-equal"">
            <AttributeValue DataType=""http://www.w3.org/2001/XMLSchema#string"">alice</AttributeValue>
            <SubjectAttributeDesignator 
              DataType=""http://www.w3.org/2001/XMLSchema#string"" 
              AttributeId=""urn:oasis:names:tc:xacml:1.0:subject:subject-id""/>
          </SubjectMatch>
        </Subject>
      </Subjects>
      <Actions>
        <Action>
          <ActionMatch 
            MatchId=""urn:oasis:names:tc:xacml:1.0:function:string-equal"">
            <AttributeValue DataType=""http://www.w3.org/2001/XMLSchema#string"">read</AttributeValue>
            <ActionAttributeDesignator 
              DataType=""http://www.w3.org/2001/XMLSchema#string"" 
              AttributeId=""urn:oasis:names:tc:xacml:1.0:action:action-id""/>
          </ActionMatch>
        </Action>
        <Action>
          <ActionMatch 
            MatchId=""urn:oasis:names:tc:xacml:1.0:function:string-equal"">
            <AttributeValue DataType=""http://www.w3.org/2001/XMLSchema#string"">write</AttributeValue>
            <ActionAttributeDesignator 
              DataType=""http://www.w3.org/2001/XMLSchema#string"" 
              AttributeId=""urn:oasis:names:tc:xacml:1.0:action:action-id""/>
          </ActionMatch>
        </Action>
        <Action>
          <ActionMatch 
            MatchId=""urn:oasis:names:tc:xacml:1.0:function:string-equal"">
            <AttributeValue DataType=""http://www.w3.org/2001/XMLSchema#string"">delete</AttributeValue>
            <ActionAttributeDesignator 
              DataType=""http://www.w3.org/2001/XMLSchema#string"" AttributeId=""urn:oasis:names:tc:xacml:1.0:action:action-id""/>
          </ActionMatch>
        </Action>
      </Actions>
    </Target>
  </Rule>
</Policy>"
0,Child axis support for XPath predicatesIt seems that Jackrabbit currently only supports the attribute axis in XPath predicates. Support for the child axis would be a nice addition.
0,"Add sort-by-term with DocValuesThere are two sorted byte[] types with DocValues (BYTES_VAR_SORTED,
BYTES_FIXED_SORTED), so you can index this type, but you can't yet
sort by it.

So I added a FieldComparator just like TermOrdValComparator, except it
pulls from the doc values instead.

There are some small diffs, eg with doc values there are never null
values (see LUCENE-3504).
"
1,"org.apache.lucene.ant.HtmlDocument creates a FileInputStream in its constructor that it doesn't closeA look through the jtidy source code doesn't show a close that i can find in parse (seems to be standard that you close your own streams anyway), so this looks like a small descriptor leak to me."
1,"jcr:mixinTypes property inconsitent, if addMixin() throws exceptionIf Node.addMixin() throws an exception, that was not due to validation checks but rather internal, the jcr:mixinTypes still contains the newly added nodetype. a subsequent call to Item.save() will store that property. The effective nodetype is not affected though."
0,Move ReusableAnalyzerBase into coreIn LUCENE-2309 it was suggested that we should make Analyzer reusability compulsory.  ReusableAnalyzerBase is a fantastic way to drive reusability so lets move it into core (so that we can then change all impls over to using it).
0,"Improve Spatial Utility like classes- DistanceUnits can be improved by giving functionality to the enum, such as being able to convert between different units, and adding tests.  

- GeoHashUtils can be improved through some code tidying, documentation, and tests.

- SpatialConstants allows us to move all constants, such as the radii and circumferences of Earth, to a single consistent location that we can then use throughout the contrib.  This also allows us to improve the transparency of calculations done in the contrib, as users of the contrib can easily see the values being used.  Currently this issues does not migrate classes to use these constants, that will happen in issues related to the appropriate classes."
0,"Avoid path resolution in case of non-wildcard ACEs (follow-up to JCR-2573)adding the ability to specify wildcard-ac-entries in the default resource based access control management lead to
always resolving the id passed to AccessControlProvider#canRead in order to be able to properly evaluate
any wildcard-aces present.

this could be improved with minor refactoring that postpones the path resolution and omitting it if there are no
wildcard-aces to compare with."
0,"Optimize bundle serializationThere are a number of ways we could use to make bundle serialization more optimized. Thomas has already done some work on this in the Jackrabbit 3 sandbox, and I'd like to apply some of the optimizations also to the trunk."
1,"TokenSources.getTokenStream() does not assign positionIncrementTokenSources.StoredTokenStream does not assign positionIncrement information. This means that all tokens in the stream are considered adjacent. This has implications for the phrase highlighting in QueryScorer when using non-contiguous tokens.

For example:
Consider  a token stream that creates tokens for both the stemmed and unstemmed version of each word - the fox (jump|jumped)
When retrieved from the index using TokenSources.getTokenStream(tpv,false), the token stream will be - the fox jump jumped

Now try a search and highlight for the phrase query ""fox jumped"". The search will correctly find the document; the highlighter will fail to highlight the phrase because it thinks that there is an additional word between ""fox"" and ""jumped"". If we use the original (from the analyzer) token stream then the highlighter works.

Also, consider the converse - the fox did not jump
""not"" is a stop word and there is an option to increment the position to account for stop words - (the,0) (fox,1) (did,2) (jump,4)
When retrieved from the index using TokenSources.getTokenStream(tpv,false), the token stream will be - (the,0) (fox,1) (did,2) (jump,3).

So the phrase query ""did jump"" will cause the ""did"" and ""jump"" terms in the text ""did not jump"" to be highlighted. If we use the original (from the analyzer) token stream then the highlighter works correctly."
1,"SSL + proxy + Host auth + Keep Alive off causes an infinite loop in HttpMethodDirectorThe combination of SSL tunnelling, host authentication, and disabled persistent
connection support (HTTPD KeepAlive off) causes an infinite loop in
HttpMethodDirector. 

The problem has been reported on the httpclient-dev list by Rindress MacDonald
<RMacDona at enterasys.com>"
1,"o.a.j.core.state.ChildNodeEntries does not override equals(Object) and hashCode() methodso.a.j.c.state.NodeStateMerger calls ChildNodeEntries.equals(ChildNodeEntries) to compare two child node entries collections.
ChildNodeEntries however doesn't override the equals(Object) method."
0,"UUIDDocId cache does not work properly because of weakReferences in combination with new instance for combined indexreader Queries that use ChildAxisQuery or DescendantSelfAxisQuery make use of getParent() functions to know wether the parents are correct and if the result is allowed. The getParent() is called recursively for every hit, and can become very expensive. Hence, in DocId.UUIDDocId, the parents are cached. 

Currently,  docId.UUIDDocId's are cached by having a WeakRefence to the CombinedIndexReader, but, this CombinedIndexReader is recreated all the time, implying that a gc() is allowed to remove the 'expensive' cache.

A much better solution is to not have a weakReference to the CombinedIndexReader, but to a reference of each indexreader segment. This means, that in getParent(int n) in SearchIndex the return 

return id.getDocumentNumber(this) needs to be replaced by return id.getDocumentNumber(subReaders[i]); and something similar in CachingMultiReader. 

That is all. Obviously, when a node/property is added/removed/changed, some parts of the cached DocId.UUIDDocId will be invalid, but mainly small indexes are updated frequently, which obviously are less expensive to recompute."
0,"Invalid redirects are not correctedIf a get is made to a page with a query argument containing a space, many web
servers, notably including Tomcat 5 can generate a redirect in which the space
in the query argument is not escaped correctly.  Most browsers including IE and
Firefox compensate for this by quoting any included spaces in the redirect
location.  Http client does not.  When this broken URL is presented to a
subsequent server, the GET command is interprted incorrectly resulting (usually)
in a 505.

The fix is to replace spaces in redirect locations with +'s.  This doesn't
entirely fix the problem but that is the job of the web server developers."
1,"Docview import fails, if attribute and childelem have same namedocimport fails, if element has same name as one of the attributes of its parent element.

example:

<?xml version=""1.0"" encoding=""UTF-8""?>
<feature plugin=""foobar"">
    <plugin>test</plugin>
</feature>

importing this results in a ItemExistsException: 'plugin'"
0,"add @Deprecated annotationsas discussed on LUCENE-2084, I think we should be consistent about use of @Deprecated annotations if we are to use it.

This patch adds the missing annotations... unfortunately i cannot commit this for some time, because my internet connection does not support heavy committing (it is difficult to even upload a large patch).

So if someone wants to take it, have fun, otherwise in a week or so I will commit it if nobody objects.
"
1,"Missing skip()-Method in ContentLengthInputStreamContentLengthInputStream is missing the skip()-Method.

This causes the internal pos variable to get out of sync with the content 
length. 
We oberseved that closing the stream caused a wait time of about 15 sec in 
routines which use the skip()-method of InputStream.

Here's a possible implementation which should solve the problem:

    public long skip(long len) throws IOException {
        long count = super.skip(len);
        pos += count;
        return count;
    }"
1,"PostgreSQL: Failed to guess validation queryWhen using PostgreSQL, the following warning appears in the log file:

> *WARN  [org.apache.jackrabbit.core.util.db.ConnectionFactory] (main)
> Failed to guess validation query for URL
> jdbc:postgresql:..."
0,Properly close resourcesJava has exceptions so resources must always be closed on a finally clause
0,"Default configuration not suitable for demo web applicationThe default configuration is not suitable for the demo application. There are no text extractors configured, which makes the populate and search demos useless.

Proposed solution: create a new repository.xml in jackrabbit-webapp with text extractors configured.

I know we should actually try to reduce the number of repository.xml files, but having one dedicated to jackrabbit-webapp seems reasonable, while we should try to achieve the same for the jackrabbit-core module."
1,"Support updateDocument() with DWPTsWith separate DocumentsWriterPerThreads (DWPT) it can currently happen that the delete part of an updateDocument() is flushed and committed separately from the corresponding new document.

We need to make sure that updateDocument() is always an atomic operation from a IW.commit() and IW.getReader() perspective.  See LUCENE-2324 for more details."
0,"Broken javadocs->site docs linksSee the java-dev mailing list discussion: [http://www.nabble.com/Broken-javadocs-%3Esite-docs-links-to20369092.html].

When the Lucene Java website transitioned to versioning some of the documentation, links from some javadocs were not modified to follow the resources.  I found broken links to gettingstarted.html and queryparsersyntax.html.  Here is one example, to gettingstarted.html (the link text is ""demo""): 

[http://lucene.apache.org/java/2_4_0/api/org/apache/lucene/document/package-summary.html]

The attached patch converts absolute URLs from javadocs to versioned docs to be relative, and modifies the ""javadocs-all"" target in build.xml to add a path element named ""all"", so that both versions of the javadocs (all: core+contrib; and separated: core, contribs) can use the same relative URLs.  Adding a path element to the ""javadocs-all"" target is necessary because currently the ""all"" javadocs have one fewer path element than the separated javadocs.

I left as-is one absolute URL, in the o.a.l.index.SegmentInfos javadocs, to fileformats.html, because SegmentInfos is a package-private class, and the javadocs targets in build.xml only generate javadocs for public classes.
"
1,"QueryParser doesn't accept empty stringfoo:"""" currently throws a parse exception
foo: bar is also parsed as foo:bar (not serious since it's arguably illegal syntax)"
1,"UsernamePasswordCredentials.equals(null) throws NPESteps to reproduce:
1. new UsernamePasswordCredentials().equals(null);

Observed:
NullPointerException is thrown

Expected:
equals() returns false"
1,"NullPointerException when deleting a property of type REFERENCEIn method org.apache.jackrabbit.rmi.value.SerialValueFactory#createValue a NPE is thrown when parameter value is null.

Solution:

Change:

   public final Value createValue(Node value) throws RepositoryException {
        return createValue(value.getUUID(), PropertyType.REFERENCE);
    }

to

   public final Value createValue(Node value) throws RepositoryException {
        if (value == null) {
           return null;
        }
        
        return createValue(value.getUUID(), PropertyType.REFERENCE);
    }"
0,"Support unicode escapes in QueryParserAs suggested by Yonik in http://issues.apache.org/jira/browse/LUCENE-573 the QueryParser should be able to handle unicode escapes, i. e. \uXXXX.

I have already working and tested code. It is based on the patch i submitted for LUCENE-573, so once this is (hopefully ;-)) committed, I will submit another patch here."
0," MalformedCookieException: distinguish cookie syntax errors from cross-domain errorsMalformedCookieException is used for both cookies with syntax errors,
and for cookies which are invalid for the particular context - e.g.
cross-domain cookies.

I think it would be helpful to be able to distinguish these without
needing to examine the message text."
0,"HuperDuperSynonymsFilterThe current synonymsfilter uses a lot of ram and cpu, especially at build time.

I think yesterday I heard about ""huge synonyms files"" three times.

So, I think we should use an FST-based structure, sharing the inputs and outputs.
And we should be more efficient with the tokenStream api, e.g. using save/restoreState instead of cloneAttributes()
"
1,"Setting a property which has been transiently removed fails with a PathNotFoundExceptionThe following tests currently all fail with a PathNotFoundException

org.apache.jackrabbit.jcr2spi.AddPropertyTest#testReplacingProperty
org.apache.jackrabbit.jcr2spi.AddPropertyTest#testReplacingProperty2
org.apache.jackrabbit.jcr2spi.AddPropertyTest#testAddingProperty
org.apache.jackrabbit.jcr2spi.AddPropertyTest#testAddingProperty2

"
0,"Unit tests TestBackwardsCompatibility and TestIndexFileDeleter might fail depending on JVMIn the two units tests TestBackwardsCompatibility and TestIndexFileDeleter several index file names are hardcoded. For example, in TestBackwardsCompatibility.testExactFileNames() it is tested if the index directory contains exactly the expected files after several operations like addDocument(), deleteDocument() and setNorm() have been performed. Apparently the unit tests pass on the nightly build machine, but in my environment (Windows XP, IBM JVM 1.5) they fail for the following reason:

When IndexReader.setNorm() is called a new norm file for the specified field is created with the file  ending .sx, where x is the number of the field. The problem is that the SegmentMerger can not guarantee to keep the order of the fields, in other words after a merge took place a field can have a different field number. This specific testcase fails, because it expects the file ending .s0, but the file has the ending .s1.

The reason why the field numbers can be different on different JVMs is the use of HashSet in SegmentReader.getFieldNames(). Depending on the HashSet implementation an iterator might not iterate over the entries in insertion order. When I change HashSet to LinkedHashSet, the two testcases pass.

However, even with a LinkedHashSet the order of the field numbers might change during a merge, because the order in which the SegmentMerger merges the FieldInfos depends on the field options like TERMVECTOR, INDEXED... (see SegmentMerger.mergeFields() for details). 

So I think we should not use LinkedHashSet but rather change the problematic testcases. Furthermore I'm not sure if we should have hardcoded filenames in the tests anyway, because if we change the index format or file names in the future these test cases would fail without modification."
0,Improve multihome supportMultihomePlainSocketFactory is basically broken and should be deprecated. Multihome logic needs to be moved to the DefaultClientConnectionOperator
0,"use the internal CND file for builtin nodetypesthe jackrabbit node type registry is reading the built in node types from a XML file.
since the CND (compact node type definition notation) is now specified by jsr283,
i would like to drop the builtin .xml file and read the builtin node typesonly from the .cnd file.
this certainly helps the developers. furthermore, all the node types in jsr283 are now speced
in CND, and converting them to XML is a pain and error prone."
0,"GCJ build fails with JDK 1.5The build.xml doesn't specify a target VM version. Using JDK 1.5, this means the compiled .class files 
are automatically made for 1.5, with java.lang.StringBuilder insidiously used for string concatenation. 
GCJ doesn't seem to include this class yet, so when it gets to the gcj build it dies trying to read the class 
files.

Steps to reproduce:
1. Install Sun JDK 1.5 for a Java compiler
2. Check out Lucene from svn
3. 'ant gcj'

Expected behavior:
Should build Lucene to .class files and .jar with the JDK compiler and then compile an .a with GCJ.

Actual behavior:
The GCJ build fails, complaining of being unable to find java.lang.StringBuilder.

Suggested fix:
Adding source=""1.3"" target=""1.3"" to the <javac> tasks seems to take care of this. Patch to be attached.

Additional notes:
Using Lucene from SVN and GCJ pulled from GCC CVS circa 2005-04-19. Ant 1.6.2."
0,"lucenetestcase ease of use improvementsI started working on this in LUCENE-2658, here is the finished patch.

There are some problems with LuceneTestCase:
* a tests beforeClass, or the test itself (its @befores and its method), might have some
  random behavior, but only the latter can be reproduced with -Dtests.seed
* if you want to do things in beforeClass, you have to use a different API: newDirectory(random)
  instead of newDirectory, etc.
* for a new user, the current output can be verbose, confusing and overwhelming.

So, I refactored this class to address these problems. 
A class still needs 2 seeds internally, as the beforeClass will only run once, 
but the methods or setUp() might run many times, especially when increasing iterations.

but lucenetestcase deals with this, and the ""seed"" is 128-bit (UUID): 
the MSB is initialized in beforeClass, the LSB varied for each method run.
if you provide a seed with a -D, they are both fixed to the UUID you provided.

I fixed the API to be consistent, so you should be able to migrate a test from 
setUp() to beforeClass() [junit3 to junit4] without changing parameters.

The codec, locale, timezone is only printed once at the end if any tests fail, 
as its per-class anyway (setup in beforeClass)

finally, when a test fails, you get a single ""reproduce with"" command line you can copy and paste to reproduce.
this way you dont have to spend time trying to figure out what the command line should be.

{noformat}
    [junit] Tests run: 2, Failures: 2, Errors: 0, Time elapsed: 0.197 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestExample -Dtestmethod=testMethodA 
              -Dtests.seed=a51e707b-6550-7800-9f8c-72622d14bf5f
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestExample -Dtestmethod=testMethodB 
              -Dtests.seed=a51e707b-6550-7800-f7eb-2efca3820738
    [junit] NOTE: test params are: codec=PreFlex, locale=ar_LY, timezone=Etc/UCT
    [junit] ------------- ---------------- ---------------
    [junit] Test org.apache.lucene.util.TestExample FAILED
{noformat}
"
1,"Document View Import: ISO 9075-encoded element/attribute names may lead to illegal node/property names reported by sridhar raman on the users-list:

importing the following xml document leads to a node of name ""abc [1]"" which is illegal:

<?xml version=""1.0""?>
<abc_x0020__x005B_1_x005D_ foo=""bar""/>
"
1,"New versions added after a restore have bad version nameI add several versions to a node (1.0, 1.1, 1.2, 1.3, 1.4). Perform a restore to version 1.2 and add more versions. After that VersionHistory is like this:

- 1.0
- 1.1
- 1.2
- 1.3
- 1.4
- 1.3.1
- 1.3.2
- 1.3.3
- 1.3.4
- 1.3.5

New versions should be 1.2.x no 1.3.x, isn't it?"
1,"VolatileIndex not closed properlyThe MultiIndex.resetVolatileIndex() method doesn't properly close the existing VolatileIndex instance before creating a new one. This can confuse the DynamicPooledExecutor reference count added in JCR-2836, leading to a background thread leak."
0,SerializationTest leaks sessionsThe class TreeComparator extends from AbstractJCRTest and opens a session in its constructor because it calls the setUp() method. The tearDown() method is never called.
0,Add supported for Wikipedia English as a corpus in the benchmarker stuffAdd support for using Wikipedia for benchmarking.
1,"SQLException with OracleBundle PM in name indexThe oracle bundle pm shows errors like:

java.lang.IllegalStateException: Unable to insert index: java.sql.SQLException:  
  ORA-01400: cannot insert NULL into  (""MARTIJNH"".""WM9_VERSIONING_PM_NAMES"".""ID"")

this is due to the fact that oracle treats empty strings as NULL values which does the schema not allow."
1,"SortField.AUTO doesn't work with longThis is actually the same as LUCENE-463 but I cannot find a way to re-open that issue. I'm attaching a test case by dragon-fly999 at hotmail com that shows the problem and a patch that seems to fix it.

The problem is that a long (as used for dates) cannot be parsed as an integer, and the next step is then to parse it as a float, which works but which is not correct. With the patch the following parsers are used in this order: int, long, float.
"
1,"Repository is corrupt after concurrent changes with the same sessionAfter concurrent write operations using the same session, the repository can get corrupt, meaning a ItemNotFoundException is thrown when trying to remove a node.

Concurrent write operations are not supported, however I believe the persistent state of the repository should not be get corrupt.

One way to solve this problem is to synchronize on the session internally."
0,"No documentation on how to use CookieSpecNone of http://hc.apache.org/httpcomponents-client/httpclient/apidocs/org/apache/http/cookie/CookieSpec.html, http://hc.apache.org/httpcomponents-client/httpclient/apidocs/org/apache/http/cookie/CookieSpecFactory.html, http://hc.apache.org/httpcomponents-client/httpclient/apidocs/org/apache/http/cookie/CookieSpecRegistry.html, or http://hc.apache.org/httpcomponents-client/httpclient/apidocs/org/apache/http/impl/client/DefaultHttpClient.html explain how to set the CookieSpec that the HttpClient actually uses. It looks like CookieSpecRegistry might be it, but it doesn't document what the ""names"" mean, so I don't know what to pick to make a factory actually get used."
0,"need the ability to also sort SpellCheck results by freq, instead of just by Edit Distance+freqThis issue was first noticed and reported in this Solr thread; http://lucene.472066.n3.nabble.com/spellcheck-issues-td489776.html#a489788

Basically, there are situations where it would be useful to sort by freq first, instead of the current ""sort by edit distance, and then subsort by freq if edit distance is equal""

The author of the thread suggested ""What I think would work even better than allowing a custom compareTo function would be to incorporate the frequency directly into the distance function.  This would allow for greater control over the trade-off between frequency and edit distance""

However, custom compareTo functions are not always be possible (ie if a certain version of Lucene must be used, because it was release with Solr) and incorporating freq directly into the distance function may be overkill (ie depending on the implementation)

it is suggested that we have a simple modification of the existing compareTo function in Lucene to allow users to specify if they want the existing sort method or if they want to sort by freq.

"
0,"Allow setting the IndexWriter docstore to be a different directoryAdd an IndexWriter.setDocStoreDirectory method that allows doc
stores to be placed in a different directory than the IW default
dir."
1,"Index recovery may fail with IllegalArgumentExceptionWhen repeatedly killed and started up again, jackrabbit may throw an IllegalArgumentException on index recovery:

Caused by: java.lang.IllegalArgumentException: already contains: _c
   at org.apache.jackrabbit.core.query.lucene.IndexInfos.addName(IndexInfos.java:170)
   at org.apache.jackrabbit.core.query.lucene.MultiIndex.deleteIndex(MultiIndex.java:716)
   at org.apache.jackrabbit.core.query.lucene.MultiIndex$DeleteIndex.execute(MultiIndex.java:1553)
   at org.apache.jackrabbit.core.query.lucene.MultiIndex.executeAndLog(MultiIndex.java:809)
   at org.apache.jackrabbit.core.query.lucene.MultiIndex.flush(MultiIndex.java:740)
   at org.apache.jackrabbit.core.query.lucene.Recovery.run(Recovery.java:160)
"
1,http client cache: SizeLimitedResponseReader is not setting content type for InputStreamEntity in constructResponse()the newly created InputStreamEntity should be populated with content-encoding and content-type.
0,"FastVectorHighlighter: add a method to set an arbitrary char that is used when concatenating multiValued dataIf the following multiValued names are in authors field:

* Michael McCandless
* Erik Hatcher
* Otis Gospodneti

Since FragmentsBuilder concatenates multiValued data with a space in BaseFragmentsBuilder.getFragmentSource():

{code}
while( buffer.length() < endOffset && index[0] < values.length ){
  if( index[0] > 0 && values[index[0]].isTokenized() && values[index[0]].stringValue().length() > 0 )
    buffer.append( ' ' );
  buffer.append( values[index[0]++].stringValue() );
}
{code}

an entire field snippet (using LUCENE-2464) will be ""Michael McCandless Erik Hatcher Otis Gospodneti"". There is a requirement an arbitrary char (e.g. '/') can be set so that client can separate the snippet easily. i.e. ""Michael McCandless/Erik Hatcher/Otis Gospodneti"""
1,"Xpath query parser accepts ""/a | /b"" and treats it as ""/a/b""The XPath query parser accepts the query

  /a | /b

and parses it into a query tree corresponging the Xpath query

  /a/b

It should be rejected instead."
1,"MMapDirectory can't create new index on WindowsWhen I set the system property to request the use of the mmap directory, and start building a large index, the process dies with an IOException trying to delete a file. Apparently, Lucene isn't closing down the memory map before deleting the file.
"
0,"More details for beginnersHi everyone,

I'm one of these beginners trying to make their way in Jackrabbit and content repositories universe.

Besides the fact that there exist but very few examples on Jackrabbit use, all of them, including the official web site of Jackrabbit miss few details, simple though essential, for beginners.

My first point is the following: once Jackrabbit source is checked out and built, time to test it using simple examples. But, where to put the example directory from the beginning. How to run it (maven java:compile....) ?

A second point is the fact that there is no help forum on the Jackrabbit web site.

 A third point, taking my case as an example, the example would just not create a new workspace configuration.
And many other troubleshoots - I repeat - basic but essential, that could avoid dozens of wasted hours and discouragment, if just mentioned on the website.

Here it is my wish :-) for the best of Jackrabbit ;-) I hope!
Regards,
Celina"
0,Add RAR META-INF/ra.xml descriptor to be used with JCA1.5I added ra.xml that lets jackrabbit jca to be used with JCA1.5 like in JBoss
0,"Similarity.Stats class for term & collection statisticsIn order to support ranking methods besides TF-IDF, we need to make the statistics they need available. These statistics could be computed in computeWeight (soon to become computeStats) and stored in a separate object for easy access. Since this object will be used solely by subclasses of Similarity, it should be implented as a static inner class, i.e. Similarity.Stats.

There are two ways this could be implemented:
- as a single Similarity.Stats class, reused by all ranking algorithms. In this case, this class would have a member field for all statistics;
- as a hierarchy of Stats classes, one for each ranking algorithm. Each subclass would define only the statistics needed for the ranking algorithm.

In the second case, the Stats class in DefaultSimilarity would have a single field, idf, while the one in e.g. BM25Similarity would have idf and average field/document length."
0,"Create EMPTY_ARGS constsant in SnowballProgram instead of allocating new Object[0]Instead of allocating new Object[0] create a proper constant in SnowballProgram. The same (for new Class[0]) is created in Among, although it's less critical because Among is called from static initializers ... Patch will follow shortly."
0,"QueryHandler should use lucene Input-/OutputStream implementationsCurrently the QueryHandler uses a jackrabbit specific implementation of the lucene Directory interface to make use of the jackrabbit FileSystem abstraction. Lucene operations on the file system however requires quite often random access on the index files. With the current FileSystem interface / abstraction random access is not possible on a FileSystemResource, therefore it is simulated by re-aquiring the InputStream and then seeking to the desired position. This it not efficient at all.

With respect to performance any other use than file based index storage does not make sense with lucene. Hence, the current abstraction using FileSystem should be dropped in favour of direct file access."
1,"Null Pointer Exception while looking for a DavProperty that hasn't been setNull pointer exception.
Exception occurs because the DavPropertySet.map does not contain an expected entry: ItemResourceConstants.JCR_NAME

Suggested fix: add the constant to the nameSet in RepositoryServiceImpl.java:760
 nameSet.add(ItemResourceConstants.JCR_NAME);

I tried that and it works. See stack trace at below.

Exception in thread ""main"" java.lang.NullPointerException
  at org.apache.jackrabbit.spi2dav.URIResolverImpl.buildPropertyId(URIResolverImpl.java:201)
  at org.apache.jackrabbit.spi2dav.RepositoryServiceImpl.getNodeInfo(RepositoryServiceImpl.java:808)
  at org.apache.jackrabbit.spi2dav.RepositoryServiceImpl.getItemInfos(RepositoryServiceImpl.java:834)
  at org.apache.jackrabbit.jcr2spi.state.WorkspaceItemStateFactory.createNodeState(WorkspaceItemStateFactory.java:88)
  at org.apache.jackrabbit.jcr2spi.state.TransientISFactory.createNodeState(TransientISFactory.java:99)
  at org.apache.jackrabbit.jcr2spi.hierarchy.NodeEntryImpl.doResolve(NodeEntryImpl.java:959)
  at org.apache.jackrabbit.jcr2spi.hierarchy.HierarchyEntryImpl.resolve(HierarchyEntryImpl.java:95)
  at org.apache.jackrabbit.jcr2spi.hierarchy.HierarchyEntryImpl.getItemState(HierarchyEntryImpl.java:212)
  at org.apache.jackrabbit.jcr2spi.ItemManagerImpl.getItem(ItemManagerImpl.java:157)
  at org.apache.jackrabbit.jcr2spi.SessionImpl.getRootNode(SessionImpl.java:225)
"
1,"JackrabbitIndexReader prevents use of DocNumberCacheThe JackrabbitIndexReader was introduced in 1.5 by JCR-1363. Unfortunately it does not overwrite the method termDocs(Term), which means the default implementation in IndexReader is used. This bypasses the DocNumberCache built into CachingIndexReader, which is used for UUID terms that look up individual documents."
1,"TestIndexWriter.testCommitThreadSafety fails on realtime_search branchHudson failed on RT with this error - I wasn't able to reproduce yet....

{noformat}
NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testCommitThreadSafety -Dtests.seed=410261592077577885:-4099127561715488589 -Dtests.multiplier=3
NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testCommitThreadSafety -Dtests.seed=410261592077577885:-4099127561715488589 -Dtests.multiplier=3
The following exceptions were thrown by threads:
*** Thread: Thread-331 ***
java.lang.RuntimeException: java.lang.AssertionError: term=f:2_0; r=DirectoryReader(segments_6 _8(4.0):Cv7) expected:<1> but was:<0>
	at org.apache.lucene.index.TestIndexWriter$5.run(TestIndexWriter.java:2416)
Caused by: java.lang.AssertionError: term=f:2_0; r=DirectoryReader(segments_6 _8(4.0):Cv7) expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:91)
	at org.junit.Assert.failNotEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:126)
	at org.junit.Assert.assertEquals(Assert.java:470)
	at org.apache.lucene.index.TestIndexWriter$5.run(TestIndexWriter.java:2410)
NOTE: test params are: codec=RandomCodecProvider: {=SimpleText, f6=MockVariableIntBlock(baseBlockSize=91), f7=MockFixedIntBlock(blockSize=1289), f8=Standard, f9=MockRandom, f1=MockSep, f0=Pulsing(freqCutoff=15), f3=Pulsing(freqCutoff=15), f2=MockFixedIntBlock(blockSize=1289), f5=MockVariableIntBlock(baseBlockSize=91), f4=MockRandom, f=MockSep, c=MockVariableIntBlock(baseBlockSize=91), termVector=SimpleText, d9=SimpleText, d8=MockSep, d5=MockVariableIntBlock(baseBlockSize=91), d4=MockRandom, d7=Standard, d6=SimpleText, d25=Standard, d0=MockVariableIntBlock(baseBlockSize=91), c29=Standard, d24=SimpleText, d1=MockFixedIntBlock(blockSize=1289), c28=MockFixedIntBlock(blockSize=1289), d23=MockVariableIntBlock(baseBlockSize=91), d2=Standard, c27=MockVariableIntBlock(baseBlockSize=91), d22=MockRandom, d3=MockRandom, d21=MockFixedIntBlock(blockSize=1289), d20=MockVariableIntBlock(baseBlockSize=91), c22=MockVariableIntBlock(baseBlockSize=91), c21=MockRandom, c20=Pulsing(freqCutoff=15), d29=MockVariableIntBlock(baseBlockSize=91), c26=SimpleText, d28=MockRandom, c25=MockSep, d27=Pulsing(freqCutoff=15), c24=MockRandom, d26=MockFixedIntBlock(blockSize=1289), c23=Standard, e9=MockRandom, e8=MockFixedIntBlock(blockSize=1289), e7=MockVariableIntBlock(baseBlockSize=91), e6=MockSep, e5=Pulsing(freqCutoff=15), c17=Standard, e3=MockFixedIntBlock(blockSize=1289), d12=SimpleText, c16=SimpleText, e4=Pulsing(freqCutoff=15), d11=MockSep, c19=MockSep, e1=MockSep, d14=Pulsing(freqCutoff=15), c18=Pulsing(freqCutoff=15), e2=SimpleText, d13=MockFixedIntBlock(blockSize=1289), e0=Standard, d10=Standard, d19=Pulsing(freqCutoff=15), c11=SimpleText, c10=MockSep, d16=MockRandom, c13=MockSep, c12=Pulsing(freqCutoff=15), d15=Standard, d18=SimpleText, c15=MockFixedIntBlock(blockSize=1289), d17=MockSep, c14=MockVariableIntBlock(baseBlockSize=91), b3=MockRandom, b2=Standard, b5=SimpleText, b4=MockSep, b7=MockSep, b6=Pulsing(freqCutoff=15), d50=MockVariableIntBlock(baseBlockSize=91), b9=MockFixedIntBlock(blockSize=1289), b8=MockVariableIntBlock(baseBlockSize=91), d43=Pulsing(freqCutoff=15), d42=MockFixedIntBlock(blockSize=1289), d41=SimpleText, d40=MockSep, d47=MockRandom, d46=Standard, b0=SimpleText, d45=MockFixedIntBlock(blockSize=1289), b1=Standard, d44=MockVariableIntBlock(baseBlockSize=91), d49=MockSep, d48=Pulsing(freqCutoff=15), c6=MockVariableIntBlock(baseBlockSize=91), c5=MockRandom, c4=Pulsing(freqCutoff=15), c3=MockFixedIntBlock(blockSize=1289), c9=MockSep, c8=MockRandom, c7=Standard, d30=MockFixedIntBlock(blockSize=1289), d32=MockRandom, d31=Standard, c1=MockVariableIntBlock(baseBlockSize=91), d34=Standard, c2=MockFixedIntBlock(blockSize=1289), d33=SimpleText, d36=MockSep, c0=MockSep, d35=Pulsing(freqCutoff=15), d38=MockVariableIntBlock(baseBlockSize=91), d37=MockRandom, d39=SimpleText, e92=MockFixedIntBlock(blockSize=1289), e93=Pulsing(freqCutoff=15), e90=MockSep, e91=SimpleText, e89=MockVariableIntBlock(baseBlockSize=91), e88=MockSep, e87=Pulsing(freqCutoff=15), e86=SimpleText, e85=MockSep, e84=MockRandom, e83=Standard, e80=MockFixedIntBlock(blockSize=1289), e81=Standard, e82=MockRandom, e77=MockVariableIntBlock(baseBlockSize=91), e76=MockRandom, e79=Standard, e78=SimpleText, e73=MockSep, e72=Pulsing(freqCutoff=15), e75=MockFixedIntBlock(blockSize=1289), e74=MockVariableIntBlock(baseBlockSize=91), binary=MockVariableIntBlock(baseBlockSize=91), f98=Pulsing(freqCutoff=15), f97=MockFixedIntBlock(blockSize=1289), f99=MockRandom, f94=Standard, f93=SimpleText, f96=MockSep, f95=Pulsing(freqCutoff=15), e95=SimpleText, e94=MockSep, e97=Pulsing(freqCutoff=15), e96=MockFixedIntBlock(blockSize=1289), e99=MockFixedIntBlock(blockSize=1289), e98=MockVariableIntBlock(baseBlockSize=91), id=MockRandom, f34=Standard, f33=SimpleText, f32=MockVariableIntBlock(baseBlockSize=91), f31=MockRandom, f30=MockFixedIntBlock(blockSize=1289), f39=Standard, f38=MockVariableIntBlock(baseBlockSize=91), f37=MockRandom, f36=Pulsing(freqCutoff=15), f35=MockFixedIntBlock(blockSize=1289), f43=MockSep, f42=Pulsing(freqCutoff=15), f45=MockFixedIntBlock(blockSize=1289), f44=MockVariableIntBlock(baseBlockSize=91), f41=SimpleText, f40=MockSep, f47=Standard, f46=SimpleText, f49=MockSep, f48=Pulsing(freqCutoff=15), content=MockSep, e19=SimpleText, e18=MockSep, e17=Standard, f12=SimpleText, e16=SimpleText, f11=MockSep, f10=MockRandom, e15=MockVariableIntBlock(baseBlockSize=91), e14=MockRandom, f16=MockRandom, e13=MockSep, f15=Standard, e12=Pulsing(freqCutoff=15), e11=Standard, f14=MockFixedIntBlock(blockSize=1289), e10=SimpleText, f13=MockVariableIntBlock(baseBlockSize=91), f19=Pulsing(freqCutoff=15), f18=Standard, f17=SimpleText, e29=MockRandom, e26=MockSep, f21=Pulsing(freqCutoff=15), e25=Pulsing(freqCutoff=15), f20=MockFixedIntBlock(blockSize=1289), e28=MockFixedIntBlock(blockSize=1289), f23=MockVariableIntBlock(baseBlockSize=91), e27=MockVariableIntBlock(baseBlockSize=91), f22=MockRandom, f25=SimpleText, e22=MockFixedIntBlock(blockSize=1289), f24=MockSep, e21=MockVariableIntBlock(baseBlockSize=91), f27=Pulsing(freqCutoff=15), e24=MockRandom, f26=MockFixedIntBlock(blockSize=1289), e23=Standard, f29=MockFixedIntBlock(blockSize=1289), f28=MockVariableIntBlock(baseBlockSize=91), e20=Pulsing(freqCutoff=15), field=MockRandom, string=Standard, e30=MockRandom, e31=MockVariableIntBlock(baseBlockSize=91), a98=Standard, e34=MockSep, a99=MockRandom, e35=SimpleText, f79=MockSep, e32=Standard, e33=MockRandom, b97=MockRandom, f77=MockRandom, e38=Standard, b98=MockVariableIntBlock(baseBlockSize=91), f78=MockVariableIntBlock(baseBlockSize=91), e39=MockRandom, b99=SimpleText, f75=MockFixedIntBlock(blockSize=1289), e36=MockVariableIntBlock(baseBlockSize=91), f76=Pulsing(freqCutoff=15), e37=MockFixedIntBlock(blockSize=1289), f73=Pulsing(freqCutoff=15), f74=MockSep, f71=SimpleText, f72=Standard, f81=MockFixedIntBlock(blockSize=1289), f80=MockVariableIntBlock(baseBlockSize=91), e40=Standard, e41=Pulsing(freqCutoff=15), e42=MockSep, e43=MockFixedIntBlock(blockSize=1289), e44=Pulsing(freqCutoff=15), e45=MockRandom, e46=MockVariableIntBlock(baseBlockSize=91), f86=SimpleText, e47=MockSep, f87=Standard, e48=SimpleText, f88=Pulsing(freqCutoff=15), e49=MockFixedIntBlock(blockSize=1289), f89=MockSep, f82=MockVariableIntBlock(baseBlockSize=91), f83=MockFixedIntBlock(blockSize=1289), f84=Standard, f85=MockRandom, f90=MockRandom, f92=SimpleText, f91=MockSep, str=MockSep, a76=SimpleText, e56=SimpleText, f59=MockVariableIntBlock(baseBlockSize=91), a77=Standard, e57=Standard, a78=Pulsing(freqCutoff=15), e54=MockRandom, f57=Pulsing(freqCutoff=15), a79=MockSep, e55=MockVariableIntBlock(baseBlockSize=91), f58=MockSep, e52=MockVariableIntBlock(baseBlockSize=91), e53=MockFixedIntBlock(blockSize=1289), e50=Pulsing(freqCutoff=15), e51=MockSep, f51=MockFixedIntBlock(blockSize=1289), f52=Pulsing(freqCutoff=15), f50=SimpleText, f55=Standard, f56=MockRandom, f53=MockVariableIntBlock(baseBlockSize=91), e58=MockFixedIntBlock(blockSize=1289), f54=MockFixedIntBlock(blockSize=1289), e59=Pulsing(freqCutoff=15), a80=MockRandom, e60=MockRandom, a82=SimpleText, a81=MockSep, a84=MockSep, a83=Pulsing(freqCutoff=15), a86=MockFixedIntBlock(blockSize=1289), a85=MockVariableIntBlock(baseBlockSize=91), a89=Standard, f68=Standard, e65=Pulsing(freqCutoff=15), f69=MockRandom, e66=MockSep, a87=MockVariableIntBlock(baseBlockSize=91), e67=MockVariableIntBlock(baseBlockSize=91), a88=MockFixedIntBlock(blockSize=1289), e68=MockFixedIntBlock(blockSize=1289), e61=Standard, e62=MockRandom, e63=MockSep, e64=SimpleText, f60=MockRandom, f61=MockVariableIntBlock(baseBlockSize=91), f62=SimpleText, f63=Standard, e69=SimpleText, f64=MockSep, f65=SimpleText, f66=MockFixedIntBlock(blockSize=1289), f67=Pulsing(freqCutoff=15), f70=MockSep, a93=MockVariableIntBlock(baseBlockSize=91), a92=MockRandom, a91=Pulsing(freqCutoff=15), e71=Pulsing(freqCutoff=15), a90=MockFixedIntBlock(blockSize=1289), e70=MockFixedIntBlock(blockSize=1289), a97=SimpleText, a96=MockSep, a95=MockRandom, a94=Standard, c58=MockFixedIntBlock(blockSize=1289), a63=MockVariableIntBlock(baseBlockSize=91), a64=MockFixedIntBlock(blockSize=1289), c59=Pulsing(freqCutoff=15), c56=MockSep, d59=MockRandom, a61=Pulsing(freqCutoff=15), c57=SimpleText, a62=MockSep, c54=SimpleText, c55=Standard, a60=SimpleText, c52=MockRandom, c53=MockVariableIntBlock(baseBlockSize=91), d53=Standard, d54=MockRandom, d51=MockVariableIntBlock(baseBlockSize=91), d52=MockFixedIntBlock(blockSize=1289), d57=Pulsing(freqCutoff=15), b62=MockFixedIntBlock(blockSize=1289), d58=MockSep, b63=Pulsing(freqCutoff=15), d55=SimpleText, b60=MockSep, d56=Standard, b61=SimpleText, b56=SimpleText, b55=MockSep, b54=MockRandom, b53=Standard, d61=SimpleText, b59=MockVariableIntBlock(baseBlockSize=91), d60=MockSep, b58=MockSep, b57=Pulsing(freqCutoff=15), c62=MockSep, c61=Pulsing(freqCutoff=15), a59=Pulsing(freqCutoff=15), c60=Standard, a58=MockFixedIntBlock(blockSize=1289), a57=MockSep, a56=Pulsing(freqCutoff=15), a55=Standard, a54=SimpleText, a72=Standard, c67=MockRandom, a73=MockRandom, c68=MockVariableIntBlock(baseBlockSize=91), a74=MockSep, c69=SimpleText, a75=SimpleText, c63=Pulsing(freqCutoff=15), c64=MockSep, a70=MockRandom, c65=MockVariableIntBlock(baseBlockSize=91), a71=MockVariableIntBlock(baseBlockSize=91), c66=MockFixedIntBlock(blockSize=1289), d62=MockSep, d63=SimpleText, d64=MockFixedIntBlock(blockSize=1289), b70=MockFixedIntBlock(blockSize=1289), d65=Pulsing(freqCutoff=15), b71=MockRandom, d66=MockVariableIntBlock(baseBlockSize=91), b72=MockVariableIntBlock(baseBlockSize=91), d67=MockFixedIntBlock(blockSize=1289), b73=SimpleText, d68=Standard, b74=Standard, d69=MockRandom, b65=Pulsing(freqCutoff=15), b64=MockFixedIntBlock(blockSize=1289), b67=MockVariableIntBlock(baseBlockSize=91), b66=MockRandom, d70=Pulsing(freqCutoff=15), b69=MockRandom, b68=Standard, d72=MockVariableIntBlock(baseBlockSize=91), d71=MockRandom, c71=MockFixedIntBlock(blockSize=1289), c70=MockVariableIntBlock(baseBlockSize=91), a69=SimpleText, c73=MockRandom, c72=Standard, a66=MockFixedIntBlock(blockSize=1289), a65=MockVariableIntBlock(baseBlockSize=91), a68=MockRandom, a67=Standard, c32=MockSep, c33=SimpleText, c30=Standard, c31=MockRandom, c36=MockVariableIntBlock(baseBlockSize=91), a41=MockRandom, c37=MockFixedIntBlock(blockSize=1289), a42=MockVariableIntBlock(baseBlockSize=91), a0=SimpleText, c34=Pulsing(freqCutoff=15), c35=MockSep, a40=Pulsing(freqCutoff=15), b84=Pulsing(freqCutoff=15), d79=MockSep, b85=MockSep, b82=SimpleText, d77=Standard, c38=SimpleText, b83=Standard, d78=MockRandom, c39=Standard, b80=Standard, d75=MockRandom, b81=MockRandom, d76=MockVariableIntBlock(baseBlockSize=91), d73=MockFixedIntBlock(blockSize=1289), d74=Pulsing(freqCutoff=15), d83=Standard, a9=MockSep, d82=SimpleText, d81=MockVariableIntBlock(baseBlockSize=91), d80=MockRandom, b79=MockSep, b78=Standard, b77=SimpleText, b76=MockVariableIntBlock(baseBlockSize=91), b75=MockRandom, a1=SimpleText, a35=Pulsing(freqCutoff=15), a2=Standard, a34=MockFixedIntBlock(blockSize=1289), a3=Pulsing(freqCutoff=15), a33=SimpleText, a4=MockSep, a32=MockSep, a5=MockFixedIntBlock(blockSize=1289), a39=MockRandom, c40=Pulsing(freqCutoff=15), a6=Pulsing(freqCutoff=15), a38=Standard, a7=MockRandom, a37=MockFixedIntBlock(blockSize=1289), a8=MockVariableIntBlock(baseBlockSize=91), a36=MockVariableIntBlock(baseBlockSize=91), c41=MockFixedIntBlock(blockSize=1289), c42=Pulsing(freqCutoff=15), c43=MockRandom, c44=MockVariableIntBlock(baseBlockSize=91), c45=Standard, a50=SimpleText, c46=MockRandom, a51=Standard, c47=MockSep, a52=Pulsing(freqCutoff=15), c48=SimpleText, a53=MockSep, b93=MockVariableIntBlock(baseBlockSize=91), d88=MockFixedIntBlock(blockSize=1289), c49=MockVariableIntBlock(baseBlockSize=91), b94=MockFixedIntBlock(blockSize=1289), d89=Pulsing(freqCutoff=15), b95=Standard, b96=MockRandom, d84=SimpleText, b90=SimpleText, d85=Standard, b91=MockFixedIntBlock(blockSize=1289), d86=Pulsing(freqCutoff=15), b92=Pulsing(freqCutoff=15), d87=MockSep, d92=MockSep, d91=Pulsing(freqCutoff=15), d94=MockFixedIntBlock(blockSize=1289), d93=MockVariableIntBlock(baseBlockSize=91), b87=MockSep, b86=Pulsing(freqCutoff=15), d90=SimpleText, b89=MockFixedIntBlock(blockSize=1289), b88=MockVariableIntBlock(baseBlockSize=91), a44=MockVariableIntBlock(baseBlockSize=91), a43=MockRandom, a46=Standard, a45=SimpleText, a48=SimpleText, a47=MockSep, c51=Standard, a49=MockFixedIntBlock(blockSize=1289), c50=SimpleText, d98=MockFixedIntBlock(blockSize=1289), d97=MockVariableIntBlock(baseBlockSize=91), d96=MockSep, d95=Pulsing(freqCutoff=15), d99=MockRandom, a20=MockRandom, c99=MockVariableIntBlock(baseBlockSize=91), c98=MockRandom, c97=Pulsing(freqCutoff=15), c96=MockFixedIntBlock(blockSize=1289), b19=MockVariableIntBlock(baseBlockSize=91), a16=SimpleText, a17=Standard, b17=Pulsing(freqCutoff=15), a14=MockRandom, b18=MockSep, a15=MockVariableIntBlock(baseBlockSize=91), a12=MockVariableIntBlock(baseBlockSize=91), a13=MockFixedIntBlock(blockSize=1289), a10=Pulsing(freqCutoff=15), a11=MockSep, b11=MockFixedIntBlock(blockSize=1289), b12=Pulsing(freqCutoff=15), b10=SimpleText, b15=Standard, b16=MockRandom, a18=MockFixedIntBlock(blockSize=1289), b13=MockVariableIntBlock(baseBlockSize=91), a19=Pulsing(freqCutoff=15), b14=MockFixedIntBlock(blockSize=1289), b30=MockSep, a31=Pulsing(freqCutoff=15), a30=MockFixedIntBlock(blockSize=1289), b28=Standard, a25=Pulsing(freqCutoff=15), b29=MockRandom, a26=MockSep, a27=MockVariableIntBlock(baseBlockSize=91), a28=MockFixedIntBlock(blockSize=1289), a21=Standard, a22=MockRandom, a23=MockSep, a24=SimpleText, b20=MockRandom, b21=MockVariableIntBlock(baseBlockSize=91), b22=SimpleText, b23=Standard, a29=SimpleText, b24=MockSep, b25=SimpleText, b26=MockFixedIntBlock(blockSize=1289), b27=Pulsing(freqCutoff=15), b41=MockFixedIntBlock(blockSize=1289), b40=MockVariableIntBlock(baseBlockSize=91), c77=MockRandom, c76=Standard, c75=MockFixedIntBlock(blockSize=1289), c74=MockVariableIntBlock(baseBlockSize=91), c79=Standard, c78=SimpleText, c80=MockVariableIntBlock(baseBlockSize=91), c83=MockSep, c84=SimpleText, c81=Standard, b39=MockSep, c82=MockRandom, b37=MockRandom, b38=MockVariableIntBlock(baseBlockSize=91), b35=MockFixedIntBlock(blockSize=1289), b36=Pulsing(freqCutoff=15), b33=Pulsing(freqCutoff=15), b34=MockSep, b31=SimpleText, b32=Standard, str2=Pulsing(freqCutoff=15), b50=MockRandom, b52=SimpleText, str3=MockRandom, b51=MockSep, c86=SimpleText, tvtest=MockSep, c85=MockSep, c88=Pulsing(freqCutoff=15), c87=MockFixedIntBlock(blockSize=1289), c89=MockVariableIntBlock(baseBlockSize=91), c90=Pulsing(freqCutoff=15), c91=MockSep, c92=MockFixedIntBlock(blockSize=1289), c93=Pulsing(freqCutoff=15), c94=MockRandom, c95=MockVariableIntBlock(baseBlockSize=91), content1=MockSep, b46=SimpleText, b47=Standard, content3=Standard, b48=Pulsing(freqCutoff=15), content4=SimpleText, b49=MockSep, content5=MockRandom, b42=MockVariableIntBlock(baseBlockSize=91), b43=MockFixedIntBlock(blockSize=1289), b44=Standard, b45=MockRandom}, locale=lv_LV, timezone=Australia/Lindeman
NOTE: all tests run in this JVM:
[TestNumericTokenStream, TestIndexFileDeleter, TestIndexInput, TestIndexReaderCloneNorms, TestIndexReaderReopen, TestIndexWriter]
NOTE: FreeBSD 8.2-RELEASE amd64/Sun Microsystems Inc. 1.6.0 (64-bit)/cpus=16,threads=1,free=44228576,total=213778432
{noformat}"
0,"Wrong schemaObjectPrefix parameter in default repository.xmlThe object schema prefix is hard-coded in the default configuration file (I think this taken from the jackrabbit-core.jar):

        <PersistenceManager class=""org.apache.jackrabbit.core.persistence.bundle.DerbyPersistenceManager"">
          <param name=""url"" value=""jdbc:derby:${wsp.home}/db;create=true""/>
          <param name=""schemaObjectPrefix"" value=""Jackrabbit Core_""/>
        </PersistenceManager>

This is probably caused by JCR-945, though I've no idea why ${wsp.name} is replaced with the name of the module...

I have marked this issue as minor because it still works with the DerbyPersistenceManager. There are separate database instances for each workspace, but it will become a problem if a data base persistence manager on a dedicated server is used."
1,"SloppyPhraseScorer returns non-deterministic results for queries with many repeatsProximity queries with many repeats (four or more, based on my testing) return non-deterministic results. I run the same query multiple times with the same data set and get different results.

So far I've reproduced this with Solr 1.4.1, 3.1, 3.2, 3.3, and latest 4.0 trunk.

Steps to reproduce (using the Solr example):
1) In solrconfig.xml, set queryResultCache size to 0.
2) Add some documents with text ""dog dog dog"" and ""dog dog dog dog"". http://localhost:8983/solr/update?stream.body=%3Cadd%3E%3Cdoc%3E%3Cfield%20name=%22id%22%3E1%3C/field%3E%3Cfield%20name=%22text%22%3Edog%20dog%20dog%3C/field%3E%3C/doc%3E%3Cdoc%3E%3Cfield%20name=%22id%22%3E2%3C/field%3E%3Cfield%20name=%22text%22%3Edog%20dog%20dog%20dog%3C/field%3E%3C/doc%3E%3C/add%3E&commit=true
3) Do a ""dog dog dog dog""~1 query. http://localhost:8983/solr/select?q=%22dog%20dog%20dog%20dog%22~1
4) Repeat step 3 many times.

Expected results: The document with id 2 should be returned.

Actual results: The document with id 2 is always returned. The document with id 1 is sometimes returned.

Different proximity values show the same bug - ""dog dog dog dog""~5, ""dog dog dog dog""~100, etc show the same behavior.

So far I've traced it down to the ""repeats"" array in SloppyPhraseScorer.initPhrasePositions() - depending on the order of the elements in this array, the document may or may not match. I think the HashSet may be to blame, but I'm not sure - that at least seems to be where the non-determinism is coming from."
0,"move DocumentStoredFieldsVisitor to o.a.l.documentwhen examining the changes to the field/document API, i noticed this class was in o.a.l.index

I think it should be in o.a.l.document, its more intuitive packaging"
0,"Add AttributeSource.copyTo(AttributeSource)One problem with AttributeSource at the moment is the missing ""insight"" into AttributeSource.State. If you want to create TokenStreams that inspect cpatured states, you have no chance. Making the contents of State public is a bad idea, as it does not help for inspecting (its a linked list, so you have to iterate).

AttributeSource currently contains a cloneAttributes() call, which returns a new AttrubuteSource with all current attributes cloned. This is the (more expensive) captureState. The problem is that you cannot copy back the cloned AS (which is the restoreState). To use this behaviour (by the way, ShingleMatrix can use it), one can alternatively use cloneAttributes and copyTo. You can easily change the cloned attributes and store them in lists and copy them back. The only problem is lower performance of these calls (as State is a very optimized class).

One use case could be:
{code}
AttributeSource state = cloneAttributes();
// .... do something ...
state.getAttribute(TermAttribute.class).setTermBuffer(foobar);
// ... more work
state.copyTo(this);
{code}"
1,"ObjectIterator may return null, which is not readily expected from an IteratorThe ObjectIterator class implements an Iterator of objects mapped from an underlying NodeIterator. This ObjectIterator may return null from next() if no mapping for a node in the iterator exists. Rather than returning null, the iterator should probably just ignore the unmappable node and return an object from the next node in the underlying iterator which is mappable."
0,"new merge policyNew merge policy developed in the course of 
http://issues.apache.org/jira/browse/LUCENE-565
http://issues.apache.org/jira/secure/attachment/12340475/newMergePolicy.Sept08.patch"
1,Jcr-Server: ItemDefinitionImpl.toXml throws NPE for the root node.ItemDefinitionImpl.toXml throws NPE for the root node due to a missing assertion regarding the declaring nodetype.
0,"Node.setPrimaryNodeType should only redefine child-definitions that are not covered by the new effective ntNodeImpl.setPrimaryNodeType changes the primary node type of an node and resets the definition of child items if required. Currently all child items that are not part of the effective node type of the new primary type get their definition reset or are removed in case not matching definition is found.
From my point of view this doesn't properly cope with mixin types present on the node: child items defined by any of the mixin node types present should probably not be touched (or removed).

I run into this while testing the latest 283 security changes and will try to provide a fix along with those changes."
0,"Always use bulk-copy when merging stored fields and term vectorsLucene has nice optimizations in place during merging of stored fields
(LUCENE-1043) and term vectors (LUCENE-1120) whereby the bytes are
bulk copied to the new segmetn.  This is much faster than decoding &
rewriting one document at a time.

However the optimization is rather brittle: it relies on the mapping
of field name to number to be the same (""congruent"") for the segment
being merged.

Unfortunately, the field mapping will be congruent only if the app
adds the same fields in precisely the same order to each document.

I think we should fix IndexWriter to assign the same field number for
a given field that has been assigned in the past.  Ie, when writing a
new segment, we pre-seed the field numbers based on past segments.
All other aspects of FieldInfo would remain fully dynamic."
1,"DataStore: garbage collection can fail when using workspace maxIdleTimeThe GarbageCollectorTest fails because some workspaces have an idle timeout. The data store garbage collector should prevent workspace close-on-idle.

Proposed solution: instead of using the 'regular' system sessions in the garbage collector, use special 'registered system sessions'. The sessions get garbage collected when no longer used, that means this patch requires that JCR-1216 ""Unreferenced sessions should get garbage collected"" is applied. So for each workspace, the code is:

// this will initialize the workspace if required
wspInfo.getSystemSession();

SessionImpl session = SystemSession.create(rep, wspInfo.getConfig());
// mark this session as 'active' for so the workspace does
// not get disposed by workspace-janitor until the garbage collector is done
rep.onSessionCreated(session);            
"
0,SPNEGO authentication schemeConsider integrating the SPNEGO auth scheme from Commons HttpClient contrib package into HttpClient 4.0
1,"RepositoryCopier does not copy open-scoped LocksIf you use the RepositoryCopier to make a backup of your repository and you have open-scoped (not session scoped) locks, these locks will not be copied. If you try to restore your copy of the repository all locks are gone."
1,"problem with edgengramtokenfilter and highlighteri ran into a problem while using the edgengramtokenfilter, it seems to report incorrect offsets when generating tokens, more specifically all the tokens have offset 0 and term length as start and end, this leads to goofy highlighting behavior when creating edge grams for tokens beyond the first one, i created a small patch that takes into account the start of the original token and adds that to the reported start/end offsets.

"
1,"SSL connections cannot be established using resolvable IP addressHttpClient 4.1 introduced a regression in establishing SSL connections to remote peers (it seems this is a common regression for major httpclient updates, see HTTPCLIENT-803).
The new SSLSocketFactory.connectSocket method calls the X509HostnameVerifier with InetSocketAddress.getHostName() parameter. When the selected IP address has a reverse lookup name, the verifier is called with the resolved name, and so the IP check fails.
4.0 release checked for original ip/hostname, but this cannot be done with the new connectSocket() method. 
The TestHostnameVerifier.java only checks 127.0.0.1/.2 and so masked the issue, because the matching certificate has both ""localhost"" and ""127.0.0.1"", but actually only ""localhost"" is matched. A test case with 8.8.8.8 would be better."
0,Add pattern matching for pathsI suggest to add utility classes to spi-commons which can be used to do pattern matching on paths similar to regular expressions. 
1,Extra </div> in populate.jspThe populate.jsp page in jackrabbit-webapp has an extra </div> that causes minor breakage to the page layout.
0,"Contributed ClassLoader project still uses commons-logging for logging.As of JCR-215 Jackrabbit core code has been migrated from Log4J to SLF4J. The ClassLoader contribution always used commons-logging. It is about time, to also migrate that project to proper SLF4J."
0,"Filter to process output of ICUTokenizer and create overlapping bigrams for CJK The ICUTokenizer produces unigrams for CJK. We would like to use the ICUTokenizer but have overlapping bigrams created for CJK as in the CJK Analyzer.  This filter would take the output of the ICUtokenizer, read the ScriptAttribute and for selected scripts (Han, Kana), would produce overlapping bigrams."
1,"Request with DIGEST authentication fails when redirectedRequest with DIGEST authentication fails when redirected due to invalid URI
parameter.

-- Client side log ----------------------------------------------------------

[DEBUG] HttpClient - -Java version: 1.2.2
[DEBUG] HttpClient - -Java vendor: Sun Microsystems Inc.
[DEBUG] HttpClient - -Operating system name: Linux
[DEBUG] HttpClient - -Operating system architecture: i386
[DEBUG] HttpClient - -Operating system version: 2.4.20-13.9-ok
[DEBUG] HttpClient - -SUN 1.2: SUN (DSA key/parameter generation; DSA signing;
SHA-1, MD5 digests; SecureRandom; X.509 certificates; JKS keystore)
[DEBUG] HttpClient - -SunJSSE 1.0301: Sun JSSE provider(implements RSA
Signatures, PKCS12, SunX509 key/trust factories, SSLv3, TLSv1)
[DEBUG] HttpConnection - -Creating connection for localhost using protocol http:80
[DEBUG] HttpConnection - -HttpConnection.setSoTimeout(0)
[DEBUG] HttpMethod - -Execute loop try 1
[DEBUG] wire - ->> ""GET /transfer HTTP/1.1[\r][\n]""
[DEBUG] HttpMethod - -Adding Host request header
[DEBUG] wire - ->> ""User-Agent: Jakarta Commons-HttpClient/2.0beta1[\r][\n]""
[DEBUG] wire - ->> ""Host: localhost[\r][\n]""
[DEBUG] wire - ->> ""[\r][\n]""
[DEBUG] wire - -<< ""HTTP/1.1 401 Authorization Required[\r][\n]""
[DEBUG] wire - -<< ""Date: Fri, 20 Jun 2003 08:30:06 GMT[\r][\n]""
[DEBUG] wire - -<< ""Server: Apache/2.0.40 (Red Hat Linux)[\r][\n]""
[DEBUG] wire - -<< ""WWW-Authenticate: Digest realm=""guest realm"",
nonce=""ei+T7oPAAwA=53c8e6d609ff81a8dcbc370b51f8aadec565009a"", algorithm=MD5,
domain=""/transfer"", qop=""auth""[\r][\n]""
[DEBUG] wire - -<< ""Vary: accept-language[\r][\n]""
[DEBUG] wire - -<< ""Accept-Ranges: bytes[\r][\n]""
[DEBUG] wire - -<< ""Content-Length: 1285[\r][\n]""
[DEBUG] wire - -<< ""Content-Type: text/html; charset=ISO-8859-1[\r][\n]""
[DEBUG] HttpMethod - -Authorization required
[DEBUG] HttpAuthenticator - -Using 'guest realm' authentication realm
[DEBUG] HttpMethod - -HttpMethodBase.execute(): Server demanded authentication
credentials, will try again.
...
[DEBUG] HttpMethod - -Resorting to protocol version default close connection policy
[DEBUG] HttpMethod - -Should NOT close connection, using HTTP/1.1.
[DEBUG] HttpMethod - -Execute loop try 2
[DEBUG] wire - ->> ""GET /transfer HTTP/1.1[\r][\n]""
[DEBUG] HttpMethod - -Request to add Host header ignored: header already added
[DEBUG] wire - ->> ""User-Agent: Jakarta Commons-HttpClient/2.0beta1[\r][\n]""
[DEBUG] wire - ->> ""Host: localhost[\r][\n]""
[DEBUG] wire - ->> ""Authorization: Digest username=""guest"", realm=""guest realm"",
nonce=""ei+T7oPAAwA=53c8e6d609ff81a8dcbc370b51f8aadec565009a"", uri=""/transfer"",
qop=""auth"", algorithm=""MD5"", nc=00000001,
cnonce=""81d4b905a4e9def944beaed8daf79283"",
response=""71394edcddf4bcee6237ea4bb50cfaa5""[\r][\n]""
[DEBUG] wire - ->> ""[\r][\n]""
[DEBUG] wire - -<< ""HTTP/1.1 301 Moved Permanently[\r][\n]""
[DEBUG] wire - -<< ""Date: Fri, 20 Jun 2003 08:30:06 GMT[\r][\n]""
[DEBUG] wire - -<< ""Server: Apache/2.0.40 (Red Hat Linux)[\r][\n]""
[DEBUG] wire - -<< ""Location: http://localhost/transfer/[\r][\n]""
[DEBUG] wire - -<< ""Content-Length: 302[\r][\n]""
[DEBUG] wire - -<< ""Content-Type: text/html; charset=iso-8859-1[\r][\n]""
[DEBUG] HttpMethod - -Redirect required
[DEBUG] HttpMethod - -Redirect requested to location 'http://localhost/transfer/'
[DEBUG] HttpMethod - -Redirecting from 'http://localhost:80/transfer' to
'http://localhost/transfer/
...
[DEBUG] HttpMethod - -Resorting to protocol version default close connection policy
[DEBUG] HttpMethod - -Should NOT close connection, using HTTP/1.1.
[DEBUG] HttpMethod - -Execute loop try 3
[DEBUG] wire - ->> ""GET /transfer/ HTTP/1.1[\r][\n]""
[DEBUG] HttpMethod - -Request to add Host header ignored: header already added
[DEBUG] wire - ->> ""User-Agent: Jakarta Commons-HttpClient/2.0beta1[\r][\n]""
[DEBUG] wire - ->> ""Host: localhost[\r][\n]""
[DEBUG] wire - ->> ""Authorization: Digest username=""guest"", realm=""guest realm"",
nonce=""ei+T7oPAAwA=53c8e6d609ff81a8dcbc370b51f8aadec565009a"", uri=""/transfer"",
qop=""auth"", algorithm=""MD5"", nc=00000001,
cnonce=""81d4b905a4e9def944beaed8daf79283"",
response=""71394edcddf4bcee6237ea4bb50cfaa5""[\r][\n]""
[DEBUG] wire - ->> ""[\r][\n]""
[DEBUG] wire - -<< ""HTTP/1.1 400 Bad Request[\r][\n]""
[DEBUG] wire - -<< ""Date: Fri, 20 Jun 2003 08:30:06 GMT[\r][\n]""
[DEBUG] wire - -<< ""Server: Apache/2.0.40 (Red Hat Linux)[\r][\n]""
[DEBUG] wire - -<< ""Vary: accept-language[\r][\n]""
[DEBUG] wire - -<< ""Accept-Ranges: bytes[\r][\n]""
[DEBUG] wire - -<< ""Content-Length: 973[\r][\n]""
[DEBUG] wire - -<< ""Connection: close[\r][\n]""
[DEBUG] wire - -<< ""Content-Type: text/html; charset=ISO-8859-1[\r][\n]""

-- End of client side log -----------------------------------------------------


-- Server side log ------------------------------------------------------------

[Fri Jun 20 10:30:06 2003] [error] [client 127.0.0.1] Digest: uri mismatch -
</transfer> does not match request-uri </transfer/>

-- End of server side log -----------------------------------------------------"
0,"Lucene Search not scallingI've noticed that when doing thousands of searches in a single thread the average time is quite low i.e. a few milliseconds. When adding more concurrent searches doing exactly the same search the average time increases drastically. 
I've profiled the search classes and found that the whole of lucene blocks on 

org.apache.lucene.index.SegmentCoreReaders.getTermsReader
org.apache.lucene.util.VirtualMethod
  public synchronized int getImplementationDistance 
org.apache.lucene.util.AttributeSourcew.getAttributeInterfaces

These cause search times to increase from a few milliseconds to up to 2 seconds when doing 500 concurrent searches on the same in memory index. Note: That the index is not being updates at all, so not refresh methods are called at any stage.


Some questions:
  Why do we need synchronization here?
  There must be a non-lockable solution for these, they basically cause lucene to be ok for single thread applications but disastrous for any concurrent implementation.

I'll do some experiments by removing the synchronization from the methods of these classes."
0,Add getTotalSize() to QueryResultsAs discussed in http://www.nabble.com/Total-size-of-a-query-result-and-setLimit%28%29-tf4280909.html#a12185543 a getTotalSize() method should be added to QueryResults.
0,"Update license termsCopyright 1999-2003 The Apache Software Foundation.

   Licensed under the Apache License, Version 2.0 (the ""License"");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an ""AS IS"" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License."
0,"Split PrivilegeRegistry in a per-session manager instance and a repository level registryin order to resolve the privilegeregistry related TODOs within jackrabbit-core, i would like to split off those 
methods from PrivilegeRegistry  that are used on a per-session level (including jcr-names) and add them
to a manager class that was present with each session context. consequently the responsibility of the
registry was then limited to read/build the privilege definitions and would be present on the repositorycontext
deprecating those methods that would be covered by the manager).
in addition the naming was then consistent with what we use to have for nodetypes and namespaces."
0,"allow different strategies when checking CN of x509 certWe're now doing a decent job for checking the CN of the x509 cert with https:

http://issues.apache.org/jira/browse/HTTPCLIENT-613

I think the patch for HTTPCLIENT-613 should cover 99.9% of the users out there.  But there are some more esoteric possibilities, so I think Oleg is right.  We need to let the user change the strategy, or provide their own strategy if they want to. 

Some additional things to think about:

- http://wiki.cacert.org/wiki/VhostTaskForce !!!   CN is depreciated?!?!   (I am not able to find a popular website on HTTPS that isn't using CN!)

- [*.example.com] matches subdomains [a.b.example.com] on Firefox, but not IE6.  The patch for HTTPCLIENT-613 allows subdomains.

- Should we support multiple CN's in the subject?

- Should we support ""subjectAltName=DNS:www.example.com"" ?  Should we support lots of them in a single cert?

- Should we support a mix of CN and subjectAltName?


If we do create some alternate strategies for people to try, I'd probably lean towards something like this:

X509NameCheckingStrategy.SUN_JAVA_6  (default)
X509NameCheckingStrategy.FIREFOX2
X509NameCheckingStrategy.IE7
X509NameCheckingStrategy.FIRST_CN_AND_NO_WILDCARDS   (aka ""STRICT"")

"
0,"New Preferences ArchitectureAn architectural solution is needed to configure various aspects of HttpClient,
Methods and Connections. 

Features:
- can configure certain properties per request / per connection
- all configuration is done in a consistant way 
- do not use system properties
- configuration is completely optional: default values should be used if no
configuration is made

This is a refactoring request / reminder. File configuration issues as
dependencies of this bug."
0,Reorganize test suitesI'd like to better organize the test setup in jackrabbit-core. The current test repository is located in applications/test and managed with custom ant tasks and explicit do_init/do_test surefire configuration. It would be better to have the test repository located in target/repository (with template content coming from src/test/repository) and managed using the Maven 2 integration test lifecycle phases.
1,"MultiThreadedHttpConnectionManager does not properly respond to thread interruptsMultiThreadedHttpConnectionManager uses interrupts to notify waiting threads when a connection is ready for them. Issues arise if the threads are interrupted by someone else while they are still waiting on a thread, because doGetConnection does not remove the threads from the queue of waiting threads when they are interrupted:

                        connectionPool.wait(timeToWait);

                        // we have not been interrupted so we need to remove ourselves from the 
                        // wait queue
                        hostPool.waitingThreads.remove(waitingThread);                        connectionPool.waitingThreads.remove(waitingThread);
                    } catch (InterruptedException e) {
                        // do nothing                    } finally {
                        if (useTimeout) {
                            endWait = System.currentTimeMillis();
                            timeToWait -= (endWait - startWait);                        }                    }

Under ordinary circumstances, the queue maintenance is done by the notifyWaitingThread method. However, if the thread is interrupted by any other part of the system, it will (1) not actually be released, since the loop in doGetConnection will force it back to the wait, and (2) will be added the waiting thread to the queue repeatedly, which basically means that the thread will eventually receive the interrupt from notifyWaitingThread at some later point, when it is no longer actually waiting for a connection.

This code could probably be re-architected to make it less error-prone, but the fundamental issue seems to be the use of interrupts to signal waiting threads, as opposed to something like a notify. "
1,"Value#getBinary() and #getStream() return internal representation for type PATH and NAMEjust found a path-related spi2dav test failing that passed some time before jackrabbit 2.0 (BatchTest#testSetPathValue).

i had a quick look at it and it seems to me that the reasons is the internal (Path, Name) value representation 
being exposed when calling Value#getBinary(), Value#getStream() and the corresponding shortcuts on Property.

from my understanding of the specification these methods should always return the standard JCR path (or name) representation as it
is exposed by Value#getString() and Property#getString() as it used to be in previous versions.



"
0,"JCRTest.java (First Steps example code): to few parameters in session.importXMLIn the code on the First Steps page:

if (!rn.hasNode(""importxml"")) {
        System.out.println(""importing xml"");
        Node n=rn.addNode(""importxml"", ""nt:unstructured"");
        session.importXML(""/importxml"", new FileInputStream(""repotest/test.xml""));
        session.save();
      }

The importXML needs a third parameter, compare to: 

http://www.day.com/maven/jsr170/javadocs/jcr-1.0/javax/jcr/Session.html

This prevents the code from the First Steps page from compiling."
0,DisjunctionMaxQuery -  Iterator code to  for ( A  a : container ) constructFor better readability  - converting the Iterable<T> to  for ( A  a : container ) constructs that is more intuitive to read. 
0,"BooleanScorer should not limit number of prohibited clausesToday it's limited to 32, because it uses a separate bit in the mask
for each clause.

But I don't understand why it does this; I think all prohibited
clauses can share a single boolean/bit?  Any match on a prohibited
clause sets this bit and the doc is not collected; we don't need each
prohibited clause to have a dedicated bit?

We also use the mask for required clauses, but this code is now
commented out (we always use BS2 if there are any required clauses);
if we re-enable this code (and I think we should, at least in certain
cases: I suspect it'd be faster than BS2 in many cases), I think we
can cutover to an int count instead of bit masks, and then have no
limit on the required clauses sent to BooleanScorer also.

Separately I cleaned a few things up about BooleanScorer: all of the
embedded scorer methods (nextDoc, docID, advance, score) now throw
UOE; pre-allocate the buckets instead of doing it lazily
per-sub-collect.
"
1,"[PATCH] Javadoc improvements and minor fixesJavadoc improvements for Scorer.java and Weight.java. 
This also fixes some recent changes introduced minor warnings when building 
the javadocs and adds a small comment in Similarity.java. 
The individual patches will be attached."
0,"Make open scoped locks recoverableThe lock tokens for open scoped locks are currently tied to the session which created the lock. If the session dies (for whatever reason) there is no way to recover the lock and unlock the node.
There is a theoretical way of adding the lock token to another session, but in most cases the lock token is not available.

Fortunately, the spec allows to relax this behaviour and I think it would make sense to allow all sessions from the same user to unlock the node - this is still in compliance with the spec but would make unlocked locked nodes possible in a programmatic way."
0,Add Compact Namespace and Node Type Definition support to spi-commonsAdd support for reading and writing of Compact Namespace and Node Type Definitions (cnd-files) to spi-commons. 
0,"DocValuesField should not overload setInt/setFloat etcSee my description on LUCENE-3687. In general we should avoid this for primitive types and give them each unique names.

So I think instead of setInt(byte), setInt(short), setInt(int), setInt(long), setFloat(float) and setFloat(double),
we should have setByte(byte), setShort(short), setInt(int), setLong(long), setFloat(float) and setDouble(double)."
0,"[PATCH] unnecessary synchronized collections used only in thread safe wayNodeTypeReader uses Vector in only a local variable thread safe way. Thus the synchronized value of Vector is not needed, and just slowing the code down for nothing. this patch switches the collections to ArrayLists."
1,"Handling of multiple residual prop defs in EffectiveNodeTypeImplorg.apache.jackrabbit.jcr2spi.nodetype.EffectiveNodeTypeImpl currently rejects multiple residual property definitions, if they do not differ in getMultiple(). In fact, it should accept all combinations, so differing values for getOnParentVersionAction and other aspects should be accepted as well.

See JSR 170, 6.7.8:

""For purposes of the above, the notion of two definitions having the same name does not apply to two residual definitions. Two (or more) residual property or child node definitions with differing subattributes must be permitted to co-exist in the same effective node type. They are interpreted as disjunctive (ORed) options."""
0,"Enable access to the freq information in a Query's sub-scorersThe ability to gather more details than just the score, of how a given
doc matches the current query, has come up a number of times on the
user's lists.  (most recently in the thread ""Query Match Count"" by
Ryan McV on java-user).

EG if you have a simple TermQuery ""foo"", on each hit you'd like to
know how many times ""foo"" occurred in that doc; or a BooleanQuery +foo
+bar, being able to separately see the freq of foo and bar for the
current hit.

Lucene doesn't make this possible today, which is a shame because
Lucene in fact does compute exactly this information; it's just not
accessible from the Collector.
"
0,"Code cleanup from all sorts of (trivial) warningsI would like to do some code cleanup and remove all sorts of trivial warnings, like unnecessary casts, problems w/ javadocs, unused variables, redundant null checks, unnecessary semicolon etc. These are all very trivial and should not pose any problem.

I'll create another issue for getting rid of deprecated code usage, like LuceneTestCase and all sorts of deprecated constructors. That's also trivial because it only affects Lucene code, but it's a different type of change.

Another issue I'd like to create is about introducing more generics in the code, where it's missing today - not changing existing API. There are many places in the code like that.

So, with you permission, I'll start with the trivial ones first, and then move on to the others."
0,Use FieldSelector in Sorted/LuceneQueryHits when reading UUIDLuceneQueryHits currently reads the complete lucene document. This also prevents usage of an underlying UUID cache.
1,"Session.save() and Session.refresh(boolean) rely on accessibility of the root nodefollow-up issue to JCR-2418:

an editing session that is only allowed to write in a subtree but isn't allowed to access the root node will not be
able to save or revert changes made in the transient space within that subtree.

the reason for this is, that both SessionImpl.save() and SessionImpl.refresh(boolean) access the root node
in order to execute the call. since it's the regular call READ permissions are checked, although the user
made no attempt to *look* at the root.

A workaround would be to call Item.save() on the modified tree itself that obviously was visible for the 
user... unfortunately that method is deprecated as of JCR 2.0. Therefore, I have the impression that we
should fix the methods mentioned above.

"
1,"incorrect HTML excerpt generation for queries on japanese text content The generated excerpt highlights single characters instead of full words. Test case (to be added to FullTextQueryTest):

     public void testJapaneseAndHighlight() throws RepositoryException {
        // http://translate.google.com/#auto|en|%E3%82%B3%E3%83%B3%E3%83%86%E3%83%B3%E3%83%88
        String jContent = ""\u30b3\u30fe\u30c6\u30f3\u30c8"";
        // http://translate.google.com/#auto|en|%E3%83%86%E3%82%B9%E3%83%88
        String jTest = ""\u30c6\u30b9\u30c8"";
        
        String content = ""some text with japanese: "" + jContent
                + "" ('content')"" + "" and "" + jTest + "" ('test')."";

        // expected excerpt; note this may change if excerpt providers change
        String expectedExcerpt = ""<div><span>some text with japanese: "" + jContent
                + "" ('content') and <strong>"" + jTest
                + ""</strong> ('test').</span></div>"";
        
        Node n = testRootNode.addNode(""node1"");
        n.setProperty(""title"", content);
        testRootNode.getSession().save();
        
        String xpath = ""/jcr:root"" + testRoot + ""/element(*, nt:unstructured)""
                + ""[jcr:contains(., '"" + jTest + ""')]/rep:excerpt(.)"";
        Query q = superuser.getWorkspace().getQueryManager()
                .createQuery(xpath, Query.XPATH);
        
        QueryResult qr = q.execute();
        RowIterator it = qr.getRows();
        int cnt = 0;
        while (it.hasNext()) {
            cnt++;
            Row found = it.nextRow();
            assertEquals(n.getPath(), found.getPath());
            String excerpt = found.getValue(""rep:excerpt(.)"").getString();
            assertEquals(expectedExcerpt, excerpt);
        }
        
        assertEquals(1, cnt);
    }
"
0,Remove the unneeded cqfs dependenciesThere's still unneeded dependencies to the cqfs libraries in jcr-server/webapp and jca.
1,"(Parallel-)MultiSearcher: using Sort object changes the scoresExample: 
Hits hits=multiSearcher.search(query);
returns different scores for some documents than
Hits hits=multiSearcher.search(query, Sort.RELEVANCE);
(both for MultiSearcher and ParallelMultiSearcher)

The documents returned will be the same and in the same order, but the scores in the second case will seem out of order.

Inspecting the Explanation objects shows that the scores themselves are ok, but there's a bug in the normalization of the scores.

The document with the highest score should have score 1.0, so all document scores are divided by the highest score.  (Assuming the highest score was>1.0)

However, for MultiSearcher and ParallelMultiSearcher, this normalization factor is applied *per index*, before merging the results together (the merge itself is ok though).

An example: if you use
Hits hits=multiSearcher.search(query, Sort.RELEVANCE);
for a MultiSearcher with two subsearchers, the first document will have score 1.0.
The next documents from the same subsearcher will have decreasing scores.
The first document from the other subsearcher will however have score 1.0 again !

The same applies for other Sort objects, but it is less visible.

I will post a TestCase demonstrating the problem and suggested patches to solve it in a moment..."
0,"AbstractRecord does inefficient List.indexOf()AbstractRecord keeps a list of already used UUIDs and references
them by index when used again in a record. Using a List does not
scale well, when the record grows larger. e.g. a transaction of
10k nodes takes more than a minute on my machine when the journal
is enabled. Most of the time is spent doing List.indexOf() in
AbstractRecord.getOrCreateIndex()."
1,"RMIRemoteBindingServlet fails to initialize if the RMI registry is not availableIf the RMI registry is not available, the RMIRemoteBindingServlet in jcr-rmi will throw an exception in the init() method and prevent the servlet from being loaded.

The same servlet can however also be mapped to the normal HTTP URL space as an alternative mechanism of making the RMI endpoint available to clients. Thus it would be better if the init() method just logged a warning instead of failing completely."
0,Support grouping by IndexDocValuesAlthough IDV is not yet finalized (More particular the SortedSource). I think we already can discuss / investigate implementing grouping by IDV.
0,"CoordConstrainedBooleanQuery + QueryParser supportAttached 2 new classes:

1) CoordConstrainedBooleanQuery
A boolean query that only matches if a specified number of the contained clauses
match. An example use might be a query that returns a list of books where ANY 2
people from a list of people were co-authors, eg:
""Lucene In Action"" would match (""Erik Hatcher"" ""Otis Gospodneti&#263;"" ""Mark Harwood""
""Doug Cutting"") with a minRequiredOverlap of 2 because Otis and Erik wrote that.
The book ""Java Development with Ant"" would not match because only 1 element in
the list (Erik) was selected.

2) CustomQueryParserExample
A customised QueryParser that allows definition of
CoordConstrainedBooleanQueries. The solution (mis)uses fieldnames to pass
parameters to the custom query."
0,"Use the Jackrabbit RMI extensions by default in jackrabbit-webappUsing the Jackrabbit RMI extensions by default in jackrabbit-webapp

Ref :  http://www.nabble.com/Custom-node-types-with-RMI-tf3728625.html

"
1,"VirtualItemStates of node types definitions not accessible with uuidThe VirtualNodeTypeStateProvider that maps node type definitions into the workspace under /jcr:system/jcr:nodeTypes does not implement the methods:

- internalGetNodeState(NodeId id)
- internalHasNodeState(NodeId id)

This has the effect that ItemStates that reflect node type definitions are not accessible directly with their uuid."
0,"Make ItemIds more stableThe ItemIds returned by spi2dav are currently not stable in the sense that they are sometimes uuid based and sometimes not: If a node is referenceable some of its properties will receive fully path based ids while others will receive ids based on the uuid of its parent node. 

The efficiency of caching introduced with JCR-2498 depends on stable ids. I therefore suggest to improve spi2dav such that property ids are always uuid based if the parent's node has a uuid. "
1,"Deadlock during checkinUnder a load of 3 threads performing checkin and restore operations it's possible for all to become deadlocked in AbstractVersionManager.checkin(). This method attempts to upgrade a read lock to a write lock with the following code

    aquireReadLock();
    ....

    try {
        aquireWriteLock();
        releaseReadLock();
        ...

If 2 or more threads acquire the read lock then neither can acquire the write lock resulting in the deadlock, and after that any other thread that calls this method will block waiting for the write lock. The release of the read lock needs to be done before acquiring the write lock, this is documented Concurrent library javadoc.

There is another area where there is an attempt to upgrade a read lock to write lock, RepositoryImpl.WorkspaceInfo.disposeIfIdle() acquires a read lock and calls dispose() which then acquires a write lock, this maybe ok, as I assume there is only 1 thread that will attempt to dispose of idle workspaces.
"
1,"When BG merge hits an exception, optimize sometimes throws an IOException missing the root cause
When IndexWriter.optimize() is called, ConcurrentMergeScheduler will
run the requested merges with background threads and optimize() will
wait for these merges to complete.

If a merge hits an exception, it records the root cause exception such
that optimize can then retrieve this root cause and throw its own
exception, with the root cause.

But there is a bug: sometimes, the fact that an exception occurred on
a merge is recorded, but the root cause is missing.  In this cause,
optimize() still throws an exception (correctly indicating that the
optimize() has not finished successfully), but it's not helpful
because it's missing the root cause.  You must then go find the root
cause in the JRE's stderr logs.

This has hit a few users on this lists, most recently:

  http://www.nabble.com/Background-merge-hit-exception-td19540409.html#a19540409

I found the isssue, and finally got a unit test to intermittently show
it.  It's a simple thread safety issue: in a finally clause in
IndexWriter.merge we record the fact that the merge hit an exception
before actually setting the root cause, and then only in
ConcurrentMergeScheduler's exception handler do we set the root
cause.  If the optimize thread is scheduled in between these two, it
can throw an exception missing its root cause.

The fix is straightforward.  I plan to commit to 2.4 & 2.9.
"
0,"Expose explicit 2-phase commit in IndexWriterCurrently when IndexWriter commits, it does so with a two-phase
commit, internally: first it prepares all the new index files, syncs
them; then it writes a new segments_N file and syncs that, and only if
that is successful does it remove any now un-referenced index files.

However, these two phases are done privately, internal to the commit()
method.

But when Lucene is involved in a transaction with external resources
(eg a database), it's very useful to explicitly break out the prepare
phase from the commit phase.

Spinoff from this thread:

  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200804.mbox/%3C16627610.post@talk.nabble.com%3E

"
0,"Add <a name=""""> anchors to documentation sectionsIn all docs, sections are missing <a name> anchors. I see that the xdocs stylesheet in the repository is supposed to generate them, yet the site is missing them at this moment.

See https://svn.apache.org/repos/asf/jakarta/site/xdocs/stylesheets/site.xsl 
template match=""section"""
0,"SorterTemplate.quickSort stack overflows on broken comparators that produce only few disticnt values in large arraysLooking at Otis's sort problem on the mailing list, he said:
{noformat}
* looked for other places where this call is made - found it in
MultiPhraseQuery$MultiPhraseWeight and changed that call from
ArrayUtil.quickSort to ArrayUtil.mergeSort
* now we no longer see SorterTemplate.quickSort in deep recursion when we do a
thread dump
{noformat}

I thought this was interesting because PostingsAndFreq's comparator
looks like it needs a tiebreaker.

I think in our sorts we should add some asserts to try to catch some of these broken comparators."
0,"Improve org.apache.lucene.search.Filter Documentation and Tests to reflect per segment readersFilter Javadoc does not mention that the Reader passed to getDocIDSet(Reader) could be on a per-segment basis.
This caused confusion on the users-list -- see http://lucene.markmail.org/message/6knz2mkqbpxjz5po?q=date:200912+list:org.apache.lucene.java-user&page=1
We should improve the javadoc and also add a testcase that reflects filtering on a per-segment basis."
1,"Saving a node deletion that has been modified externally throws a ConstraintViolationExceptionDeleting a node ""a"" and saving its parent might result in a ConstraintViolationException if node ""a"" has been modified externally, where an InvalidItemStateException with message ""item x has been modified externally"" would be more intuitive.
"
1,"Transient states should be persisted in depth-first traversal orderInside Node.save(), when filling the list of transient (modified) items, the node itself is added first (if transient) and all transient descendant nodes in depth-first order. This can lead to the following problem with shareable nodes and path-based access management: 

1) assume a node N has a shared child S, which is shared with at least one other node N'
2) S.removeShare is invoked: this removes S from the list of child nodes in N
3) N.save is invoked

N is persisted first, then S. If a path-based access manager tries to build the path of S after N has been persisted, S will no longer be returned in the list of removed child node entries, and an exception will be thrown. This can be circumvented by adding N last."
1,"SQL2 query - supplying column selector fails with NPE on getColumnName()I am preparing and executing an SQL2 query (JCR 2.0) as follows:

QueryManager qm = jcrSession.getWorkspace().getQueryManager();
String queryString = ""select order.[customerAccountUUID] as cust from [atl:order] as order"";
Query query = qm.createQuery(queryString, Query.JCR_SQL2);
QueryResult queryResult = query.execute();

The following query fails:

select order.[customerAccountUUID] from [atl:order] as order

java.lang.NullPointerException
        org.apache.jackrabbit.commons.query.sql2.QOMFormatter.isSimpleName(QOMFormatter.java:577)
        org.apache.jackrabbit.commons.query.sql2.QOMFormatter.formatName(QOMFormatter.java:567)
        org.apache.jackrabbit.commons.query.sql2.QOMFormatter.format(QOMFormatter.java:452)
        org.apache.jackrabbit.commons.query.sql2.QOMFormatter.format(QOMFormatter.java:123)
        org.apache.jackrabbit.commons.query.sql2.QOMFormatter.format(QOMFormatter.java:117)

line 452: c.getColumnName() returns null.

The following query is fine:

select order.[customerAccountUUID] as cust from [atl:order] as order

I have been using the test case (here: http://svn.apache.org/viewvc/jackrabbit/trunk/jackrabbit-spi-commons/src/test/resources/org/apache/jackrabbit/spi/commons/query/sql2/test.sql2.txt?view=markup) as a guideline.

Cheers,

James "
1,"Empty response body is not properly handled when chunked encoding is usedIIS 5.0 server, when returning no content in response to an HTTP/1.1 request,
still includes ""Transfer-Encoding: chunked"" response header. As HttpClient
always expects chunk-encoded stream to be properly terminated, an
HttpRecoverableException exception results, when no content is sent back

=====================================================================

POST /someurl.aspx HTTP/1.1
Content-Length: 1132
Host: xxx.xxx.xxx.xxx
User-Agent: Jakarta Commons-HttpClient/2.0alpha2
Content-Type: multipart/form-data; boundary=----------------314159265358979323846

------------------314159265358979323846
Content-Disposition: form-data; name=""nmFile""; filename=""xxxxxxxxx.xml""
Content-Type: application/octet-stream

<... content removed ...>

------------------314159265358979323846--

HTTP/1.1 200 OK
Server: Microsoft-IIS/5.0
Date: Sat, 08 Feb 2003 15:22:26 GMT
Transfer-Encoding: chunked
Cache-Control: private
Content-Type: text/html

=====================================================================

Bug reported by Jim Crossley"
0,"NearSpansOrdered does not lazy load payloads as the PayloadSpans javadoc impliesBest would be to lazy load, but I don't see how with the current algorithm. Short that, we should add an option to ignore payloads - otherwise, if you are doing non payload searching, but the payloads are present, they will be needlessly loaded.

Already added this to LUCENE-1748, but spinning from that issue to this - patch to follow when LUCENE-1748 is committed."
1,"coord should still apply to missing terms/clausesMissing terms in a boolean query ""disappear"" (i.e. they don't even affect the coord factor)."
0,"FVH: uncontrollable color tagsThe multi-colored tags is a feature of FVH. But it is uncontrollable (or more precisely, unexpected by users) that which color is used for each terms."
1,"Phrase query with term repeated 3 times requires more slop than expectedConsider a document with the text ""A A A"".
The phrase query ""A A A"" (exact match) succeeds.
The query ""A A A""~1 (same document and query, just increasing the slop value by one) fails.
""A A A""~2 succeeds again.

If the exact match succeeds, I wouldn't expect the same query but with more slop to fail.  The fault seems to require some term to be repeated at least three times in the query, but the three occurrences do not need to be adjacent.  I will attach a file that contains a set of JUnit tests that demonstrate what I mean."
0,"Add more unit tests on BeanConvertersSome BeanConverters are not yet stable. We have to add more unit tests.  It seems that null values for bean attributes are not well supported. We have to test that for all BeanConverters. 

Here is a good scenario to test : 

Model : Class A contains an attribute ""b"" based on class B. 

Create an instance of A with ""b"" = null
insert A / save
Get instance of A
set the attribute of B
update A /save




"
1,"Lucene is not fsync'ing files on commitThanks to hurricane Irene, when Mark's electricity became unreliable, he discovered that on power loss Lucene could easily corrumpt the index, which of course should never happen...

I was able to easily repro, by pulling the plug on an Ubuntu box during indexing.  On digging, I discovered, to my horror, that Lucene is failing to fsync any files, ever!

This bug was unfortunately created when we committed LUCENE-2328... that issue added tracking, in FSDir, of which files have been closed but not sync'd, so that when sync is called during IW.commit we only sync those files that haven't already been sync'd.

That tracking is done via the FSDir.onIndexOutputClosed callback, called when an FSIndexOutput is closed.  The bug is that we only call it on exception during close:

{noformat}

    @Override
    public void close() throws IOException {
      // only close the file if it has not been closed yet
      if (isOpen) {
        boolean success = false;
        try {
          super.close();
          success = true;
        } finally {
          isOpen = false;
          if (!success) {
            try {
              file.close();
              parent.onIndexOutputClosed(this);
            } catch (Throwable t) {
              // Suppress so we don't mask original exception
            }
          } else
            file.close();
        }
      }
    }
{noformat}

And so FSDir thinks no files need syncing when its sync method is called....

I think instead we should call it up-front; better to over-sync than under-sync.

The fix is trivial (move the callback up-front), but I'd love to somehow have a test that can catch such a bad regression in the future.... still I think we can do that test separately and commit this fix first.

Note that even though LUCENE-2328 was backported to 2.9.x and 3.0.x, this bug wasn't, ie the backport was a much simpler fix (to just address the original memory leak); it's 3.1, 3.2, 3.3 and trunk when this bug is present."
0,"Cleanup DR.getCurrentVersion/DR.getUserData/DR.getIndexCommit().getUserData()Spinoff from Ryan's dev thread ""DR.getCommitUserData() vs DR.getIndexCommit().getUserData()""... these methods are confusing/dups right now."
0,promote TestExternalCodecs.PerFieldCodecWrapper to corePerFieldCodecWrapper lets you set the Codec for each field; I'll promote to core & mark experimental.
1,"Bug in UtilDateTypeConverterImplIn this converter following line is used:
return this.getValueFactory().createValue(((java.util.Date) propValue).getTime());
but propValue must be converted to java.util.Calendar, not into long! ValueFactory than converts to LongValue not DateValue as expected.

Following code works OK:
final long timeInMilis = ((java.util.Date) propValue).getTime();
final Calendar calendar = Calendar.getInstance();
calendar.setTimeInMillis(timeInMilis);
return this.getValueFactory().createValue( calendar );

but I dont know better Date-> Calendar conversion.
"
0,"Binary value may leave temp file behindThe following call leaves a temp file behind that is never deleted:

InputStream in = ...
ValueFactory vf = ....
vf.createBinary(in).dispose();

Only happens when the datastore is disabled."
0,"remove deprecated classes from spatialspatial has not been released, so we can remove the deprecated classes"
1,"PathFactoryImpl creates illegal Path objectsit is currently possible to create illegal/inconsistent paths using the default path factory.
Path objects are expected to represent syntactically correct paths.

some examples:

            PathFactory pf = PathFactoryImpl.getInstance();
            Path.Element re = pf.getRootElement();
            Path illegalPath = pf.create(new Path.Element[]{re, re});
            
            Path.Element pe = pf.getParentElement();
            Path nonNormalizedPath = pf.create(new Path.Element[]{pe, pe});    // ""../..""
            assertFalse(nonNormalizedPath.isNormalized());

"
1,"FSDirectory.getDirectory always creates index pathThis was reported to me as a Luke bug, but going deeper it proved to be a non-intuitive (broken?) behavior of FSDirectory.

If you use FSDirectory.getDirectory(File nonexistent) on a nonexistent path, but one that is located under some existing parent path, then FSDirectory:174 uses file.mkdirs() to create this directory. One would expect a variant of the method with a boolean flag to decide whether or not to create the output path. However, the API with ""create"" flag is now deprecated, with a comment that points to IndexWriter's ""create"" flag. This comment is misleading, because the indicated path is created anyway in the file system just by calling FSDirectory.getDirectory().

I propose to do one of the following:

* reinstate the variant of the method with ""create"" flag. In case if this flag is false, and the index directory is missing, either return null or throw an IOException,

* keep the API as it is now, but either return null or throw IOException if the index dir is missing. This breaks the backwards compatibility, because now users are required to do file.mkdirs() themselves prior to calling FSDirectory.getDirectory()."
1,"TCK: Incorrect check of namespace mappings in System View XML exportorg.apache.jackrabbit.test.api.SysViewContentHandler. In endDocument(), two issues:

1. line 351: tries to go through a table of prefixes but uses a fixed index inside the loop;
2. The mapping for the 'xml' prefix should be skipped (it must be registered in the Session but must not be registered during export since this is a built-in XML mapping."
0,"Implement anonymous login with credentialsJackrabbit currently implements anonymous login by detecting a null credentials argument on login. This is actually not compliant to the specification.

<spec>
If credentials is null, it is assumed that authentication is handled by a mechanism external to the repository itself (for example, through the JAAS framework) and that the repository implementation exists within a context (for example, an application server) that allows it to handle authorization of the request for access to the specified workspace.
</spec>

Jackrabbit should rather support anonymous login with a defined credential, either some subclass of SimpleCredentials or a predefined / known userId that has read-only access."
0,Refactor RewriteMethods out of MultiTermQueryPoliceman work :-) - as usual
1,"ParameterParser parse method for authentication headers does not appear to deal with empty value stringsHi, I have found an issue with HTTPClient due to the way it parses parameter 
strings.

In particular, consider the following WWW-Authenticate header:

WWW-Authenticate: Digest realm="""", algorithm=MD5, qop=""auth"", 
domain=""/content"", nonce=""0e11dcf146563c3a89e5327f0c5f5bad""
 
The realm is definitely specified, but is equal to the empty string.  It is not 
a null value.

However, the extractParams method of AuthChallengeParser which in turn calls 
ParameterParser will actually parse the value as Null  instead of an empty 
string.

This is due to parseQuotedToken getToken(true) call which essentially returns a 
null String result  as the condition i2>i1 fails :-

        String result = null;
        if (i2 > i1) {
            result = new String(chars, i1, i2 - i1);
        }
        return result;

As the processChallenge method of DigestScheme throws an exception when 
getParameter(""realm"") == null, HTTPClient is not able to process the digest 
request when an empty string realm value is present."
0,"Change remaining contrib streams/filters to use new TokenStream APIAll other contrib streams/filters have already been converted with LUCENE-1460.

The two shingle filters are the last ones we need to convert."
0,"another highlighterI've written this highlighter for my project to support bi-gram token stream (general token stream (e.g. WhitespaceTokenizer) also supported. see test code in patch). The idea was inherited from my previous project with my colleague and LUCENE-644. This approach needs highlight fields to be TermVector.WITH_POSITIONS_OFFSETS, but is fast and can support N-grams. This depends on LUCENE-1448 to get refined term offsets.

usage:
{code:java}
TopDocs docs = searcher.search( query, 10 );
Highlighter h = new Highlighter();
FieldQuery fq = h.getFieldQuery( query );
for( ScoreDoc scoreDoc : docs.scoreDocs ){
  // fieldName=""content"", fragCharSize=100, numFragments=3
  String[] fragments = h.getBestFragments( fq, reader, scoreDoc.doc, ""content"", 100, 3 );
  if( fragments != null ){
    for( String fragment : fragments )
      System.out.println( fragment );
  }
}
{code}

features:
- fast for large docs
- supports not only whitespace-based token stream, but also ""fixed size"" N-gram (e.g. (2,2), not (1,3)) (can solve LUCENE-1489)
- supports PhraseQuery, phrase-unit highlighting with slops
{noformat}
q=""w1 w2""
<b>w1 w2</b>
---------------
q=""w1 w2""~1
<b>w1</b> w3 <b>w2</b> w3 <b>w1 w2</b>
{noformat}
- highlight fields need to be TermVector.WITH_POSITIONS_OFFSETS
- easy to apply patch due to independent package (contrib/highlighter2)
- uses Java 1.5
- looks query boost to score fragments (currently doesn't see idf, but it should be possible)
- pluggable FragListBuilder
- pluggable FragmentsBuilder

to do:
- term positions can be unnecessary when phraseHighlight==false
- collects performance numbers
"
0,"httpclient doesn't read and parse response from certain types of proxy servers when POST method is usedIt was determined that when sending post data to server via Squid proxy server
of version 2.4.STABLE2 and Squid responds 
with ""407 proxy authentication required"" response, httpclient doesn't read this
response in order to parse, but rather
fails with soket exception ""java.net.SocketException: Software caused connection
abort: recv failed"".

This behaviour is reproduced with the latest nigtly build of httpclient version
3.0. (from 9 of February 2005) as
well as 3.0. RC1, 2.0.2 and 2.0.

This is the piece of code that sends post data using httpclient:

try
{
	HttpClientParams httpClientParams = new HttpClientParams();
	HttpClient client = new HttpClient(httpClientParams);

	HostConfiguration hostconfig = new HostConfiguration();
	hostconfig.setProxy(""db00-devl.eps.agfa.be"", 3128); // SQUID proxy server
version 2.4.STABLE2
	client.setHostConfiguration(hostconfig);
	PostMethod postMethod = new
PostMethod(""http://brugge.eps.agfa.be/portal03/servlet/selectFiles"");

	postMethod.addParameter(""data"", ""some data"");
	int status = client.executeMethod(postMethod);
	System.out.println(""status = "" + status);
	if (status == HttpStatus.SC_OK)
		System.out.println(""Ok"");
	else if (status == HttpStatus.SC_PROXY_AUTHENTICATION_REQUIRED)
		System.out.println(""Proxy authentication required."");
}
catch (Exception e)
{
	System.out.println(""Socket exception."");
	e.printStackTrace();
}

Look at ""debug log of the problem"" attachment to see all output from httpclient
and mentioned piece of code.
In ""problem_request_response_interaction"" attacment it is possible to see
interaction beetween httpclient and Squid proxy server: httpclient sends initial
request and headers, then squid responds with ""proxy authentication required""
response and afterwards httpclient tries to send post data(without reading the
response) but fails because connection is already closed.

For more details look at ""ethereal_problem"" attachment for all network traffic
during running of mentioned piece of code:
Ethereal protocol analyzer can be used to open this file(http://www.ethereal.com/).

Most likely this particular version of Squid closes connection after it sends
proxy athentication response back,
which causes httpclient to fail while sending post data.

Let's have a look at what writeRequest(...) method of HttpMethodBase class does:

1) sends request line and headers to server,
2) handles 'Expect: 100-continue' handshake if needed,
3) sends post data to server.

My question is should HTTPClient send initial request and headers before data
even if it is not going to read 
a response from the server(proxy server), or this should be done only in case of
'Expect: 100-continue' handshake 
(this seems the only case when HTTPClient is going to listen to server
in-between of steps 1 and 3)?

My understanding is that the command

        // make sure the status line and headers have been sent
        conn.flushRequestOutputStream();
        
which actually splits sending of data in two parts are needed only for 'Expect:
100-continue' handshake case.
Just by moving ""flush"" command to appropriate place inside 'Expect:
100-continue' handshake case:
		.....
                try {
	            conn.flushRequestOutputStream(); // moved
                    conn.setSocketTimeout(RESPONSE_WAIT_TIME_MS);
		.....
it is posible to solve described problem.

I created PostMethodEx that overrides writeRequest(...) method of
HttpMethodBase(look at ""PostMethodEx"" attachment) 
and for all cases but the 'Expect: 100-continue' handshake it sends request
line, headers and post data to server 
at once.

When mentioned piece of code(with PostMethod changed to PostMethodEx) is
executed everyting works fine:
look at ""debug log of the fix"", ""fix_request_response_interaction"" and
""ethereal_fix""(all network trafic) 
attachments.

According to mentioned logs httpclient sends all post data at once and then
reads and parses ""proxy authentication required"" 
response from squid and sets status code to 407. Correct."
0,[PATCH] Comment corrections in MMapDirectory.javaThese comments ended up on the wrong lines after the last changes
0,"Cleanup highlighter test classcleanup highlighter test class - did some of this in another issue, but there is a bit more to do"
0,"Move 'good' contrib/queries classes to Queries moduleWith the Queries module now filled with the FunctionQuery stuff, we should look at closing down contrib/queries.  While not a huge contrib, it contains a number of pretty useful classes and some that should go elsewhere.

Heres my proposed plan:

- similar.* -> suggest module
- regex.* -> queries module
- BooleanFilter -> queries module under .filters package
- BoostingQuery -> queries module
- ChainedFilter -> queries module under .filters package
- DuplicateFilter -> queries module under .filters package
- FieldCacheRewriteMethod -> This doesn't belong in this contrib or the queries module.  I think we should push it to contrib/misc for the time being.  It seems to have quite a few constraints on when its useful.  If indeed CONSTANT_SCORE_AUTO rewrite is better, then I dont see a purpose for it.
- FilterClause -> class inside BooleanFilter
- FuzzyLikeThisQuery -> suggest module. This class seems a mess with its Similarity hardcoded.  With all that said, it does seem to do what it claims and with some cleanup, it could be good.
- TermsFilter -> queries module under .filters package
- SlowCollated* -> They can stay in the module till we have a better place to nuke them.

One of the implications of the above moves, is that the xml-query-parser, which supports many of the queries, will need to have a dependency on the queries module.  But that seems unavoidable at this stage.



"
0,Add init method to CloseableThreadLocalJava ThreadLocal has an init method that allows subclasses to easily instantiate an initial value.  
1,"HttpMethod#getResponseBody throws NPEHttpMethod#getResponseBody throws an NPE if the response from the server was 
204.  Shouldn't getResponseBody return null by contract instead of throwing 
NPE?"
1,"Incorrect node position after importI have found a behavior that does not seem to be consistent with the
spec:

After replacing a node with importXML using IMPORT_UUID_COLLISION_REPLACE_EXISTING the new node is not at the position of the replaced node (talking about the position among the siblings).

The origininal node is removed, but the new node is created as the last child of the parent node, and not spec-compliant at the position of the replaced node.

Here how I use it:

// assume Session s, Node n, String text (holding XML data)

s.importXML(
	n.getPath(), 
	new ByteArrayInputStream (text.getBytes(""UTF-8"")),
	ImportUUIDBehavior.IMPORT_UUID_COLLISION_REPLACE_EXISTING
);
s.save();
 

And here a quote from the spec section 7.3.6

ImportUUIDBehavior.IMPORT_UUID_COLLISION_REPLACE_EXISTING: 
If an incoming referenceable node has the same UUID as a node already existing in the workspace then the already existing node is replaced by the incoming node in the same position as the existing node.
[note ""same position""]
"
1,Query may throw ArrayIndexOutOfBoundsExceptionThere's a bug in DescendantSelfAxisQuery.DescendantSelfAxisScorer.skipTo() that causes the exception.
0,"Create a Size Estimator model for Lucene and SolrIt is often handy to be able to estimate the amount of memory and disk space that both Lucene and Solr use, given certain assumptions.  I intend to check in an Excel spreadsheet that allows people to estimate memory and disk usage for trunk.  I propose to put it under dev-tools, as I don't think it should be official documentation just yet and like the IDE stuff, we'll see how well it gets maintained."
1,NearSpansOrdered.getPayload does not return the payload from the minimum match span
0,"Need stopwords and stoptags lists for default Japanese configurationStopwords and stoptags lists for Japanese needs to be developed, tested and integrated into Lucene."
1,"CharFilters not being invoked in Solr
On Solr trunk, *all* CharFilters have been non-functional since LUCENE-3396 was committed in r1175297 on 25 Sept 2011, until Yonik's fix today in r1235810; Solr 3.x was not affected - CharFilters have been working there all along."
1,"SpanMultiTermQueryWrapper with Prefix Query issueIf we try to do a search with SpanQuery and a PrefixQuery this message is returned:

""You can only use SpanMultiTermQueryWrapper with a suitable SpanRewriteMethod.""

The problem is in the WildcardQuery rewrite function.

If the wildcard query is a prefix, a new prefix query is created, the rewrite method is set with the SpanRewriteMethod and the prefix query is returned.

But, that's the rewritten prefix query which should be returned:

-      return rewritten;
+      return rewritten.rewrite(reader);

I will attach a patch with a unit test included.



"
0,"JSR 283: JCR Pathwith jsr 283 the jcr path is defined to consist of a combination of the following segments

	a name segment, (J, I), where J is a JCR name and I is an integer index (I  1).
	an identifier segment, U, where U is a JCR identifier.
	the root segment.
	the self segment.
	the parent segment.

-> the name segment can be in extended or qualified form -> see issue JCR-1712
-> the identifier segment is new for jsr283 and always identifies a node (-> see new method Node.getIdentifier())

Non-standard parts always need to be standardized. Any of the following makes a path non-standard:
- expanded name segments
- trailing /
- index [1]

Identifier-segments
- get resolved upon being passed to any API calls that take path to an existing Node
- don't get resolved when being used to create a PATH value object.

Except for PATH values, all jcr paths returned by the API are normalized and standard, thus never identifier-based.

PATH values in contrast:
- must be converted to standard form
- must NOT be normalized. i.e. redundant segments and identifiers must be preserved.
"
0,"Missing possibility to supply custom FieldParser when sorting search resultsWhen implementing the new TrieRangeQuery for contrib (LUCENE-1470), I was confronted by the problem that the special trie-encoded values (which are longs in a special encoding) cannot be sorted by Searcher.search() and SortField. The problem is: If you use SortField.LONG, you get NumberFormatExceptions. The trie encoded values may be sorted using SortField.String (as the encoding is in such a way, that they are sortable as Strings), but this is very memory ineffective.

ExtendedFieldCache gives the possibility to specify a custom LongParser when retrieving the cached values. But you cannot use this during searching, because there is no possibility to supply this custom LongParser to the SortField.

I propose a change in the sort classes:
Include a pointer to the parser instance to be used in SortField (if not given use the default). My idea is to create a SortField using a new constructor
{code}SortField(String field, int type, Object parser, boolean reverse){code}

The parser is ""object"" because all current parsers have no super-interface. The ideal solution would be to have:

{code}SortField(String field, int type, FieldCache.Parser parser, boolean reverse){code}

and FieldCache.Parser is a super-interface (just empty, more like a marker-interface) of all other parsers (like LongParser...). The sort implementation then must be changed to respect the given parser (if not NULL), else use the default FieldCache.getXXXX without parser."
1,"DateTools needs to use UTC for correct collation,If your local timezone is Europe/London then the times Sun, 30 Oct 2005 00:00:00 +0000 and exactly one hour later are both converted to 200530010000 by DateTools.dateToString() with minute resolution.   The Linux date command is useful in seeing why:

    $ date --date ""Sun, 30 Oct 2005 00:00:00 +0000""
    Sun Oct 30 01:00:00 BST 2005

    $ date --date ""Sun, 30 Oct 2005 01:00:00 +0000""
    Sun Oct 30 01:00:00 GMT 2005

Both times are 1am in the morning, but one is when DST is in force, the other isn't.   Of course, these are actually different times!

Of course, if dates are stored in the index with implicit timezone information then not only do we get problems when the clocks go back at the end of summer, but we also have problems crossing timezones.   If a database is created in California and used in Paris then the times are going to be badly skewed (there's a nine hour time difference most of the year).
"
0,Discovery of privileges of any set of Principalsjsr 283 defines means to discover the privileges for the editing session. however there is no way to determine the privileges for other principals.
0,"Source packaging fails if ${dist.dir} does not existpackage-tgz-src and package-zip-src fail if ${dist.dir} does not exist, since these two targets do not call the package target, which is responsible for making the dir.

I have a fix and will commit shortly."
0,Remove QueryResultImpl and rename LazyQueryResultImpl to QueryResultImplQueryResultImpl isn't used in Jackrabbit anymore. Instead LazyQueryResultImpl is now used. See the discussion in JCR-1073.
0,"optimize automatonqueryMike found a few cases in flex where we have some bad behavior with automatonquery.
The problem is similar to a database query planner, where sometimes simply doing a full table scan is faster than using an index.

We can optimize automatonquery a little bit, and get better performance for fuzzy,wildcard,regex queries.

Here is a list of ideas:
* create commonSuffixRef for infinite automata, not just really-bad linear scan cases
* do a null check rather than populating an empty commonSuffixRef
* localize the 'linear' case to not seek, but instead scan, when ping-ponging against loops in the state machine
* add a mechanism to enable/disable the terms dict cache, e.g. we can disable it for infinite cases, and maybe fuzzy N>1 also.
* change the use of BitSet to OpenBitSet or long[] gen for path-tracking
* optimize the backtracking code where it says /* String is good to go as-is */, this need not be a full run(), I think...
"
1,"ChunkedInputStream incorrectly handles chunksize without semicolonChunkedInputStream does not correctly read the chunk size when a semicolon does
not appear in the first line of the chunk.  If whitespace exists between the
chunk size value and the end of line and no semicolon is present, the whitespace
is not removed before parseInt is called resulting in an IOException ""Bad chunk
size""

I can not tell from RFC2616 if whitespace is legal here, but I have received it
from at least one web server.  The relevant section is 3.6.1.

A small patch repairs the problem.  I will attach it immediately."
0,"Cloned SegmentReaders fail to share FieldCache entriesI just hit this on LUCENE-1516, which returns a cloned readOnly
readers from IndexWriter.

The problem is, when cloning, we create a new [thin] cloned
SegmentReader for each segment.  FieldCache keys directly off this
object, so if you clone the reader and do a search that requires the
FieldCache (eg, sorting) then that first search is always very slow
because every single segment is reloading the FieldCache.

This is of course a complete showstopper for LUCENE-1516.

With LUCENE-831 we'll switch to a new FieldCache API; we should ensure
this bug is not present there.  We should also fix the bug in the
current FieldCache API since for 2.9, users may hit this.
"
0,"Deprecate all non-bundle persistence managersBundle persistence has been the recommended default since Jackrabbit 1.3, and there is little reason for anyone to be using non-bundle persistence anymore. Thus I'd like to deprecate all non-bundle PMs in Jackrabbit 2.2 and target for their removal in Jackrabbit 3.0."
1,"""overwriting cached entry"" warningswhen using multiple concurrent sessions you'll find *lots* of log entries like:

    03.11.2010 21:17:03 *WARN * ItemStateReferenceCache: overwriting cached entry ad79ca57-5eb1-4b7d-a439-a9fd73cc8c5a (ItemStateReferenceCache.java, line 176)

those are actually legitimate warnings since there's a siginificant risk of data loss/inconsistency involved.

this is apparently a regression of changes introduced by JCR-2699, specifically svn r1004223"
1,"Moved node disappearsMoving a node and then refreshing it can make it disappear.

deleteDirectory(new File(""repository""));
Repository rep = new TransientRepository();
Session session = rep.login(new SimpleCredentials("""", new char[0]));
Node root = session.getRootNode();
Node a = root.addNode(""a"");
Node b = a.addNode(""b"");
session.save();
session.move(""/a/b"", ""/b"");
b.refresh(false);
// session.save(); // no effect
for (NodeIterator it = root.getNodes(); it.hasNext();) {
    Node n = it.nextNode();
    System.out.println(n.getName());
    for (NodeIterator it2 = n.getNodes(); it2.hasNext();) {
        System.out.println(""  "" + it2.nextNode().getName());
    }
}

In the trunk, the node 'b' is not listed after the refresh (not under the root page, and not under a). The output is:
jcr:system
  jcr:versionStorage
  jcr:nodeTypes
a


Jackrabbit 1.4.x throws an exception:

jcr:system
  jcr:versionStorage
  jcr:nodeTypes
a
Exception in thread ""main"" javax.jcr.RepositoryException: failed to resolve name of acee31c4-c33b-4ed4-b1b5-39db6f17fb09
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getName(HierarchyManagerImpl.java:451)
	at org.apache.jackrabbit.core.CachingHierarchyManager.getName(CachingHierarchyManager.java:287)
	at org.apache.jackrabbit.core.NodeImpl.getName(NodeImpl.java:1931)
	at org.apache.jackrabbit.core.fuzz.TestMoveRemoveRefresh.test(TestMoveRemoveRefresh.java:33)
	at org.apache.jackrabbit.core.fuzz.TestMoveRemoveRefresh.main(TestMoveRemoveRefresh.java:15)


void deleteDirectory(File file) {
    if (file.isDirectory()) {
        File[] list = file.listFiles();
        for(int i=0; i<list.length; i++) {
            deleteDirectory(list[i]);
        }
    }
    file.delete();
}
"
1,"JCR2SPI; setProperty(name, date-string) fails when property is added and property type is PropertyType.DATE.Example code:

        Node l_parent = (Node)session.getItem(this.m_path);
        
        Node l_test = l_parent.addNode(""createcontenttest"", ""nt:file"");
        Node l_content = l_test.addNode(""jcr:content"", ""nt:resource"");
        
        l_content.setProperty(""jcr:encoding"", ""UTF-8"");
        l_content.setProperty(""jcr:mimeType"", ""text/plain"");
        l_content.setProperty(""jcr:data"", new ByteArrayInputStream(""foobar"".getBytes()));
        l_content.setProperty(""jcr:lastModified"", ""2007-07-25T17:04:00.000Z""); // TODO: this should work as well, bug in JCR2SPI?
        session.save();

This will fail when the property is defined as DATE, what should happen is that a value comparison is attempted (note that it works when the property already exists and just is overwritten).

The exception is:

javax.jcr.nodetype.ConstraintViolationException: no matching property definition found for {http://www.jcp.org/jcr/1.0}lastModified
        at org.apache.jackrabbit.jcr2spi.nodetype.ItemDefinitionProviderImpl.getQPropertyDefinition(ItemDefinitionProviderImpl.java:269)
        at org.apache.jackrabbit.jcr2spi.nodetype.ItemDefinitionProviderImpl.getQPropertyDefinition(ItemDefinitionProviderImpl.java:159)
        at org.apache.jackrabbit.jcr2spi.NodeImpl.getApplicablePropertyDefinition(NodeImpl.java:1672)
        at org.apache.jackrabbit.jcr2spi.NodeImpl.createProperty(NodeImpl.java:1369)
        at org.apache.jackrabbit.jcr2spi.NodeImpl.setProperty(NodeImpl.java:264)
        at org.apache.jackrabbit.jcr2spi.NodeImpl.setProperty(NodeImpl.java:345)
        at org.apache.jackrabbit.jcr2spi.NodeImpl.setProperty(NodeImpl.java:336)
"
0,"Similarity.java javadocs and simplifications for 4.0As part of adding additional scoring systems to lucene, we made a lower-level Similarity
and the existing stuff became e.g. TFIDFSimilarity which extends it.

However, I always feel bad about the complexity introduced here (though I do feel there
are some ""excuses"", that its a difficult challenge).

In order to try to mitigate this, we also exposed an easier API (SimilarityBase) on top of 
it that makes some assumptions (and trades off some performance) to try to provide something 
consumable for e.g. experiments.

Still, we can cleanup a few things with the low-level api: fix outdated documentation and
shoot for better/clearer naming etc.
"
0,"Remove deprecated Directory stuff and IR/IW open/ctor hellThis patch removes primarily the deprecated Directory stuff. This also removes parts of the ctor/open hell in IR and IW. IndexModifier is completely removed as deprecated, too."
0,"Add sort missing first/last ability to SortField and ValueComparatorWhen SortField and ValueComparator use EntryCreators (from LUCENE-2649) they use a special sort value when the field is missing.

This enables lucene to implement 'sort missing last' or 'sort missing first' for numeric values from the FieldCache.
"
0,"ReorderReferenceableSNSTest failureI have checked out the Jackrabbit 1.4 branch to a new directory, and called:

mvn clean install

The error is:

Building Jackrabbit JCR to SPI
  task-segment: [clean, install]
---------------------------------
...
testRevertReorder(org.apache.jackrabbit.jcr2spi.ReorderReferenceableSNSTest)
junit.framework.AssertionFailedError: Reorder added a child node.
       at junit.framework.Assert.fail(Assert.java:47)
       at org.apache.jackrabbit.jcr2spi.ReorderTest.testOrder(ReorderTest.java:90)
       at org.apache.jackrabbit.jcr2spi.ReorderTest.testRevertReorder(ReorderTest.java:122)
"
0,"Stop using BaseExceptionThe o.a.j.BaseException class is deprecated (since JCR-1169) and not caught anywhere, so there's no need to keep using it."
0,"Incorrect decodedAttributeValue in AbstractImportXmlTestThe string literal is not correctly escaped.

Later on the decoded attribute value should be used to check the imported value. There is currently an odd test that checks the encoded attribute value twice."
0,"lucene benchmark has some unnecessary fileslucene/contrib/benchmark/.rsync-filter is only in the source pack (and in SVN), I was not aware of this file, though it was added long ago in https://issues.apache.org/jira/browse/LUCENE-848?focusedCommentId=12491404&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12491404
Not a blocker for this RC, just interesting to note.

maybe this is related to LUCENE-3155 too, in that we could consider this one for automatic exclusion (like DS_Store), but we should fix it if its committed in SVN too.
"
0,"Factor out ByteSliceWriter from DocumentsWriterFieldDataDocumentsWriter uses byte slices into shared byte[]'s to hold the
growing postings data for many different terms in memory.  This is
probably the trickiest (most confusing) part of DocumentsWriter.

Right now it's not cleanly factored out and not easy to separately
test.  In working on this issue:

  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200805.mbox/%3c126142c0805061426n1168421ya5594ef854fae5e4@mail.gmail.com%3e

which eventually turned out to be a bug in Oracle JRE's JIT compiler,
I factored out ByteSliceWriter and created a unit test to stress test
the writing & reading of byte slices.  The test just randomly writes N
streams interleaved into shared byte[]'s, then reads them back
verifying the results are correct.

I created the stress test to try to find any bugs in that code.  The
test ran fine (no bugs were found) but I think the refactoring is
still very much worthwhile.

I expected the changes to reduce indexing throughput, so I ran a test
indexing first 200K Wikipedia docs using this alg:

{code}
analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker

docs.file=/Volumes/External/lucene/wiki.txt
doc.stored = true
doc.term.vector = true
doc.add.log.step=2000

directory=FSDirectory
autocommit=false
compound=true

ram.flush.mb=256

{ ""Rounds""
  ResetSystemErase
  { ""BuildIndex""
    - CreateIndex
     { ""AddDocs"" AddDoc > : 200000
    - CloseIndex
  }
  NewRound
} : 4

RepSumByPrefRound BuildIndex

{code}

Ok trunk it produces these results:
{code}
Operation   round   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem
BuildIndex      0        1       200000        791.7      252.63   338,552,096  1,061,814,272
BuildIndex -  - 1 -  -   1 -  -  200000 -  -   793.1 -  - 252.18 - 605,262,080  1,061,814,272
BuildIndex      2        1       200000        794.8      251.63   601,966,528  1,061,814,272
BuildIndex -  - 3 -  -   1 -  -  200000 -  -   782.5 -  - 255.58 - 608,699,712  1,061,814,272
{code}

and with the patch:

{code}
Operation   round   runCnt   recsPerRun        rec/s  elapsedSec    avgUsedMem    avgTotalMem
BuildIndex      0        1       200000        745.0      268.47   338,318,784  1,061,814,272
BuildIndex -  - 1 -  -   1 -  -  200000 -  -   792.7 -  - 252.30 - 605,331,776  1,061,814,272
BuildIndex      2        1       200000        786.7      254.24   602,915,712  1,061,814,272
BuildIndex -  - 3 -  -   1 -  -  200000 -  -   795.3 -  - 251.48 - 602,378,624  1,061,814,272
{code}

So it looks like the performance cost of this change is negligible (in
the noise).

"
1,"UserManager throws javax.jcr.query.InvalidQueryException on createUserThe UserManager method createUser(String userID, String password) throws an exception (javax.jcr.query.InvalidQueryException) if the user name contains a '@' character.

Stack trace:
Exception in thread ""main"" javax.jcr.query.InvalidQueryException: Lexical error at line 1, column 76.  Encountered: ""@"" (64), after : """" for statement: for $v in /jcr:root/rep:security/rep:authorizables/rep:groups//element(test@example.com,rep:Group) return $v: Lexical error at line 1, column 76.  Encountered: ""@"" (64), after : """": Lexical error at line 1, column 76.  Encountered: ""@"" (64), after : """"
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathQueryBuilder.<init>(XPathQueryBuilder.java:302)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathQueryBuilder.createQuery(XPathQueryBuilder.java:331)
        at org.apache.jackrabbit.spi.commons.query.xpath.QueryBuilder.createQueryTree(QueryBuilder.java:39)
        at org.apache.jackrabbit.spi.commons.query.QueryParser.parse(QueryParser.java:57)
        at org.apache.jackrabbit.core.query.lucene.QueryImpl.<init>(QueryImpl.java:91)
        at org.apache.jackrabbit.core.query.lucene.SearchIndex.createExecutableQuery(SearchIndex.java:615)
        at org.apache.jackrabbit.core.query.QueryImpl.init(QueryImpl.java:128)
        at org.apache.jackrabbit.core.SearchManager.createQuery(SearchManager.java:282)
        at org.apache.jackrabbit.core.query.QueryManagerImpl.createQuery(QueryManagerImpl.java:102)
        at org.apache.jackrabbit.core.security.user.IndexNodeResolver.buildQuery(IndexNodeResolver.java:105)
        at org.apache.jackrabbit.core.security.user.IndexNodeResolver.findNode(IndexNodeResolver.java:50)
        at org.apache.jackrabbit.core.security.user.UserManagerImpl.getAuthorizable(UserManagerImpl.java:93)
        at org.apache.jackrabbit.core.security.user.UserManagerImpl.createUser(UserManagerImpl.java:177)
        at org.apache.jackrabbit.core.security.user.UserManagerImpl.createUser(UserManagerImpl.java:158)
        at FirstHop.main(FirstHop.java:20)
Caused by: org.apache.jackrabbit.spi.commons.query.xpath.TokenMgrError: Lexical error at line 1, column 76.  Encountered: ""@"" (64), after : """"
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathTokenManager.getNextToken(XPathTokenManager.java:13263)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.jj_ntk(XPath.java:9187)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ElementTest(XPath.java:8745)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.KindTest(XPath.java:8120)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.NodeTest(XPath.java:5041)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AbbrevForwardStep(XPath.java:4891)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ForwardStep(XPath.java:4747)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AxisStep(XPath.java:4692)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.StepExpr(XPath.java:4597)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.RelativePathExpr(XPath.java:4547)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.PathExpr(XPath.java:4396)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ValueExpr(XPath.java:4125)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.UnaryExpr(XPath.java:4032)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.CastExpr(XPath.java:3935)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.CastableExpr(XPath.java:3898)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.TreatExpr(XPath.java:3861)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.InstanceofExpr(XPath.java:3824)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.IntersectExceptExpr(XPath.java:3748)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.UnionExpr(XPath.java:3672)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.MultiplicativeExpr(XPath.java:3586)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AdditiveExpr(XPath.java:3510)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.RangeExpr(XPath.java:3451)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ComparisonExpr(XPath.java:3353)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AndExpr(XPath.java:3290)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.OrExpr(XPath.java:3227)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ExprSingle(XPath.java:2214)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ForClause(XPath.java:2337)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.FLWORExpr(XPath.java:2233)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ExprSingle(XPath.java:2133)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.Expr(XPath.java:2094)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.QueryBody(XPath.java:2066)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.MainModule(XPath.java:512)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.Module(XPath.java:387)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.QueryList(XPath.java:151)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.XPath2(XPath.java:118)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathQueryBuilder.<init>(XPathQueryBuilder.java:295)
        ... 14 more
org.apache.jackrabbit.spi.commons.query.xpath.TokenMgrError: Lexical error at line 1, column 76.  Encountered: ""@"" (64), after : """"
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathTokenManager.getNextToken(XPathTokenManager.java:13263)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.jj_ntk(XPath.java:9187)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ElementTest(XPath.java:8745)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.KindTest(XPath.java:8120)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.NodeTest(XPath.java:5041)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AbbrevForwardStep(XPath.java:4891)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ForwardStep(XPath.java:4747)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AxisStep(XPath.java:4692)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.StepExpr(XPath.java:4597)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.RelativePathExpr(XPath.java:4547)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.PathExpr(XPath.java:4396)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ValueExpr(XPath.java:4125)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.UnaryExpr(XPath.java:4032)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.CastExpr(XPath.java:3935)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.CastableExpr(XPath.java:3898)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.TreatExpr(XPath.java:3861)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.InstanceofExpr(XPath.java:3824)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.IntersectExceptExpr(XPath.java:3748)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.UnionExpr(XPath.java:3672)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.MultiplicativeExpr(XPath.java:3586)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AdditiveExpr(XPath.java:3510)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.RangeExpr(XPath.java:3451)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ComparisonExpr(XPath.java:3353)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.AndExpr(XPath.java:3290)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.OrExpr(XPath.java:3227)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ExprSingle(XPath.java:2214)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ForClause(XPath.java:2337)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.FLWORExpr(XPath.java:2233)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.ExprSingle(XPath.java:2133)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.Expr(XPath.java:2094)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.QueryBody(XPath.java:2066)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.MainModule(XPath.java:512)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.Module(XPath.java:387)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.QueryList(XPath.java:151)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPath.XPath2(XPath.java:118)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathQueryBuilder.<init>(XPathQueryBuilder.java:295)
        at org.apache.jackrabbit.spi.commons.query.xpath.XPathQueryBuilder.createQuery(XPathQueryBuilder.java:331)
        at org.apache.jackrabbit.spi.commons.query.xpath.QueryBuilder.createQueryTree(QueryBuilder.java:39)
        at org.apache.jackrabbit.spi.commons.query.QueryParser.parse(QueryParser.java:57)
        at org.apache.jackrabbit.core.query.lucene.QueryImpl.<init>(QueryImpl.java:91)
        at org.apache.jackrabbit.core.query.lucene.SearchIndex.createExecutableQuery(SearchIndex.java:615)
        at org.apache.jackrabbit.core.query.QueryImpl.init(QueryImpl.java:128)
        at org.apache.jackrabbit.core.SearchManager.createQuery(SearchManager.java:282)
        at org.apache.jackrabbit.core.query.QueryManagerImpl.createQuery(QueryManagerImpl.java:102)
        at org.apache.jackrabbit.core.security.user.IndexNodeResolver.buildQuery(IndexNodeResolver.java:105)
        at org.apache.jackrabbit.core.security.user.IndexNodeResolver.findNode(IndexNodeResolver.java:50)
        at org.apache.jackrabbit.core.security.user.UserManagerImpl.getAuthorizable(UserManagerImpl.java:93)
        at org.apache.jackrabbit.core.security.user.UserManagerImpl.createUser(UserManagerImpl.java:177)
        at org.apache.jackrabbit.core.security.user.UserManagerImpl.createUser(UserManagerImpl.java:158)
        at FirstHop.main(FirstHop.java:20)

"
0,"HttpMethodBase does not compile on JDK prior to 1.3reason is the use of URL.getPath() and URL.getQuery() within method
processRedirectResponse.

should use URIUtil.getPath and URIUtil.getQuery instead.

so, HttpMethodBase around line 952:

//update the current location with the redirect location
setPath(URIUtil.getPath(redirectUrl.toString()));
setQueryString(URIUtil.getQuery(redirectUrl.toString()));

thanks,

marius"
0,"GData - Server wrong commit does not buildThe last GData - Server commit  does not build due to a wrong commit.
Yonik did not commit all the files in the diff file. There are several sources and packages missing.
  
The diff - file with the date of 26.06.06 should be applied.
--> http://issues.apache.org/jira/browse/LUCENE-598
26.06.06.diff (644 kb)

could any of the lucene committers apply this patch. Yonik is on the way to Dublin.

Thanks Simon
"
1,"Revision 949509 (LUCENE-2480) causes IOE ""read past EOF"" when processing older format SegmentInfo data when JVM assertion processing is disabled.At revision 949509 in org.apache.lucene.index.SegmentInfo at line 155, there is the following code:
{noformat} 
    if (format > SegmentInfos.FORMAT_4_0) {
      // pre-4.0 indexes write a byte if there is a single norms file
      assert 1 == input.readByte();
    }
{noformat} 
Note that the assert statement invokes input.readByte().
If asserts are disabled for the JVM, input.readByte() will not be invoked, causing the following readInt() to return a bogus value, and then causing an IOE during the (mistakenly entered) loop at line 165.
This can occur when processing old format (format ""-9"") index data under Tomcat (whose startup scripts by default do not turn on asserts).

Full stacktrace:
{noformat} 
SEVERE: java.lang.RuntimeException: java.io.IOException: read past EOF
	at org.apache.solr.core.SolrCore.getSearcher(SolrCore.java:1066)
	at org.apache.solr.core.SolrCore.<init>(SolrCore.java:581)
	at org.apache.solr.core.CoreContainer.create(CoreContainer.java:431)
	at org.apache.solr.core.CoreContainer.load(CoreContainer.java:286)
	at org.apache.solr.core.CoreContainer$Initializer.initialize(CoreContainer.java:125)
	at org.apache.solr.servlet.SolrDispatchFilter.init(SolrDispatchFilter.java:86)
	at org.apache.catalina.core.ApplicationFilterConfig.getFilter(ApplicationFilterConfig.java:275)
	at org.apache.catalina.core.ApplicationFilterConfig.setFilterDef(ApplicationFilterConfig.java:397)
	at org.apache.catalina.core.ApplicationFilterConfig.<init>(ApplicationFilterConfig.java:108)
	at org.apache.catalina.core.StandardContext.filterStart(StandardContext.java:3800)
	at org.apache.catalina.core.StandardContext.start(StandardContext.java:4450)
	at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:791)
	at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:771)
	at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:526)
	at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:850)
	at org.apache.catalina.startup.HostConfig.deployWARs(HostConfig.java:724)
	at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:493)
	at org.apache.catalina.startup.HostConfig.start(HostConfig.java:1206)
	at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:314)
	at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:119)
	at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1053)
	at org.apache.catalina.core.StandardHost.start(StandardHost.java:722)
	at org.apache.catalina.core.ContainerBase.start(ContainerBase.java:1045)
	at org.apache.catalina.core.StandardEngine.start(StandardEngine.java:443)
	at org.apache.catalina.core.StandardService.start(StandardService.java:516)
	at org.apache.catalina.core.StandardServer.start(StandardServer.java:710)
	at org.apache.catalina.startup.Catalina.start(Catalina.java:583)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.catalina.startup.Bootstrap.start(Bootstrap.java:288)
	at org.apache.catalina.startup.Bootstrap.main(Bootstrap.java:413)
Caused by: java.io.IOException: read past EOF
	at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:154)
	at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)
	at org.apache.lucene.store.ChecksumIndexInput.readByte(ChecksumIndexInput.java:40)
	at org.apache.lucene.store.DataInput.readInt(DataInput.java:76)
	at org.apache.lucene.store.DataInput.readLong(DataInput.java:99)
	at org.apache.lucene.index.SegmentInfo.<init>(SegmentInfo.java:165)
	at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:230)
	at org.apache.lucene.index.DirectoryReader$1.doBody(DirectoryReader.java:91)
	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:649)
	at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:87)
	at org.apache.lucene.index.IndexReader.open(IndexReader.java:415)
	at org.apache.lucene.index.IndexReader.open(IndexReader.java:294)
	at org.apache.solr.core.StandardIndexReaderFactory.newReader(StandardIndexReaderFactory.java:38)
	at org.apache.solr.core.SolrCore.getSearcher(SolrCore.java:1055)
	... 32 more
{noformat} "
1,"intermittent failure in TestIndexWriter. testExceptionDuringSync {code}
common.test:

    [mkdir] Created dir: C:\Projects\lucene\trunk-full1\build\test

    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter

    [junit] Tests run: 102, Failures: 0, Errors: 1, Time elapsed: 100,297sec

    [junit]

    [junit] Testcase: testExceptionDuringSync(org.apache.lucene.index.TestIndexWriter): Caused an ERROR

    [junit] _a.fnm

    [junit] java.io.FileNotFoundException: _a.fnm

    [junit]     at org.apache.lucene.store.MockRAMDirectory.openInput(MockRAMDirectory.java:226)

    [junit]     at org.apache.lucene.index.FieldInfos.<init>(FieldInfos.java:68)

    [junit]     at org.apache.lucene.index.SegmentReader$CoreReaders.<init>(SegmentReader.java:116)

    [junit]     at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:620)

    [junit]     at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:590)

    [junit]     at org.apache.lucene.index.DirectoryReader.<init>(DirectoryReader.java:104)

    [junit]     at org.apache.lucene.index.ReadOnlyDirectoryReader.<init>(ReadOnlyDirectoryReader.java:27)

    [junit]     at org.apache.lucene.index.DirectoryReader$1.doBody(DirectoryReader.java:74)

    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:704)

    [junit]     at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:69)

    [junit]     at org.apache.lucene.index.IndexReader.open(IndexReader.java:307)

    [junit]     at org.apache.lucene.index.IndexReader.open(IndexReader.java:193)

    [junit]     at org.apache.lucene.index.TestIndexWriter.testExceptionDuringSync(TestIndexWriter.java:2723)

    [junit]     at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:206)

    [junit]

    [junit]

    [junit] Test org.apache.lucene.index.TestIndexWriter FAILED
{code}"
0,"Optimize TermInfosWriter.addI found one more optimization, in how terms are written in
TermInfosWriter.  Previously, each term required a new Term() and a
new String().  Looking at the cpu time (using YourKit), I could see
this was adding a non-trivial cost to flush() when indexing Wikipedia.

I changed TermInfosWriter.add to accept char[] directly, instead.

I ran a quick test building first 200K docs of Wikipedia.  With this
fix it took 231.31 sec (best of 3) and without the fix it took 236.05
sec (best of 3) = ~2% speedup.
"
0,"Improved Payloads APIWe want to make some optimizations to the Payloads API.

See following thread for related discussions:
http://www.gossamer-threads.com/lists/lucene/java-dev/54708"
0,"HttpClient drops connection to the proxy when an invalid 'connection: close' directive is encountered in 'connection established' responseOne of our customer is using our application to connect to our servlet using 
https.  We are using httpClient for http protocol handling.  The customer has a 
IBM proxy (see log file).  The connect failed with a null pointer exception.

The log seem to indicate that the proxy server is returning 200 for ""CONNECT"", 
but the proxy also sends a ""Connection:close"" header.  The httpClient closed 
the connection and then tried to create the SSL socket.  If the proxy server is 
incorrect in sending 200 with ""Connection:close"", then httpClient should throw 
exception for invalid state (IllegalStateException ?).

I will attach the log file."
1,"fix analyzer bugs found by MockTokenizerIn LUCENE-3064, we beefed up MockTokenizer with assertions, and I've switched over the analysis tests to use MockTokenizer for better coverage.

However, this found a few bugs (one of which is LUCENE-3106):
* incrementToken() after it returns false in CommonGramsQueryFilter, HyphenatedWordsFilter, ShingleFilter, SynonymFilter
* missing end() implementation for PrefixAwareTokenFilter
* double reset() in QueryAutoStopWordAnalyzer and ReusableAnalyzerBase
* missing correctOffset()s in MockTokenizer itself.

I think it would be nice to just fix all the bugs on one issue... I've fixed everything except Shingle and Synonym"
0,"Remove getSafeJCRPath methods in HierarchyManagerImplThe getSafeJCRPath utility methods in the HierarchyManagerImpl class have not been used since revision 485720, but their presence still causes the hierarchy managers to depend on namespace mapping information. I'll remove the methods to simplify things."
1,"bugs in ByteArrayDataInputByteArrayDataInput has a byte[] ctor, but it doesn't actually work (some things like readVint will work, others will fail due to asserts).

The problem is it doesnt set things like limit in the ctor... I think the ctor should call reset()
Most code using this passes null to the ctor to initialize it, then uses reset(), instead they could just call ByteArrayInput(BytesRef.EMPTY_BYTES) if they want to do that.
finally, reset()'s limit looks like it should be offset + len"
0,"JSR 283: Repository ComplianceJSR 283 defines a huge set of new (or changed) repository descriptors as well as a couple of new methods on the
Repository interface that allows the API consumer to determine the feature set exposed by an implementation.

The new methods are

- Repository.isStandardDescriptor(String key)
- Repository.isSingleValueDescriptor(String key)
- Repository.getDescriptorValue(String key) Value
- Repository.getDescriptorValues(String key) Value[]"
0,"Clicking on the ""More Results"" link in luceneweb.war demo results in ArrayIndexOutOfBoundsExceptionSummary says it all."
0,"Precompile JavaCC parsers in jackrabbit-spi-commonsThe JavaCC-generated Java source files in jackrabbit-spi-commons require special configuration when importing Jackrabbit sources to an IDE like Eclipse. To make IDE integration smoother it would be nice if precompiled copies of the Java files existed the src/main/java folder.

Precompiling the sources would also allow us to avoid the JavaCC processing step during each Jackrabbit build. Instead we could have a separate profile for explicitly recompiling the JavaCC sources when they have been modified. In the past three years that has happened only once (JCR-952), so I think a bit of extra complexity there is justified by the simplification we can achieve in normal builds and IDE integration."
0,"Adding Event interface and isLocal()when a repository cluster is used, it seems that a common problem many people have is to detect if an observation event is send because of changes on the local instance or a remote instance of the cluster.

This is especially important if you want to do post processing of data
based on observation (the post processing should only be done by one instance in the cluster).

A current solution is to cast the jcr event object to the EventImpl of jackrabbit core which is obviously not a nice solution :)

So what about adding an event interface to jackrabbit api which extends the jcr event interface and adds the isLocal() method? "
0,"randomize skipInterval in testswe probably don't test the multi-level skipping very well, but skipInterval etc is now private to the codec, so for better test coverage we should parameterize it to the postings writers, and randomize it via mockrandomcodec."
0,"TCK: PropertyReadMethodsTest#testGetValues fails if it cannot find a single valued STRING propertyIf the JCR repository being tested does not contain any single valued property of type STRING, PropertyReadMethodsTest#testGetValues fails with the following exception:

java.lang.NullPointerException at org.apache.jackrabbit.test.api.PropertyReadMethodsTest.testGetValues(PropertyReadMethodsTest.java:273) at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:401)

The test should either try to find a single valued property of any type (it is guaranted that it will at least find jcr:primaryType) or should throw NotExecutableException if is does not find the property it needs."
0,"yank SegmentReader.norm out of SegmentReader.javaWhile working on flex scoring branch and LUCENE-3012, I noticed it was difficult to navigate 
the norms handling in SegmentReader's code.

I think we should yank this inner class out into a separate file as a start."
0,"remove unused code in SmartChineseAnalyzer hmm pkgthere is some unused code in the hmm package.

I would like to remove it before I supply a fix for LUCENE-1817.

only after this can we refactor any of this analyzer, otherwise we risk breaking custom dictionary support."
1,"TestIndexWriterExceptions reproducible AOOBE in MockVariableIntBlockCodec{code}
  [junit] Testsuite: org.apache.lucene.index.TestIndexWriterExceptions
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.739 sec
    [junit]
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testDocumentsWriterAbort -Dtests.seed=4579947455
682149564:-7960989923752018504
    [junit] NOTE: test params are: codec=RandomCodecProvider: {content=MockVariableIntBlock(baseBlockSize=32)}, locale=bg_BG, timezone=Brazil
/Acre
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestIndexWriterExceptions]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=94363216,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testDocumentsWriterAbort(org.apache.lucene.index.TestIndexWriterExceptions):      Caused an ERROR
    [junit] 66
    [junit] java.lang.ArrayIndexOutOfBoundsException: 66
    [junit]     at org.apache.lucene.index.codecs.mockintblock.MockVariableIntBlockCodec$MockIntFactory$2.add(MockVariableIntBlockCodec.java:
114)
    [junit]     at org.apache.lucene.index.codecs.intblock.VariableIntBlockIndexOutput.close(VariableIntBlockIndexOutput.java:118)
    [junit]     at org.apache.lucene.index.codecs.sep.SepPostingsWriterImpl.close(SepPostingsWriterImpl.java:320)
    [junit]     at org.apache.lucene.index.codecs.BlockTermsWriter.close(BlockTermsWriter.java:137)
    [junit]     at org.apache.lucene.index.PerFieldCodecWrapper$FieldsWriter.close(PerFieldCodecWrapper.java:81)
    [junit]     at org.apache.lucene.index.FreqProxTermsWriter.flush(FreqProxTermsWriter.java:103)
    [junit]     at org.apache.lucene.index.TermsHash.flush(TermsHash.java:118)
    [junit]     at org.apache.lucene.index.DocInverter.flush(DocInverter.java:80)
    [junit]     at org.apache.lucene.index.DocFieldProcessor.flush(DocFieldProcessor.java:75)
    [junit]     at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:457)
    [junit]     at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:417)
    [junit]     at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:309)
    [junit]     at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:381)
    [junit]     at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1469)
    [junit]     at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1229)
    [junit]     at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1210)
    [junit]     at org.apache.lucene.index.TestIndexWriterExceptions.testDocumentsWriterAbort(TestIndexWriterExceptions.java:555)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1333)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1251)
    [junit]
    [junit]
    [junit] Test org.apache.lucene.index.TestIndexWriterExceptions FAILED
{code}

trunk: r1127871"
1,"Search with sort fails when a document has a missing valueTesting on version: lucene-1.4-rc2

Call in question: IndexSearcher.search(Query query, Filter filter, int nDocs, 
Sort sort) 

Description: I'm making a call to search with a sort field - in my case I'm 
sorting by date. If any document in the results set (Hits) has a missing value 
in the sort field, the entire call throws an [uncaught] exception during the 
sorting process with no results returned. 

This is an undesireable result, and the prospects for patching this problem 
outside the search classes are ugly, e.g. trying to patch the index itself.

This is actually a critical function in my application. Thank you for 
addressing it.

-Dan"
1,"PostingsConsumer#merge does not call finishDocWe discovered that the current merge function in PostingsConsumer is not calling the #finishDoc method. This does not have consequences for the standard codec (since the lastPosition is set to 0 in #startDoc, and its #finishDoc method is empty), but for the SepCodec, this results in position file corruption (the lastPosition is set to 0 in #finishDoc for the SepCodec)."
0,"Lazy Atomic Loading Stopwords in SmartCN The default constructor in SmartChineseAnalyzer loads the default (jar embedded) stopwords each time the constructor is invoked. 
This should be atomically loaded only once in an unmodifiable set.

"
0,"Added New Token API impl for ASCIIFoldingFilterI added an implementation of incrementToken to ASCIIFoldingFilter.java and extended the existing  testcase for it.
I will attach the patch shortly.
Beside this improvement I would like to start up a small discussion about this filter. ASCIIFoldingFitler is meant to be a replacement for ISOLatin1AccentFilter which is quite nice as it covers a superset of the latter. I have used this filter quite often but never on a as it is basis. In the most cases this filter does the correct thing (replace a special char with its ascii correspondent) but in some cases like for German umlaut it does not return the expected result. A german umlaut  like '' does not translate to a but rather to 'ae'. I would like to change this but I'n not 100% sure if that is expected by all users of that filter. Another way of doing it would be to make it configurable with a flag. This would not affect performance as we only check if such a umlaut char is found. 
Further it would be really helpful if that filter could ""inject"" the original/unmodified token with the same position increment into the token stream on demand. I think its a valid use-case to index the modified and unmodified token. For instance, the german word ""sd"" would be folded to ""sud"". In a query q:(sd) the filter would also fold to sud and therefore find sud which has a totally different meaning. Folding works quite well but for special cases would could add those options to make users life easier. The latter could be done in a subclass while the umlaut problem should be fixed in the base class.

simon "
0,"client cache does not respect 'Cache-Control: no-store' on requests""The purpose of the no-store directive is to prevent the inadvertent release or retention of sensitive information (for example, on backup tapes). The no-store directive applies to the entire message, and MAY be sent either in a response or in a request. If sent in a request, a cache MUST NOT store any part of either this request or any response to it.""

http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.2

The current implementation will incorrectly cache responses to requests containing 'Cache-Control: no-store'."
0,"Skip sync delay when changes are foundThe cluster synchronization on a slave does always wait for some time (as specified in the sync delay) before fetching changes. If a lot of changes are being written to the master, a slave will considerably fall behind the master in term of revisions, which may endanger the integrity of the cluster if the master will crash. I therefore suggest that a slave should rather immediately contact the master again after some changes have been found, until it sees no more changes."
0,"Replacement for TermAttribute+Impl with extended capabilities (byte[] support, CharSequence, Appendable)For flexible indexing terms can be simple byte[] arrays, while the current TermAttribute only supports char[]. This is fine for plain text, but e.g NumericTokenStream should directly work on the byte[] array.
Also TermAttribute lacks of some interfaces that would make it simplier for users to work with them: Appendable and CharSequence

I propose to create a new interface ""CharTermAttribute"" with a clean new API that concentrates on CharSequence and Appendable.
The implementation class will simply support the old and new interface working on the same term buffer. DEFAULT_ATTRIBUTE_FACTORY will take care of this. So if somebody adds a TermAttribute, he will get an implementation class that can be also used as CharTermAttribute. As both attributes create the same impl instance both calls to addAttribute are equal. So a TokenFilter that adds CharTermAttribute to the source will work with the same instance as the Tokenizer that requested the (deprecated) TermAttribute.

To also support byte[] only terms like Collation or NumericField needs, a separate getter-only interface will be added, that returns a reusable BytesRef, e.g. BytesRefGetterAttribute. The default implementation class will also support this interface. For backwards compatibility with old self-made-TermAttribute implementations, the indexer will check with hasAttribute(), if the BytesRef getter interface is there and if not will wrap a old-style TermAttribute (a deprecated wrapper class will be provided): new BytesRefGetterAttributeWrapper(TermAttribute), that is used by the indexer then."
0,"Exception when missing namespace in CND file should have clearer messageUsing the attached CND file, when calling CndImporter.registerNodeTypes(..) the following message is in the returned exception:

""Unable to parse CND Input: Error setting name of sling:resourceType to sling:resourceType (bundle://23.0:0/SLING-INF/nodetypes/types.cnd, line 24)""

The issue with line 24 is that the ""sling"" namespace has not been included.  The message should state that a namespace is missing and what prefix is not understood."
0,"Let the AbstractISMLockingTest tests fail properlyThe tests in the AbstractISMLockingTest class call junit.framework.Assert.fail() on threads that are not managed by the JUnit framework. Therefore, such calls to fail are not interpreted as test failures, but are merely logged to the console and the build succeeds. This is easy to see with a stub implementation of the ISMLocking type which returns non-null references from the two acquire methods and the downgrade method: many of the following stacktraces appear, but the build succeeds.

Exception in thread ""Thread-1"" junit.framework.AssertionFailedError: acquireWriteLock must block
	at junit.framework.Assert.fail(Assert.java:47)
	at org.apache.jackrabbit.core.state.AbstractISMLockingTest.checkBlocking(AbstractISMLockingTest.java:214)
	at org.apache.jackrabbit.core.state.AbstractISMLockingTest$1.run(AbstractISMLockingTest.java:88)
	at java.lang.Thread.run(Thread.java:613)


"
1,"TestIndexWriter#testThreadInterruptDeadlock fails with OOM Selckin reported a repeatedly failing test that throws OOM Exceptions. According to the heapdump the MockDirectoryWrapper#createdFiles HashSet takes about 400MB heapspace containing 4194304 entries. Seems kind of way too many though :)

{noformat}
 [junit] java.lang.OutOfMemoryError: Java heap space
    [junit] Dumping heap to /tmp/java_pid25990.hprof ...
    [junit] Heap dump file created [520807744 bytes in 4.250 secs]
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter
    [junit] Testcase: testThreadInterruptDeadlock(org.apache.lucene.index.TestIndexWriter):	FAILED
    [junit] 
    [junit] junit.framework.AssertionFailedError: 
    [junit] 	at org.apache.lucene.index.TestIndexWriter.testThreadInterruptDeadlock(TestIndexWriter.java:2249)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1282)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1211)
    [junit] 
    [junit] 
    [junit] Testcase: testThreadInterruptDeadlock(org.apache.lucene.index.TestIndexWriter):	FAILED
    [junit] Some threads threw uncaught exceptions!
    [junit] junit.framework.AssertionFailedError: Some threads threw uncaught exceptions!
    [junit] 	at org.apache.lucene.util.LuceneTestCase.tearDown(LuceneTestCase.java:557)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1282)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1211)
    [junit] 
    [junit] 
    [junit] Tests run: 67, Failures: 2, Errors: 0, Time elapsed: 3,254.884 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] FAILED; unexpected exception
    [junit] java.lang.OutOfMemoryError: Java heap space
    [junit] 	at org.apache.lucene.store.RAMFile.newBuffer(RAMFile.java:85)
    [junit] 	at org.apache.lucene.store.RAMFile.addBuffer(RAMFile.java:58)
    [junit] 	at org.apache.lucene.store.RAMOutputStream.switchCurrentBuffer(RAMOutputStream.java:132)
    [junit] 	at org.apache.lucene.store.RAMOutputStream.copyBytes(RAMOutputStream.java:171)
    [junit] 	at org.apache.lucene.store.MockIndexOutputWrapper.copyBytes(MockIndexOutputWrapper.java:155)
    [junit] 	at org.apache.lucene.index.CompoundFileWriter.copyFile(CompoundFileWriter.java:223)
    [junit] 	at org.apache.lucene.index.CompoundFileWriter.close(CompoundFileWriter.java:189)
    [junit] 	at org.apache.lucene.index.SegmentMerger.createCompoundFile(SegmentMerger.java:138)
    [junit] 	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3344)
    [junit] 	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2959)
    [junit] 	at org.apache.lucene.index.SerialMergeScheduler.merge(SerialMergeScheduler.java:37)
    [junit] 	at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1763)
    [junit] 	at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1758)
    [junit] 	at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1754)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1373)
    [junit] 	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1230)
    [junit] 	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1211)
    [junit] 	at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:2154)
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testThreadInterruptDeadlock -Dtests.seed=7183538093651149:3431510331342554160
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testThreadInterruptDeadlock -Dtests.seed=7183538093651149:3431510331342554160
    [junit] The following exceptions were thrown by threads:
    [junit] *** Thread: Thread-379 ***
    [junit] java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_3r1n_0.tib=1, _3r1n_0.frq=1, _3r1n_0.pos=1, _3r1m.cfs=1, _3r1n_0.doc=1, _3r1n.tvf=1, _3r1n.tvd=1, _3r1n.tvx=1, _3r1n.fdx=1, _3r1n.fdt=1, _3r1q.cfs=1, _3r1o.cfs=1, _3r1n_0.skp=1, _3r1n_0.pyl=1}
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:448)
    [junit] 	at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:2217)
    [junit] Caused by: java.lang.RuntimeException: unclosed IndexOutput
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.createOutput(MockDirectoryWrapper.java:367)
    [junit] 	at org.apache.lucene.index.FieldInfos.write(FieldInfos.java:563)
    [junit] 	at org.apache.lucene.index.DocFieldProcessor.flush(DocFieldProcessor.java:82)
    [junit] 	at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:381)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:378)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:505)
    [junit] 	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:2621)
    [junit] 	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:2598)
    [junit] 	at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2464)
    [junit] 	at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2537)
    [junit] 	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2519)
    [junit] 	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2503)
    [junit] 	at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:2156)
    [junit] NOTE: test params are: codec=RandomCodecProvider: {=MockVariableIntBlock(baseBlockSize=105), f6=MockFixedIntBlock(blockSize=1372), f7=Pulsing(freqCutoff=11), f8=MockRandom, f9=MockVariableIntBlock(baseBlockSize=105), f1=MockSep, f0=Pulsing(freqCutoff=11), f3=Pulsing(freqCutoff=11), f2=MockFixedIntBlock(blockSize=1372), f5=MockVariableIntBlock(baseBlockSize=105), f4=MockRandom, f=Standard, c=Pulsing(freqCutoff=11), termVector=MockFixedIntBlock(blockSize=1372), d9=MockVariableIntBlock(baseBlockSize=105), d8=MockRandom, d5=MockSep, d4=Pulsing(freqCutoff=11), d7=MockFixedIntBlock(blockSize=1372), d6=MockVariableIntBlock(baseBlockSize=105), d25=SimpleText, d0=Pulsing(freqCutoff=11), c29=SimpleText, d24=MockSep, d1=MockSep, c28=MockVariableIntBlock(baseBlockSize=105), d23=MockRandom, d2=MockVariableIntBlock(baseBlockSize=105), c27=MockRandom, d22=Standard, d3=MockFixedIntBlock(blockSize=1372), d21=MockVariableIntBlock(baseBlockSize=105), d20=MockRandom, c22=SimpleText, c21=MockSep, c20=MockRandom, d29=MockFixedIntBlock(blockSize=1372), c26=MockFixedIntBlock(blockSize=1372), d28=MockVariableIntBlock(baseBlockSize=105), c25=MockVariableIntBlock(baseBlockSize=105), d27=MockSep, c24=MockSep, d26=Pulsing(freqCutoff=11), c23=Pulsing(freqCutoff=11), e9=MockSep, e8=Standard, e7=SimpleText, e6=MockVariableIntBlock(baseBlockSize=105), e5=MockRandom, c17=MockSep, e3=SimpleText, d12=Pulsing(freqCutoff=11), c16=Pulsing(freqCutoff=11), e4=Standard, d11=MockFixedIntBlock(blockSize=1372), c19=MockFixedIntBlock(blockSize=1372), e1=MockRandom, d14=MockVariableIntBlock(baseBlockSize=105), c18=MockVariableIntBlock(baseBlockSize=105), e2=MockVariableIntBlock(baseBlockSize=105), d13=MockRandom, e0=MockFixedIntBlock(blockSize=1372), d10=MockSep, d19=Pulsing(freqCutoff=11), c11=MockVariableIntBlock(baseBlockSize=105), c10=MockRandom, d16=MockRandom, c13=MockRandom, c12=Standard, d15=Standard, d18=SimpleText, c15=SimpleText, d17=MockSep, c14=MockSep, b3=SimpleText, b2=MockSep, b5=Pulsing(freqCutoff=11), b4=MockFixedIntBlock(blockSize=1372), b7=MockFixedIntBlock(blockSize=1372), b6=MockVariableIntBlock(baseBlockSize=105), d50=Pulsing(freqCutoff=11), b9=MockRandom, b8=Standard, d43=MockRandom, d42=Standard, d41=MockFixedIntBlock(blockSize=1372), d40=MockVariableIntBlock(baseBlockSize=105), d47=MockSep, d46=Pulsing(freqCutoff=11), b0=MockFixedIntBlock(blockSize=1372), d45=Standard, b1=Pulsing(freqCutoff=11), d44=SimpleText, d49=Pulsing(freqCutoff=11), d48=MockFixedIntBlock(blockSize=1372), c6=MockRandom, c5=Standard, c4=MockFixedIntBlock(blockSize=1372), c3=MockVariableIntBlock(baseBlockSize=105), c9=Pulsing(freqCutoff=11), c8=Standard, c7=SimpleText, d30=SimpleText, d32=Pulsing(freqCutoff=11), d31=MockFixedIntBlock(blockSize=1372), c1=Standard, d34=MockFixedIntBlock(blockSize=1372), c2=MockRandom, d33=MockVariableIntBlock(baseBlockSize=105), d36=MockRandom, c0=MockFixedIntBlock(blockSize=1372), d35=Standard, d38=Standard, d37=SimpleText, d39=Pulsing(freqCutoff=11), e92=MockFixedIntBlock(blockSize=1372), e93=Pulsing(freqCutoff=11), e90=MockSep, e91=SimpleText, e89=Pulsing(freqCutoff=11), e88=Standard, e87=SimpleText, e86=MockRandom, e85=Standard, e84=MockFixedIntBlock(blockSize=1372), e83=MockVariableIntBlock(baseBlockSize=105), e80=MockVariableIntBlock(baseBlockSize=105), e81=SimpleText, e82=Standard, e77=MockFixedIntBlock(blockSize=1372), e76=MockVariableIntBlock(baseBlockSize=105), e79=MockRandom, e78=Standard, e73=SimpleText, e72=MockSep, e75=Pulsing(freqCutoff=11), e74=MockFixedIntBlock(blockSize=1372), binary=Pulsing(freqCutoff=11), f98=MockSep, f97=Pulsing(freqCutoff=11), f99=MockVariableIntBlock(baseBlockSize=105), f94=MockRandom, f93=Standard, f96=SimpleText, f95=MockSep, e95=MockSep, e94=Pulsing(freqCutoff=11), e97=MockFixedIntBlock(blockSize=1372), e96=MockVariableIntBlock(baseBlockSize=105), e99=MockVariableIntBlock(baseBlockSize=105), e98=MockRandom, id=MockRandom, f34=SimpleText, f33=MockSep, f32=MockRandom, f31=Standard, f30=MockVariableIntBlock(baseBlockSize=105), f39=MockRandom, f38=MockFixedIntBlock(blockSize=1372), f37=MockVariableIntBlock(baseBlockSize=105), f36=MockSep, f35=Pulsing(freqCutoff=11), f43=MockSep, f42=Pulsing(freqCutoff=11), f45=MockFixedIntBlock(blockSize=1372), f44=MockVariableIntBlock(baseBlockSize=105), f41=SimpleText, f40=MockSep, f47=MockVariableIntBlock(baseBlockSize=105), f46=MockRandom, f49=Standard, f48=SimpleText, content=MockFixedIntBlock(blockSize=1372), e19=Standard, e18=SimpleText, e17=MockRandom, f12=Standard, e16=Standard, f11=SimpleText, f10=MockVariableIntBlock(baseBlockSize=105), e15=MockFixedIntBlock(blockSize=1372), e14=MockVariableIntBlock(baseBlockSize=105), f16=Pulsing(freqCutoff=11), e13=Pulsing(freqCutoff=11), f15=MockFixedIntBlock(blockSize=1372), e12=MockFixedIntBlock(blockSize=1372), e11=SimpleText, f14=SimpleText, e10=MockSep, f13=MockSep, f19=Standard, f18=MockFixedIntBlock(blockSize=1372), f17=MockVariableIntBlock(baseBlockSize=105), e29=MockFixedIntBlock(blockSize=1372), e26=Standard, f21=SimpleText, e25=SimpleText, f20=MockSep, e28=MockSep, f23=Pulsing(freqCutoff=11), e27=Pulsing(freqCutoff=11), f22=MockFixedIntBlock(blockSize=1372), f25=MockFixedIntBlock(blockSize=1372), e22=MockFixedIntBlock(blockSize=1372), f24=MockVariableIntBlock(baseBlockSize=105), e21=MockVariableIntBlock(baseBlockSize=105), f27=MockRandom, e24=MockRandom, f26=Standard, e23=Standard, f29=Standard, f28=SimpleText, e20=Pulsing(freqCutoff=11), field=MockRandom, string=Pulsing(freqCutoff=11), e30=MockSep, e31=SimpleText, a98=MockRandom, e34=MockVariableIntBlock(baseBlockSize=105), a99=MockVariableIntBlock(baseBlockSize=105), e35=MockFixedIntBlock(blockSize=1372), f79=MockVariableIntBlock(baseBlockSize=105), e32=Pulsing(freqCutoff=11), e33=MockSep, b97=Pulsing(freqCutoff=11), f77=MockFixedIntBlock(blockSize=1372), e38=SimpleText, b98=MockSep, f78=Pulsing(freqCutoff=11), e39=Standard, b99=MockVariableIntBlock(baseBlockSize=105), f75=MockSep, e36=MockRandom, f76=SimpleText, e37=MockVariableIntBlock(baseBlockSize=105), f73=SimpleText, f74=Standard, f71=MockRandom, f72=MockVariableIntBlock(baseBlockSize=105), f81=MockFixedIntBlock(blockSize=1372), f80=MockVariableIntBlock(baseBlockSize=105), e40=MockSep, e41=MockVariableIntBlock(baseBlockSize=105), e42=MockFixedIntBlock(blockSize=1372), e43=MockRandom, e44=MockVariableIntBlock(baseBlockSize=105), e45=SimpleText, e46=Standard, f86=MockVariableIntBlock(baseBlockSize=105), e47=MockSep, f87=MockFixedIntBlock(blockSize=1372), e48=SimpleText, f88=Standard, e49=MockFixedIntBlock(blockSize=1372), f89=MockRandom, f82=MockSep, f83=SimpleText, f84=MockFixedIntBlock(blockSize=1372), f85=Pulsing(freqCutoff=11), f90=MockVariableIntBlock(baseBlockSize=105), f92=Standard, f91=SimpleText, str=MockFixedIntBlock(blockSize=1372), a76=MockVariableIntBlock(baseBlockSize=105), e56=MockRandom, f59=MockRandom, a77=MockFixedIntBlock(blockSize=1372), e57=MockVariableIntBlock(baseBlockSize=105), a78=Standard, e54=MockFixedIntBlock(blockSize=1372), f57=MockFixedIntBlock(blockSize=1372), a79=MockRandom, e55=Pulsing(freqCutoff=11), f58=Pulsing(freqCutoff=11), e52=Pulsing(freqCutoff=11), e53=MockSep, e50=SimpleText, e51=Standard, f51=Standard, f52=MockRandom, f50=MockFixedIntBlock(blockSize=1372), f55=Pulsing(freqCutoff=11), f56=MockSep, f53=SimpleText, e58=Standard, f54=Standard, e59=MockRandom, a80=MockVariableIntBlock(baseBlockSize=105), e60=MockRandom, a82=Standard, a81=SimpleText, a84=SimpleText, a83=MockSep, a86=Pulsing(freqCutoff=11), a85=MockFixedIntBlock(blockSize=1372), a89=Pulsing(freqCutoff=11), f68=Standard, e65=Standard, f69=MockRandom, e66=MockRandom, a87=SimpleText, e67=MockSep, a88=Standard, e68=SimpleText, e61=MockFixedIntBlock(blockSize=1372), e62=Pulsing(freqCutoff=11), e63=MockRandom, e64=MockVariableIntBlock(baseBlockSize=105), f60=SimpleText, f61=Standard, f62=Pulsing(freqCutoff=11), f63=MockSep, e69=Pulsing(freqCutoff=11), f64=MockFixedIntBlock(blockSize=1372), f65=Pulsing(freqCutoff=11), f66=MockRandom, f67=MockVariableIntBlock(baseBlockSize=105), f70=MockRandom, a93=Pulsing(freqCutoff=11), a92=MockFixedIntBlock(blockSize=1372), a91=SimpleText, e71=MockSep, a90=MockSep, e70=Pulsing(freqCutoff=11), a97=MockRandom, a96=Standard, a95=MockFixedIntBlock(blockSize=1372), a94=MockVariableIntBlock(baseBlockSize=105), c58=MockFixedIntBlock(blockSize=1372), a63=Pulsing(freqCutoff=11), a64=MockSep, c59=Pulsing(freqCutoff=11), c56=MockSep, d59=MockSep, a61=SimpleText, c57=SimpleText, a62=Standard, c54=SimpleText, c55=Standard, a60=MockRandom, c52=MockRandom, c53=MockVariableIntBlock(baseBlockSize=105), d53=MockVariableIntBlock(baseBlockSize=105), d54=MockFixedIntBlock(blockSize=1372), d51=Pulsing(freqCutoff=11), d52=MockSep, d57=SimpleText, b62=Standard, d58=Standard, b63=MockRandom, d55=MockRandom, b60=MockVariableIntBlock(baseBlockSize=105), d56=MockVariableIntBlock(baseBlockSize=105), b61=MockFixedIntBlock(blockSize=1372), b56=MockSep, b55=Pulsing(freqCutoff=11), b54=Standard, b53=SimpleText, d61=SimpleText, b59=MockRandom, d60=MockSep, b58=Pulsing(freqCutoff=11), b57=MockFixedIntBlock(blockSize=1372), c62=MockFixedIntBlock(blockSize=1372), c61=MockVariableIntBlock(baseBlockSize=105), a59=MockRandom, c60=MockSep, a58=Standard, a57=MockVariableIntBlock(baseBlockSize=105), a56=MockRandom, a55=Pulsing(freqCutoff=11), a54=MockFixedIntBlock(blockSize=1372), a72=MockFixedIntBlock(blockSize=1372), c67=MockVariableIntBlock(baseBlockSize=105), a73=Pulsing(freqCutoff=11), c68=MockFixedIntBlock(blockSize=1372), a74=MockRandom, c69=Standard, a75=MockVariableIntBlock(baseBlockSize=105), c63=MockSep, c64=SimpleText, a70=Pulsing(freqCutoff=11), c65=MockFixedIntBlock(blockSize=1372), a71=MockSep, c66=Pulsing(freqCutoff=11), d62=MockRandom, d63=MockVariableIntBlock(baseBlockSize=105), d64=SimpleText, b70=MockRandom, d65=Standard, b71=SimpleText, d66=MockSep, b72=Standard, d67=SimpleText, b73=Pulsing(freqCutoff=11), d68=MockFixedIntBlock(blockSize=1372), b74=MockSep, d69=Pulsing(freqCutoff=11), b65=Pulsing(freqCutoff=11), b64=MockFixedIntBlock(blockSize=1372), b67=MockVariableIntBlock(baseBlockSize=105), b66=MockRandom, d70=MockSep, b69=MockRandom, b68=Standard, d72=MockFixedIntBlock(blockSize=1372), d71=MockVariableIntBlock(baseBlockSize=105), c71=MockVariableIntBlock(baseBlockSize=105), c70=MockRandom, a69=Pulsing(freqCutoff=11), c73=Standard, c72=SimpleText, a66=MockRandom, a65=Standard, a68=SimpleText, a67=MockSep, c32=Standard, c33=MockRandom, c30=MockVariableIntBlock(baseBlockSize=105), c31=MockFixedIntBlock(blockSize=1372), c36=Pulsing(freqCutoff=11), a41=MockSep, c37=MockSep, a42=SimpleText, a0=MockSep, c34=SimpleText, c35=Standard, a40=MockRandom, b84=SimpleText, d79=MockSep, b85=Standard, b82=MockRandom, d77=Standard, c38=MockFixedIntBlock(blockSize=1372), b83=MockVariableIntBlock(baseBlockSize=105), d78=MockRandom, c39=Pulsing(freqCutoff=11), b80=MockVariableIntBlock(baseBlockSize=105), d75=MockRandom, b81=MockFixedIntBlock(blockSize=1372), d76=MockVariableIntBlock(baseBlockSize=105), d73=MockFixedIntBlock(blockSize=1372), d74=Pulsing(freqCutoff=11), d83=MockSep, a9=Standard, d82=Pulsing(freqCutoff=11), d81=Standard, d80=SimpleText, b79=MockVariableIntBlock(baseBlockSize=105), b78=Pulsing(freqCutoff=11), b77=MockFixedIntBlock(blockSize=1372), b76=SimpleText, b75=MockSep, a1=SimpleText, a35=MockFixedIntBlock(blockSize=1372), a2=Standard, a34=MockVariableIntBlock(baseBlockSize=105), a3=Pulsing(freqCutoff=11), a33=MockSep, a4=MockSep, a32=Pulsing(freqCutoff=11), a5=MockFixedIntBlock(blockSize=1372), a39=Standard, c40=Pulsing(freqCutoff=11), a6=Pulsing(freqCutoff=11), a38=SimpleText, a7=MockRandom, a37=MockVariableIntBlock(baseBlockSize=105), a8=MockVariableIntBlock(baseBlockSize=105), a36=MockRandom, c41=SimpleText, c42=Standard, c43=Pulsing(freqCutoff=11), c44=MockSep, c45=MockFixedIntBlock(blockSize=1372), a50=Pulsing(freqCutoff=11), c46=Pulsing(freqCutoff=11), a51=MockSep, c47=MockRandom, a52=MockVariableIntBlock(baseBlockSize=105), c48=MockVariableIntBlock(baseBlockSize=105), a53=MockFixedIntBlock(blockSize=1372), b93=MockSep, d88=Pulsing(freqCutoff=11), c49=Standard, b94=SimpleText, d89=MockSep, b95=MockFixedIntBlock(blockSize=1372), b96=Pulsing(freqCutoff=11), d84=Standard, b90=MockVariableIntBlock(baseBlockSize=105), d85=MockRandom, b91=SimpleText, d86=MockSep, b92=Standard, d87=SimpleText, d92=Pulsing(freqCutoff=11), d91=MockFixedIntBlock(blockSize=1372), d94=MockVariableIntBlock(baseBlockSize=105), d93=MockRandom, b87=MockFixedIntBlock(blockSize=1372), b86=MockVariableIntBlock(baseBlockSize=105), d90=MockSep, b89=MockRandom, b88=Standard, a44=MockVariableIntBlock(baseBlockSize=105), a43=MockRandom, a46=Standard, a45=SimpleText, a48=SimpleText, a47=MockSep, c51=MockRandom, a49=MockFixedIntBlock(blockSize=1372), c50=Standard, d98=MockRandom, d97=Standard, d96=MockFixedIntBlock(blockSize=1372), d95=MockVariableIntBlock(baseBlockSize=105), d99=SimpleText, a20=Standard, c99=MockSep, c98=Pulsing(freqCutoff=11), c97=Standard, c96=SimpleText, b19=Standard, a16=Standard, a17=MockRandom, b17=MockVariableIntBlock(baseBlockSize=105), a14=MockVariableIntBlock(baseBlockSize=105), b18=MockFixedIntBlock(blockSize=1372), a15=MockFixedIntBlock(blockSize=1372), a12=MockFixedIntBlock(blockSize=1372), a13=Pulsing(freqCutoff=11), a10=MockSep, a11=SimpleText, b11=SimpleText, b12=Standard, b10=MockVariableIntBlock(baseBlockSize=105), b15=MockFixedIntBlock(blockSize=1372), b16=Pulsing(freqCutoff=11), a18=SimpleText, b13=MockSep, a19=Standard, b14=SimpleText, b30=Standard, a31=Pulsing(freqCutoff=11), a30=MockFixedIntBlock(blockSize=1372), b28=SimpleText, a25=SimpleText, b29=Standard, a26=Standard, a27=Pulsing(freqCutoff=11), a28=MockSep, a21=MockVariableIntBlock(baseBlockSize=105), a22=MockFixedIntBlock(blockSize=1372), a23=Standard, a24=MockRandom, b20=MockSep, b21=SimpleText, b22=MockFixedIntBlock(blockSize=1372), b23=Pulsing(freqCutoff=11), a29=MockFixedIntBlock(blockSize=1372), b24=MockVariableIntBlock(baseBlockSize=105), b25=MockFixedIntBlock(blockSize=1372), b26=Standard, b27=MockRandom, b41=MockVariableIntBlock(baseBlockSize=105), b40=MockRandom, c77=SimpleText, c76=MockSep, c75=MockRandom, c74=Standard, c79=MockSep, c78=Pulsing(freqCutoff=11), c80=MockSep, c83=MockRandom, c84=MockVariableIntBlock(baseBlockSize=105), c81=MockFixedIntBlock(blockSize=1372), b39=MockRandom, c82=Pulsing(freqCutoff=11), b37=MockVariableIntBlock(baseBlockSize=105), b38=MockFixedIntBlock(blockSize=1372), b35=Pulsing(freqCutoff=11), b36=MockSep, b33=MockSep, b34=SimpleText, b31=Standard, b32=MockRandom, str2=Standard, b50=MockRandom, b52=SimpleText, str3=Pulsing(freqCutoff=11), b51=MockSep, c86=MockSep, tvtest=Pulsing(freqCutoff=11), c85=Pulsing(freqCutoff=11), c88=MockFixedIntBlock(blockSize=1372), c87=MockVariableIntBlock(baseBlockSize=105), c89=MockRandom, c90=MockRandom, c91=MockVariableIntBlock(baseBlockSize=105), c92=Standard, c93=MockRandom, c94=MockSep, c95=SimpleText, content1=SimpleText, b46=MockRandom, b47=MockVariableIntBlock(baseBlockSize=105), content3=MockRandom, b48=SimpleText, content4=Standard, b49=Standard, content5=MockVariableIntBlock(baseBlockSize=105), b42=Pulsing(freqCutoff=11), b43=MockSep, b44=MockVariableIntBlock(baseBlockSize=105), b45=MockFixedIntBlock(blockSize=1372)}, locale=it_CH, timezone=Europe/Chisinau
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestMergeSchedulerExternal, TestToken, TestCodecs, TestFieldInfos, TestFlushByRamOrCountsPolicy, TestIndexReaderReopen, TestIndexWriter]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=275548088,total=309395456
{noformat}"
1,"LockableFileRevision not thread-safeLockableFileRevision works well across process boundaries, but does not within the same JVM. The methods lock() and unlock() must be synchronized similar to DatabaseRevision."
0,"Add myqsql ddl for clustering (DatabaseJournal)the default ddl for clustering does't work with mysql, so it would be nice to include a mysql specific ddl also for clustering."
0,"FST should differentiate between final vs non-final stop nodesI'm breaking out this one improvement from LUCENE-2948...

Currently, if a node has no outgoing edges (a ""stop node"") the FST
forcefully marks this as a final node, but it need not do this.  Ie,
whether that node is final or not should be orthogonal to whether it
has arcs leaving or not.
"
1,"Calling PropertyDef.getDefaultValue() via RMI results in Exceptionhi jukka

30.03.2005 15:28:23 *MARK * servletengine: Servlet threw exception: 
org.apache.jackrabbit.rmi.client.RemoteRuntimeException: java.rmi.UnmarshalException: error unmarshalling return; nested exception is: 
	java.io.WriteAbortedException: writing aborted; java.io.NotSerializableException: javax.jcr.BooleanValue
	at org.apache.jackrabbit.rmi.client.ClientPropertyDef.getDefaultValues(ClientPropertyDef.java:76)

[...]

regards
angela"
0,"Expert API to specify indexing chainIt would be nice to add an expert API to specify an indexing chain, so that
we can make use of Mike's nice LUCENE-1301 feature.

This patch simply adds a package-protected expert API to IndexWriter and 
DocumentsWriter. It adds a inner, abstract class to DocumentsWriter called 
IndexingChain, and a default implementation that is the currently used one.

This might not be the final solution, but a nice way to play with different
modules in the indexing chain.

Could you take a look at the patch, Mike? "
0,Fix for deprecations in contrib/surroundFix for deprecations in contrib/surround.
1,"FieldSortedHitQueue - subsequent String sorts with different locales sort identicallyFrom my own post to the java-user list. I have looked into this further and am sure it's a bug.

---

It seems to me that there's a possible bug in FieldSortedHitQueue, specifically in getCachedComparator(). This is showing up on our 1.4.3 install, but it seems from source code inspection that if it's a bug, it's in 1.9.1 also.

The issue shows up when you need to sort results from a given IndexReader multiple times, using different locales. On line 180 (all line numbers from the 1.9.1 code), we have this:

ScoreDocComparator comparator = lookup (reader, fieldname, type, factory);

Then, if no comparator is found in the cache, a new one is created (line 193) and then stored in the cache (line 202). HOWEVER, both the cache lookup() and store() do NOT take into account locale; if we, on the same index reader, try to do one search sorted by Locale.FRENCH and one by Locale.ITALIAN, the first one will result in a cache miss, a new French comparator will be created, and stored in the cache. Second time through, lookup() finds the cached French comparator -- even though this time, the locale parameter to getCachedComparator() is an Italian locale. Therefore, we don't create a new comparator and we use the wrong one to sort the results.

It looks to me (unless I'm mistaken) that the FieldCacheImpl.Entry class should have an additional property, .locale, to ensure that different locales get different comparators.

---

Patch (well, most of one) to follow immediately."
1,"304 response status handlingI have an IBM WebSphere server that returns 304 responses with a Content-
Length header set to something other than 0 and the server is not closing the 
connection.  According to the HTTP RFC 
(http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.3.5):

""The 304 response MUST NOT contain a message-body, and thus is always 
terminated by the first empty line after the header fields.""

Obviously, the web server is returning a bad response but the HTTPClient 
blocks waiting on data in the response even though there shouldn't be any.  
Other HTTP clients (browsers) do not have this issue and seem to ignore the 
fact that the server set an invalid Content-Length in the response."
0,"Support only-if-cached directiveAdd support for only-if-cached Cache-Control directive- If the request is not servable from the cache, return a 504 Gateway Timeout.  See http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.4"
1,"Readers wrapping other readers don't prevent usage if any of their subreaders was closedOn recent trunk test we got this problem:
org.apache.lucene.index.TestReaderClosed.test
fails because the inner reader is closed but the wrapped outer ones are still open.

I fixed the issue partially for SlowCompositeReaderWrapper and ParallelAtomicReader but it failed again. The cool thing with this test is the following:

The test opens an DirectoryReader and then creates a searcher, closes the reader and executes a search. This is not an issue, if the reader is closed that the search is running on. This test uses LTC.newSearcher(wrap=true), which randomly wraps the passed Reader with SlowComposite or ParallelReader - or with both!!! If you then close the original inner reader, the close is not detected when excuting search. This can cause SIGSEGV when MMAP is used.

The problem in (in Slow* and Parallel*) is, that both have their own Fields instances thats are kept alive until the reader itsself is closed. If the child reader is closed, the wrapping reader does not know and still uses its own Fields instance that delegates to the inner readers. On this step no more ensureOpen checks are done, causing the failures.

The first fix done in Slow and Parallel was to call ensureOpen() on the subReader, too when requesting fields(). This works fine until you wrap two times: ParallelAtomicReader(SlowCompositeReaderWrapper(StandardDirectoryReader(segments_1:3:nrt _0(4.0):C42)))

One solution would be to make ensureOpen also check all subreaders, but that would do the volatile checks way too often (with n is the total number of subreaders and m is the number of hierarchical levels this is n^m) - we cannot do this. Currently we only have n*m which is fine.

The proposal how to solve this (closing subreaders under the hood of parent readers is to use the readerClosedListeners. Whenever a composite or slow reader wraps another readers, it registers itself as interested in readerClosed events. When a subreader is then forcefully closed (e.g by a programming error or this crazy test), we automatically close the parents, too.

We should also fix this in 3.x, if we have similar problems there (needs investigation)."
1,"[PATCH] BooleanScorer2 ArrayIndexOutOfBoundsException + alternative NearSpansFrom Erik's post at java-dev: 
 
>   [java] Caused by: java.lang.ArrayIndexOutOfBoundsException: 4 
>   [java]   at org.apache.lucene.search.BooleanScorer2  
> $Coordinator.coordFactor(BooleanScorer2.java:54) 
>   [java]   at org.apache.lucene.search.BooleanScorer2.score  
> (BooleanScorer2.java:292) 
... 
 
and my answer: 
 
Probably nrMatchers is increased too often in score() by calling score() 
more than once."
1,"2.4.x index cannot be opened with 2.9-devSorry for the lack of proper testcase.

In 2.4.1, if you created an index with the (stupid) options below, then it will not create a .prx file. 2.9 expects this file and will not open the index.
The reason i used these stupid options is because i changed the field from indexed=yes to indexed=no, but forgot to remove the .setOmitTf()

{code}
public class Testcase {
	public static void main(String args[]) throws Exception {
		/* run this part with lucene 2.4.1 */
		IndexWriter iw = new IndexWriter(""test"", new WhitespaceAnalyzer(), IndexWriter.MaxFieldLength.LIMITED);
		iw.setUseCompoundFile(false);
		Document doc = new Document();
		Field field1 = new Field(""field1"", ""foo"", Field.Store.YES, Field.Index.NO);
		field1.setOmitTf(true); // 2.9 will create a 0-byte .prx file, but 2.4.x will NOT. This is the problem. 2.9 expects this file!
		doc.add(field1);
		iw.addDocument(doc);
		iw.close(); 
		/* run this with lucene 2.9 */
		IndexReader ir = IndexReader.open(FSDirectory.getDirectory(""test""), true); 
	}
}
{code}"
0,"Improve maven artifactsThere are a couple of things we can improve for the next release:
- ""*pom.xml"" files should be renamed to ""*pom.xml.template""
- artifacts ""lucene-parent"" should extend ""apache-parent""
- add source jars as artifacts
- update <generate-maven-artifacts> task to work with latest version of maven-ant-tasks.jar
- metadata filenames should not contain ""local"""
0,"LuceneTestCase.afterClass does not print enough information if a temp-test-dir fails to deleteI've hit an exception from LTC.afterClass when _TestUtil.rmDir failed (on write.lock, as if some test did not release resources). However, I had no idea which test caused that (i.e. opened the temp directory and did not release resources).

I think we should do the following:
* Track in LTC a map from dirName -> StackTraceElement
* In afterClass if _TestUtil.rmDir fails, print the STE of that particular dir, so we know where was this directory created from
* Make tempDirs private and create accessor method, so that we control the inserts to this map (today the Set is updated by LTC, _TestUtils and TestBackwards !)"
1,"weird error when adding a node using an abstract/mixin nodetypewhen trying to add a node ""files"" with an abstract nodetype, i.e. nt:base, the following error is reported:

javax.jcr.nodetype.ConstraintViolationException: {}files is abstract  be used as primary node type.

the correct wording could be:

javax.jcr.nodetype.ConstraintViolationException: not allowed to add node {}files: {http://www.jcp.org/jcr/nt/1.0}base is abstract and cannot be used as primary node type.
"
0,Add settings to IWC to optimize IDV indices for CPU or RAM respectivlyspinnoff from LUCENE-3496 - we are seeing much better performance if required bits for PackedInts are rounded up to a 8/16/32/64. We should add this option to IWC and default to round up ie. more RAM & faster lookups.
1,"Typo in the deploy/jboss/4.x/jcr-ds.xml file
The datasource descriptor in the jboss 4.x example xml file has a type in this line:

<config-property name=""bindSessionToTrasaction"" type=""java.lang.Boolean"">true</config-property>

bindSessionToTrasaction ought to be bindSessionToTransaction - there is an 'n' missing from Transaction.

Found this on the tagged release in the subversion repo."
0,"Pass a context struct to Weight#scorer instead of naked booleansWeight#scorer(AtomicReaderContext, boolean, boolean) is hard to extend if another boolean like ""needsScoring"" or similar flags / information need to be passed to Scorers. An immutable struct would make such an extension trivial / way easier. "
0,"Authentication does not respond to stale nonceWhen using digest authentication, HTTP allows the server to mark the nonce value
as stale. The client then must re-authenticate with a new nonce value provided
by the server. Currently, HttpClient does not support this functionality. I've
created a patch that allows HttpClient to support stale nonce values. It is
attached below. The patch should be applied to HttpMethodBase.java


***
/home/scohen/downloads/httpclient-src/commons-httpclient-2.0-rc1/src/java/org/apache/commons/httpclient/HttpMethodBase.java
2003-07-31 22:15:26.000000000 -0400
--- org/apache/commons/httpclient/HttpMethodBase.java   2003-08-20
17:22:52.000000000 -0400
***************
*** 1351,1384 ****
       *
       * @throws IOException when errors occur reading or writing to/from the
       *         connection
       * @throws HttpException when a recoverable error occurs
       */
!     protected void addAuthorizationRequestHeader(HttpState state,
!                                                  HttpConnection conn)
!     throws IOException, HttpException {
!         LOG.trace(""enter HttpMethodBase.addAuthorizationRequestHeader(""
!                   + ""HttpState, HttpConnection)"");
   
          // add authorization header, if needed
!         if (getRequestHeader(HttpAuthenticator.WWW_AUTH_RESP) == null) {
!             Header[] challenges = getResponseHeaderGroup().getHeaders(
!                                                HttpAuthenticator.WWW_AUTH);
!             if (challenges.length > 0) {
!                 try {
!                     AuthScheme authscheme =
HttpAuthenticator.selectAuthScheme(challenges);
                      HttpAuthenticator.authenticate(authscheme, this, conn, state);
!                 } catch (HttpException e) {
!                     // log and move on
!                     if (LOG.isErrorEnabled()) {
!                         LOG.error(e.getMessage(), e);
!                     }
                  }
              }
          }
      }
                                                                                
      /**
       * Adds a <tt>Content-Length</tt> or <tt>Transfer-Encoding: Chunked</tt>
       * request header, as long as no <tt>Content-Length</tt> request header
       * already exists.
       *
--- 1351,1391 ----
       *
       * @throws IOException when errors occur reading or writing to/from the
       *         connection
       * @throws HttpException when a recoverable error occurs
       */
!     protected void addAuthorizationRequestHeader(HttpState state,
HttpConnection conn)
!         throws IOException, HttpException {
!         LOG.trace(""enter HttpMethodBase.addAuthorizationRequestHeader("" +
""HttpState, HttpConnection)"");
                                                                                
          // add authorization header, if needed
!
!         Header[] challenges =
getResponseHeaderGroup().getHeaders(HttpAuthenticator.WWW_AUTH);
!         if (challenges.length > 0) {
!
!             try {
!                 AuthScheme authscheme =
HttpAuthenticator.selectAuthScheme(challenges);
!                 if (getRequestHeader(HttpAuthenticator.WWW_AUTH_RESP) == null
!                     || isNonceStale(authscheme) ) {
                      HttpAuthenticator.authenticate(authscheme, this, conn, state);
!                 }
!             } catch (HttpException e) {
!                 // log and move on
!                 if (LOG.isErrorEnabled()) {
!                     LOG.error(e.getMessage(), e);
                  }
              }
          }
      }
                                                                                
+
+     private boolean isNonceStale(AuthScheme authscheme) {
+         return authscheme.getSchemeName().equalsIgnoreCase(""digest"")
+             && ""true"".equalsIgnoreCase(authscheme.getParameter(""stale""));
+     }
+
+
      /**
       * Adds a <tt>Content-Length</tt> or <tt>Transfer-Encoding: Chunked</tt>
       * request header, as long as no <tt>Content-Length</tt> request header
       * already exists.
       *
***************
*** 2419,2430 ****
                  buffer.append(port);
              }
              buffer.append('#');
              buffer.append(authscheme.getID());
              String realm = buffer.toString();
!
              if (realmsUsed.contains(realm)) {
                  if (LOG.isInfoEnabled()) {
                      LOG.info(""Already tried to authenticate to \""""
                               + realm + ""\"" but still receiving ""
                               + statusCode + ""."");
                  }
--- 2426,2442 ----
                  buffer.append(port);
              }
              buffer.append('#');
              buffer.append(authscheme.getID());
              String realm = buffer.toString();
!
!                       // check to see if the server has made our nonce stale.
!                       // if it has, re-auth
              if (realmsUsed.contains(realm)) {
+               if ( isNonceStale(authscheme)) {
+                       return false;
+               }
                  if (LOG.isInfoEnabled()) {
                      LOG.info(""Already tried to authenticate to \""""
                               + realm + ""\"" but still receiving ""
                               + statusCode + ""."");
                  }"
0,Caching client has a class for common headers that was not being used consistently in the codeThe HttpCachingClient has a class called HeaderConstants that contains all the cache interesting headers that are used in the code base.  This class of string constants was not being used consistently in the code base.  The attached patch cleans this up.
0,"Hostname verification:  turn off wildcards when CN is an IP addressHostname verification:   turn off wildcards when CN is an IP address.  This is a further improvement on HTTPCLIENT-613 and HTTPCLIENT-614.

Example - don't allow:
CN=*.114.102.2

I'm thinking of grabbing the substring following the final dot, and running it through ""Integer.parseInt()"".  If the NumberFormatException isn't thrown (so Integer.parseInt() actually worked!), then I'll turn off wildcard matching.  Notice that this won't be a problem with IP6 addresses, since they don't use dots.  It's only a problem with IP4, where the meaning of the dots clashes with dots in domain names.

Note:  when I turn off wildcard matching, I still attempt an exact match with the hostname.  If through some weird mechanism the client is actually able to use a hostname such as ""https://*.114.102.2/"", then they will be okay if that's what the certificate on the server contains."
0,"Spatial Filters not SerializableI am using Lucene in a distributed setup. 

The Filters in the spatial project aren't Serializable even though it inherits it from Filter. Filter is a Serializable class. 

DistanceFilter contains the non-Serializable class WeakHashMap.
CartesianShapeFilter contains the non-Serializable class java.util.logging.Logger
"
1,Several DocsEnum / DocsAndPositionsEnum return wrong docID when next() / advance(int) return NO_MORE_DOCSDuring work on LUCENE-2878 I found some minor problems in PreFlex and Pulsing Codec - they are not returning NO_MORE_DOCS but the last docID instead from DocsEnum#docID() when next() or advance(int) returned NO_MORE_DOCS. The JavaDoc clearly says that it should return NO_MORE_DOCS.
0,"HttpURLConnection wrapperInitial comments from Vincent Massol for this feature:

I am moving Jakarta Cactus from using the JDK HttpURLConnection to
Commons HttpClient. However, I have some public interface that return
HttpURLConnection and I cannot break that contract with Cactus users.

I propose to write a HttpURLConnection wrapper for HttpMethod (I have
actually already written it but I am currently testing it on Cactus and
will make a proper donation once I am sure it works - i.e all the Cactus
tests pass as before ... ).

I attach a preview of it for those interested.

What do you think of including it in HttpClient distribution ?

Thanks
-Vincent"
0,Remove deprecated RangeQuery classesRemove deprecated RangeQuery classes
0,"AdministratorTest.testAdminNodeCollidingWithRandomNode failureI see the following test failure with the latest trunk. It seems to affect also Sbastien as commented in JCR-2389. However, it doesn't break the Hudson build or Angela's checkout.

I'm filing this as a bug and will disable the test for now to be able to cut the 2.0-beta3 release. We can look at this later in more detail.

The detailed failure message is:

-------------------------------------------------------------------------------
Test set: org.apache.jackrabbit.core.security.user.TestAll
-------------------------------------------------------------------------------
Tests run: 144, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 4.078 sec <<< FAILURE!
testAdminNodeCollidingWithRandomNode(org.apache.jackrabbit.core.security.user.AdministratorTest)  Time elapsed: 0.072 sec  <<< FAILURE!
junit.framework.AssertionFailedError
        at junit.framework.Assert.fail(Assert.java:47)
        at junit.framework.Assert.assertTrue(Assert.java:20)
        at junit.framework.Assert.assertFalse(Assert.java:34)
        at junit.framework.Assert.assertFalse(Assert.java:41)
        at org.apache.jackrabbit.core.security.user.AdministratorTest.testAdminNodeCollidingWithRandomNode(AdministratorTest.java:205)
"
0,"Clone supportIt would be nice to have a clone method for some of the classes that don't have getters & setters exposed for all of their fields. Where relevant, the clone method could be in the interface, so that it doesn't matter which implementing class is being used. The main interfaces that I would like to clone are HttpRequest and Cookie. I know that HttpRequest is technically part of HttpCore, but the primary implementations of it are in HttpClient, so I thought I would post it here. 

Thanks,
David Byrne"
1,jcr2spi: wrong status change upon conflicting removal (CacheBehaviour.OBSERVATION)with CacheBehaviour.OBSERVATION the external removal of a transiently removed item results in wrong status change that brings it back to life.
0,"jackrabbit JCA pom.xmldo not see a way to add attachments, so here it is below inline.
Note, need to move the src/rar/META-INF/ra.xml to src/main/rar/META-INF/ra.xml (which is the default location with maven rar packager).
==========================================
<?xml version=""1.0"" encoding=""UTF-8""?>

<!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the ""License""); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an ""AS IS"" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
  -->

<project xmlns=""http://maven.apache.org/POM/4.0.0""
         xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0
                             http://maven.apache.org/maven-v4_0_0.xsd "">
  <modelVersion>4.0.0</modelVersion>

<!-- ====================================================================== -->
<!-- P R O J E C T  D E S C R I P T I O N                                   -->
<!-- ====================================================================== -->
  <groupId>org.apache.jackrabbit</groupId>
  <artifactId>jackrabbit-jca</artifactId>
  <packaging>rar</packaging>
  <name>Jackrabbit JCA</name>
  <version>1.1-SNAPSHOT</version>
  <!--
    Keep the description on a single line. Otherwise Maven might generate
    a corrupted MANIFEST.MF (see http://jira.codehaus.org/browse/MJAR-4)
   -->
  <description>
A resource adapter for Jackrabbit as specified by JCA 1.0.
</description>
  <url>http://jackrabbit.apache.org/</url>
  <prerequisites>
    <maven>2.0</maven>
  </prerequisites>
  <issueManagement>
    <system>Jira</system>
    <url>http://issues.apache.org/jira/browse/JCR</url>
  </issueManagement>
  <inceptionYear>2005</inceptionYear>




<!-- ====================================================================== -->
<!-- M A I L I N G   L I S T S                                              -->
<!-- ====================================================================== -->
  <mailingLists>
    <mailingList>
      <name>Jackrabbit Announce List</name>
      <subscribe>announce-subscribe@jackrabbit.apache.org</subscribe>
      <unsubscribe>announce-unsubscribe@jackrabbit.apache.org</unsubscribe>
      <archive>http://mail-archives.apache.org/mod_mbox/jackrabbit-announce/</archive>
    </mailingList>
    <mailingList>
      <name>Jackrabbit Users List</name>
      <subscribe>users-subscribe@jackrabbit.apache.org</subscribe>
      <unsubscribe>users-unsubscribe@jackrabbit.apache.org</unsubscribe>
      <post>users at jackrabbit.apache.org</post>
      <archive>http://mail-archives.apache.org/mod_mbox/jackrabbit-users/</archive>
      <otherArchives>
        <otherArchive>
          http://dir.gmane.org/gmane.comp.apache.jackrabbit.user
        </otherArchive>
        <otherArchive>
          http://www.mail-archive.com/users@jackrabbit.apache.org/
        </otherArchive>
      </otherArchives>
    </mailingList>
    <mailingList>
      <name>Jackrabbit Development List</name>
      <subscribe>dev-subscribe@jackrabbit.apache.org</subscribe>
      <unsubscribe>dev-unsubscribe@jackrabbit.apache.org</unsubscribe>
      <post>dev at jackrabbit.apache.org</post>
      <archive>http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/</archive>
      <otherArchives>
        <otherArchive>
          http://dir.gmane.org/gmane.comp.apache.jackrabbit.devel
        </otherArchive>
        <otherArchive>
          http://www.mail-archive.com/dev@jackrabbit.apache.org/
        </otherArchive>
        <otherArchive>
          http://www.mail-archive.com/jackrabbit-dev@incubator.apache.org/
        </otherArchive>
      </otherArchives>
    </mailingList>
    <mailingList>
      <name>Jackrabbit Source Control List</name>
      <subscribe>commits-subscribe@jackrabbit.apache.org</subscribe>
      <unsubscribe>commits-unsubscribe@jackrabbit.apache.org</unsubscribe>
      <archive>http://mail-archives.apache.org/mod_mbox/jackrabbit-commits/</archive>
    </mailingList>
  </mailingLists>


  <licenses>
    <license>
      <name>The Apache Software License, Version 2.0</name>
      <url>http://www.apache.org/licenses/LICENSE-2.0</url>
      <distribution>repo</distribution>
    </license>
  </licenses>
  <scm>
    <connection>scm:svn:http://svn.apache.org/repos/asf/jackrabbit/trunk/jca</connection>
    <developerConnection>scm:svn:https://svn.apache.org/repos/asf/jackrabbit/trunk/jca</developerConnection>
    <url>http://svn.apache.org/viewvc/jackrabbit/trunk/jca</url>
  </scm>
  <organization>
    <name>The Apache Software Foundation</name>
    <url>http://www.apache.org/</url>
  </organization>
  <build>
    <resources>
      <resource>
        <directory>src/java</directory>
      </resource>
    </resources>
<!-- 
   <testResources>
      <testResource>
        <directory>applications/test</directory>
        <includes>
          <include>*.properties</include>
          <include>*.xml</include>
        </includes>
      </testResource>
      <testResource>
        <directory>src/test/java</directory>
        <includes>
          <include>**/*.xml</include>
          <include>**/*.txt</include>
        </includes>
      </testResource>
    </testResources>
-->
    <plugins>
      <plugin>
        <artifactId>maven-compiler-plugin</artifactId>
        <configuration>
          <target>1.4</target>
          <source>1.4</source>
        </configuration>
      </plugin>
<!-- 
      <plugin>
        <artifactId>maven-surefire-plugin</artifactId>
        <configuration>
          <excludes>
            <exclude>**/init/*</exclude>
          </excludes>
          <includes>
            <include>**/*TestAll.java</include>
          </includes>
          <forkMode>once</forkMode>
          <argLine>-Xmx128m -enableassertions</argLine>
          <systemProperties>
            <property>
              <name>derby.system.durability</name>
              <value>test</value>
            </property>
            <property>
              <name>known.issues</name>
              <value>org.apache.jackrabbit.core.xml.DocumentViewTest#testMultiValue org.apache.jackrabbit.value.BinaryValueTest#testBinaryValueEquals</value>
            </property>
          </systemProperties>          
        </configuration>
      </plugin>
-->
    </plugins>
  </build>

  <dependencies>
  
  	<dependency>
	    <groupId>org.apache.jackrabbit</groupId>
	    <artifactId>jackrabbit-core</artifactId>
	    <version>1.1-SNAPSHOT</version>
	</dependency>
    <dependency>
      <groupId>concurrent</groupId>
      <artifactId>concurrent</artifactId>
      <version>1.3.4</version>
    </dependency>
    <dependency>
      <groupId>commons-collections</groupId>
      <artifactId>commons-collections</artifactId>
      <version>3.1</version>
    </dependency>
    <dependency>
      <groupId>org.apache.geronimo.specs</groupId>
      <artifactId>geronimo-jta_1.0.1B_spec</artifactId>
      <version>1.0.1</version>
    </dependency>
    <dependency>
      <groupId>javax.jcr</groupId>
      <artifactId>jcr</artifactId>
      <version>1.0</version>
    </dependency>
    <dependency>
      <groupId>log4j</groupId>
      <artifactId>log4j</artifactId>
      <version>1.2.8</version>
    </dependency>
    <dependency>
      <groupId>org.slf4j</groupId>
      <artifactId>slf4j-log4j12</artifactId>
      <version>1.0</version>
    </dependency>
    <dependency>
      <groupId>lucene</groupId>
      <artifactId>lucene</artifactId>
      <version>1.4.3</version>
    </dependency>
    <dependency>
      <groupId>org.apache.derby</groupId>
      <artifactId>derby</artifactId>
      <version>10.1.1.0</version>
      <optional>true</optional>
    </dependency>
    <dependency>
      <groupId>junit</groupId>
      <artifactId>junit</artifactId>
      <version>3.8.1</version>
      <scope>test</scope>
    </dependency>
  </dependencies>

  <distributionManagement>
    <repository>
      <id>apache.releases</id>
      <name>Apache Repository for PMC approved releases</name>
      <url>scp://people.apache.org/www/www.apache.org/dist/maven-repository/</url>
    </repository>
    <snapshotRepository>
      <id>apache.snapshots</id>
      <name>Apache Development Repository</name>
      <url>scp://people.apache.org/www/cvs.apache.org/maven-snapshot-repository</url>
    </snapshotRepository>
    <site>
      <id>website</id>
      <url>scp://people.apache.org/www/jackrabbit.apache.org/</url>
    </site>
  </distributionManagement>

</project>"
0,"Handle conditional requests in cacheReturn 304 if incoming request has ""If-None-Match"" or ""If-Modified-Since"" headers and can be served from cache.  Currently we return a 200 which is correct but not optimal."
0,"Optimize the core tokenizers/analyzers & deprecate Token.termTextThere is some ""low hanging fruit"" for optimizing the core tokenizers
and analyzers:

  - Re-use a single Token instance during indexing instead of creating
    a new one for every term.  To do this, I added a new method ""Token
    next(Token result)"" (Doron's suggestion) which means TokenStream
    may use the ""Token result"" as the returned Token, but is not
    required to (ie, can still return an entirely different Token if
    that is more convenient).  I added default implementations for
    both next() methods in TokenStream.java so that a TokenStream can
    choose to implement only one of the next() methods.

  - Use ""char[] termBuffer"" in Token instead of the ""String
    termText"".

    Token now maintains a char[] termBuffer for holding the term's
    text.  Tokenizers & filters should retrieve this buffer and
    directly alter it to put the term text in or change the term
    text.

    I only deprecated the termText() method.  I still allow the ctors
    that pass in String termText, as well as setTermText(String), but
    added a NOTE about performance cost of using these methods.  I
    think it's OK to keep these as convenience methods?

    After the next release, when we can remove the deprecated API, we
    should clean up Token.java to no longer maintain ""either String or
    char[]"" (and the initTermBuffer() private method) and always use
    the char[] termBuffer instead.

  - Re-use TokenStream instances across Fields & Documents instead of
    creating a new one for each doc.  To do this I added an optional
    ""reusableTokenStream(...)"" to Analyzer which just defaults to
    calling tokenStream(...), and then I implemented this for the core
    analyzers.

I'm using the patch from LUCENE-967 for benchmarking just
tokenization.

The changes above give 21% speedup (742 seconds -> 585 seconds) for
LowerCaseTokenizer -> StopFilter -> PorterStemFilter chain, tokenizing
all of Wikipedia, on JDK 1.6 -server -Xmx1024M, Debian Linux, RAID 5
IO system (best of 2 runs).

If I pre-break Wikipedia docs into 100 token docs then it's 37% faster
(1236 sec -> 774 sec), I think because of re-using TokenStreams across
docs.

I'm just running with this alg and recording the elapsed time:

  analyzer=org.apache.lucene.analysis.LowercaseStopPorterAnalyzer
  doc.tokenize.log.step=50000
  docs.file=/lucene/wikifull.txt
  doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker
  doc.tokenized=true
  doc.maker.forever=false

  {ReadTokens > : *

See this thread for discussion leading up to this:

  http://www.gossamer-threads.com/lists/lucene/java-dev/51283

I also fixed Token.toString() to work correctly when termBuffer is
used (and added unit test).
"
0,"Upgrade commons-codec 1.4 -> 1.6commons-codec 1.4 is buggy, see for example https://issues.apache.org/jira/browse/CODEC-99"
0,"use reusable collation keys in ICUCollationFilterICUCollationFilter need not create a new CollationKey object for each token.
In ICU there is a mechanism to use a reusable key.
"
1,"NullPointerException in ServerRowA NullPointerException occurs in ServerRow.getValues() when the underlying Value array contains a null reference. See http://www.nabble.com/exception-after-calling-webdav-search-command-tf2826750.html for the details.

java.lang.NullPointerException
     at org.apache.jackrabbit.rmi.value.StatefulValueAdapter.getType(StatefulValueAdapter.java:98)
     at org.apache.jackrabbit.rmi.value.SerialValue.<init>(SerialValue.java:65)
     at org.apache.jackrabbit.rmi.value.SerialValueFactory.makeSerialValue(SerialValueFactory.java:100)
     at org.apache.jackrabbit.rmi.value.SerialValueFactory.makeSerialValueArray(SerialValueFactory.java:77)
     at org.apache.jackrabbit.rmi.server.ServerRow.getValues(ServerRow.java:58)

The best solution would be to explicitly handle nulls in SerialValueFactory.makeSerialValue()."
1,"when you clone or reopen an IndexReader with pending changes, the new reader doesn't commit the changesWhile working on LUCENE-1647, I came across this issue... we are failing to carry over hasChanges, norms/deletionsDirty, etc, when cloning the new reader."
0,Benchmark: Improve transparency of test resultsas discussed in JCR-1501.
1,"IndexReader.setTermInfosIndexDivisor doesn't carry over to reopened readersWhen you reopen a reader, some segments are shared (and thus properly inherit the index divisor) but others are newly opened and use the default index divisor.  You then have no way to change the index divisor of those newly opened ones.  The only workaround is to not use reopen (always open a new reader).

I'd like to make termInfosDivisor an up-front param to IndexReader, anyway, for LUCENE-1609, so likely I'll fix both of these issues there."
1,"Bug with textfilters and classloadersI'm having problems with text filter service. I built the contrib/textfilters package and I included the resulting jackrabbit-textfilters-1.0-SNAPSHOT.jar in my application classpath. The problem is that TextFilterService class is unable to find any filters, even though that a services/org...TextFilterService file is wihin the META-INF jar's directory.

I think that this must to be with Eclipse RCP classloader mechanism, but the fact is that it does not work. I find a little bit strange this way to load services, and as you can see, it seems problematic in some scenarios.

----

Marcel Reutegger 	
<marcel.reutegger@gmx.net> to jackrabbit-dev
	 More options	  11:01 am (55 minutes ago)
Hi Martin,

we had a similar problem with the query languages, but I solved that one
by telling the registry to use a specific classloader. this seemed to work.
I'm not sure this will also work for the text filters, because the jar
file might be in another classloader.

could you please post a jira bug? I'll then change the discovery
mechanism to use good old xml config ;)
"
0,"Remove deprecated methods in BooleanQueryRemove deprecated methods setUseScorer14 and getUseScorer14 in BooleanQuery, and adapt javadocs."
0,"implement reusableTokenStream for all contrib analyzersmost contrib analyzers do not have an impl for reusableTokenStream

regardless of how expensive the back compat reflection is for indexing speed, I think we should do this to mitigate any performance costs. hey, overall it might even be an improvement!

the back compat code for non-final analyzers is already in place so this is easy money in my opinion."
1,"Possible hidden exception on SegmentInfos commitI am not sure if this is that big of a deal, but I just ran into it and thought I might mention it.

SegmentInfos.commit removes the Segments File if it hits an exception. If it cannot remove the Segments file (because its not there or on Windows something has a hold of it), another Exception is thrown about not being able to delete the Segments file. Because of this, you lose the first exception, which might have useful info, including why the segments file might not be there to delete.

- Mark"
1,HttpClient treats URI fragments in redirect URIs incosistentlyHttpClient treats URI fragments in redirect URIs incosistently. It strips fragments from relative URIs but leaves absolute ones unchanges.  
0,"Persian Arabic Analyzer cleanupWhile browsing through the code I found some places for minor improvements in the new Arabic / Persian Analyzer code. 

- prevent default stopwords from being loaded each time a default constructor is called
- replace if blocks with a single switch
- marking private members final where needed
- changed protected visibility to final in final class.

"
0,Add HTMLStripReader and WordDelimiterFilter from SOLRSOLR has two classes HTMLStripReader and WordDelimiterFilter which are very useful for a wide variety of use cases.  It would be good to place them into core Lucene.
0,make spi query code compatible with JCR 2.0SPI-Commons currently has it's own outdated copy of the new JCR 2.0 query interfaces.
0,"Add log information when node/property type determination failsgetQNodeDefinition() and getQPropertyDefinition() of o.a.j.jcr2spi.nodetype.ItemDefinitionProviderImpl silently ignore errors which might occur on determination of node and property types. Instead these methods use RepositoryService.getNodeDefinition() and RepositoryService.getPropertyDefinition(), respectively to determine the types. This might lead to difficult to track down problems when the RepositoryService call occurs because of an error in the node type definition. I suggest to add logging statements to these methods. "
0,"JSP page compilation errors when depoyed using oc4jAn error in the Welcome.jsp was produced as follows:

cannot find symbol symbol : method log(java.lang.String,java.lang.Throwable)

In a response from the user group it was determined that there should be no expectation in Jackrabbit that the JspPage implementation will inherit from the GeneralServlet base class.
"
0,"add shortcut method to CndImporter which makes it easier to rereigster node typesCndImporter has a two argument registerNodeTypes method which is a nice shortcut, but in order to rereigster node types, you have to use the non-shortcut method. Attached patch adds a three-argument method which provide a shortcut for doing reregistration."
0,"contrib/benchmark config does not play nice with doubles with the flush.by.ram valueIn the o.a.l.benchmark.byTask.utils.Config.java file, the nextRound and various other methods do not handle doubles in the ""round"" property configuration syntax.

To replicate this, copy the micro-standard.alg and replace 
merge.factor=mrg:10:100:10:100
max.buffered=buf:10:10:100:100

with

ram.flush.mb=ram:32:40:48:56

and you will get various ClassCastExceptions in Config (one in newRound() and, when that is fixed, in getColsValuesForValsByRound.

The fix seems to be to just to mirror the handling of int[].

The fix seems relatively minor.  Patch shortly and will plan to commit tomorrow evening."
1,"NodeTypeDefDiff.PropDefDiff.init() constraints change check bugsTwo bugs have been found in NodeTypeDefDiff.PropDefDiff.init() when try to modify property constraints of an already registered node type:

1) according to the java doc it should be possible to remove all constraints from a property, but it is not (marked as a MAJOR change).
 
2) it's allowed (TRIVIAL) to set a constraint to a property that had no constraint at all before, which is wrong, because it could affect the consistency of existing repository content."
0,Database Data Store: support database type 'mssql'MS SQL Server is referred to with the schema name 'mssql' in the persistence managers and the cluster journal. For the DbDataStore it is called 'sqlserver'. This is not consistent.
0,"Revise PagedBytes#fillUsingLengthPrefix* methods namesPagedBytes has 3 different variants of fillUsingLengthPrefix. We need better names for that since CSFBranch already added a 4th one.


here are some suggestions:

{code}
/** Reads length as 1 or 2 byte vInt prefix, starting @ start */
    public BytesRef fillLengthAndOffset(BytesRef b, long start) 
//    was: public BytesRef fillUsingLengthPrefix(BytesRef b, long start) 


 /** @lucene.internal  Reads length as 1 or 2 byte vInt prefix, starting @ start.  Returns the block number of the term. */
    public int getBlockAndFill(BytesRef b, long start) 
//    was: public BytesRef fillUsingLengthPrefix2(BytesRef b, long start) 

/** @lucene.internal  Reads length as 1 or 2 byte vInt prefix, starting @ start. 
     * Returns the start offset of the next part, suitable as start parameter on next call
     * to sequentially read all BytesRefs. */
    public long getNextOffsetAndFill(BytesRef b, long start) 
//    was: public BytesRef fillUsingLengthPrefix3(BytesRef b, long start) 

{code}"
1,"ArrayIndexOutOfBoundsException in HttpStatus.getStatusText(508)Try the following:

    System.out.println(""Status text = "" + HttpStatus.getStatusText(507));
    try {
      System.out.println(""Status text = "" + HttpStatus.getStatusText(508));
    }
    catch (Exception ex) {
      System.err.println(""Exception! msg = "" + ex.getMessage());
      ex.printStackTrace();
    }
    System.out.println(""Status text = "" + HttpStatus.getStatusText(509));

507 -> returns a message as expected
508 -> ArrayIndexOutOfBoundsException
509 -> null as expected"
0,"Do not use deletable anymoreThe query handler implementation currently uses a deletable file to keep track of index segments that are not needed anymore and can be deleted. In general index segments are deleted right away when they are not needed anymore, but it may happen that index readers are still open (because of a time consuming query) and the index segment cannot be deleted at the moment. In this case the index segment name is written to the deletable file and the index periodically tries to delete the segment later.

The implementation should rather infer from the indexes file on startup, which segments are still needed and in use."
0,New method on NodeTypeManagerImpl to reregister nodetypesAdd a method to NodeTypeManagerImpl to allow reregistering of existing nodetypes. The method takes an inputstream in either XML or CND format and registers all new nodetypes and reregisters existing nodetypes.
0,"Some typos in the English Manualin section 2.8.4
Per default this implementation will create no more than than 2 concurrent connections per given route and no more 20 connections in total.
Here are 2 ""than"" in this statement.

in section 3.1
Netscape engineers used to refer to it as as a ""magic cookie"" and the name stuck.
Also, here are 2 ""as"" in the sentence.

in section 5.2 'http.protocol.handle-redirects'
If this parameter is not HttpClient will handle redirects automatically.
here, a ""set"" should be put after not

in section 6.1
In certain situations it may be necessary to customize the way HTTP messages get transmitted across
the wire beyond what is possible possible using HTTP parameters in order to be able to deal nonstandard,
non-compliant behaviours.
here are 2 ""possible""."
1,"SearchManager might throw when handling cluster eventWhen handling events that are generated from another node in the cluster, the SearchManager might try to index an index that does no longer exist. This results in an error message or even a NPE. The scenario looks as follows (A and B are nodes in a repository cluster)

1: A adds node N
2: A saves changes
3: A removes node N
4: A saves changes

Upon receiving the event of a newly created node, B starts indexing node N. If this process hasn't been concluded before step 3 above, it will throw.
"
0,"replacing an extended mixin with it's supertype is problematicnode.addMixin() / node.removeMixin() have some checks to avoid redundant mixin settings on a node and not only when the node is saved.

eg: have 2 mixins: mix:A and mix:AA where mix:AA > mix:A and a node (N with mix:AA) on it.

then, N.addMixin(mix:A)  has no effect, since it's regarded as redundant.  so you have to remove mix:AA first and then add mix:A.
there is the first problem when applying mixin types programmatically, just be sure to remove them first before adding new ones.

the 2nd problem occurs when mix:A has a mandatory property. then somehow when downgrading from mix:AA to mix:A, some information is lost, and a save call results in

Unable to save node 'N': javax.jcr.nodetype.ConstraintViolationException: /test/A: mandatory property {}prop does not exist.
you need to ""touch"" the property, otherwise it will not work.

so only this works:

N.removeMixin(""mix:AA"");
N.addMixin(""mix:A"");
N.setProperty(""prop"", N.getProperty(""prop"").getValue());
session.save();



"
1,"Exception handling in HttpClient requires redesignWhen I use httpclient2.0-alpha3 and setTimeout(60000), after the specified 
time, I would like to see InterruptedIOException thrown, but I got 
HttpRecoverableException instead, which is pretty general. I would like to see 
the original exception. Thanks"
1,"hrefs in dav responses should be url-escapedthe url in an href element of a dav response should be url-escaped. currently at least one webdav client (os x's webdavfs) chokes on unescaped urls (such as /home/bcm/file with spaces in its name.txt).
"
0,"improve test coverage of multi-segment indicesSimple patch that adds a test-only helper class, RandomIndexWriter, that lets you add docs, but it will randomly do things like use a different merge policy/scheduler, flush by doc count instead of RAM, flush randomly (so we get multi-segment indices) but also randomly optimize in the end (so we also sometimes test single segment indices)."
0,"Make the extraction of Session UserIDs from Subjects configurableThe SessionImpl class must extract a string name from the Prinicpals in a Subject to use as the Session userID.  In 1.4 the SessionImpl class directly selects the first available Principal.  In 1.5, this is delegated to the SecurityManager, which chooses the first  non-group principal.

It would be useful to be able to configure specific selection criteria for the Principal used for the Session userID.  A simple mechanism would involve specifying a Principal implementation classname in the configuration, and the first instance of that class found in the Subject would be used for the userID.  One way to implement this in 1.4 would be to extend AuthContext to include a method getSessionPrincipal() which encapsulates the selection logic, and adding an option the LoginModuleConfig to specify the class name of the Principal to select.

A particular use case is using the LDAP LoginModule from Sun JDK 6 with the repository.  The first Principal LdapLoginModule populates into the Subject is an instance of LdapPrincipal, which renders the userID as the full DN of the user.  The LoginModule also adds an instance of UserPrincipal, whose name is the simple username/uid attribute, which would be more appropriate as the Session userId since it corresponds to the username provided by the user to application authentication mechanisms (the provided username is expanded into the full DN prior to authentication by the login module).  If the above configuration mechanism were available, one could configure the LdapLoginModule, and specify that the userID be extracted from the first instance of com.sun.security.auth.UserPrincipal.  Since rewriting LoginModules is not always possible or desirable, this change would enable the stable integration of 3rd-party login modules that may populate the Subject with several principals."
0,"Performance improvement for merging stored, compressed fieldsHello everyone,

currently the merging of stored, compressed fields is not optimal for the following reason: every time a stored, compressed field is being merged, the FieldsReader uncompresses the data, hence the FieldsWriter has to compress it again when it writes the merged fields data (.fdt) file. The uncompress/compress step is unneccessary and slows down the merge performance significantly.

This patch improves the merge performance by avoiding the uncompress/compress step. In the following I give an overview of the changes I made:
   * Added a new FieldSelectorResult constant named ""LOAD_FOR_MERGE"" to org.apache.lucene.document.FieldSelectorResult
   * SegmentMerger now uses an FieldSelector to get stored fields from the FieldsReader. This FieldSelector's accept() method returns the FieldSelectorResult ""LOAD_FOR_MERGE"" for every field.
   * Added a new inner class to FieldsReader named ""FieldForMerge"", which extends  org.apache.lucene.document.AbstractField. This class holds the field properties and its data. If a field has the FieldSelectorResult ""LOAD_FOR_MERGE"", then the FieldsReader creates an instance of ""FieldForMerge"" and does not uncompress the field's data.
   * FieldsWriter checks if the field it is about to write is an instanceof FieldsReader.FieldForMerge. If true, then it does not compress the field data.


To test the performance I index about 350,000 text files and store the raw text in a stored, compressed field in the lucene index. I use a merge factor of 10. The final index has a size of 366MB. After building the index, I optimize it to measure the pure merge performance.

Here are the performance results:

old version:
   * Time for Indexing:  36.7 minutes
   * Time for Optimizing: 4.6 minutes

patched version:
   * Time for Indexing:  20.8 minutes
   * Time for Optimizing: 0.5 minutes

The results show that the index build time improved by about 43%, and the optimizing step is more than 8x faster. 

A diff of the final indexes (old and patched version) shows, that they are identical. Furthermore, all junit testcases succeeded with the patched version. 

Regards,
  Michael Busch"
0,"TestIndexWriter.testOptimizeTempSpaceUsage fails w/ SimpleText codec{noformat}
   [junit] Testsuite: org.apache.lucene.index.TestIndexWriter
   [junit] Testcase: testOptimizeTempSpaceUsage(org.apache.lucene.index.TestIndexWriter):      FAILED
   [junit] optimized used too much temporary space: starting usage was 117867 bytes; max temp usage was 363474 but should have been 353601 (= 3X starting usage)
   [junit] junit.framework.AssertionFailedError: optimized used too much temporary space: starting usage was 117867 bytes; max temp usage was 363474 but should have been 353601 (= 3X starting usage)
   [junit]     at org.apache.lucene.index.TestIndexWriter.testOptimizeTempSpaceUsage(TestIndexWriter.java:662)
   [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
   [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
   [junit]
   [junit]
   [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 5.284 sec
   [junit]
   [junit] ------------- Standard Output ---------------
   [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testOptimizeTempSpaceUsage -Dtests.seed=-3299990090561349208:2824386407253661541
   [junit] NOTE: test params are: codec=SimpleText, locale=el_GR, timezone=Africa/Dar_es_Salaam
{noformat}

It's not just SimpleText (because -Dtests.codec=SimpleText, alone, sometimes passes)... there must be something else about the RIWC settings.
"
1,Fix exception handling and thread safety in realtime branchSeveral tests are currently failing in the realtime branch - most of them due to thread safety problems (often exceptions in ConcurrentMergeScheduler) and in tests that test for aborting and non-aborting exceptions.
0,"Hudson build doesn't detect Java 5 class referencesDue to the fact that the Maven 2 support in Hudson only works on Java 5, our current CI build at http://hudson.zones.apache.org/hudson/job/Jackrabbit-trunk/ doesn't detect references to classes and methods that are only available in the Java 5 class library."
0,"Rename contrib/queryparser project to queryparser-contribMuch like with contrib/queries, we should differentiate the contrib/queryparser from the queryparser module.  No directory structure changes will be made, just ant and maven."
0,"Support more queries (other than just title) in Trec quality pkgNow that we can properly parse descriptions and narratives from TREC queries (LUCENE-2210), it would be nice to allow the user to easily run quality evaluations on more than just ""Title""

This patch adds an optional commandline argument to QueryDriver (the default is Title as before), where you can specify something like:
T: Title-only
D: Description-only
N: Narrative-only
TD: Title + Description,
TDN: Title+Description+Narrative,
DN: Description+Narrative

The SimpleQQParser has an additional constructor that simply accepts a String[] of these fields, forming a booleanquery across all of them.
"
1,"disk full can cause index corruption in certain casesRobert uncovered this nasty bug, in adding more randomness to
oal.index tests...

I got a standalone test to show the issue; the corruption path is
as follows:

  * The merge hits an initial exception (eg disk full when merging the
    postings).

  * In handling this exception, IW closes all the sub-readers,
    suppressing any further exceptions.

  * If one of these sub-readers has pending deletions, which happens
    if readers are pooled in IW, it will flush them.  If that flush
    hits a 2nd exception (eg disk full), then SegmentReader
    [incorrectly] leaves the SegmentInfo's delGen advanced by 1,
    referencing a corrupt file, yet the SegmentReader is still
    forcefully closed.

  * If enough disk frees up such that a later IW.commit/close
    succeeds, the resulting segments file will reference an invalid
    deletions file.
"
0,"TCK: AddNodeTest requires implementation to support one-parameter addNode method on test nodeThis test requires a repository to support addNode(String) [one argument] on the test node.  However, JSR-170 does not require an implementation to have at least one child node definition with a default primary type.  For such repositories, this test will fail, regardless of configuration.

Proposal: introduce a configuration property which, if set, causes calls to addNode(String) to be replaced with addNode(String, String)."
0,"JCR2SPI: Move test execution to SPI2JCRproposed patches see  issue JCR-1629

this allows to
- remove dependency to jackrabbit-spi2jcr and jackrabbit-core from jcr2spi
- remove duplicated tests from sandbox/spi"
0,"TextFilters get called three times within checkin() methodIf you want to add a PDF document to a repository using a PdfTextFilter, and you do the following steps:

session.save()
node.checkin();

The method PdfTextFilter.doFilter() gets called 4 times!!!

session's save method calls doFilter one time. This is normal

But checkin method calls doFilter three times. Is this normal? I do not see the sense.

------------------

		
Marcel Reutegger 	
<marcel.reutegger@gmx.net> to jackrabbit-dev
	 More options	  11:43 am (13 minutes ago)
Hi Martin,

this is unfortunate and should be improved. the reason why this happens
is the following:
the search index implementation always indexes a node as a whole to
improve query performance. that means even if a single property changes
the parent node with all its properties is re-indexed.

unfortunately the checkin method sets properties in three separate
'transactions', causing the search to re-index the according node three
times.

usually this is not an issue, because the index implementation keeps a
buffer for pending index work. that is, if you change the same property
several times and save after each setProperty() call, it won't actually
get re-indexed several times. but text filters behave differently here,
because they extract the text even though the text will never be used.

eventually this will improve without any change to the search index
implementation, because as soon as versioning participates properly in
transactions there will only be one call to index a node on checkin().

as a quick fix we could improve the text filter classes to only parse
the binary when the returned reader is acutally used."
0,"OracleFileSystem uses getClass().getResourceAsStream to load schema fileorg.apache.jackrabbit.core.fs.db.OracleFileSystem loads the schema via getClass().getResourceAsStream(...).
This makes it impossible to extend the class without either copying the schema ddl file, or overwriting checkSchema(...),
as the schema file is not accessible.

The solution is to use OracleFilesystem.class.getResourceAsStream(...).

See JCR-595 which fixed this already for org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager."
1,"[PATCH] PerFieldAnalyzerWrapper fails to implement getPositionIncrementGap()The attached patch causes PerFieldAnalyzerWrapper to delegate calls to getPositionIncrementGap() to the analyzer that is appropriate for the field in question.  The current behavior without this patch is to always use the default value from Analyzer, which is a bug because PerFieldAnalyzerWrapper should behave just as if it was the analyzer for the selected field.
"
0,"IP address of the server of a HttpConnectionAFAIK it's not possible to get the IP address of the server of a HttpConnection.

I propose to add a getServerAddress() method to the HttpConnection class that returns the IP address of the server, if the connection has been opened.
And either returns null or throws an Exception if the IP address is not available, i.e. the connection is not open.

Below is a workaround for getting the IP address in current versions.

-----------------------
package org.apache.commons.httpclient;

import java.io.IOException;
import java.net.InetAddress;

public class InetAddressFetcher {
	private HttpConnection hc;

	public InetAddressFetcher(HttpConnection hc) {
		this.hc = hc;
	}

	public InetAddress getInetAddress() throws IOException {
		if (!hc.isOpen()) {
			hc.open();
		}
		return hc.getSocket().getInetAddress();
	}
}"
0,"http.connection-manager.timeout is a LONG not an INTEGERDocumentation is wrong.

Table in Preference Architecture page states http.connection-manager.timeout 
is an Integer.

Doing:

setParameter(""http.connection-manager.timeout"", new Integer(n));

Causes:

java.lang.ClassCastException
	at 
org.apache.commons.httpclient.params.DefaultHttpParams.getLongParameter
(DefaultHttpParams.java:171)
	at 
org.apache.commons.httpclient.params.HttpClientParams.getConnectionManagerTimeo
ut(HttpClientParams.java:143)
	at org.apache.commons.httpclient.HttpMethodDirector.executeMethod
(HttpMethodDirector.java:161)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:437)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:324)"
0,"Improve indexing performance by increasing internal buffer sizesIn working on LUCENE-843, I noticed that two buffer sizes have a
substantial impact on overall indexing performance.

First is BufferedIndexOutput.BUFFER_SIZE (also used by
BufferedIndexInput).  Second is CompoundFileWriter's buffer used to
actually build the compound file.  Both are now 1 KB (1024 bytes).

I ran the same indexing test I'm using for LUCENE-843.  I'm indexing
~5,500 byte plain text docs derived from the Europarl corpus
(English).  I index 200,000 docs with compound file enabled and term
vector positions & offsets stored plus stored fields.  I flush
documents at 16 MB RAM usage, and I set maxBufferedDocs carefully to
not hit LUCENE-845.  The resulting index is 1.7 GB.  The index is not
optimized in the end and I left mergeFactor @ 10.

I ran the tests on a quad-core OS X 10 machine with 4-drive RAID 0 IO
system.

At 1 KB (current Lucene trunk) it takes 622 sec to build the index; if
I increase both buffers to 8 KB it takes 554 sec to build the index,
which is an 11% overall gain!

I will run more tests to see if there is a natural knee in the curve
(buffer size above which we don't really gain much more performance).

I'm guessing we should leave BufferedIndexInput's default BUFFER_SIZE
at 1024, at least for now.  During searching there can be quite a few
of this class instantiated, and likely a larger buffer size for the
freq/prox streams could actually hurt search performance for those
searches that use skipping.

The CompoundFileWriter buffer is created only briefly, so I think we
can use a fairly large (32 KB?) buffer there.  And there should not be
too many BufferedIndexOutputs alive at once so I think a large-ish
buffer (16 KB?) should be OK.
"
0,"apply delete-by-Term and docID immediately to newly flushed segmentsSpinoff from LUCENE-2324.

When we flush deletes today, we keep them as buffered Term/Query/docIDs that need to be deleted.  But, for a newly flushed segment (ie fresh out of the DWPT), this is silly, because during flush we visit all terms and we know their docIDs.  So it's more efficient to apply the deletes (for this one segment) at that time.

We still must buffer deletes for all prior segments, but these deletes don't need to map to a docIDUpto anymore; ie we just need a Set.

This issue should wait until LUCENE-1076 is in since that issue cuts over buffered deletes to a transactional stream."
0,add missing name constants for mix:title
0,"connection wrapper prevents GC of TSCCMEven if a connection is released back to the ThreadSafeClientConnManager, a hard reference to the connection wrapper will prevent GC of the TSCCM.
Make sure the connection wrapper is properly detached on release. Then update TestTSCCMWithServer.testConnectionManagerGC() accordingly. 
"
1,"DirectIOLinuxDirectory hardwires buffer size and creates files with invalid permissionsTestDemo fails if I use the DirectIOLinuxDirectory (using Robert's new -Dtests.directory=XXX), because when it O_CREATs a file, it fails to specify the mode, so [depending on C stack!] you can get permission denied.

Also, we currently hardwire the buffer size to 1 MB (Mark found this)... I plan to add a ""forcedBufferSize"" to the DirectIOLinuxDir's ctor, to optionally override lucene's default buffer sizes (which are way too small for direct IO to get barely OK performance).  If you pass 0 for this then you get Lucene's default buffer sizes..."
0,"Support contains queries with wildcard prefixThe current implementation only allows wildcards in the middle or at the end of a search term. 

The following queries work right now:
//*[jcr:contains(., 'foo*')
//*[jcr:contains(., 'fo?o')

But the following does not:
//*[jcr:contains(., '*bar')

There was already a thread in the mailing list on this topic:
http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/3304"
0,"orm-persistence package doesn't compile against cvs head.Corrected QName import (which I guess was moved). 
This patch has no meaning besides making the src/java compile and to update the dependencies in the project.xml.
"
1,"Term's equals() throws ClassCastException if passed something other than a TermTerm.equals(Object) does a cast to Term without checking if the other object is a Term.

It's unlikely that this would ever crop up but it violates the implied contract of Object.equals()."
0,"WebDAV: Allow for Extensions of MimeResolver in the Configuration.Currently mime type detection is done using the content type header or (if missing) using a static MimeResolver instance in 
the IOUtil class. The MimeResolver itself reads from a properties file, that obviously does not list all possible extensions and
mimetypes.

This could be improved by:

- extending the resource configuration.
- extend the ImportContext and ExportContext interfaces
- replacing the current usages of IOUti#MIMERESOLVER by the corresponding calls on the Context classes which 
  themselves get a MimeResolver that is retrieved from the resource configuration."
0,"remove unused benchmark dependenciesBenchmark has a huge number of jar files in its lib/ (some of which even have different versions than the same libs used in e.g. solr)

But the worst thing is, most of these it doesn't even use.
* commons-collection: unused
* commons-beanutils: unused
* commons-logging: unused
* commons-digetser: unused

"
1,"VersionHistory.removeVersion() does not throw ReferentialIntegrityExceptionInside an XATransaction immediately removing a version that was created by a checkin succeeds, even though it should fail because referential integrity is violated. The reason seems to be that the created version does not return any references.

In the end the transaction fails because referential integrity is checked again in the SharedItemStateManager, which is correct. But IMO removeVersion() should fail first.

Added test case: org.apache.jackrabbit.core.version.CheckinRemoveVersionTest"
1,"QueryNodeImpl throws ConcurrentModificationException on add(List<QueryNode>)on adding a List of children to a QueryNodeImplemention a ConcurrentModificationException is thrown.
This is due to the fact that QueryNodeImpl instead of iteration over the supplied list, iterates over its internal clauses List.

Patch:
Index: QueryNodeImpl.java
===================================================================
--- QueryNodeImpl.java    (revision 911642)
+++ QueryNodeImpl.java    (working copy)
@@ -74,7 +74,7 @@
           .getLocalizedMessage(QueryParserMessages.NODE_ACTION_NOT_SUPPORTED));
     }
 
-    for (QueryNode child : getChildren()) {
+    for (QueryNode child : children) {
       add(child);
     }
 "
0,"URI.parseUriReference treats strings with leading ':' as absolute URIs with zero-length schemeURI.parseUriReference treats strings with leading ':' as absolute URIs with a
zero-length scheme. If you then try to derelativize such a URI against a base
URI, you just get the same URI with leading ':'. 

IE and Firefox treat URI strings with a leading ':' as relative URIs. For
example, an HREF of "":foo"" in the context of base URI
""http://www.example.com/path/page"" would derelativize as
""http://www.example.com/path/:foo"". (Only if another character comes before the
colon is it interpreted as a URI scheme.)

It'd be desirable for HTTPClient URI to do the same thing.

Example code to demonstrate:

import org.apache.commons.httpclient.URI;
URI base = new URI(""http://www.example.com/path/page"");
URI rel1 = new URI("":foo/boo"");
System.out.println((new URI(base,rel1)).toString()); // displays just "":foo""

A potential fix would be for URI.parseUriReference() to avoid interpreting a ':'
in the zero position as indicating a zero-length scheme:

-       if (atColon < 0 || (atSlash >= 0 && atSlash < atColon)) {
+       if (atColon <= 0 || (atSlash >= 0 && atSlash < atColon)) {

and

-        if (at < length && tmp.charAt(at) == ':') {
+        if (at > 0 && at < length && tmp.charAt(at) == ':') {"
0,"Rethrow exception with cause in BundleDbPersistenceManagerAn exception forwarded from SQL should have a cause for better diagnosis.

Index: jackrabbit-core/src/main/java/org/apache/jackrabbit/core/persistence/bundle/BundleDbPersistenceManager.java
===================================================================
--- jackrabbit-core/src/main/java/org/apache/jackrabbit/core/persistence/bundle/BundleDbPersistenceManager.java (revision 585555)
+++ jackrabbit-core/src/main/java/org/apache/jackrabbit/core/persistence/bundle/BundleDbPersistenceManager.java (working copy)
@@ -604,7 +604,7 @@
             }
             return nameIndex;
         } catch (Exception e) {
-            throw new IllegalStateException(""Unable to create nsIndex: "" + e);
+            throw new IllegalStateException(""Unable to create nsIndex"", e);
         }
     }
 "
0,"Create ScoreNode on demand in SortedLuceneQueryHitsScoreNodes are current created for the full result fetch. Instead, the ScoreNodes should be created on demand when requested in nextScoreNode()."
0,javacc ant task for contrib/misc precedence query parserAdd a javacc task in contrib/misc for the precedence query parser.
0,"Fix StandardAnalyzer to not mis-identify HOST as ACRONYM by defaultComing out of the discussion around back compatibility, it seems best to default StandardAnalyzer to properly fix LUCENE-1068, while preserving the ability to get the back-compatible behavior in the rare event that it's desired.

This just means changing the replaceInvalidAcronym = false with = true, and, adding a clear entry to CHANGES.txt that this very slight non back compatible change took place.

Spinoff from here:

    http://www.gossamer-threads.com/lists/lucene/java-dev/57517#57517

I'll commit that change in a day or two."
0,"Provide an convenience AttributeFactory that implements all default attributes with TokenI found some places in contrib tests, where the Token.class was added using addAttributeImpl(). The problem here is, that you cannot be sure, that the attribute is really added and you may fail later (because you only update your local instance). The tests in contrib will partially fail with 3.0 without backwards layer (because the backwards layer uses Token/TokenWrapper internally and copyTo() will work.

The correct way to achieve this is using an AttributeFactory. The AttributeFactory is currently private in SingleTokenTokenStream. I want to move it to Token.java as a static class / static member. In this case the tests can be rewritten.

I also want to mark addAttributeImpl() as EXPERT, because you must really know whats happening and what are the traps."
1,"fix more position corrumptions in 4.0 codecsSpinoff of LUCENE-3876.

Some codecs have invalid asserts, wrong shift operators etc.

If a position exceeds Integer.MAX_VALUE/2 and then also has a payload,
it will produce corrumpt indexes or other strange errors.

Easiest way to trigger the bugs is to sometimes add a payload to the test from LUCENE-3876."
0,"Use Iterable<? extends UrlEncodedFormEntity> instead of List<? extends UrlEncodedFormEntity> in URLEncodedUtils.format and UrlEncodedFormEntityUrlEncodedFormEntity requires a List<? extends UrlEncodedFormEntity> to pass it to URLEncodedUtils.format. It would be nice to use Iterable<? extends UrlEncodedFormEntity> to be able to use other collections, e.g. a Set<? extends UrlEncodedFormEntity>"
1,"StatusLine IndexOutOfBoundsReported by Sam Berlin on the developers mailing list:

I'm not sure if this problem is still on CVS HEAD, but we're seeing it  
against 2.0rc2.  In StatusLine (line 139 in my version), when it walks  
through the spaces, it is possible that the entire line was spaces (and  
thus a malformed response).  The code will throw an  
StringIndexOutOfBoundsException now instead of the correct  
HttpException.  See the following bug:

http://bugs.limewire.com/bugs/ 
searching.jsp?disp1=l&disp2=c&disp3=o&disp4=j&l=151&c=204&m=416_205

Thanks,
  Sam"
0,"optionally support naist-jdic for kuromojiThis is an alternative dictionary, somewhat larger (~25%).

we can support it in build.xml so if a user wants to build with it, they can (the resulting jar file will be 500KB larger)"
0,"Add a simple FST impl to Lucene
I implemented the algo described at
http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.3698 for
incrementally building a finite state transducer (FST) from sorted
inputs.

This is not a fully general FST impl -- it's only able to build up an
FST incrementally from input/output pairs that are pre-sorted.

Currently the inputs are BytesRefs, and the outputs are pluggable --
NoOutputs gets you a simple FSA, PositiveIntOutputs maps to a long,
ByteSequenceOutput maps to a BytesRef.

The implementation has a low memory overhead, so that it can handle a
fairly large set of terms.  For example, it can build the FSA for the
9.8M terms from a 10M document wikipedia index in ~8 seconds (on
beast), using ~256 MB peak RAM, resulting in an FSA that's ~60 MB.

It packs the FST as-it-builds into a compact byte[], and then exposes
the API to read nodes/arcs directly from the byte[].  The FST can be
quickly saved/loaded to/from a Directory since it's just a big byte[].

The format is similar to what Morfologik uses
(http://sourceforge.net/projects/morfologik/).

I think there are a number of possible places we can use this in
Lucene.  For example, I think many apps could hold the entire terms
dict in RAM, either at the multi-reader level or maybe per-segment
(mapping to file offset or to something else custom to the app), which
may possibly be a good speedup for certain MTQs (though, because the
format is packed into a byte[], there is a decode cost when visiting
arcs).

The builder can also prune as it goes, so you get a prefix trie pruned
according to how many terms run through the nodes, which makes it
faster and even less memory consuming.  This may be useful as a
replacement for our current binary search terms index since it can
achieve higher term density for the same RAM consumption of our
current index.

As an initial usage to make sure this is exercised, I cutover the
SimpleText codec, which currently fully loads all terms into a
TreeMap (and has caused intermittent OOME in some tests), to use an FST
instead.  SimpleText uses a PairOutputs which is able to ""pair up"" any
two other outputs, since it needs to map each input term to an int
docFreq and long filePosition.

All tests pass w/ SimpleText forced codec, and I think this is
committable except I'd love to get some help w/ the generics
(confession to the policeman: I had to add
@SuppressWarnings({""unchecked""})) all over!!  Ideally an FST is
parameterized by its output type (Integer, BytesRef, etc.).

I even added a new @nightly test that makes a largeish set of random
terms and tests the resulting FST on different outputs :)

I think it would also be easy to make a variant that uses char[]
instead of byte[] as its inputs, so we could eg use this during analysis
(Robert's idea).  It's already be easy to have a CharSequence
output type since the outputs are pluggable.

Dawid Weiss (author of HPPC -- http://labs.carrotsearch.com/hppc.html -- and
Morfologik -- http://sourceforge.net/projects/morfologik/)
was very helpful iterating with me on this (thank you!).
"
0,"Optimize refresh operations With the current implementation (recursive) refresh operations cause a full traversal of the sub-tree rooted at the item causing the refresh. This is potentially expensive. 

Instead of invalidating each item in the respective sub-tree I propose to mark the root of the sub-tree as invalidated. Such a mark would include a time stamp. Also individual items would be time stamped with their resolution time. When an item is accessed, it would check if its resolution time stamp is older than the latest invalidation time stamp. If so, it checks whether the invalidation applies to it at all (by traversing up the path) and if so it would re-resolve itself. In any case its resolution time stamp will be updated.

This approach would make invalidation much cheaper without putting much additional load to simple item access. Moreover most of the additional load (traversing up the path) only applies when an invalidation is pending."
0,"[PATCH] Remove equals() from internal Comparator of ConjunctionScorerAs written, the equals() method is not used. 
The docs of java.util.Comparator have an equals() with a single 
arg to compare the Comparator itself to another one, which is 
hardly ever useful. 
Patch follows"
1,"MatchAllDocsQuery, MultiSearcher and a custom HitCollector throwing exceptionI have encountered an issue with lucene1.9.1. It involves MatchAllDocsQuery, MultiSearcher and a custom HitCollector. The following code throws  java.lang.UnsupportedOperationException.

If I remove the MatchAllDocsQuery  condition (comment whole //1 block), or if I dont use the custom hitcollector (ms.search(mbq); instead of ms.search(mbq, allcoll);) the exception goes away. By stepping into the source I can see it seems due to MatchAllDocsQuery no implementing extractTerms()....


           Searcher searcher = new
IndexSearcher(""c:\\projects\\mig\\runtime\\index\\01Aug16\\"");
           Searchable[] indexes = new IndexSearcher[1];
           indexes[0] = searcher;
           MultiSearcher ms = new MultiSearcher(indexes);

           AllCollector allcoll = new AllCollector(ms);

           BooleanQuery mbq = new BooleanQuery();
           mbq.add(new TermQuery(new Term(""body"", ""value1"")),
BooleanClause.Occur.MUST_NOT);
// 1
           MatchAllDocsQuery alld = new MatchAllDocsQuery();
           mbq.add(alld, BooleanClause.Occur.MUST);
//

           System.out.println(""Query: "" + mbq.toString());

           // 2
           ms.search(mbq, allcoll);
           //ms.search(mbq);"
0,"IndexWriter.addIndexes can make any incoming segment into CFS if it isn't alreadyToday, IW.addIndexes(Directory) does not modify the CFS-mode of the incoming segments. However, if IndexWriter's MP wants to create CFS (in general), there's no reason why not turn the incoming non-CFS segments into CFS. We anyway copy them, and if MP is not against CFS, we should create a CFS out of them.

Will need to use CFW, not sure it's ready for that w/ current API (I'll need to check), but luckily we're allowed to change it (@lucene.internal).

This should be done, IMO, even if the incoming segment is large (i.e., passes MP.noCFSRatio) b/c like I wrote above, we anyway copy it. However, if you think otherwise, speak up :).

I'll take a look at this in the next few days."
0,"CacheManager interval between recalculation of cache sizes should be configurableCurrently interval between recaluclation of cahce size is hard coded to 1000 ms. Resizing/recalculation of cache size is quite expensive method (especially getMemoryUsed on MLRUItemStateCache is time consuming)

Depending on the configuration, we realized that under some load up to 10-15% percent of CPU time (profiler metrics) could be spend doing such recalculations. It does not seem to be needed to resize cache every second. Best this interval should be configurable in external config. file with other cache settings (like memory sizes)."
0,"Move common validation checks to a single placecheck for effective locks, nodes being checked-in, protection of item definitions etc. are abundant throughout jackrabbit-core. now that in addition retention and holds will be added and need to be checked as well, i suggest to move those checks to a common utility class and pass item and a list of checks to be performed.

batcheditemoperations already provides a similar pattern for the operations on item states.
i therefore suggest to move the flags to its base (ItemValidator) and add the utility methods needed."
0,"Allow database as backend for clusteringCurrently, clustering (see JCR-623) uses a shared file system folder in order to store modifications and synchronize all nodes in the cluster. Alternatively, a database backend should be available."
0,"RFC 2965 Support (Port sensitive cookies)RFC 2109 doesn't consider port numbers when matching and filtering cookies. RFC
2965 does. Modify the Cookie class so that it (optionally?) supports RFC 2965,
while maintaining support for RFC 2109-style (portless) cookies."
0,"[PATCH] TermVectorReader and TermVectorWriterTermVectorReader.close() closes all streams now under any condition. If an
excpetion is catched, it is remembered an thrown when all streams are closed.
Unnecessary variable assignment removed from code. 
Fix typo in TermVectorReader and TermVectorWriter."
0,"Replace spatial contrib module with LSP's spatial-lucene moduleI propose that Lucene's spatial contrib module be replaced with the spatial-lucene module within Lucene Spatial Playground (LSP).  LSP has been in development for approximately 1 year by David Smiley, Ryan McKinley, and Chris Male and we feel it is ready.  LSP is here: http://code.google.com/p/lucene-spatial-playground/  and the spatial-lucene module is intuitively in svn/trunk/spatial-lucene/.

I'll add more comments to prevent the issue description from being too long."
0,"Webdav: Review usage of command chainsi'd like to review the usage of command chains for import/export within the simple webdav server.

while the concept of command chains offers a lot of flexibility, it showed that the implementation generates some drawbacks. a new mechanism should take advantage of the experiences made with the command chains.

from my point of view the following issues should be taken into consideration:

- provide means to extend and modify the import/export logic with minimal effort

- consistent import/export functionality for both collections and non-collections
  > export/import should not be completely separated.
  > interfaces should encourage consistency
  > increase maintainability, reduce no of errors

- distinction of collections and non-collections for import/export behaviour
  > PUT must result in non-coll, MKCOL in collection

- allow to defined a set of import/export-handlers with a given order.

- the different handlers must not rely on each other.

- an import/export should be completed after the first handler indicates success. there 
  should not be other classes involved in order to complete the import/export.

- avoid huge configuration files and if possible, avoid program flow being defined outside of java code.

- avoid duplicate configuration (e.g. resource-filtering), duplicate code, duplicate logic, that is 
  hard to maintain.

- additonal logic should be defined within a given import/export handler.
  however, in case of webdav i see limited value of using extra logic such as addMixin or checkin, 
  that are covered by  webdav methods (such as LOCK, VERSION-CONTROL or CHECKIN).

regards
angela


"
1,"Cached Item could be lost in cachesince items and itemstate are cache in week/soft reference maps, they can disappear from the caches after a GC cycle, and code like this:

if (isCached(id)) {
  return retrieveItem(id);
}

has potential to fail. log entries like:

failed to build path of c2eeecbe-6126-45a2-a38a-002361095107: 3334d748-2790-4004-8bfa-09463624c7c4 has no child entry for 4897c961-f36f-4d46-87bd-f24f152138a4

are the result."
1,"PATCH MultiLevelSkipListReader NullPointerException When Reconstructing Document Using Luke Tool, received NullPointerException.

java.lang.NullPointerException
        at org.apache.lucene.index.MultiLevelSkipListReader.loadSkipLevels(MultiLevelSkipListReader.java:188)
        at org.apache.lucene.index.MultiLevelSkipListReader.skipTo(MultiLevelSkipListReader.java:97)
        at org.apache.lucene.index.SegmentTermDocs.skipTo(SegmentTermDocs.java:164)
        at org.getopt.luke.Luke$2.run(Unknown Source)

Luke version 0.7.1

I emailed with Luke author Andrzej Bialecki and he suggested the attached patch file which fixed the problem.
"
0,"On missing child node, automatically remove the entry (when a session attribute is set)If a node points to a non-existing child node (which is a repository inconsistency), currently this child node is silently ignored for read operations (as far as I can tell). However, when trying to add another child node with the same name, an exception is thrown with a message saying a child node with this name already exists. Also, the parent node can't be removed.

One solution is to remove the bad child node entry, but only if the session attribute ""org.apache.jackrabbit.autoFixCorruptions"" is set (so by default the repository is not changed 'secretly'):

    SimpleCredentials cred = new SimpleCredentials(...);
    cred.setAttribute(""org.apache.jackrabbit.autoFixCorruptions"", ""true"");
    rep.login(cred);

It's not a perfect solution, but it might be better than throwing an exception and basically preventing changes.

Another solution (not implemented) would be to rename the missing child node entry when trying to add a child node with the same name (for example add the current date/time, or a random digit until there is no conflict), and then continue with adding the new child node.

"
0,"remove Byte/CharBuffer wrapping for collation key generationWe can remove the overhead of ByteBuffer and CharBuffer wrapping in CollationKeyFilter and ICUCollationKeyFilter.

this patch moves the logic in IndexableBinaryStringTools into char[],int,int and byte[],int,int based methods, with the previous Byte/CharBuffer methods delegating to these.
Previously, the Byte/CharBuffer methods required a backing array anyway.
"
0,Use executor service from repository for index mergingThe index merger currently starts its own threads for index merges. Using the repository wide executor service would simplify things and make configuration easier.
0,"Update site level documentationugg - a fun one - my brain is sliding to the bottom of my skull from excitement.

Must update all of the site level pages to current API usage."
0,"Remove dependency on Jackrabbit-coreWe should remove the dependency on Jackrabit core in the OCM subprojects ""jcr-mapping"" and ""annotation"". We can use Jackrabbit core only for the unit tests. 

We can also split the jcr-nodemanagement into several subprojects (one per JCR repo impl).  We will have only one subproject for Jackrabbit but contributions for other JCR repo impl are welcome.  A specific jcr-nodemanagement jar can be produce for each JCR repo impl. When the JCR will support the node creation, we can refactor the jcr-nodemanagement. 

"
0,"Occasional testDataStoreGarbageCollection test failuresIn the past few days our Hudson build started failing every now and then with the following jackrabbit-core test failure:

javax.jcr.NoSuchWorkspaceException: security
	at org.apache.jackrabbit.core.RepositoryImpl.getWorkspaceInfo(RepositoryImpl.java:786)
	at org.apache.jackrabbit.core.RepositoryImpl.getSystemSession(RepositoryImpl.java:985)
	at org.apache.jackrabbit.core.RepositoryImpl.getSecurityManager(RepositoryImpl.java:471)
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1496)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:380)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:123)
	at org.apache.jackrabbit.core.data.DataStoreAPITest.testDataStoreGarbageCollection(DataStoreAPITest.java:55)"
1,"EventConsumer.canRead() should rely on AccessManager.isGranted()The current implementation of EventConsumer.canRead() uses
AccessManager.canRead(), which might cause issues if the item
does not exist anymore. AccessManager.isGranted() explicitly
mentions and supports checks on paths for items that do not
yet exist or not exist anymore.

See also JCR-3271."
0,"ConjunctionScorer - more tuneup(See also: #LUCENE-443)
I did some profile testing with the new ConjuctionScorer in 2.1 and discovered a new bottleneck in ConjunctionScorer.sortScorers. The java.utils.Arrays.sort method is cloning the Scorers array on every sort, which is quite expensive on large indexes because of the size of the 'norms' array within, and isn't necessary. 

Here is one possible solution:

  private void sortScorers() {
// squeeze the array down for the sort
//    if (length != scorers.length) {
//      Scorer[] temps = new Scorer[length];
//      System.arraycopy(scorers, 0, temps, 0, length);
//      scorers = temps;
//    }
    insertionSort( scorers,length );
    // note that this comparator is not consistent with equals!
//    Arrays.sort(scorers, new Comparator() {         // sort the array
//        public int compare(Object o1, Object o2) {
//          return ((Scorer)o1).doc() - ((Scorer)o2).doc();
//        }
//      });
  
    first = 0;
    last = length - 1;
  }
  private void insertionSort( Scorer[] scores, int len)
  {
      for (int i=0; i<len; i++) {
          for (int j=i; j>0 && scores[j-1].doc() > scores[j].doc();j-- ) {
              swap (scores, j, j-1);
          }
      }
      return;
  }
  private void swap(Object[] x, int a, int b) {
    Object t = x[a];
    x[a] = x[b];
    x[b] = t;
  }
 
The squeezing of the array is no longer needed. 
We also initialized the Scorers array to 8 (instead of 2) to avoid having to grow the array for common queries, although this probably has less performance impact.

This change added about 3% to query throughput in my testing.

Peter
"
0,"Clean caches in node type registry on session logoutWhen running the JCR tests the memory consumption increases steadily. At the end of the test run it consumes about 300 Mb on my machine. There's not really a memory leak in jcr2spi, because the JUnit tests keep references to Session and Node instances until the end of the test run, but  it would be nice if those instances were a bit more lightweight."
0,"SPI: Introduce NodeInfo.getChildInfos()Improvement suggested by Marcel:

ChildInfo is basically a stripped down NodeInfo. With little effort it would even be possible to have NodeInfo extends ChildInfo. Not sure how useful that is, but since we don't have that inheritance in code and at the same time nearly a 100% overlap it makes me suspicious.

Here's another idea:

introduce a method ChildInfo[] NodeInfo.getChildInfos(). The method either returns:

- all child infos, which also gives the correct number of child nodes. this may also mean that an empty array is returned to indicate there are no child nodes.
- null, to indicate that there are *lots* of child nodes and the method RepositoryService.getChildInfos() with the iterator should be used. 


"
0,Missing third party notices and license infoThe components that bundle external libraries (jackrabbit-webapp and jackrabbit-jca) should come with appropriate copyright notices and license information.
0,"Add CharArrayMap to lucene and make CharAraySet an proxy on the keySet() of itThis patch adds a CharArrayMap<V> to Lucene's analysis package as compagnon of CharArraySet. It supports fast retrieval of char[] keys like CharArraySet does. This is important for some stemmers and other places in Lucene.

Stemers generally use CharArrayMap<String>, which has then get(char[]) returning String. Strings are compact and can be easily copied into termBuffer. A Map<String,String> would be slow as the termBuffer would be first converted to String, then looked up. The return value as String is perfectly legal, as it can be copied easily into termBuffer.

This class borrows lots of code from Solr's pendant, but has additional features and more consistent API according to CharArraySet. The key is always <?>, because as of CharArraySet, anything that has a toString() representation can be used as key (of course with overhead). It also defines a unmodifiable map and correct iterators (returning the native char[]).

CharArraySet was made consistent and now returns for matchVersion>=3.1 also an iterator on char[]. CharArraySet's code was almost completely copied to CharArrayMap and removed in the Set. CharArraySet is now a simple proxy on the keySet().

In future we can think of making CharArraySet/CharArrayMap/CharArrayCollection an interface so the whole API would be more consistent to the Java collections API. But this would be a backwards break. But it would be possible to use better impl instead of hashing (like prefix trees)."
0,"Logging per test caseThanks to the switch to Logback a while ago (JCR-2584) we can start taking advantage of some nice new features like the one described in [1]. With this trick we'll be able to split the currently pretty large jcr.log test log file we have to separate log files per each test case. This will make it much easier to review the logs written during any particular test.

[1] http://www.nalinmakar.com/2010/07/28/logging-tests-to-separate-files/"
0,"SimpleJbossAccessManagerhttp://wiki.apache.org/jackrabbit/SimpleJbossAccessManager

Code contribution
"
0,"Java 5 port phase II LUCENE-1257 addresses the public API changes ( generics , mainly ) and other j.u.c. package changes related to the API .  The changes are frozen and closed for 3.0 . This would be a placeholder JIRA for 3.0+ version to address the pending changes ( tests for generics etc.) and any other internal API changes as necessary. "
0,"New PostSOAP example (for src/examples)I have a slightly modified version of PostXML which invokes SOAP requests. The
only difference to PostXML is that PostSOAP takes the SOAPAction as an extra
commndline arg and adds that as a header into the request."
1,"NullPointerException in GarbageCollector.scan() if no DataStore configured
I am running the garbage collector in a separate thread every 5 minutes.

            GarbageCollector gc = ((SessionImpl)mSession).createDataStoreGarbageCollector();
            gc.scan();
            gc.stopScan();
            int du = gc.deleteUnused();

When using Jackrabbit v1.5.2 I get sometimes a null pointer exception;

java.lang.NullPointerException
        at org.apache.jackrabbit.core.data.GarbageCollector.scan(GarbageCollector.java:153)



"
0,"Replacing mixin type doesn't preserve propertiesNodeImpl.setPrimaryType(String) attempts to ""redefine"" nodes and properties that were defined by the previous node type if they also appear in the new type. If there is no matching definition for a node/property in the new type - or value conversion for matched node/property fails - only then are children removed. For example, say I have a node ""harry"", with a primary type ""Human"" that defines a ""bloodgroup"" property. If I set the primary type to be an unrelated type ""Animal"" that has a similar ""bloodgroup"" property, then its property value will survive the call to setPrimaryType(""Animal"").

The same is apparently not possible with mixins. NodeImpl.removeMixin(Name) immediately removes all children that were defined by the mixin (strictly, those that are not present in the effective node type resulting from the mixin being removed). In addition, NodeImpl.addMixin(Name) immediately throws a NodeTypeConflictException if you attempt to add a mixin defining an identically-named property prior to calling removeMixin. For example, say I have a node ""matrix"", with a mixin type ""movie"" that defines a ""title"" property. If I wish to replace the ""movie"" mixin on that node with another ""jcr:title"" mixin type, the existing ""title"" property will be deleted.

This occurs regardless of the order in which removeMixin and addMixin are called, and without session.save() being called between them. One option for coding this is to defer validation (and possible node/property removal) until session.save() is called.

This is not strictly a bug, as JSR-283 seems to leave the details of assigning node types (section 5.5) unspecified. However, it does say for Node.removeMixin(String) that ""Both the semantic change in effective node type and the persistence of the
change to the jcr:mixinTypes property occur on save"" and ideally we could emulate the nice behaviour in NodeImpl.setPrimaryType(String) for mixin types."
1,"OOM erros with CheckIndex with indexes containg a lot of fields with normsAll index readers have a cache of the last used norms (SegmentReader, MultiReader, MultiSegmentReader,...). This cache is never cleaned up, so if you access norms of a field, the norm's byte[maxdoc()] array is not freed until you close/reopen the index.

You can see this problem, if you create an index with many fields with norms (I tested with about 4,000 fields) and many documents (half a million). If you then call CheckIndex, that calls norms() for each (!) field in the Segment and each of this calls creates a new cache entry, you get OutOfMemoryExceptions after short time (I tested with the above index: I was not able to do a CheckIndex even with ""-Xmx 16GB"" on 64bit Java).

CheckIndex opens and then tests each segment of a index with a separate SegmentReader. The big index with the OutOfMemory problem was optimized, so consisting of one segment with about half a million docs and about 4,000 fields. Each byte[] array takes about a half MiB for this index. The CheckIndex funtion created the norm for 4000 fields and the SegmentReader cached them, which is about 2 GiB RAM. So OOMs are not unusal.

In my opinion, the best would be to use a Weak- or better a SoftReference so norms.bytes gets java.lang.ref.SoftReference<byte[]> and used for caching. With proper synchronization (which is done on the norms cache in SegmentReader) you can do the best with SoftReference, as this reference is garbage collected only when an OOM may happen. If the byte[] array is freed (but it is only freed if no other references exist), a lter call to getNorms() creates a new array. When code is hard referencing the norms array, it will not be freed, so no problem. The same could be done for the other IndexReaders.

Fields without norm() do not have this problem, as all these fields share a one-time allocated dummy norm array. So the same index without norms enabled for most of the fields checked perfectly.

I will prepare a patch tomorrow.

Mike proposed another quick fix for CheckIndex:
bq. we could do something first specifically for CheckIndex (eg it could simply use the 3-arg non-caching bytes method instead) to prevent OOM errors when using it.
"
0,JSR 283: JCR Names
0,"Use NIO positional read to avoid synchronization in FSIndexInputAs suggested by Doug, we could use NIO pread to avoid synchronization on the underlying file.
This could mitigate any MT performance drop caused by reducing the number of files in the index format."
0,Jcr2Spi: configuration entry for size of ItemCachein order to make the size of the ItemCache configurable (see TODO in jcr2spi SessionImpl) i'd like to extend the jcr2spi RepositoryConfig and have a default value being provided with AbstractRepositoryConfig in the tests section.
0,ExtendableQueryParser should allow extensions to access the toplevel parser settings/ propertiesBased on the latest discussions in LUCENE-2039 this issue will expose the toplevel parser via the ExtensionQuery so that ExtensionParsers can access properties like getAllowLeadingWildcards() from the top level parser.
0,"Factor maxMergeSize into findMergesForOptimize in LogMergePolicyLogMergePolicy allows you to specify a maxMergeSize in MB, which is taken into consideration in regular merges, yet ignored by findMergesForOptimze. I think it'd be good if we take that into consideration even when optimizing. This will allow the caller to specify two constraints: maxNumSegments and maxMergeMB. Obviously both may not be satisfied, and therefore we will guarantee that if there is any segment above the threshold, the threshold constraint takes precedence and therefore you may end up w/ <maxNumSegments (if it's not 1) after optimize. Otherwise, maxNumSegments is taken into consideration.

As part of this change, I plan to change some methods to protected (from private) and members as well. I realized that if one wishes to implement his own LMP extension, he needs to either put it under o.a.l.index or copy some code over to his impl.

I'll attach a patch shortly."
0,"Unable to get the status line from a http method objectThe status line (typically the first line returned from a http connection read)
is hidden inside httpclient with no way for client code to retrieve it intact.
readStatusLine() in HttpMethodBase is where the status line is
read, but it is never stored and is not available outside the method.

We could store the status line as a string and add a getStatusLine
method to the HttpMethod interface and HttpMethodBase class.  Alternatively, we
could create a header for it with the name StatusLine (or perhaps just null) so
that it could be retrieved with getHeader(""StatusLine"").  This would preserve
the interface but would be a bit of a kludge."
0,"Allow TaskSequence to run for certain timeTo help the perf testing for LUCENE-1483, I added simple ability to specify a fixed run time (seconds) for a task sequence, eg:
{code}
{ ""XSearchWithSort"" SearchWithSort(doctitle:string) > : 2.7s
{code}
iterates on that subtask until 2.7 seconds have elapsed, and then sets the repetition count to how many iterations were done.  This is useful when you are running searches whose runtime may vary drastically."
0,Move Content-Type to the RequestEntityThe content type is really a property of the RequestEntity.  It should be moved there.
0,"Remove RepositoryService.getRootId()For consistency reasons jcr2spi should use idFactory.createNodeId((String) null, pathFactory.getRootPath()) everywhere to build the NodeId of the root node. having two separate methods is confusing."
0,More javadocs for WeightFrom Doug's reply of 21 Feb 2005 in bug 31841
0,"Mavenize ProjectMavenize project pending mailing list feedback.

I'm attaching an out-dated zip that has some useful ""stuff"" to review or build upon.

Proposed flow:

1) Review the maven specific elements in the zip:
- project.xml, header.txt, license.txt, notice.txt, checkstyle.xml, project.properties, xdocs directory. NOTE: I know the license.txt and maybe header.txt are not correct per earlier discussions.
2) Once new package layout/code is in svn - we can refactor this and move the maven related files into place.

Thoughts,
  a) One of the artifacts generated is the bat file and supporting directory with the jar to run the sample app - could use some ideas on how maven should generate this.
  b) How the local maven repo gets the jcr-api.jar? My current thinking is 'same as any non-redistributable artifact'. The user is responsible for downloading it to a local repository."
0,"TestCachingSpanFilter sometimes failsif I run 
{noformat} 
ant test -Dtestcase=TestCachingSpanFilter -Dtestmethod=testEnforceDeletions -Dtests.seed=5015158121350221714:-3342860915127740146 -Dtests.iter=100
{noformat} 

I get two failures on my machine against current trunk
{noformat} 

junit-sequential:
    [junit] Testsuite: org.apache.lucene.search.TestCachingSpanFilter
    [junit] Testcase: testEnforceDeletions(org.apache.lucene.search.TestCachingSpanFilter):	FAILED
    [junit] expected:<2> but was:<3>
    [junit] junit.framework.AssertionFailedError: expected:<2> but was:<3>
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit] 	at org.apache.lucene.search.TestCachingSpanFilter.testEnforceDeletions(TestCachingSpanFilter.java:101)
    [junit] 
    [junit] 
    [junit] Testcase: testEnforceDeletions(org.apache.lucene.search.TestCachingSpanFilter):	FAILED
    [junit] expected:<2> but was:<3>
    [junit] junit.framework.AssertionFailedError: expected:<2> but was:<3>
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit] 	at org.apache.lucene.search.TestCachingSpanFilter.testEnforceDeletions(TestCachingSpanFilter.java:101)
    [junit] 
    [junit] 
    [junit] Tests run: 100, Failures: 2, Errors: 0, Time elapsed: 2.297 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestCachingSpanFilter -Dtestmethod=testEnforceDeletions -Dtests.seed=5015158121350221714:-3342860915127740146
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestCachingSpanFilter -Dtestmethod=testEnforceDeletions -Dtests.seed=5015158121350221714:-3342860915127740146
    [junit] NOTE: test params are: codec=MockVariableIntBlock(baseBlockSize=43), locale=fr, timezone=Africa/Bangui
    [junit] ------------- ---------------- ---------------
    [junit] Test org.apache.lucene.search.TestCachingSpanFilter FAILED
{noformat}

not sure what it is but it seems likely to be a WeakRef / GC issue in the cache. "
1,ChangeLogRecord throws NullPointerExceptionHappens when the userId of the session that created the events is null.
1,"DigestScheme.authenticate returns invalid authorization string when algorithm is nullDigestScheme.authenticate returns invalid authorization string when algorithm 
is null. 
I traced the bug and found the following from the method call:
authenticate(Credentials credentials, String method, String uri) calls
authenticate(UsernamePasswordCredentials credentials,
            Map params) calls
createDigest(String uname, String pwd,
            Map params)
  and properly defaults algorithm to MD5 if null
but the final call to createDigestHeader(String uname, Map params,
            String digest) does not default algorithm to MD5 if null"
0,"Update namespace uri for prefix fnThe SearchManager class still uses an outdated namespace uri for the 'fn' prefix: http://www.w3.org/2004/10/xpath-functions

The prefix should be re-mapped to the now offical namespace: http://www.w3.org/2005/xpath-functions

See: http://www.w3.org/TR/xquery-operators/#namespace-prefixes

To keep a minimum of backward compatibility, the existing namespace uri should still exist in the namespace registry, but refer to another prefix. E.g. fn_old."
0,"short circuit FuzzyQuery.rewrite when input token length is small compared to minSimilarityI found this (unreplied to) email floating around in my Lucene folder from during the holidays...

{noformat}
From: Timo Nentwig
To: java-dev
Subject: Fuzzy makes no sense for short tokens
Date: Mon, 31 Dec 2007 16:01:11 +0100
Message-Id: <200712311601.12255.lucene@nitwit.de>

Hi!

it generally makes no sense to search fuzzy for short tokens because changing
even only a single character of course already results in a high edit
distance. So it actually only makes sense in this case:

           if( token.length() > 1f / (1f - minSimilarity) )

E.g. changing one character in a 3-letter token (foo) results in an edit
distance of 0.6. And if minSimilarity (which is by default: 0.5 :-) is higher
we can save all the expensive rewrite() logic.
{noformat}

I don't know much about FuzzyQueries, but this reasoning seems sound ... FuzzyQuery.rewrite should be able to completely skip all TermEnumeration in the event that the input token is shorter then some simple math on the minSimilarity.  (i'm not smart enough to be certain that the math above is right however ... it's been a while since i looked at Levenstein distances ... tests needed)
"
1,"If the NRT reader hasn't changed then IndexReader.openIfChanged should return nullI hit a failure in TestSearcherManager (NOTE: doesn't always fail):

{noformat}
  ant test -Dtestcase=TestSearcherManager -Dtestmethod=testSearcherManager -Dtests.seed=459ac99a4256789c:-29b8a7f52497c3b4:145ae632ae9e1ecf
{noformat}

It was tripping the assert inside SearcherLifetimeManager.record,
because two different IndexSearcher instances had different IR
instances sharing the same version.  This was happening because
IW.getReader always returns a new reader even when there are no
changes.  I think we should fix that...

Separately I found a deadlock in
TestSearcherManager.testIntermediateClose, if the test gets
SerialMergeScheduler and needs to merge during the second commit.
"
1,"WebDAV method invocation trying to create a new resource should fail with 409 (Conflict) if parent resource does not existThis is Litmus test case copy_nodestcoll. An attempt is made to COPY an existing resource to a new location, where the parent collection of the resource-to-be-created does not exist. RFC2518 asks for status code 409 (Conflict) instead of 403 (Forbidden) in this case.
"
1,"restore sometime throws error about missing tmp filesCaused by: javax.jcr.RepositoryException: file backing binary value not
found: /server/apache-tomcat-5.5.15/temp/bin4435.tmp (No such file or
directory): /server/apache-tomcat-5.5.15/temp/bin4435.tmp (No such file
or directory)
   at
org.apache.jackrabbit.core.value.BLOBFileValue.getStream(BLOBFileValue.java:454)
   at
org.apache.jackrabbit.core.state.util.Serializer.serialize(Serializer.java:197)"
1,"DefaultHttpRequestRetryHandler#retryRequest should not retry aborted requestsDefaultHttpRequestRetryHandler#retryRequest incorrectly retries aborted requests; I have seen the following log messages in JMeter:

org.apache.http.impl.client.DefaultHttpClient: I/O exception (java.net.SocketException) caught when processing request: socket closed
org.apache.http.impl.client.DefaultHttpClient: Retrying request

and

org.apache.http.impl.client.DefaultHttpClient: I/O exception (java.net.BindException) caught when connecting to the target host: Address already in use: connect
org.apache.http.impl.client.DefaultHttpClient: Retrying connect

The abort() method sets the isAborted() flag, but the retry handler does not check it."
1,"FSDirectory doesn't detect double-close nor usage after closeFSDirectory.close implements logic to ensure only a single instance of FSDirectory per canonical directory exists.  This means code that synchronizes on the FSDirectory instance is also synchronized against that canonical directory.  I think only IndexModifier (now deprecated) actually makes use of this, but I'm not certain. 

But, the close() method doesn't detect double close, and doesn't catch usage after being closed, and so one can easily get two instances of FSDirectory for the same canonical directory."
1,"Problem with IndexWriter.mergeFinishI'm getting a (very) infrequent assert in IndexWriter.mergeFinish from TestIndexWriter.testAddIndexOnDiskFull. The problem occurs during the rollback when the merge hasn't been registered. I'm not 100% sure this is the correct fix, because it's such an infrequent event. 

{code:java}
  final synchronized void mergeFinish(MergePolicy.OneMerge merge) throws IOException {
    
    // Optimize, addIndexes or finishMerges may be waiting
    // on merges to finish.
    notifyAll();

    if (merge.increfDone)
      decrefMergeSegments(merge);

    assert merge.registerDone;

    final SegmentInfos sourceSegments = merge.segments;
    final int end = sourceSegments.size();
    for(int i=0;i<end;i++)
      mergingSegments.remove(sourceSegments.info(i));
    mergingSegments.remove(merge.info);
    merge.registerDone = false;
  }
{code}

Should  be something like:

{code:java}
  final synchronized void mergeFinish(MergePolicy.OneMerge merge) throws IOException {
    
    // Optimize, addIndexes or finishMerges may be waiting
    // on merges to finish.
    notifyAll();

    if (merge.increfDone)
      decrefMergeSegments(merge);

    if (merge.registerDone) {
      final SegmentInfos sourceSegments = merge.segments;
      final int end = sourceSegments.size();
      for(int i=0;i<end;i++)
        mergingSegments.remove(sourceSegments.info(i));
      mergingSegments.remove(merge.info);
      merge.registerDone = false;
    }
  }
{code}"
1,"AccessManager asks for property (jcr:created) permissions before the actual creation of the object
When implementing a custom AccessManager for Jackrabbit v1.5+ there a bug when creating a new object.

I perform an addNode() and my own accessmanager the isGranted() method is override'd and performs the following code;

            ......
            String perm = null;
            NodeId     nodeId = mHierMgr.resolveNodePath( pPath );
            PropertyId propId = null;
            if (nodeId==null) {
              propId = mHierMgr.resolvePropertyPath( pPath );
     System.out.println(""path = "" + pPath.toString() );
              // **** TODO is this ok?... it happens when a new object is created and the accessmanager ask for read access on a property.
//              if (propId==null) return true;
              nodeId = propId.getParentId();
            }

this is the System.out.println result

path = {}        {}JeCARS        {}default        {}Groups        {}testGroup        {http://www.jcp.org/jcr/1.0}created


and the stacktrace


	at org.jecars.CARS_AccessManager.isGranted(CARS_AccessManager.java:844)
	at org.jecars.CARS_AccessManager.isGranted(CARS_AccessManager.java:806)
	at org.apache.jackrabbit.core.ItemManager.canRead(ItemManager.java:339)
	at org.apache.jackrabbit.core.ItemManager.canRead(ItemManager.java:326)
	at org.apache.jackrabbit.core.ItemManager.createItemData(ItemManager.java:696)
	at org.apache.jackrabbit.core.ItemManager.getItemData(ItemManager.java:291)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:228)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:493)
	at org.apache.jackrabbit.core.NodeImpl.createChildProperty(NodeImpl.java:479)
	at org.apache.jackrabbit.core.NodeImpl.createChildNode(NodeImpl.java:535)
	at org.apache.jackrabbit.core.NodeImpl.internalAddChildNode(NodeImpl.java:795)
	at org.apache.jackrabbit.core.NodeImpl.internalAddNode(NodeImpl.java:729)
	at org.apache.jackrabbit.core.NodeImpl.internalAddNode(NodeImpl.java:677)
	at org.apache.jackrabbit.core.NodeImpl.addNode(NodeImpl.java:2110)


It seems that READ permission for the jcr:created property is requested before the object is actually created


"
1,"LOWER operand with nested LOCALNAME operand not work with SQL2Below query was running successful using Query.SQL languange:
SELECT * FROM nt:file WHERE (CONTAINS(*, 'Jon') OR  LOWER(fn:name()) LIKE '%jon%') AND jcr:path LIKE '/Resources/%' ORDER BY jcr:score()

But equivalent next query in Query.JCR_SQL2 will fail with exception UnsupportedRepositoryOperationException():
SELECT * FROM [nt:file] WHERE (CONTAINS([nt:file].*, 'Jon') OR  LOWER(LOCALNAME()) LIKE '%jon%') AND ISDESCENDANTNODE('/Resources') ORDER BY SCORE()

From my investigation seems LOWER function will not work with nested function LOCALNAME. According to section ""6.7.32 LowerCase"" JCR 2.0 Specs, LOWER operand able to work on DynamicOperand argument."
0,"[PATCH] demo HTML parser corrupts foreign charactersWe are using HTML parser for parsing English and other NL documents in 
Eclipse.  Post Lucene 1.2 there has been a regression in the parser.  
Characters coming from Reader (obtained from getReader() ) are corrupted.  
Only the characters that can be encoded using the default machine encoding go 
through correctly.  For example, parsing Chinese document on an English 
machine results with all characters, except the few English words, corrupted."
1,"SPI2DAVex: HttpClient StringPart uses charset US-ASCII by defaultif the diff is sent as multipart instead of a urlencoded post string properties may be garbeled.
reason: instances of httpclient StringPart are created without specifying the charset in which case US-ASCII is used by default."
1,"Incorrect ShingleFilter behavior when outputUnigrams == falseShingleFilter isn't working as expected when outputUnigrams == false. In particular, it is outputting unigrams at least some of the time when outputUnigrams==false.

I'll attach a patch to ShingleFilterTest.java that adds some test cases that demonstrate the problem.

I haven't checked this, but I hypothesize that the behavior for outputUnigrams == false got changed when the class was upgraded to the new TokenStream API?"
0,"Creation of JavaDoc fails on jackrabbit-jcr-serverThe creation of JavaDoc fails on project jackrabbit-jcr-server if the command ""mvn javadoc:javadoc"" is called on the latest checkout of jackrabbit. See attached log..."
0,"Correctly handle concurrent calls to addIndexes, optimize, commitSpinoff from here:

    http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200807.mbox/%3Cc7b302c50807111018j58b6d08djd56b5889f6b3780d@mail.gmail.com%3E"
0,"Add a document describing the HttpClient release processThe commons release process (http://jakarta.apache.org/commons/releases.html) is
a good starting place, but is out of date.  When we do our own releases, there
are some other steps particular to maven, the test-local and the webapp tests
that must be documented."
1,"NPE in StopFilter caused by StandardAnalyzer(boolean replaceInvalidAcronym) constructorI think that I found a problem with the new code (https://issues.apache.org/jira/browse/LUCENE-1068).
Usage of the new constructor StandardAnalyzer(boolean replaceInvalidAcronym) causes NPE in
StopFilter:

java.lang.NullPointerException
        at org.apache.lucene.analysis.StopFilter.<init>(StopFilter.java:74)
        at org.apache.lucene.analysis.StopFilter.<init>(StopFilter.java:86)
        at
org.apache.lucene.analysis.standard.StandardAnalyzer.tokenStream(StandardAnalyzer.java:151)
        at
org.apache.lucene.queryParser.QueryParser.getFieldQuery(QueryParser.java:452)
        at
org.apache.lucene.queryParser.QueryParser.Term(QueryParser.java:1133)
        at
org.apache.lucene.queryParser.QueryParser.Clause(QueryParser.java:1020)
        at
org.apache.lucene.queryParser.QueryParser.Query(QueryParser.java:948)
        at
org.apache.lucene.queryParser.QueryParser.Clause(QueryParser.java:1024)
        at
org.apache.lucene.queryParser.QueryParser.Query(QueryParser.java:948)
        at
org.apache.lucene.queryParser.QueryParser.TopLevelQuery(QueryParser.java:937)
        at
org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:147)

The reason is that new constructor forgets to initialize the stopSet field:
  public StandardAnalyzer(boolean replaceInvalidAcronym) {
    this.replaceInvalidAcronym = replaceInvalidAcronym;
  }

I guess this should be changed to something like this:
  public StandardAnalyzer(boolean replaceInvalidAcronym) {
    this(STOP_WORDS);
    this.replaceInvalidAcronym = replaceInvalidAcronym;
  }

The bug is present in RC3. Fix is one line, it'll be great to have it in 2.3
release.
"
0,"[PATCH] Differently configured Lucene 'instances' in same JVMCurrently Lucene can be configured using system properties. When running multiple 'instances' of Lucene for different purposes in the same JVM, it is not possible to use different settings for each 'instance'.

I made changes to some Lucene classes so you can pass a configuration to that class. The Lucene 'instance' will use the settings from that configuration. The changes do not effect the API and/or the current behavior so are backwards compatible.

In addition to the changes above I also made the SegmentReader and SegmentTermDocs extensible outside of their package. I would appreciate the inclusion of these changes but don't mind creating a separate issue for them.

"
1,"Journal log file rotation overwrites old filesJournal log files are rotated as follows:

  journal.log.N -> journal.log.(N+1)

Because the list of files to be rotated is created with alphanumeric sort order
(descending), it may destroy files in the following situation:

  journal.log.9  -> journal.log.10
  ..
  journal.log.2  -> journal.log.3
  journal.log.10 -> journal.log.11 (!)
  journal.log.1  -> journal.log.2

i.e. journal.log.10 is overwritten by journal.log.9 and then moved."
0,"trappy ignoreCase behavior with StopFilter/ignoreCaseSpinoff from LUCENE-3751:

{code}
* If <code>stopWords</code> is an instance of {@link CharArraySet} (true if
* <code>makeStopSet()</code> was used to construct the set) it will be
* directly used and <code>ignoreCase</code> will be ignored since
* <code>CharArraySet</code> directly controls case sensitivity.
{code}

This is really confusing and trappy... we need to change something here."
0,"MultiThreadedHttpConnectionManager never reclaims unused connectonsThere is no limit on the number of connections that will get created by the
MultiThreadedHttpConnectionManager.  Unused connections are never destroyed."
0,"Minor changes to SimpleHTMLFormatterI'd like to make few minor changes to SimpleHTMLFormatter.

1. Define DEFAULT_PRE_TAG and DEFAULT_POST_TAG and use them in the default constructor. This will not trigger String lookups by the JVM whenever the highlighter is instantiated.

2. Create the StringBuffer in highlightTerm with the right number of characters from the beginning. Even though StringBuffer's default constructor allocates 16 chars, which will probably be enough for most highlighted terms (pre + post tags are 7 chars, which leaves 9 chars for terms), I think it's better to allocate SB with the right # of chars in advance, to avoid char[] allocations in the middle."
0,"Speed up hierarchy cache initializationInitializing a workspace can take quite a long time if there is a big number of nodes and some search indexes involved. The reason is that the setup of the CachingIndexReader is processed using chunks of a certain size (actually 400K) in order to reduce the memory footprint. As soon as the number of documents exceeds this limit some operations (actually traversing complete indexes) are performed again and again.

It seems that the current algorithm ""initializeParents"" in the CachingIndexReader class can't be optimized without increasing the memory consumption. Therefore it should be a promising approach to persist the ""state"" of this class (actually it's main member array and map) and reload it on startup.

The ""load"" of the state can be done implicitly in the initializing phase of the cache. This is obvious. The correct point of time to call the ""save"" operation isn't obvious at all. I tried the ""doClose"" method of the class and it seems sufficient."
1,Index migration fails for property names that are prefixes of othersThe automatic index migration (JCR-1363) introduced in Jackrabbit version 1.5.0 replaces the separator char for named term text in PROPERTIES field. Util 1.4.x '\uFFFF' was used and after migration '[' is used. This changes the overall order of  PROPERTIES terms and leads to assertion failures. See attached test.
0,"Preserve whitespace in <code> sections in the Changes.html generated from CHANGES.txt by changes2html.plThe Trunk section of CHANGES.txt sports use of a new feature: <code> sections, for the two mentions of LUCENE-1575.

This looks fine in the text rendering, but looks crappy in the HTML version, since changes2html.pl escapes HTML metacharacters to appear as-is in the HTML rendering, but the newlines in the code are converted to a single space. 

I think this should be fixed by modifying changes2html.pl to convert <code> and </code> into (unescaped) <code><pre> and </pre></code>, respectively, since just passing through <code> and </code>, without </?pre>, while changing the font to monospaced (nice), still collapses whitespace (not nice). 

See the java-dev thread that spawned this issue here: http://www.nabble.com/CHANGES.txt-td23102627.html"
0,"Provide access to port of Host headerWe use a load balancer that connects to the HTTP server and the HTTP server
connects to the application server. We use port translation in our load
balancer. So when e.g. a client connects to 90 of the load balancer, the load
balancer connects to port 100 of the HTTP server. The load balancer doesn't
change the Host request header, so in the host request header is still the
original virtual host name and port, in this case port 90. For this reason, the
virtual hosts of the HTTP server and application server are configured based on
the external port numbers, so in this case port 90.
 
For test purposes, we sometimes want to connect directly to the HTTP server or
the application server, bypassing the load balancer. To do this, we need to
connect to the same port as the load balancer would, in this example port 100,
but the host header of this request should be the same as if the request would
go through the load balancer, so in this example port 90, because the HTTP
server and application server's virtual hosts are configured for this port.

The attached patch adds the possibility to specify the port number for virtual
hosts.

Here's a code snippet that uses the patched code:

HttpClient httpClient = new HttpClient();
HttpMethod method = new GetMethod();
HostConfiguration hostConfiguration = new HostConfiguration();
hostConfiguration.setHost(""localhost"", 80, ""http"");
HostParams params = new HostParams();
params.setVirtualHost(""localhost"");
params.setVirtualHostPort(100);
hostConfiguration.setParams(params);
httpClient.executeMethod(hostConfiguration, method);
System.out.println(method.getResponseBodyAsString());
method.releaseConnection();"
0,"Avoid creation of more than one jackrabbit instance with the same configurationbased on the mailing list archive, it seems new users often run more than one jackrabbit instance with the same configuration. I propose to lock the repository by creating an empty file called "".lock"" at the repository home on startup and remove it on shutdown.
If the lock file is found on jackrabbit startup the following message will be logged:
""The repository home at "" + home.getAbsolutePath() + "" appears to be in use. If you are sure it's not in use please delete the file at  "" + lock.getAbsolutePath() + "". Probably the repository was not shutdown properly.""
"
0,"refactoring of Similarity.sloppyFreq() and Similarity.scorePayloadCurrently these are top-level, but they only affect the SloppyDocScorer.
So it makes more sense to put these into the SloppyDocScorer api, this gives you additional flexibility
(e.g. combining payloads with CSF or whatever the hell you want to do), and is cleaner.

Furthermore, there are the following confusing existing issues:
* scorePayload should take bytesref
* PayloadTermScorer passes a *null* byte[] array to the sim if there are no payloads. I don't think it should do this, and its inconsistent with PayloadNearQuery, which does not do this. Its an undocumented conditional you need to have in the scoring algorithm which we should remove.
* there is an unused constant for scorepayload (NO_DOC_ID_PROVIDED), which is a documented, but never used anywhere. I think we should remove this conditional too, because its not possible to have a payload without a docid, and we shouldn't be passing fake document ids (-1) to our scoring APIs anyway.
"
1,"ClassCastException MultiReader(See original message below)
Sure.  'Bugzilla it', please.

Otis
P.S.
That line 274 should be line 273 in the CVS HEAD as of now.

--- Rasik Pandey <rasik.pandey@ajlsm.com> wrote:
> Howdy,
> 
> This exception was thrown with 1.4rc3. Do you need a test case for 
> this one?
> 
> java.lang.ClassCastException
>         at
> org.apache.lucene.index.MultiTermEnum.<init>(MultiReader.java:274)
>         at
> org.apache.lucene.index.MultiReader.terms(MultiReader.java:187)
> 
> 
> Regards,
> RBP
> 
> 
> 
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: lucene-dev-unsubscribe@jakarta.apache.org
> For additional commands, e-mail: lucene-dev-help@jakarta.apache.org
> 


---------------------------------------------------------------------
To unsubscribe, e-mail: lucene-dev-unsubscribe@jakarta.apache.org
For additional commands, e-mail: lucene-dev-help@jakarta.apache.org"
1,"'OR' in XPath query badly interpretedexecuting query: //*[@a=1 and @b=2 or @c=3] leads to creating wrong query tree. The builded tree looks like for query: //*[@a=1 and @b=2 and @c=3](see attachement). using brackets resolves the problem, but without brackets output query is different from input query. When AND and OR are switched(so the OR is in first palce - //*[@a=1 or @b=2 and @c=3]) everything is ok."
1,"Explanation.toHtml outputs invalid HTMLIf you want an HTML representation of an Explanation, you might call the toHtml() method.  However, the output of this method looks like the following:

<ul>
  <li>some value = some description</li>
  <ul>
    <li>some nested value = some description</li>
  </ul>
</ul>

As it is illegal in HTML to nest a UL directly inside a UL, this method will always output unparseable HTML if there are nested explanations.

What Lucene probably means to output is the following, which is valid HTML:

<ul>
  <li>some value = some description
    <ul>
      <li>some nested value = some description</li>
    </ul>
  </li>
</ul>
"
0,"Replace commons-logging with jcl-over-slf4j in jackrabbit-webdavIn JCR-1631 we moved the exclusion rule against the transitive commons-logging dependency from commons-httpclient to higher level components like jackrabbit-jca and jackrabbit-webapp.

However, I'm having some trouble with commons-logging showing up in other downstream projects that depend directly on jackrabbit-webapp or jackrabbit-jcr-server. Thus I'd like to revert the JCR-1631 solution and push the exclusion rule back to jackrabbit-webapp and replace the commons-logging dependency with jcl-over-slf4j. Downstream projects that already use commons-logging can still exclude the jcl-over-slf4j dependency."
0,"Logging documentation refers to nonexisting level ERROR in java.util.logging sectionPage ""Logging Practices""  Section ""java.util.logging Examples"" 
(http://hc.apache.org/httpcomponents-client/logging.html) 
says:
""org.apache.http.level = FINEST
org.apache.http.wire.level = ERROR""

However, there is no such thing as java.util.logging.Level.ERROR

Did you mean SEVERE as in
Logger.getLogger(""org.apache.http.wire"").setLevel(Level.SEVERE);

Thanks"
0,"ManageableCollectionUtil should not throw ""unsupported"" JcrMapping exceptionMany times, the object model'd code cannot be altered for ocm.

To avoid the ""unsupported"" exception in almost all such cases, use a delegating wrapper class to encapsulate a Collection.    The wrapper class implements MaangeableCollection.

Since delegation is a performance hit, make the test below the last resort for *object*  conversion in the method:
public static ManageableCollection getManageableCollection(Object object) 

Proposed ""catchall"" test and program action:

            if (object instanceof Collection) {
                return new ManageableCollectionImpl((Collection)object);
            }

"
0,HttpMethodBase Javadoc patchesClarify that HTTP 1.1 is the default. Attaching.
0,"Create OSGi Bundle Manifest HeadersTo be able to easily uses libraries from Jackrabbit inside an OSGi framework, for example in Apache Sling, it would be very helpfull if some of the Jackrabbit libraries include OSGi Bundle Manifest headers. It will of course not be possible to define such manifest header definition for all libraries, but jackrabbit-api, jackrabbit-jcr-commons and jackrabbit-jcr-rmi are certainly good candidates."
0,VFS backed file systemFile System implementation backed by commons VFS. 
0,"tweak AppendingCodec to write segments_N compatible with 'normal' LuceneJust an easy improvement from LUCENE-3490:

Currently AppendingCodec writes a different segments_N format (it writes no checksum at all in commit())
If you don't configure your codecprovider correctly in IndexReader, you will get read past EOF.
(we have some proposed fixes for this stuff in LUCENE-3490 branch)

But besides this, all it really needs to do is no-op prepareCommit(), it can still write the 'final' checksum
which is a good thing."
1,"[Lock] weird number for ""infinite""(this is a follow-up of JCR-3205)

i am surprised by the davex reply to a lock request with infinite timeout (before and after the fix from JCR-3205):


<D:timeout>Second-2147483</D:timeout>

this number is
2^21+50331

which seems pretty random to me. coincidally, this number is exactly 2^31 - 1 (2147483647) without the last 3 digits. can it be that there are some weird string operations happening on server side?
"
0,"TermsFilter: reuse TermDocsTermsFilter currently calls termDocs(Term) once per term in the TermsFilter.  If we sort the terms it's filtering on, this can be optimised to call termDocs() once and then skip(Term) once per term, which should significantly speed up this filter.
"
1,"FastVectorHighlighter SimpleBoundaryScanner does not work well when highlighting at the beginning of the text The SimpleBoundaryScanner still breaks text not based on characters provided when highlighting text that end up scanning to the beginning of the text to highlight. In this case, just use the start of the text as the offset."
0,"importXML still depends on XercesPrzemo Pakulski commented on JCR-367:
> Jackrabbit-core is still dependent on Xerces directly during runtime, SessionImpl.importWorkspace,
> Workspacempl.importWorkspace methods contains folliwng lines :
>
>             XMLReader parser =
>                     XMLReaderFactory.createXMLReader(""org.apache.xerces.parsers.SAXParser"");
>
> It works in maven1 probably because maven1 itself needs xerces to run test goal.
>
> I suggest reopening the issue.

Creating a new issue since JCR-367 is already closed after the 1.1 release."
1,"MultiPhraseQuery has incorrect hashCode() implementation - Leads to Solr Cache missesI found this while hunting for the cause of Solr Cache misses.

The MultiPhraseQuery class hashCode() implementation is non-deterministic. It uses termArrays.hashCode() in the computation. The contents of that ArrayList are actually arrays themselves, which return there reference ID as a hashCode instead of returning a hashCode which is based on the contents of the array. I would suggest an implementation involving the Arrays.hashCode() method.

I will try to submit a patch soon, off for today."
0,"Problematic exception handling in Jackrabbit WebAppIn this project, the cause of the exception is often ignored, and only the message of the cause is used, as in:

} catch (Exception e) {
    log.error(""Error in configuration: {}"", e.toString());
    throw new ServletException(""Error in configuration: "" + e.toString());
}

An additional problem is that when using ServletException(String message, Throwable rootCause), the rootCause is not used in printStackTrace(), that means the cause is not logged. See also: http://closingbraces.net/2007/11/27/servletexceptionrootcause/

It is therefore better to convert 
  throw new ServletException(""Unable to create RMI repository. jcr-rmi.jar might be missing."", e);
to
  ServletException s = new ServletException(""Unable to create RMI repository. jcr-rmi.jar might be missing."");
  s.initCause(e);
  throw s;




"
1,StackOverflowError if too many versions of a node are createdIn org.apache.jackrabbit.core.version.VersionIteratorImpl addVersion() is called recursively which can cause StackOverflowErrors if there are too many versions.
0,"Proxy authentication does not handle multiple multiple authentication schemesMy proxy server returns the following header lines in the response:

    Proxy-Authenticate: NTLM
    Proxy-Authenticate: Basic realm=""10.105.20.201""

i.e., it returns two Proxy-Authenticate header lines. Unfortunately this does 
not work. In line 253 of class Authenticator (method: authenticate(HttpMethod, 
HttpState, Header, String)) I see this comment:

    // FIXME: Note that this won't work if there is more than one realm within 
the challenge

so it looks like this is something that isn't yet implemented. In the log, I 
can see that the Authenticator attempts to parse the realm, but it looks like
this is not being done correctly:

   411 DEBUG Attempting to authenticate challenge: Proxy-Authenticate: NTLM, 
Basic realm=""10.105.20.201""

   411 DEBUG Parsed realm ""ealm=""10.105.20.201"" from challenge ""NTLM, Basic 
realm=""10.105.20.201"""".
   421 WARN  Exception thrown authenticating
java.lang.UnsupportedOperationException: Authentication type ""NTLM,"" is not 
recognized.
    at org.apache.commons.httpclient.Authenticator.authenticate
(Authenticator.java:274)
    at org.apache.commons.httpclient.Authenticator.authenticateProxy
(Authenticator.java:178)
    at 
org.apache.commons.httpclient.HttpMethodBase.processAuthenticationResponse
(HttpMethodBase.java:580)
    at org.apache.commons.httpclient.HttpMethodBase.execute
(HttpMethodBase.java:668)
    at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:355)
    at com.cmg.httptest.Main.main(Main.java:34)

It looks wrong to me that the realm name seems to be parsed as: 
ealm=""10.105.20.201

I understand that Authenticator does not know what NTLM is but I would like it 
to use Basic authentication in this case.

If there are more authentication methods possible, how can I specify which one 
I want to use?

Jesper de Jong"
0,"optional normsFor applications with many indexed fields, the norms cause memory problems both during indexing and querying.
This patch makes norms optional on a per-field basis, in the same way that term vectors are optional per-field.

Overview of changes:
 - Field.omitNorms that defaults to false
 - backward compatible lucene file format change: FieldInfos.FieldBits has a bit for omitNorms
 - IndexReader.hasNorms() method
 - During merging, if any segment includes norms, then norms are included.
 - methods to get norms return the equivalent 1.0f array for backward compatibility

The patch was designed for backward compatibility:
 - all current unit tests pass w/o any modifications required
 - compatible with old indexes since the default is omitNorms=false
 - compatible with older/custom subclasses of IndexReader since a default hasNorms() is provided
 - compatible with older/custom users of IndexReader such as Weight/Scorer/explain since a norm array is produced on demand, even if norms were not stored

If this patch is accepted (or if the direction is acceptable), performance for scoring  could be improved by assuming 1.0f when hasNorms(field)==false.
"
0,"Site search powered by Lucene/SolrFor a number of years now, the Lucene community has been criticized for not eating our own ""dog food"" when it comes to search. My company has built and hosts a site search (http://www.lucidimagination.com/search) that is powered by Apache Solr and Lucene and we'd like to donate it's use to the Lucene community. Additionally, it allows one to search all of the Lucene content from a single place, including web, wiki, JIRA and mail archives. See also http://www.lucidimagination.com/search/document/bf22a570bf9385c7/search_on_lucene_apache_org

You can see it live on Mahout, Tika and Solr

Lucid has a fault tolerant setup with replication and fail over as well as monitoring services in place. We are committed to maintaining and expanding the search capabilities on the site.

The following patch adds a skin to the Forrest site that enables the Lucene site to search Lucene only content using Lucene/Solr. When a search is submitted, it automatically selects the Lucene facet such that only Lucene content is searched. From there, users can then narrow/broaden their search criteria.


I plan on committing in a 3 or 4 days."
0,"bad assumptions/error handling in SetValueVersionExceptionTest:SetValueVersionExceptionTest makes several assumptions that may not be true in all repositories:

- nodes can be created without specifying a node type
- nodes are not referenceable by default (thus addMixin fails)

Also, if a repository does not allow creating a reference property, the associated test should be aborted with NotExecutableException.
"
0,"add getFinalOffset() to TokenStreamIf you add multiple Fieldable instances for the same field name to a document, and you then index those fields with TermVectors storing offsets, it's very likely the offsets for all but the first field instance will be wrong.

This is because IndexWriter under the hood adds a cumulative base to the offsets of each field instance, where that base is 1 + the endOffset of the last token it saw when analyzing that field.

But this logic is overly simplistic.  For example, if the WhitespaceAnalyzer is being used, and the text being analyzed ended in 3 whitespace characters, then that information is lost and then next field's offsets are then all 3 too small.  Similarly, if a StopFilter appears in the chain, and the last N tokens were stop words, then the base will be 1 + the endOffset of the last non-stopword token.

To fix this, I'd like to add a new getFinalOffset() to TokenStream.  I'm thinking by default it returns -1, which means ""I don't know so you figure it out"", meaning we fallback to the faulty logic we have today.

This has come up several times on the user's list."
0,ConsistencyCheck uses too much memoryA consistency check loads all lucene documents into memory. On a large repository this may lead to an OutOfMemoryError. The consistency check should rather read the lucene documents on demand and discard them afterwards.
0,"Demo HTML parser doesn't work for international documentsJavacc assumes ASCII so it won't work with, say, japanese documents. Ideally it would read the charset from the HTML markup, but that can by tricky. For now assuming unicode would do the trick:

Add the following line marked with a + to HTMLParser.jj:

options {
  STATIC = false;
  OPTIMIZE_TOKEN_MANAGER = true;
  //DEBUG_LOOKAHEAD = true;
  //DEBUG_TOKEN_MANAGER = true;
+  UNICODE_INPUT = true;
}
"
0,"Provide general HTTP date parsingAdd generally accessible support for parsing HTTP dates as used in headers/cookies.

Initially submitted to HttpClient dev by Chris Brown."
0,"improve efficiency of snowballfiltersnowball stemming currently creates 2 new strings and 1 new stringbuilder for every word.

all of this is unnecessary, so don't do it.
"
1,o.a.j.spi.commons.nodetype.NodeTypeDefinitionFactory does not set required typeNodeTypeDefinitionFactory does not set required type for property definitions.
0,"Add numDeletedDocs to IndexReaderAdd numDeletedDocs to IndexReader. Basically, the implementation is as simple as doing:
public int numDeletedDocs() {
  return deletedDocs == null ? 0 : deletedDocs.count();
}
in SegmentReader.
Patch to follow to include in all IndexReader extensions."
0,"Add utitily class to manage NRT reopeningI created a simple class, NRTManager, that tries to abstract away some
of the reopen logic when using NRT readers.

You give it your IW, tell it min and max nanoseconds staleness you can
tolerate, and it privately runs a reopen thread to periodically reopen
the searcher.

It subsumes the SearcherManager from LIA2.  Besides running the reopen
thread, it also adds the notion of a ""generation"" containing changes
you've made.  So eg it has addDocument, returning a long.  You can
then take that long value and pass it back to the getSearcher method
and getSearcher will return a searcher that reflects the changes made
in that generation.

This gives your app the freedom to force ""immediate"" consistency (ie
wait for the reopen) only for those searches that require it, like a
verifier that adds a doc and then immediately searches for it, but
also use ""eventual consistency"" for other searches.

I want to also add support for the new ""applyDeletions"" option when
pulling an NRT reader.

Also, this is very new and I'm sure buggy -- the concurrency is either
wrong over overly-locking.  But it's a start...
"
1,"Thai token type() bugWhile adding tests for offsets & type to ThaiAnalyzer, i discovered it does not type Thai numeric digits correctly.
ThaiAnalyzer uses StandardTokenizer, and this is really an issue with the grammar, which adds the entire [:Thai:] block to ALPHANUM.

i propose that alphanum be described a little bit differently in the grammar.
Instead, [:letter:] should be allowed to have diacritics/signs/combining marks attached to it.

this would allow the [:thai:] hack to be completely removed, would allow StandardTokenizer to parse complex writing systems such as Indian languages, and would fix LUCENE-1545.
"
0,"deprecate IndexWriter.addIndexes(Directory[])Since addIndexesNoOptimize accomplishes the same thing, more efficiently, and you can always then call optimize() if you really wanted to, I think we should deprecate the older addIndexes(Directory[])."
0,"Token implementation needs improvementsThis was discussed in the thread (not sure which place is best to reference so here are two):
http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200805.mbox/%3C21F67CC2-EBB4-48A0-894E-FBA4AECC0D50@gmail.com%3E
or to see it all at once:
http://www.gossamer-threads.com/lists/lucene/java-dev/62851

Issues:
1. JavaDoc is insufficient, leading one to read the code to figure out how to use the class.
2. Deprecations are incomplete. The constructors that take String as an argument and the methods that take and/or return String should *all* be deprecated.
3. The allocation policy is too aggressive. With large tokens the resulting buffer can be over-allocated. A less aggressive algorithm would be better. In the thread, the Python example is good as it is computationally simple.
4. The parts of the code that currently use Token's deprecated methods can be upgraded now rather than waiting for 3.0. As it stands, filter chains that alternate between char[] and String are sub-optimal. Currently, it is used in core by Query classes. The rest are in contrib, mostly in analyzers.
5. Some internal optimizations can be done with regard to char[] allocation.
6. TokenStream has next() and next(Token), next() should be deprecated, so that reuse is maximized and descendant classes should be rewritten to over-ride next(Token)
7. Tokens are often stored as a String in a Term. It would be good to add constructors that took a Token. This would simplify the use of the two together.
"
0,"TCK: XPathQueryLevel2Test uses optional column specifier syntaxTest assumes the implementation uses a terminal attribute step as the column specifier.  This is allowed, but not required, by JCR.

Proposal: remove column specifier and process results using getNodes instead of getRows.

--- XPathQueryLevel2Test.java   (revision 422074)
+++ XPathQueryLevel2Test.java   (working copy)
@@ -85,7 +85,7 @@
         checkResult(result, 1);
  
         // evaluate result
-        checkValue(result.getRows(), propertyName1, ""b"");
+        checkValue(result.getNodes(), propertyName1, ""b"");
     }
  
     /**
@@ -101,7 +101,7 @@
         checkResult(result, 1);
  
         // evaluate result
-        checkValue(result.getRows(), propertyName1, ""existence"");
+        checkValue(result.getNodes(), propertyName1, ""existence"");
     }
  
     /**
@@ -147,7 +147,6 @@
         tmp.append(jcrRoot).append(testRoot);
         tmp.append(""/*[@"").append(propertyName2).append("" = 'two'"");
         tmp.append("" and @"").append(propertyName1).append("" = 'existence']"");
-        tmp.append(""/@"").append(propertyName1);
         return new Statement(tmp.toString(), Query.XPATH);
     }
  
@@ -161,7 +160,7 @@
         tmp.append(propertyName1);
         tmp.append("" <= 'b' and @"");
         tmp.append(propertyName1);
-        tmp.append("" > 'a']/@"").append(propertyName1);
+        tmp.append("" > 'a']"");
         return new Statement(tmp.toString(), Query.XPATH);
     }
 }

--- AbstractQueryLevel2Test.java        (revision 422074)
+++ AbstractQueryLevel2Test.java        (working copy)
@@ -19,6 +19,7 @@
 import org.apache.jackrabbit.test.NotExecutableException;
  
 import javax.jcr.nodetype.NodeType;
+import javax.jcr.NodeIterator;
 import javax.jcr.query.RowIterator;
 import javax.jcr.query.Row;
 import javax.jcr.Value;
@@ -115,4 +116,16 @@
                     expectedValue, value.getString());
         }
     }
+
+    protected void checkValue(NodeIterator itr,
+                              String propertyName,
+                              String expectedValue) throws RepositoryException {
+        while (itr.hasNext()) {
+            Node node = itr.nextNode();
+            // check fullText
+            Value value = node.getProperty(propertyName).getValue();
+            assertEquals(""Value in query result row does not match expected value"",
+                    expectedValue, value.getString());
+        }
+    }
 }
"
0,"Add fake charfilter to BaseTokenStreamTestCase to find offsets bugsRecently lots of issues have been fixed about broken offsets, but it would be nice to improve the
test coverage and test that they work across the board (especially with charfilters).

in BaseTokenStreamTestCase.checkRandomData, we can sometimes pass the analyzer a reader wrapped
in a ""MockCharFilter"" (the one in the patch sometimes doubles characters). If the analyzer does
not call correctOffsets or does incorrect ""offset math"" (LUCENE-3642, etc) then eventually
this will create offsets and the test will fail.

Other than tests bugs, this found 2 real bugs: ICUTokenizer did not call correctOffset() in its end(),
and ThaiWordFilter did incorrect offset math."
0,"Large fetch sizes have potentially deleterious effects on VM memory requirements when using OracleSince Release 10g, Oracle JDBC drivers use the fetch size to allocate buffers for caching row data.
cf. http://www.oracle.com/technetwork/database/enterprise-edition/memory.pdf

r1060431 hard-codes the fetch size for all ResultSet-returning statements to 10,000. This value has significant, potentially deleterious, effects on the heap space required for even moderately-sized repositories. For example, the BUNDLE table (from 'oracle.ddl') has two columns -- NODE_ID raw(16) and BUNDLE_DATA blob -- which require 16 b and 4 kb of buffer space, respectively. This requires a buffer of more than 40 mb [(16+4096) * 10000 = 41120000].

If the issue described in JCR-2832 is truly specific to PostgreSQL, I think its resolution should be moved to a PostgreSQL-specific ConnectionHelper subclass. Failing that, there should be a way to override this hard-coded value in OracleConnectionHelper."
1,"IndexWriter applies wrong deletes during concurrent flush-allYonik uncovered this with the TestRealTimeGet test: if a flush-all is
underway, it is possible for an incoming update to pick a DWPT that is
stale, ie, not yet pulled/marked for flushing, yet the DW has cutover
to a new deletes queue.  If this happens, and the deleted term was
also updated in one of the non-stale DWPTs, then the wrong document is
deleted and the test fails by detecting the wrong value.

There's a 2nd failure mode that I haven't figured out yet, whereby 2
docs are returned when searching by id (there should only ever be 1
doc since the test uses updateDocument which is atomic wrt
commit/reopen).

Yonik verified the test passes pre-DWPT, so my guess is (but I
have yet to verify) this test also passes on 3.x.  I'll backport
the test to 3.x to be sure.
"
1,"Destination header not containing URI scheme causes NPEIn WebDAVRequestImpl. getDestinationLocator assumes that URI.getAuthority is always non-null.

In RFC2518, a full URI is indeed required, but the NPE causes a status of 500, instead of 400 as expected.

In RFC4918, an absolute path is allowed.

Proposal: delegate to gethrefLocator, which already does the right thing.
"
0,Improved javadocs for PriorityQueue#lessThanIt kills me that I have to inspect the code every time I implement a PriorityQueue. :)
0,"Add tool to upgrade all segments of an index to last recent supported index format without optimizingCurrently if you want to upgrade an old index to the format of your current Lucene version, you have to optimize your index or use addIndexes(IndexReader...) [see LUCENE-2893] to copy to a new directory. The optimize() approach fails if your index is already optimized.

I propose to add a custom MergePolicy to upgrade all segments to the last format. This MergePolicy could simply also ignore all segments already up-to-date. All segments in prior formats would be merged to a new segment using another MergePolicy's optimize strategy.

This issue is different from LUCENE-2893, as it would only support upgrading indexes from previous Lucene versions in-place using the official path. Its a tool for the end user, not a developer tool.

This addition should also go to Lucene 3.x, as we need to make users with pre-3.0 indexes go the step through 3.x, else they would not be able to open their index with 4.0. With this tool in 3.x the users could safely upgrade their index without relying on optimize to work on already-optimized indexes."
1,"PredefinedNodeTypeTest..getNodeTypeSpec handling unknown super typesThis method tries to filter out custom super types, but produces a broken spec when *all* super types are custom (in which case it should emit ""[]"", but doesn't).
"
0,"Searchable.java: The info in the @deprecated tags do not refer to the search(Weight, etc...) versions...E.g.

The javadoc for 
          void search(Query query, Filter filter, HitCollector results)
states:
          Deprecated. use search(Query, Filter, HitCollector) instead.
instead of:
          Deprecated. use search(Weight, Filter, HitCollector) instead.
"
0,"QueryParser should use reusable token streamsJust like indexing, the query parser should use reusable token streams"
0,"Implement getDistance() on DirectSpellChecker.INTERNAL_LEVENSHTEINDirectSpellChecker.INTERNAL_LEVENSHTEIN is currently not a full-fledged implementation of StringDistance.  But an full implementation is needed for Solr's SpellCheckComponent.finishStage(), and also would be helpful for those trying to take the advice given in LIA 2nd ed section sect8.5.3."
0,"Refactor ObservationManagerFactoryThe current o.a.j.core.observation.ObservationManagerFactory class has two main responsibilities:

    1) Create new ObservationManagerImpl instances as an observation manager factory
    2) Manage event consumers and dispatch events within a workspace

These two responsibilities are quite unrelated and the factory responsibility essentially boils down to the following method that is only ever invoked within WorkspaceImpl.getObservationManager():

    public ObservationManagerImpl createObservationManager(SessionImpl session, ItemManager itemMgr) {
        return new ObservationManagerImpl(this, session, itemMgr);
    }

To simplify the design I'd inline this method and rename ObservationManagerFactory to ObservationDispatcher to better reflect the one remaining responsibility."
0,"change jdk & icu collation to use byte[]Now that term is byte[], we should switch collation to use byte[] instead of 'indexablebinarystring'.

This is faster and results in much smaller sort keys.

I figure we can work it out here, and fix termrangequery to use byte in parallel, but we can already test sorting etc now."
1,"InputStream.read return value is ignored.RepositoryImpl.loadRootNodeId reads from an InputStreamReader using read(...) and ignores the return values.

This results in a problem if the input stream doesn't read all bytes."
0,"Very inefficient implementation of MultiTermDocs.skipToIn our application anytime the index was unoptimized/contained more than one segment there was a sharp drop in performance, which amounted to over 50ms per search on average.  We would consistently see this drop anytime an index went from an optimized state to an unoptimized state.

I tracked down the issue to the implementation of MultiTermDocs.skipTo function (found in MultiReader.java).  Optimized indexes do not use this class during search but unoptimized indexes do.  The comment on this function even explicitly states 'As yet unoptimized implementation.'  It was implemented just by calling 'next' over and over so even if it knew it could skip ahead hundreds of thousands of hits it would not.

So I re-implemented the function very similar to how the MultiTermDocs.next function was implemented and tested it out on or application for correctness and performance and it passed all our tests and the performance penalty of having multiple segments vanished.  We have already put the new jar onto our production machines.

Here is my implementation of skipTo, which closely mirrors the accepted implementation of 'next', please feel free to test it and commit it.

  /** Much more optimized implementation. Could be
   * optimized fairly easily to skip entire segments */
  public boolean skipTo(int target) throws IOException {
    if (current != null && current.skipTo(target-base)) {
      return true;
    } else if (pointer < readers.length) {
      base = starts[pointer];
      current = termDocs(pointer++);
      return skipTo(target);
    } else
      return false;
  }"
0,"DataStore: gc.stopScan() should be optionalData Store garbage collection currently works like this:

gc.scan();
gc.stopScan();
gc.deleteUnused();

Currently, if stopScan() is not called, an exception is thrown. This is not user friendly. Instead, stopScan() should be optional, and should be allowed any time. It may be used to indicate garbage collection has finished:

try {
  gc.scan();
  ...
  gc.deleteUnused();
} finally {
  gc.stopScan();
}

Or when sharing the repository:

gc1.scan();
gc2.scan();
gc1.deleteUnused();
gc2.stopScan();
"
1,"OutOfMemoryError When repeat login and the logout many timesWhen repeat login and the logout many times, I encountered?OutOfMemoryError.

javax.jcr.RepositoryException: Cannot instantiate persistence manager org.apache.jackrabbit.core.state.db.DerbyPersistenceManager: Java exception: 'Java heap space: java.lang.OutOfMemoryError'.: Java exception: 'Java heap space: java.lang.OutOfMemoryError'.
	at org.apache.jackrabbit.core.RepositoryImpl.createPersistenceManager(RepositoryImpl.java:1095)
	at org.apache.jackrabbit.core.RepositoryImpl.createVersionManager(RepositoryImpl.java:300)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:245)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:498)
	at org.apache.jackrabbit.core.TransientRepository$2.getRepository(TransientRepository.java:245)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:265)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:333)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:388)
	at Test.tryLoginAndLogout(Test.java:19)
	at Test.test1(Test.java:13)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:164)
	at junit.framework.TestCase.runBare(TestCase.java:130)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:120)
	at junit.framework.TestSuite.runTest(TestSuite.java:230)
	at junit.framework.TestSuite.run(TestSuite.java:225)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: SQL Exception: Java exception: 'Java heap space: java.lang.OutOfMemoryError'.
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.javaException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.<init>(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement20.<init>(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement30.<init>(Unknown Source)
	at org.apache.derby.jdbc.Driver30.newEmbedPreparedStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.prepareStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.prepareStatement(Unknown Source)
	at org.apache.jackrabbit.core.state.db.DatabasePersistenceManager.init(DatabasePersistenceManager.java:224)
	at org.apache.jackrabbit.core.RepositoryImpl.createPersistenceManager(RepositoryImpl.java:1091)
	... 27 more
java.lang.OutOfMemoryError: Java heap space

"
0,"better payload testing with mockanalyzerMockAnalyzer currently always indexes some fixed-length payloads.

Instead it should decide for each field randomly (and remember it for that field):
* if the field should index no payloads at all
* field should index fixed length payloads
* field should index variable length payloads.
"
0,The DisjunctionMaxQuery lacks an implementation of extractTerms().The DisjunctionMaxQuery lacks an implementation of extractTerms(). 
1,"ArrayIndexOutofBoundException while setting a reference propertyI have a node whith a multivalued reference property.
I try to add a reference as follows (the spec is outdated at this point, so I'm not sure if I use the right approach to add a reference):

  ReferenceValue rv = new ReferenceValue(rn.getNode(""pages/mjo:page""));
  Value[] values = {rv};
  tstN.setProperty(""mjo:testCon"",values);
  session.save();

This results in a 

2004-12-09 16:11:43,614 WARN org.apache.jackrabbit.core.ItemManager - node at /pages/mjo:page has invalid definitionId (1512950840)
java.lang.ArrayIndexOutOfBoundsException: 0
	at org.apache.jackrabbit.core.ItemImpl.validateTransientItems(ItemImpl.java:626)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1148)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:633)
	at de.freaquac.test.JCRTest.main(JCRTest.java:174)

ArrayIndexOutOfBoundsException is never a good sign, so I assume it's a bug. I should say that crx throws the same exception if I try it there.
"
0,"Use hash codes instead of sequence numbers for string indexesWe use index numbers instead of namespace URIs or other strings in many places. The two-way mapping between namespace URIs and index numbers is by default stored in the repository-global ns_idx.properties file, and the index numbers are allocated using a linear sequence. The problem with this approach is that two repositories will easily end up with different string index mappings, which makes it practically impossible to make low-level copies of workspace content across repositories.

The ultimate solution for this problem would be to store the namespace URIs closer to the stored content, ideally as an implementation detail of a persistence manager.

An easier short-term solution would be to decrease the chances of two repositories having different string index mappings. A simple (and backwards-compatible) way to do this is to use the hash code of a namespace URI as the basis of allocating a new index number. Hash collisions are fairly unlikely, and can be handled by incrementing the intial hash code until the collision is avoided. In the common case of no collisions (with a uniform hash function the chance of a collision is less than 1% even with tousands of registered namespaces) this solution allows workspaces to be copied between repositories without worrying about the namespace index mappings."
0,"Upload Lucene 2.0 artifacts in the Maven 1 repositoryThe Lucene 2.0 artifacts can be found in the Maven 2 repository, but not in the Maven 1 repository. There are still projects using Maven 1 who might be interested in upgrading to Lucene 2, so having the artifacts also in the Maven 1 repository would be very helpful."
0,Improve aggregate node indexing codeCurrently the aggregate nodes indexing code uses a sub-optimal way of copying and sorting the aggregated fields.
0,"move log4j initialization out of RepositoryStartupServletthe RepositoryStartupServlet initializes/configures the Log4J environment. this might not be desirable since other applications might already done so.

"
1,"KeywordTokenizer/Analyzer cannot be re-used
The new reusableTokenStream API in KeywordAnalyzer fails to reset the tokenizer when it re-uses it.

This issue came from this thread:

    http://www.gossamer-threads.com/lists/lucene/java-dev/55929

Thanks to Hideaki Takahashi for finding this!"
1,"NodeIdImpl is not really serializable I've been trying to get jcr2spi - rmi - spi2jcr to work. 

The error I'm seeing is reported as:
java.io.NotSerializableException: org.apache.jackrabbit.spi.commons.identifier.IdFactoryImpl

I believe I tracked this down.  It is because NodeIdImpl is implicitly referencing its containing instance IdFactoryImpl which is not serializable.

NodeIdImpl is attempted to be serialized, in my case, with the following stack:

at org.apache.jackrabbit.spi.rmi.client.ClientRepositoryService.getItemInfos(ClientRepositoryService.java:258)
at org.apache.jackrabbit.jcr2spi.state.WorkspaceItemStateFactory.createNodeState(WorkspaceItemStateFactory.java:94)
at org.apache.jackrabbit.jcr2spi.state.TransientISFactory.createNodeState(TransientISFactory.java:99)
at org.apache.jackrabbit.jcr2spi.hierarchy.NodeEntryImpl.doResolve(NodeEntryImpl.java:972)
at org.apache.jackrabbit.jcr2spi.hierarchy.HierarchyEntryImpl.resolve(HierarchyEntryImpl.java:95)
at org.apache.jackrabbit.jcr2spi.hierarchy.HierarchyEntryImpl.getItemState(HierarchyEntryImpl.java:212)
at org.apache.jackrabbit.jcr2spi.ItemManagerImpl.getItem(ItemManagerImpl.java:170)
at org.apache.jackrabbit.jcr2spi.SessionImpl.getRootNode(SessionImpl.java:216)

I think I must be doing something wrong, because it seems like this is a fundamental problem with doing jcr2spi - rmi - spi2jcr, and looking at the SVN history I don't see how this ever could have worked.  
So either session.getRootNode() has never been tested using jcr2spi - rmi - spi2jcr, or I've got something setup wrong."
1,"Duplicate request headers when connect through proxiesWhen negotiating proxy servers or during write-failure retries, the httpClient 
adds duplicate request headers to each retry.  The result is that each header 
is duplicated multiple times (number of retries).  This only impact the headers 
that allow multiple values (the others were prevented byt the code).  In  
Particular, it affects ""cookie"" header.  It happens more often when going 
through proxy server with tunnelling connections (https), but also happens on 
http on NTLM proxy server (need multiple round trips to get authenticated).

Steps to Reproduce:
Setup a client application to go to a website that requires going through a 
proxy server that supports tunnelling for https connections and authenticate 
users (Basic and/or NTLM).  The website also need to support keep-alive and 
support https. Initilize the HttpState with a cookie. Turn httpClient 
logging ""wire, debug and trace"" logging on.  Set the proxy credential with 
valid user id and password.

Then run the application against any url on the website.

Test Results and fixes:
1. When connect to https through a (Netscape/Basic auth) proxy that does 
the ""tunnelling"", the initial ""CONNECT"" adds the cookie header once.  The proxy 
returns the 407.  Then the code will use the proxy credential to do 
the ""CONNECT"" again.  After successful connection (200), the code will do the 
proper ""POST"".  The httpClient code adds the same cookie one more time in here.

This case was caused by the wrapper class ConnectMethod using the wrapped 
method ""addRequestHeaders"" to build headers for the ""CONENCT"".  It can be fixed 
by having the ConnectMethod only adds headers it needs (""addHostRequestHeader"" 
and ""addProxyAuthorizationRequestHeader"").

2. When connect to http through proxy.  The client first sends a ""POST"" without 
proxy credentials, and gets a 407 back.  The cookie header is added before that 
happens.  Then (loop in the HttpMethodBase.execute) the client will retry 
the ""POST"" with the credentials (the logic require to have a response header to 
submit credentials).  In the retry, the cookie is added again 
(addRequestHeaders is called inside the writeRequest).

3. When using kee-alive with no-proxy on http, if the connection times out, the 
next call of the method will get a socket error on write.  The retry loop in 
the processRequest method will retry the request again.  It will add the cookie 
again.  To deal with both case 2 and 3, the addRequestHeadres call need to be 
moved up to the beginning of the HttpMethodBase.execute.  We tested this 
approach and it worked for us.

4. When negotiating NTLM proxy for https, multiple round trips are needed to 
get user authenticated.  The same ConnectMethod instance is used.  So the 
adding of the proxy authenticate headers need to be done with the ConnectMethod 
instance (vs. the wrapped method for Host header).  Otherwise, the 
Authenticator class would not be able to find the information for 
authenticating user.


I will send in our suggested fixes later.

Build: This is based on 0307 nightly build.  We run into some other problems 
when trying the 0410 nightly build."
0,"improve BaseTokenStreamTestCase to test end()If offsetAtt/end() is not implemented correctly, then there can be problems with highlighting: see LUCENE-2207 for an example with CJKTokenizer.

In my opinion you currently have to write too much code to test this.

This patch does the following:
* adds optional Integer finalOffset (can be null for no checking) to assertTokenStreamContents
* in assertAnalyzesTo, automatically fill this with the String length()

In my opinion this is correct, for assertTokenStreamContents the behavior should be optional, it may not even have a Tokenizer. If you are using assertTokenStreamContents with a Tokenizer then simply provide the extra expected value to check it.

for assertAnalyzesTo then it is implied there is a tokenizer so it should be checked.

the tests pass for core but there are failures in contrib even besides CJKTokenizer (apply Koji's patch from LUCENE-2207, it is correct). Specifically ChineseTokenizer has a similar problem.
"
0,Refrase javadoc 1st sentence for IndexReader.deleteDocuments
1,"NullPointerException may be thrown when trying to enumerate observation event listenersWhen calling the ObservationManager.getRegisteredEventListeners() a NullPointerException may be thrown if no event listener has been registered (yet). The reason for this is, that in the ClientObservationManager.getRegisteredEventListener method an internal field is access directly, which is created on demand and thus may be null."
0,Remove unnecessary memory barriers in DWPTCurrently DWPT still uses AtomicLong to count the bytesUsed. Each write access issues an implicite memory barrier which is totally unnecessary since we doing everything single threaded on that level. This might be very minor but we shouldn't issue unnecessary memory barriers causing processors to lock their instruction pipeline for no reason.
0,"relax the per-segment max unique term limitLucene can't handle more than 2.1B (limit of signed 32 bit int) unique terms in a single segment.

But I think we can improve this to termIndexInterval (default 128) * 2.1B.  There is one place (internal API only) where Lucene uses an int but should use a long."
0,use AtomicInteger/Boolean to track IR.refCount and IW.closedLess costly than synchronized methods we have now...
0,Add link to irc channel #lucene on the websiteWe should add a link to #lucene IRC channel on chat.freenode.org. 
1,"DefaultSimilarity.queryNorm() should never return InfinityCurrently DefaultSimilarity.queryNorm() returns Infinity if sumOfSquaredWeights=0.
This can result in a score of NaN (e. g. in TermScorer) if boost=0.0f.

A simple fix would be to return 1.0f in case zero is passed in.

See LUCENE-698 for discussions about this."
1,"dead lock while locking or unlocking nodesJackRabbit is still hanging on the Node.lock() or Node.unlock() function.

... everything fine until here...
s13: 4
s13: 5
s13: 6
s13: 7   -> unlock()
s14: started.
s14: 1   -> session.getRootNode()
s15: started.
s15: 1
s16: started.

I just find this failure during the first run (emtpy repository home directory). 2nd and 3th run are fine after killing the vm from first run, but with already initialized repository directory these time.

1. rm -rf repository.home
2. run -> hang
3. kill
4. run -> ok
5. run -> ok
"
0,"org.apache.lucene.ant.HtmlDocument added Tidy config file passthrough availabilityParsing HTML documents using the org.apache.lucene.ant.HtmlDocument.Document method resulted in many error messages such as this:

    line 152 column 725 - Error: <as-html> is not recognized!
    This document has errors that must be fixed before
    using HTML Tidy to generate a tidied up version.

The solution is to configure Tidy to accept these abnormal tags by adding the tag name to the ""new-inline-tags"" option in the Tidy config file (or the command line which does not make sense in this context), like so:

    new-inline-tags: as-html

Tidy needs to know where the configuration file is, so a new constructor and Document method can be added.  Here is the code:

{code}
    /**                                                                                                                                                                                            
     *  Constructs an <code>HtmlDocument</code> from a {@link                                                                                                                                      
     *  java.io.File}.                                                                                                                                                                             
     *                                                                                                                                                                                             
     *@param  file             the <code>File</code> containing the                                                                                                                                
     *      HTML to parse                                                                                                                                                                          
     *@param  tidyConfigFile   the <code>String</code> containing                                                                                                                                  
     *      the full path to the Tidy config file                                                                                                                                                  
     *@exception  IOException  if an I/O exception occurs                                                                                                                                          
     */
    public HtmlDocument(File file, String tidyConfigFile) throws IOException {
        Tidy tidy = new Tidy();
        tidy.setConfigurationFromFile(tidyConfigFile);
        tidy.setQuiet(true);
        tidy.setShowWarnings(false);
        org.w3c.dom.Document root =
                tidy.parseDOM(new FileInputStream(file), null);
        rawDoc = root.getDocumentElement();
    }

    /**                                                                                                                                                                                            
     *  Creates a Lucene <code>Document</code> from a {@link                                                                                                                                       
     *  java.io.File}.                                                                                                                                                                             
     *                                                                                                                                                                                             
     *@param  file                                                                                                                                                                                 
     *@param  tidyConfigFile the full path to the Tidy config file                                                                                                                                 
     *@exception  IOException                                                                                                                                                                      
     */
    public static org.apache.lucene.document.Document
        Document(File file, String tidyConfigFile) throws IOException {

        HtmlDocument htmlDoc = new HtmlDocument(file, tidyConfigFile);

        org.apache.lucene.document.Document luceneDoc = new org.apache.lucene.document.Document();

        luceneDoc.add(new Field(""title"", htmlDoc.getTitle(), Field.Store.YES, Field.Index.ANALYZED));
        luceneDoc.add(new Field(""contents"", htmlDoc.getBody(), Field.Store.YES, Field.Index.ANALYZED));

        String contents = null;
        BufferedReader br =
            new BufferedReader(new FileReader(file));
        StringWriter sw = new StringWriter();
        String line = br.readLine();
        while (line != null) {
            sw.write(line);
            line = br.readLine();
        }
        br.close();
        contents = sw.toString();
        sw.close();

        luceneDoc.add(new Field(""rawcontents"", contents, Field.Store.YES, Field.Index.NO));

        return luceneDoc;
    }
{code}

I am using this now and it is working fine.  The configuration file is being passed to Tidy and now I am able to index thousands of HTML pages with no more Tidy tag errors.

"
0,"Make Authorizable.setProperty more noisy in case of failuressetProperty fails with an unspecific warning, when there's an exception, but it doesn' print any useful information from this exception."
0,"IntelliJ IDEA and Eclipse setupSetting up Lucene/Solr in IntelliJ IDEA or Eclipse can be time-consuming.

The attached patches add a new top level directory {{dev-tools/}} with sub-dirs {{idea/}} and {{eclipse/}} containing basic setup files for trunk, as well as top-level ant targets named ""idea"" and ""eclipse"" that copy these files into the proper locations.  This arrangement avoids the messiness attendant to in-place project configuration files directly checked into source control.

The IDEA configuration includes modules for Lucene and Solr, each Lucene and Solr contrib, and each analysis module.  A JUnit run configuration per module is included.

The Eclipse configuration includes a source entry for each source/test/resource location and classpath setup: a library entry for each jar.

For IDEA, once {{ant idea}} has been run, the only configuration that must be performed manually is configuring the project-level JDK.  For Eclipse, once {{ant eclipse}} has been run, the user has to refresh the project (right-click on the project and choose Refresh).

If these patches is committed, Subversion svn:ignore properties should be added/modified to ignore the destination IDEA and Eclipse configuration locations.

Iam Jambour has written up on the Lucene wiki a detailed set of instructions for applying the 3.X branch patch for IDEA: http://wiki.apache.org/lucene-java/HowtoConfigureIntelliJ"
0,"2.1 Locking documentation in ""Apache Lucene - Index File Formats"" section ""6.2 Lock File"" out datedI am in the process to migrate from Lucene 2.0 to Lucene 2.1.

From reading the Changes document I understand that the write locks are now written into the index folder instead of the java.io.tmpdir. 

In the ""Apache Lucene - Index File Formats"" document in section ""6.2 Lock File"" I read that there is a write lock used to indicate that another process is writing into the index and that this file is stored in the java.io.tempdir.

This is confusing to me.  I had the impression all lock files go into the index folder now.  And using the the java.io.tempdir is only local and does not support access to shared index folders.

Do I miss something here or is the documentation not updated?
"
0,"SecureProtocolFactoryWrapper class for using the socket factory created by Java Web StartAs smartcards and SSL are becoming more and more prevelant, Java Web Start has started to become better equiped to handle these situations.  When running an app within webstart, it can access the browser's keystore, which (at least in our case) accessed the users smartcard to make the SSL connection.

I wanted to start using HttpClient, but needed a way to do so while still mainaining access to the browser's keystore.

My initial tests show that getting the default socket factory from the java.net.HttpURLConnection and wrapping it in a class that implements org.apache.commons.httpclient.protocol.SecureProtocolSocketFactory is sufficient."
0,"Rename GenericRepositoryFactory to JndiRepositoryFactoryThe GenericRepositoryFactory class introduced in JCR-2360 has since been refactored so that most of its functionality is now distributed among the more implementation-specific RepositoryFactory classes. Now the GenericRepositoryFactory only contains support for looking the repository up in JNDI, so it would be better to rename the class to JndiRepositoryFactory.

The only troublesome part of the rename is the GenericRepositoryFactory.URI constant that was for a while documented on our wiki as a part of the canonical code snippet for accessing a remote repository based on the repository URI. The latest recommendation is to use the JcrUtils.getRepository(String uri) method so the constant is no longer needed in client code, but for backwards compatibility with earlier Jackrabbit 2.0 betas it may be good to keep the deprecated constant for at least the next beta release."
0,"Index nodes in parallelCPUs with multiple cores are now standard and Jackrabbit should make use of it where it makes sense. Analyzing content while a node is indexed is quite costly, but can be broken easily into task that can be executed in parallel. E.g. index multiple nodes in parallel."
0,Inner classes of FilterAtomicReader (trunk) / FilterIndexReader (3.x) do not override all methods to be filteredThis issue adds missing checks in the FilterReader test to also check overridden methods in the enum implementations (inner classes) similar to the checks added by Shai Erea.
1,"Node.getWeakReferences throws UnsupportedOperationException if not referenceable.... while Node.getReferences() doesn't and returns an empty propertyiterator.

From my point of view both methods should behave the same way and i prefer not throwing.

"
0,"When resolving deletes, IW should resolve in term sort orderSee java-dev thread ""IndexWriter.updateDocument performance improvement""."
0,lucene jars should include LiCENSE and NOTICEThe Lucene jars created by the build should include the LICENSE and NOTICE files in META-INF.
0,"New node type namespaces should be automatically registeredA user currently needs to explicitly register any new namespaces used in new node types before registering the node types. See for example the problem on the mailing list:

    http://mail-archives.apache.org/mod_mbox/incubator-jackrabbit-dev/200603.mbox/%3c1142091097.13136.0.camel@localhost.localdomain%3e

The node type registration should be changed so that new namespaces are automatically registered."
0,"javadocs very very ugly if you generate with java7Java7 changes its javadocs to look much nicer, but this involves different CSS styles.

Lucene overrides the CSS with stylesheet+prettify.css which is a combination of java5/6 stylesheet + google prettify:
but there are problems because java7 has totally different styles.

So if you generate javadocs with java7, its like you have no stylesheet at all.

A solution might be to make stylesheet7+prettify.css and conditionalize a property in ant based on java version."
0,"Add IndexReader.flush(commitUserData)IndexWriter offers a commit(String commitUserData) method.
IndexReader can commit as well using the flush/close methods and so
needs an analogous method that accepts commitUserData."
1,"RegexQuery matches terms the input regex doesn't actually matchI was writing some unit tests for our own wrapper around the Lucene regex classes, and got tripped up by something interesting.

The regex ""cat."" will match ""cats"" but also anything with ""cat"" and 1+ following letters (e.g. ""cathy"", ""catcher"", ...)  It is as if there is an implicit .* always added to the end of the regex.

Here's a unit test for the behaviour I would expect myself:

    @Test
    public void testNecessity() throws Exception {
        File dir = new File(new File(System.getProperty(""java.io.tmpdir"")), ""index"");
        IndexWriter writer = new IndexWriter(dir, new StandardAnalyzer(), true);
        try {
            Document doc = new Document();
            doc.add(new Field(""field"", ""cat cats cathy"", Field.Store.YES, Field.Index.TOKENIZED));
            writer.addDocument(doc);
        } finally {
            writer.close();
        }

        IndexReader reader = IndexReader.open(dir);
        try {
            TermEnum terms = new RegexQuery(new Term(""field"", ""cat."")).getEnum(reader);
            assertEquals(""Wrong term"", ""cats"", terms.term());
            assertFalse(""Should have only been one term"", terms.next());
        } finally {
            reader.close();
        }
    }

This test fails on the term check with terms.term() equal to ""cathy"".

Our workaround is to mangle the query like this:

    String fixed = String.format(""(?:%s)$"", original);
"
0,"Provide a clean mechanism to attatch user define attributes to connectionsIt would be nice to have a way to attach user defined attributes to a connection.
Ideally it'd be nice if such support could be added to HttpClientConnection, but understandably this may not be possible due to back-compatibility issues.
So, we could have something like HttpConnectionContext perhaps (or similar) with:

HttpConnectionContext#setAttribute(String name, Object value)
Object HttpConnectionContext#getAttribute(String name)

This would be made available in the HttpContext of a request (like the connection is today):

HttpConnectionContext connectionContext = (HttpConnectionContext) httpContext.getAttribute(ExecutionContext.HTTP_CONNECTION_CONTEXT);

This would make a few things much cleaner to implement than they are today: The most obvious being my current use case of wanting connection isolated cookies.

Currently to achieve this goal we need to provide custom client connection + connection operator + connection manager implementations. Then there is no clean way to currently obtain the actual connection instance created by a custom operator in the HttpContext: As it's wrapped by the connection pool and #getWrappedConnection is protected - so we need to resort to reflection in interceptors.

Providing a clean mechanism for attaching user defined attributes to a connection instance as described above would make such implementations far far simpler.
"
0,Remove unused method RedoLog.clear()This method is not used anymore and can be removed.
0,Make ItemInfoBuilder name space awareCurrently there is no way to to have NodeInfoBuilder/PropertyInfoBuilder build NodeInfos/ItemInfos on a name space other than the default one. I suggest to add appropriate methods to NodeInfoBuilder/PropertyInfoBuilder to set the name space.
0,Webdav: Drop xerces dependencyjukka is the final assignee :) thanks for taking over.
1,"HttpState.setCookiePolicy() is completely ignoredThough this method is deprecated, it currently has no effect and gives no warning that it does nothing.  
A patch that fixes this problem is coming shortly.

Mike"
0,PrivilegeHandlerTest fails on WindowsThe test fails on Windows because there are differences in line breaks between expected and actual results.
1,"DocumentsWriter.abort fails to clear docStoreOffsetI hit this in working on LUCENE-1044.

If you disk full event during flush, then DocumentsWriter will abort
(clear all buffered docs).  Then, if you then add another doc or two,
and then close your writer, and this time succeed in flushing (say
because it's only a couple buffered docs so the resulting segment is
smaller), you can flush a corrupt segment (that incorrectly has a
non-zero docStoreOffset).

I modified the TestConcurrentMergeScheduler test to show this bug.
I'll attach a patch shortly.
"
0,"Extract the public API interfaces from o.a.j.core to o.a.j.apiTo better document and track the public JCR extensions and component API provided by Jackrabbit and to allow more room for refactoring within the Jackrabbit core, we shoud move (or create) the supported API interfaces to a new org.apache.jackrabbit.api package.

At least the following interfaces should be moved along with any supporting implementation-independent classes:

    * PersistenceManager
    * FileSystem
    * AccessManager
    * QueryHandler
    * TextFilter

Possible dependencies to implementation-specific classes should preferably be abstracted using extra interfaces.

Also the workspace and node type administration methods should be published as Jackrabbit-specific extensions to the JCR API interfaces.
"
0,"Catch Throwables while calling TextExtractorsThere are different Exception Handlings in the current TextExtractors.
The Method Signature throws IOException but the internal Handling is different.

For example in the MsExcelTextExtractor there will be RuntimeException's catched but not in all Extractors.
@see JCR-574

I think we should catch Throwables in the NodeIndexer to prevent OutOfMemoryErros while indexing a node."
0,"Modify LazyQueryResultImpl to allow resultFetchSize to be set programmaticallyIn our application we have a search which only shows part of a query result. We always know which part of the result needs to be shown. This means we know in advance how many results need to be fetched. I would like to be able to programmatically set resultFetchSize to minimize the number of loaded lucene docs and therefore improve the performance.
I know it is already possible to the set the resultFetchSize via the index configuration, but this number is fixed and doesn't work well in environments where you use paging for your results because if you set this number too low the query will be executed multiple times and if you set it too high too many lucene docs are loaded."
0,"Slightly more readable code in Token/TermAttributeImplNo big deal. 

growTermBuffer(int newSize) was using correct, but slightly hard to follow code. 

the method was returning null as a hint that the current termBuffer has enough space to the upstream code or reallocated buffer.

this patch simplifies logic   making this method to only reallocate buffer, nothing more.  
It reduces number of if(null) checks in a few methods and reduces amount of code. 
all tests pass.

This also adds tests for the new basic attribute impls (copies of the Token tests)."
0,move wordnet based synonym code out of contrib/memory and into contrib/wordnet (or somewhere else)see LUCENE-387 ... some synonym related code has been living in contrib/memory for a very long time ... it should be refactored out.
1,"InternalVersionManager deadlockThe changes in JCR-2753 exposed the InternalVersionManager classes to the following deadlock scenario:

   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x0000000085edb5a0> (a org.apache.jackrabbit.core.state.DefaultISMLocking)
	at java.lang.Object.wait(Object.java:485)
	at org.apache.jackrabbit.core.state.DefaultISMLocking.acquireReadLock(DefaultISMLocking.java:92)
	- locked <0x0000000085edb5a0> (a org.apache.jackrabbit.core.state.DefaultISMLocking)
	at org.apache.jackrabbit.core.version.InternalVersionManagerBase.acquireReadLock(InternalVersionManagerBase.java:192)
	at org.apache.jackrabbit.core.version.InternalVersionManagerImpl.getItem(InternalVersionManagerImpl.java:324)
	at org.apache.jackrabbit.core.version.InternalVersionManagerBase.createInternalVersionItem(InternalVersionManagerBase.java:761)
	at org.apache.jackrabbit.core.version.InternalVersionManagerImpl.getItem(InternalVersionManagerImpl.java:329)
	- locked <0x0000000085edb770> (a org.apache.commons.collections.map.ReferenceMap)
	at org.apache.jackrabbit.core.version.InternalVersionManagerBase.getVersionHistory(InternalVersionManagerBase.java:130)
	at org.apache.jackrabbit.core.version.InternalVersionManagerImpl.getVersionHistory(InternalVersionManagerImpl.java:70)
	at org.apache.jackrabbit.core.version.InternalVersionManagerImpl$4.run(InternalVersionManagerImpl.java:415)
	at org.apache.jackrabbit.core.version.InternalVersionManagerImpl$DynamicESCFactory.doSourced(InternalVersionManagerImpl.java:720)
	at org.apache.jackrabbit.core.version.InternalVersionManagerImpl.checkin(InternalVersionManagerImpl.java:407)
	at org.apache.jackrabbit.core.version.InternalXAVersionManager.checkin(InternalXAVersionManager.java:251)
	at org.apache.jackrabbit.core.version.VersionManagerImplBase.checkoutCheckin(VersionManagerImplBase.java:190)
	at org.apache.jackrabbit.core.VersionManagerImpl.access$100(VersionManagerImpl.java:72)
	at org.apache.jackrabbit.core.VersionManagerImpl$1.perform(VersionManagerImpl.java:121)
	at org.apache.jackrabbit.core.VersionManagerImpl$1.perform(VersionManagerImpl.java:114)
	at org.apache.jackrabbit.core.session.SessionState.perform(SessionState.java:200)
	at org.apache.jackrabbit.core.VersionManagerImpl.perform(VersionManagerImpl.java:95)
	at org.apache.jackrabbit.core.VersionManagerImpl.checkin(VersionManagerImpl.java:114)
	at org.apache.jackrabbit.core.VersionManagerImpl.checkin(VersionManagerImpl.java:100)

   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.jackrabbit.core.version.InternalVersionManagerImpl.getItem(InternalVersionManagerImpl.java:327)
	- waiting to lock <0x0000000085edb770> (a org.apache.commons.collections.map.ReferenceMap)
	at org.apache.jackrabbit.core.version.InternalXAVersionManager.getItem(InternalXAVersionManager.java:442)
	at org.apache.jackrabbit.core.version.InternalVersionManagerBase.getVersionHistory(InternalVersionManagerBase.java:130)
	at org.apache.jackrabbit.core.version.InternalXAVersionManager.getVersionHistory(InternalXAVersionManager.java:58)
	at org.apache.jackrabbit.core.version.VersionHistoryImpl.getInternalVersionHistory(VersionHistoryImpl.java:78)
	at org.apache.jackrabbit.core.version.VersionHistoryImpl.isSame(VersionHistoryImpl.java:278)
	at org.apache.jackrabbit.core.version.VersionHistoryImpl.checkOwnVersion(VersionHistoryImpl.java:326)
	at org.apache.jackrabbit.core.version.VersionHistoryImpl.getVersionLabels(VersionHistoryImpl.java:218)

The problem is the ReferenceMap synchronization (object 0x0000000085edb770) that now interferes with the more general read/write locking mechanism."
0,"PersistenceManager API change breaks backward compatibilityPersistence Manager API change introduced in JCR-1428 breaks backward compatibility. although this is not a public visible API it renders 3rd party PMs invalid that do not extend from AbstractPersistenceManager.
at least for the 1.4.3 patch release, we should not do this.

suggest to revert the API change for the next 1.4.4 release, but leave the method on the abstract pm, and introduce it only for 1.5.
"
0,"additional conditional compliance tests for the caching module for Content-Encoding, Content-Location, Date, Expires, Server, Transfer-Encoding, and Vary headersPatch is forthcoming."
0,"AttributeSource's methods for accessing attributes should be final, else its easy to corrupt the internal statesThe methods that operate and modify the internal maps of AttributeSource should be final, which is a backwards break. But anybody that overrides such methods simply creates a buggy AS either case.

I want to makeall impls final (in general the class should be final at all, but it is made for extension in TokenStream). So its important that the implementations are final!"
1,Prevent persistence of faulty back-referencesThe SharedItemStateManager updates reference data. Sometimes the back-references to reference properties are not updated correctly with the result that nodes cannot removed anymore. The attached patch contains JUnit test cases and a possible solution.
1,"shouldn't automatically set Content-Length in request headercurrently, httpclient automatically add Content-Length: 0 in the request 
header, this is causing problems with some web servers, particularly, with

ar.atwola.com

Try the following URL
http://ar.atwola.com/file/adsWrapper.js

It will block indefinitely. This problem can be fixed by not sending the 
Content-Length header, this is the browser's behavior. I'm not sure why this 
casue problem, but let's conform to a standard browser's practice and avoid 
troubles."
0,"Further steps towards flexible indexingI attached a very rough checkpoint of my current patch, to get early
feedback.  All tests pass, though back compat tests don't pass due to
changes to package-private APIs plus certain bugs in tests that
happened to work (eg call TermPostions.nextPosition() too many times,
which the new API asserts against).

[Aside: I think, when we commit changes to package-private APIs such
that back-compat tests don't pass, we could go back, make a branch on
the back-compat tag, commit changes to the tests to use the new
package private APIs on that branch, then fix nightly build to use the
tip of that branch?o]

There's still plenty to do before this is committable! This is a
rather large change:

  * Switches to a new more efficient terms dict format.  This still
    uses tii/tis files, but the tii only stores term & long offset
    (not a TermInfo).  At seek points, tis encodes term & freq/prox
    offsets absolutely instead of with deltas delta.  Also, tis/tii
    are structured by field, so we don't have to record field number
    in every term.
.
    On first 1 M docs of Wikipedia, tii file is 36% smaller (0.99 MB
    -> 0.64 MB) and tis file is 9% smaller (75.5 MB -> 68.5 MB).
.
    RAM usage when loading terms dict index is significantly less
    since we only load an array of offsets and an array of String (no
    more TermInfo array).  It should be faster to init too.
.
    This part is basically done.

  * Introduces modular reader codec that strongly decouples terms dict
    from docs/positions readers.  EG there is no more TermInfo used
    when reading the new format.
.
    There's nice symmetry now between reading & writing in the codec
    chain -- the current docs/prox format is captured in:
{code}
FormatPostingsTermsDictWriter/Reader
FormatPostingsDocsWriter/Reader (.frq file) and
FormatPostingsPositionsWriter/Reader (.prx file).
{code}
    This part is basically done.

  * Introduces a new ""flex"" API for iterating through the fields,
    terms, docs and positions:
{code}
FieldProducer -> TermsEnum -> DocsEnum -> PostingsEnum
{code}
    This replaces TermEnum/Docs/Positions.  SegmentReader emulates the
    old API on top of the new API to keep back-compat.
    
Next steps:

  * Plug in new codecs (pulsing, pfor) to exercise the modularity /
    fix any hidden assumptions.

  * Expose new API out of IndexReader, deprecate old API but emulate
    old API on top of new one, switch all core/contrib users to the
    new API.

  * Maybe switch to AttributeSources as the base class for TermsEnum,
    DocsEnum, PostingsEnum -- this would give readers API flexibility
    (not just index-file-format flexibility).  EG if someone wanted
    to store payload at the term-doc level instead of
    term-doc-position level, you could just add a new attribute.

  * Test performance & iterate.
"
1,"NPE in PredefinedNodeTypeTest.getPropertyDefSpecOccurs when PropertyDefinition.getValueConstraints returns null, which is allowed by the spec.
"
1,AbstractSession should not synchronize on the session instanceThe local namespace mapping methods in AbstractSession are synchronized to protect against concurrent access. That's troublesome since our observation delivery needs to be able to get those mappings even when the session is synchronized to do something else.
0,"DerbyPersistenceManager only usable for embedded databasesDerbyPersistenceManager always shuts down the database on exit, which makes it unusable for standalone databases."
0,Contrib CharTokenizer classes should be instantiated using their new Version based ctorsContrib CharTokenizer classes should be instantiated using their new Version based ctors introduced by LUCENE-2183 and LUCENE-2240
0,"Two-stage state expansion for the FST: distance-from-root and child-count criteria.In the current implementation FST states are expanded into a binary search on labels (from a vector of transitions) when the child count of a state exceeds a given predefined threshold (NUM_ARCS_FIXED_ARRAY). This threshold affects automaton size and traversal speed (as it turns out when benchmarked). For some degenerate (?) data sets, close-to-the-root nodes could have a small number of children (below the threshold) and yet be traversed on every single seek.

A fix of this is to introduce two control thresholds: 
  EXPAND state if (distance-from-root <= MIN_DISTANCE || children-count >= NUM_ARCS_FIXED_ARRAY)

My plan is to create a data set that will prove this first and then to implement the workaround above."
1,"Auto Reconnect for RMI RepositoryIf i bind the RepositoryAccessServlet to a RMI Repository and then reboot the Repository i get a
NullpointerException. 
Stack :
java.lang.NullPointerException
	at org.apache.jackrabbit.webdav.jcr.JcrDavException.<init>(JcrDavException.java:111)
	at org.apache.jackrabbit.webdav.simple.DavSessionProviderImpl.attachSession(DavSessionProviderImpl.java:99)
	at org.apache.jackrabbit.server.AbstractWebdavServlet.service(AbstractWebdavServlet.java:181)

If i deploy jackrabbit in a Model 3 Environment this Situation can happen very often.
thanks
claus"
1,"IndexWriter should never pool readers for external segmentsEG when addIndexes is called, it enrolls external segment infos, which are then merged.  But merging will simply ask the pool for the readers, and if writer is pooling (NRT reader has been pooled) it incorrectly pools these readers.

It shouldn't break anything but it's a waste because these readers are only used for merging, once, and they are not opened by NRT reader."
0,"Strengthen CheckIndex a bitA few small improvements to CheckIndex to detect possible ""docs out of order"" cases."
0,Migrate to maven 2 the other OCM subprojectsMigrate to maven 2 the other OCM subprojects : jcr-nodemanagement & spring. 
1,WeightedHighlighter does not encode XML markup charactersSee JCR-2611; the same problem applies to WeightedHighlighter.
0,"AppendRecord writes single bytes to diskThe AppendRecord initially buffers writes in memory and starts
to write it to a temp file as soon as it occupies more than
64k heap. After switching to the temp file, data is written
unbuffered."
1,"Deadlock on concurrent commitsAs reported in the followup to JCR-1979, there's a case where two transactions may be concurrently inside a commit. This is bad as it breaks the main assumption in http://jackrabbit.apache.org/concurrency-control.html about all transactions first acquiring the versioning write lock.

Looking deeper into this I find that the versioning write lock is only acquired if the transaction being committed contains versioning operations. This is incorrect as all transactions in any case need to access the version store when checking for references."
0,"provide an ehcache implementation for HttpCacheProvide an implementation of the HttpCache interface that stores cache entries in ehcache.
"
1,"Path is not indexed when inserting a new node with SNSUsing Jackrabbit OCM, when inserting two nodes with the same path, the second node's path is not indexed. 
Both nodes have the same path, and a search by path retrieves the first node only. 

The node mapping included the following annotations:

@Node(jcrMixinTypes=""mix:referenceable,mix:lockable,mix:versionable"") 
public class Article { 

        @Field(uuid=true) 
        private String id = null; 
        
        @Field(path=true) 
        private String path = null; 

        ....
}"
0,"authentication order has changed from 1.4.x to 1.5.xIn 1.4.x inside RepositoryImpl.login(...) at first the local configuration is checked for configured LoginModules and after it was unsuccessful, the JAAS component is asked:

          AuthContext authCtx;
            LoginModuleConfig lmc = repConfig.getLoginModuleConfig();
            if (lmc == null) {
                        authCtx = new AuthContext.JAAS(repConfig.getAppName(), credentials);
            } else {
...

With 1.5.x this behaviour has moved to SimpleSecurityManager.init(..) and is changed:
        LoginModuleConfig loginModConf = config.getLoginModuleConfig();
        authCtxProvider = new AuthContextProvider(config.getAppName(), loginModConf);
        if (authCtxProvider.isJAAS()) {
            log.info(""init: using JAAS LoginModule configuration for "" + config.getAppName());
        } else if (authCtxProvider.isLocal()) {
...

The problem is with JBoss JAAS implemantation, that authCtxProvider.isJAAS()  is always true.
Because for any reason, the result of Configuration.getAppConfigurationEntry(appName) is never empty,
when a jaas.config is specified for Liferay. Using different appName takes no effect, always the configuration inside the jaas.config is used.

I think still first the local configuration should be concerned, before using JAAS."
1,"Deadlock due different Thread access while prepare and commit in same TransactionSince we have configured a j2c resource adapter any modification to the repository ends
with a deadlock."
0,"SimpleHttpConnectionManager is used incorrectly by tutorial codeUsing pretty well standard (from the tutorial) code causes the 
SimpleHttpConnectionManager to print its ""being used incorrectly"" warning if 
the connection times out (or other I/O exception occurs).

I will attach a simple test I made to demonstrate this."
1,"NumericRangeQuery errors with endpoints near long min and max valuesThis problem first reported in Solr:

http://lucene.472066.n3.nabble.com/range-query-on-TrieLongField-strange-result-tt970974.html#a970974"
0,"Automatic repository shutdownCurrently Jackrabbit relies on two mechanisms for safely shutting down a repository:

    1) client application invoking RepositoryImpl.shutdown(), or
    2) the shutdown hook installed by RepositoryImpl being run

Both of these mechanisms have problems:

    1) The shutdown() method is not a part of the JCR API, thus making the client application depend on a Jackrabbit-specific feature
    2) In some cases the shutdown hook is not properly run (see issues JCR-120 and JCR-233)

I think the JCR spec thinks of the Repository and Session interfaces as being somewhat similar to the JDBC DataSource and Connection interfaces. The Repository instances have no real lifecycle methods while the Session instances have clearly specified login and logout steps. (DataSource.getConnection() = Repository.login(), Session.logout() = Connection.close()) However the Jackrabbit implementation defines an explicit lifecycle for the RepositoryImpl instances.

This causes problems especially for container environments (JNDI, Spring) where it is hard or even impossible to specify a shutdown mechanism for resource factories like the Repository instances. The current solution for such environments is to use a shutdown hook, but as reported this solution does not work perfectly in all cases.

How about if we bound the RepositoryImpl lifecycle to the lifecycles of the instantiated Sessions. A RepositoryImpl instance could initialize (and lock) the repository when the first session is opened and automatically shut down when the last session has logged out. As long as the sessions are properly logged out (or finalized by the garbage collector) there would be no need for an explicitly RepositoryImpl.shutdown() call. The current behaviour of pre-initializing the repository and shutting down during a shutdown hook could be enabled with a configuration option for environments (like global JNDI resources) in which the shutdown hooks work well.
"
1,"SSLSocketFactory.connectSocket() possible NPE - or use of wrong variable?SSLSocketFactory.connectSocket() has a possible NPE at line 324:

            sock.connect(remoteAddress, connTimeout);

Or perhaps this should really be:

            sslsock.connect(remoteAddress, connTimeout);"
1,"Node.restore fails with multiple mixin typesRestoring a node that has more than one mixin type causes the exception below. 

java.lang.NullPointerException
	at org.apache.jackrabbit.core.NodeImpl.restoreFrozenState(NodeImpl.java:3286)
	at org.apache.jackrabbit.core.NodeImpl.internalRestore(NodeImpl.java:3243)
	at org.apache.jackrabbit.core.NodeImpl.internalRestore(NodeImpl.java:3210)
	at org.apache.jackrabbit.core.NodeImpl.restore(NodeImpl.java:2821)
	at com.gtnet.jcr.VersionedNodeTest.testNodeVersionAndRestore(VersionedNodeTest.java:311)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at com.intellij.rt.execution.junit2.JUnitStarter.main(JUnitStarter.java:31)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:78)
"
1,Incorrect iterator position in JCR-RMI when skipping large number of entriesThe positionOfBuffer variable in ClientIterator gets out of sync more than ServerAdapter.bufferLength items.
1,"indexing doesn't reset token stateIndexWriter (DocumentsWriter) forgets to reset the token state resulting in incorrect positionIncrements, payloads, and token types."
1,"if index is too old you should hit an exception saying soIf you create an index in 2.3.x (I used demo's IndexFiles) and then try to read it in 4.0.x (I used CheckIndex), you hit a confusing exception like this:
{noformat}
java.io.IOException: read past EOF
        at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:154)
        at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)
        at org.apache.lucene.store.ChecksumIndexInput.readByte(ChecksumIndexInput.java:40)
        at org.apache.lucene.store.DataInput.readInt(DataInput.java:76)
        at org.apache.lucene.index.SegmentInfo.<init>(SegmentInfo.java:171)
        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:230)
        at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:269)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:649)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:484)
        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:265)
        at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:308)
        at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:287)
        at org.apache.lucene.index.CheckIndex.main(CheckIndex.java:930)
{noformat}

I think instead we should throw an IndexTooOldException or something like that?"
1,"RAMDirectory reports incorrect EOF on seekIf you create a file whose length is a multiple of 1024 (BUFFER_SIZE),
and then try to seek to the very end of the file, you hit
EOFException.

But this is actually ""legal"" as long as you don't try to read any
bytes at that point.

I'm hitting this (rarely) with the bulk-merging logic for term vectors
(LUCENE-1120), which can seek to the very end of the file but not read
any bytes if conditions are right.

"
0,"Some tests try to add new nodes without specifying the node typeSome tests try to add new nodes without specifying the node type, which may not be supported by a repository.

In particular:

- NodeAddMixinTest.testAddNonExisting
- NodeCanAddMixinTest.testNonExisting
- ValueFactoryTest.testValueType
- ValueFactoryTest.testValueFormatException

Proposal: update test cases to obtain the node type from the test config.
"
0,"Unique ID for org.apache.jackrabbit.value.BinaryValueBinaryValue should have a method get the unique identifier (if one is available). That way an application may not have to read the stream if that value is already processed.

When the DataStore is used, a unique identifier is available, so probably this feature is quite simple to implement.

See also http://www.nabble.com/Workspace.copy()-Question-...-td20435164.html (but please don't reply to this thread from now on - instead add comments to this issue).

Another feature is getFileName() to get the file name if it is stored in the file system. This method may need a security mechanism, for example getFileName(Session s) so that the system can check it. In any case the file should not be modified, but maybe knowing the file name is already too dangerous in some cases."
0,"Add a TypeTokenFilterIt would be convenient to have a TypeTokenFilter that filters tokens by its type, either with an exclude or include list. This might be a stupid thing to provide for people who use Lucene directly, but it would be very useful to later expose it to Solr and other Lucene-backed search solutions."
0,"Improve CachingWrapperFilter to optionally also cache acceptDocs, if identical to liveDocsSpinoff from LUCENE-1536: This issue removed the different cache modes completely and always applies the acceptDocs using BitsFilteredDocIdSet.wrap(), the cache only contains raw DocIdSet without any deletions/acceptDocs. For IndexReaders that are seldom reopened, this might not be as performant as it could be. If the acceptDocs==IR.liveDocs, those DocIdSet could also be cached with liveDocs applied."
1,"Using transactions leads to memory leakThere is global static map in XASessionImpl class which stores all Xids and TransactionContexts

    /**
     * Global transactions
     */
    private static final Map txGlobal = new HashMap();

It looks like this map is never cleared, even after end of transaction. It leads to memory leak because TransactionContexts and all nested objects (including XASessionImpl) are still referenced and couldn't be freed.

Proposed solution : Is it posssible to add just single line which will remove TransactionContext from static map at the end of transaction ?

      if (flags == TMSUCCESS || flags == TMFAIL) {
            associate(null);
-->       txGlobal.remove(xid);
        } else  if (flags == TMSUSPEND) {
            associate(null);
        } else {
            throw new XAException(XAException.XAER_INVAL);
        }

If this is not acceptable, then we have to unreference TransactionContext in another way."
1,"JCR-Server: Allow header misses colong (RootCollection, WorkspaceResourceImpl)List of supported dav methods misses some colons."
1,"Dead code in SpellChecker.java (branch never executes)SpellChecker contains the following lines of code:

    final int goalFreq = (morePopular && ir != null) ? ir.docFreq(new Term(field, word)) : 0;
    // if the word exists in the real index and we don't care for word frequency, return the word itself
    if (!morePopular && goalFreq > 0) {
      return new String[] { word };
    }

The branch will never execute: the only way for goalFreq to be greater than zero is if morePopular is true, but if morePopular is true, the expression in the if statement evaluates to false.

"
0,"make frozenbuffereddeletes more efficient for termswhen looking at LUCENE-3340, I thought its also ridiculous how much ram we use for delete by term.

so we can save a lot of memory, especially object overhead by being a little more efficient."
0,"Customizable Cookie PolicyEven if the server is not complying with the cookie specification, sometimes 
you still need to talk to it.

It would be nice that when setting the Cookie Policy for the HttpMethod, you 
could specify a custom policy that implements the CookieSpecBase but may 
handle certain problems more leniently.  This would be instead of specifying 
one of the three hard-coded policies."
1,"Restoring a node fails (partially) if done within a XA transactionA problem occurs with the following sequence of steps: 

1) Create a versionable node that has a child and a grandchild.
2) Perform a check-in of the versionable node and give a version-label.
3) Perform a restore by using the version-label.
4) Access the grandchild.

Step 4 fails, if step 3 is executed within a transaction. If no transaction is used, then step 4 succeeds. 
The test-case attached below can be executed within XATest.java (http://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-core/src/test/java/org/apache/jackrabbit/core/XATest.java).


public void testRestore() throws Exception {
        Session session = null;
        try {
            session = getHelper().getSuperuserSession();

            // make sure that 'testNode' does not exist at the beginning of the test
            for (NodeIterator ni = session.getRootNode().getNodes(); ni.hasNext();) {
                Node aNode = ni.nextNode();
                if (aNode.getName().equals(""testNode"")) {
                    aNode.remove();
                }
            }

            // 1) create 'testNode' that has a child and a grandchild
            session.getRootNode().addNode(""testNode"").addMixin(NodeType.MIX_VERSIONABLE);
            session.getRootNode().getNode(""testNode"").addNode(""child"").addNode(""grandchild"");
            session.save();

            // 2) check in 'testNode' and give a version-label
            Version version = session.getWorkspace().getVersionManager().checkin(
                    session.getRootNode().getNode(""testNode"").getPath());
            session.getWorkspace().getVersionManager().getVersionHistory(
                    session.getRootNode().getNode(""testNode"").getPath()).addVersionLabel(version.getName(),
                    ""testLabel"", false);

            // 3) do restore by label
            UserTransaction utx = new UserTransactionImpl(session);
            utx.begin();
            session.getWorkspace().getVersionManager().restoreByLabel(
                    session.getRootNode().getNode(""testNode"").getPath(), ""testLabel"", true);
            utx.commit();

            // 4) try to get the grandchild (fails if the restoring has been done within a transaction)
            session.getRootNode().getNode(""testNode"").getNode(""child"").getNode(""grandchild"");
        } finally {
            if (session != null) {
                session.logout();
            }
        }
    } "
1,"InvalidQueryException thrown for a SQL query using WHERE CONTAINS(., 'someword')The following SQL query:
SELECT * FROM es:document WHERE CONTAINS(., 'software')
throws an InvalidQueryException exception.

javax.jcr.query.InvalidQueryException: Encountered ""."" at line 1, column 42.
Was expecting one of:
    ""BY"" ...
    ""IN"" ...
    ""OR"" ...
    ""IS"" ...
    ""AND"" ...
    ""LIKE"" ...
    ""NULL"" ...
    ""FROM"" ...
    ""ORDER"" ...
    ""WHERE"" ...
    ""SELECT"" ...
    ""BETWEEN"" ...
    ""*"" ...
    <REGULAR_IDENTIFIER> ...
    <DELIMITED_IDENTIFIER> ...
    
This syntax seems correct according to the latest jcr spec (1.0.1).
Using an asterisk (*) instead of a dot (.) as the first parameter of CONTAINS() works fine."
0,Introduce spellchecker functionality based on content in the workspaceProvide a way to spell check a fulltext statement based on the content present in the workspace.
1,"offset gap should be added regardless of existence of tokens in DocInverterPerFieldProblem: If a multiValued field which contains a stop word (e.g. ""will"" in the following sample) only value is analyzed by StopAnalyzer when indexing, the offsets of the subsequent tokens are not correct.

{code:title=indexing a multiValued field}
doc.add( new Field( F, ""Mike"", Store.YES, Index.ANALYZED, TermVector.WITH_OFFSETS ) );
doc.add( new Field( F, ""will"", Store.YES, Index.ANALYZED, TermVector.WITH_OFFSETS ) );
doc.add( new Field( F, ""use"", Store.YES, Index.ANALYZED, TermVector.WITH_OFFSETS ) );
doc.add( new Field( F, ""Lucene"", Store.YES, Index.ANALYZED, TermVector.WITH_OFFSETS ) );
{code}

In this program (soon to be attached), if you use WhitespaceAnalyzer, you'll get the offset(start,end) for ""use"" and ""Lucene"" will be use(10,13) and Lucene(14,20). But if you use StopAnalyzer, the offsets will be use(9,12) and lucene(13,19). When searching, since searcher cannot know what analyzer was used at indexing time, this problem causes out of alignment of FVH.

Cause of the problem: StopAnalyzer filters out ""will"", anyToken flag set to false then offset gap is not added in DocInverterPerField:

{code:title=DocInverterPerField.java}
if (anyToken)
  fieldState.offset += docState.analyzer.getOffsetGap(field);
{code}

I don't understand why the condition is there... If always the gap is added, I think things are simple."
0,"Upgrade to latest Apache parent POMWe're quite a bit behind the latest and greatest of the Apache parent POMs (org.apache:apache), mostly since we're inheriting it through the now mostly unused org.apache.jackrabbit:parent POM.

I'd like to move things back from the o.a.j:parent POM to the jackrabbit-parent POM that's located inside trunk. This will allow us to upgrade to the latest Apache parent POM without the trouble of an extra release of the o.a.j:parent POM."
0,"Czech StemmerCurrently, the CzechAnalyzer is merely stopwords, and there isn't a czech stemmer in snowball.

This patch implements the light stemming algorithm described in: http://portal.acm.org/citation.cfm?id=1598600

In their measurements, it improves MAP 42%

The analyzer does not use this stemmer if LUCENE_VERSION <= 3.0, for back compat.
"
0,"QueryObjectModelImpl should execute queries as SessionOperation(s)QueryObjectModelImpl doesn't leverage the SessionOperation closure approach (like the QueryImpl does). 

Switching to this style of running a query yields some gains in speed (I ran 50 queries per test):
 - #1. old style   (no code change)       avg was 14.26 ms
 - #2. new style (as session operation) avg was 12.14 ms
 - #3. new style (as session operation) avg was   6.44 ms
 - #4. new style (as session operation) avg was   6.68 ms
 - #5. old style  (no code change)        avg was 11.62 ms
 - #6. old style  (no code change)        avg was 11.66 ms"
1,"SpanNotQuery.hashCode ignores excludefiling as bug for tracking/refrence...

On May 16, 2006, at 3:33 AM, Chris Hostetter wrote:

> SpanNodeQuery's hashCode method makes two refrences to  
> include.hashCode(),
> but none to exclude.hashCode() ... this is a mistake yes/no?

Date: Tue, 16 May 2006 05:57:15 -0400
From: Erik Hatcher
To: java-dev@lucene.apache.org
Subject: Re: SpanNotQuery.hashCode cut/paste error?

Yes, this is a mistake.  I'm happy to fix it, but looks like you have  
other patches in progress.

"
1,spi2davex: uri-lookup not cleared after reordering referenceable same-name-siblings
1,"Exception in DocumentsWriter.addDocument can corrupt stored fields file (fdt)DocumentsWriter writes the number of stored fields, up front, into the
fdtLocal buffer.  Then, as each field is processed, it writes each
stored field into this buffer.  When the document is done, in a
finally clause, it flushes the buffer to the real fdt file in the
Directory.

The problem is, if an exception is hit, that number of stored fields
can be too high, which corrupts the fdt file.

The solution is to not write it up front, and instead write only the
number of fields we actually saw."
1,"Unsynchronized NameFactoryImpl initializationorg.apache.jackrabbit.spi.commons.name.NameFactoryImpl uses an unsafe pattern when initializing:

    private static NameFactory FACTORY;
    private NameFactoryImpl() {};
    public static NameFactory getInstance() {
        if (FACTORY == null) {
            FACTORY = new NameFactoryImpl();
        }
        return FACTORY;
    }

This is bad in a multi-threaded environment (see http://www.ibm.com/developerworks/library/j-dcl.html for details)."
1,"DbDataStore: garbage collection deadlockSometimes, the unit tests hangs with the following threads blocked. It looks like a database level deadlock caused by the DbDataStore implementation. The database used is Apache Derby.

org.apache.jackrabbit.core.data.db.DbDataStore.addRecord line=298
org.apache.jackrabbit.core.value.BLOBInDataStore.getInstance line=120
org.apache.jackrabbit.core.value.InternalValue.getBLOBFileValue line=644
org.apache.jackrabbit.core.value.InternalValue.create line=123
org.apache.jackrabbit.core.PropertyImpl.setValue line=609
org.apache.jackrabbit.core.PropertyImpl.setValue line=525
org.apache.jackrabbit.core.NodeImpl.setProperty line=2312
org.apache.jackrabbit.core.data.CopyValueTest.doTestCopy line=64
org.apache.jackrabbit.core.data.CopyValueTest.testCopyStream line=45

org.apache.jackrabbit.core.data.db.DbDataStore.updateLastModifiedDate line=641
org.apache.jackrabbit.core.data.db.DbDataStore.touch line=631
org.apache.jackrabbit.core.data.db.DbDataStore.getRecord line=484
org.apache.jackrabbit.core.value.BLOBInDataStore.getDataRecord line=136
org.apache.jackrabbit.core.value.BLOBInDataStore.getLength line=92
org.apache.jackrabbit.core.data.GarbageCollector.scanPersistenceManagers
org.apache.jackrabbit.core.data.GarbageCollector.scan line=161
org.apache.jackrabbit.core.data.GCThread.run line=52"
0,"DateTools.stringToDate() can cause lock contention under loadLoad testing our application (the JIRA Issue Tracker) has shown that threads spend a lot of time blocked in DateTools.stringToDate().

The stringToDate() method uses a singleton SimpleDateFormat object to parse the dates.
Each call to SimpleDateFormat.parse() is *synchronized* because SimpleDateFormat is not thread safe.

"
0,"Transaction-safe versioningI've been working on a partial fix to JCR-630. Instead of implementing fully transactional versioning (i.e. a checkin will disappear when a transactin is rolled back), I'm ensuring that all versioning operations within a transaction will leave the version store in a consistent state even if the transaction otherwise fails at any point."
1,"LuceneTestCase's check for uncaught exceptions in threads causes collateral damage?Eg see these failures:

    https://hudson.apache.org/hudson/job/Lucene-3.x/214/

Multiple test methods failed in TestIndexWriterOnDiskFull, but, I think only 1 test had a real failure but somehow our ""thread hit exc"" tracking incorrectly blames the other 3 cases?

I'm not sure about this but it seems like something like that is going on...

So, one problem is that LuceneTestCase.tearDown fails on any thread excs, but if CMS had also hit a failure, then fails to clear CMS's thread failures.  I think we should just remove CMS's thread failure tracking?  (It's static so it can definitely bleed across tests).  Ie, just rely on LuceneTestCase's tracking."
0,"Trunk fails tests, FSD.open() - related    [junit] Testcase: testReadAfterClose(org.apache.lucene.index.TestCompoundFile):	FAILED
    [junit] expected readByte() to throw exception
    [junit] junit.framework.AssertionFailedError: expected readByte() to throw exception
    [junit] 	at org.apache.lucene.index.TestCompoundFile.demo_FSIndexInputBug(TestCompoundFile.java:345)
    [junit] 	at org.apache.lucene.index.TestCompoundFile.testReadAfterClose(TestCompoundFile.java:313)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)

This one is a non-bug, if you ask me. The test should fail on SimpleFSD, but on my system FSD.open() creates MMapD and that one cannot be closed, so the read succeeds.

    [junit] ------------- Standard Output ---------------
    [junit] Thread[Thread-34,5,main]: exc
    [junit] java.nio.BufferUnderflowException
    [junit] 	at java.nio.Buffer.nextGetIndex(Buffer.java:474)
    [junit] 	at java.nio.DirectByteBuffer.get(DirectByteBuffer.java:229)
    [junit] 	at org.apache.lucene.store.MMapDirectory$MMapIndexInput.readByte(MMapDirectory.java:67)
    [junit] 	at org.apache.lucene.store.ChecksumIndexInput.readByte(ChecksumIndexInput.java:36)
    [junit] 	at org.apache.lucene.store.IndexInput.readInt(IndexInput.java:70)
    [junit] 	at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:238)
    [junit] 	at org.apache.lucene.index.DirectoryIndexReader$1.doBody(DirectoryIndexReader.java:106)
    [junit] 	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:699)
    [junit] 	at org.apache.lucene.index.DirectoryIndexReader.open(DirectoryIndexReader.java:126)
    [junit] 	at org.apache.lucene.index.IndexReader.open(IndexReader.java:374)
    [junit] 	at org.apache.lucene.index.IndexReader.open(IndexReader.java:260)
    [junit] 	at org.apache.lucene.search.IndexSearcher.<init>(IndexSearcher.java:76)
    [junit] 	at org.apache.lucene.index.TestStressIndexing$SearcherThread.doWork(TestStressIndexing.java:109)
    [junit] 	at org.apache.lucene.index.TestStressIndexing$TimedThread.run(TestStressIndexing.java:52)
    [junit] NOTE: random seed of testcase 'testStressIndexAndSearching' was: -7374705829444180151
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testStressIndexAndSearching(org.apache.lucene.index.TestStressIndexing):	FAILED
    [junit] null
    [junit] junit.framework.AssertionFailedError
    [junit] 	at org.apache.lucene.index.TestStressIndexing.runStressTest(TestStressIndexing.java:155)
    [junit] 	at org.apache.lucene.index.TestStressIndexing.testStressIndexAndSearching(TestStressIndexing.java:178)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)

This one suceeds sometimes, sometimes (mostly) fails. Is obviously linked with switch to MMapD, but what is the real cause - I don't know.

    [junit] ------------- Standard Output ---------------
    [junit] NOTE: random seed of testcase 'testSetBufferSize' was: 8481546620770090440
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testSetBufferSize(org.apache.lucene.store.TestBufferedIndexInput):	Caused an ERROR
    [junit] org.apache.lucene.store.MMapDirectory$MMapIndexInput cannot be cast to org.apache.lucene.store.BufferedIndexInput
    [junit] java.lang.ClassCastException: org.apache.lucene.store.MMapDirectory$MMapIndexInput cannot be cast to org.apache.lucene.store.BufferedIndexInput
    [junit] 	at org.apache.lucene.store.TestBufferedIndexInput$MockFSDirectory.tweakBufferSizes(TestBufferedIndexInput.java:226)
    [junit] 	at org.apache.lucene.store.TestBufferedIndexInput.testSetBufferSize(TestBufferedIndexInput.java:181)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)

Broken assumptions.
"
1,"LazyField use of IndexInput not thread safeHypothetical problem: IndexInput.clone() of an active IndexInput could result in a corrupt copy.
LazyField clones the FieldsReader.fieldsStream, which could be in use via IndexReader.document()"
0,"remove IndexDocValuesFieldIts confusing how we present CSF functionality to the user, its actually not a ""field"" but an ""attribute"" of a field like  STORED or INDEXED.

Otherwise, its really hard to think about CSF because there is a mismatch between the APIs and the index format."
0,"Document the problem with MS impl of digest authentication with older JREs and stale connection checkIt seems like digest authentication, when used with a username of the format:
domain\username fails in httpclient-3.0-rc2.

I did confirm that digest authentication does work by connecting to a local
Apache HTTP 2.0 server, using just a username and password (no domain\username).
However, it does not support the MD5-sess algorithm, and the server I am getting
the failure from is using MD5-sess. 

It may turn out the username is not causing the problem, but one thing is
consistent--I can connect to the site in the logs below using httpclient-2.0.2.
It fails when I use identical Java code, with the addition of AuthScope, when
using httpclient-3.0-rc2. I will also attach Java code that reproduces the problem.

The following are wire and debug logs from httpclient-2.0.2 and
httpclient-3.0-rc2 respectively. The first one connects and gets an 'HTTP 200'
response. The second one, using 3.0-rc2 fails with an 'HTTP 401'.

LOGS
===============================================================
commons-httpclient-2.0.2 (works):
2005/05/13 11:05:15:185 EDT [DEBUG] HttpClient - Java version: 1.3.1
2005/05/13 11:05:15:185 EDT [DEBUG] HttpClient - Java vendor: IBM Corporation
2005/05/13 11:05:15:185 EDT [DEBUG] HttpClient - Java class path: 
2005/05/13 11:05:15:205 EDT [DEBUG] HttpClient - Operating system name: Windows XP
2005/05/13 11:05:15:205 EDT [DEBUG] HttpClient - Operating system architecture: x86
2005/05/13 11:05:15:205 EDT [DEBUG] HttpClient - Operating system version: 5.1
2005/05/13 11:05:15:205 EDT [DEBUG] HttpClient - SUN 1.2: SUN (DSA key/parameter
generation; DSA signing; SHA-1, MD5 digests; SecureRandom; X.509 certificates;
JKS keystore)
2005/05/13 11:05:15:205 EDT [DEBUG] HttpClient - SunJCE 1.22: SunJCE Provider
(implements DES, Triple DES, Blowfish, PBE, Diffie-Hellman, HMAC-MD5, HMAC-SHA1)
2005/05/13 11:05:15:205 EDT [DEBUG] HttpClient - SunJSSE 1.0303: Sun JSSE
provider(implements RSA Signatures, PKCS12, SunX509 key/trust factories, SSLv3,
TLSv1)
2005/05/13 11:05:20:893 EDT [DEBUG] HttpConnection - HttpConnection.setSoTimeout(0)
2005/05/13 11:05:20:893 EDT [DEBUG] HttpMethodBase - Execute loop try 1
2005/05/13 11:05:20:913 EDT [DEBUG] header - >> ""GET
/CustomerData-30/CustomerDataService.asmx HTTP/1.1[\r][\n]""
2005/05/13 11:05:20:913 EDT [DEBUG] HttpMethodBase - Adding Host request header
2005/05/13 11:05:20:913 EDT [DEBUG] header - >> ""User-Agent: Jakarta
Commons-HttpClient/2.0.2[\r][\n]""
2005/05/13 11:05:20:913 EDT [DEBUG] header - >> ""Host:
mappoint-css.partners.extranet.microsoft.com[\r][\n]""
2005/05/13 11:05:21:173 EDT [DEBUG] header - >> ""[\r][\n]""
2005/05/13 11:05:21:273 EDT [DEBUG] header - << ""HTTP/1.1 401 Unauthorized[\r][\n]""
2005/05/13 11:05:21:273 EDT [DEBUG] header - << ""Content-Length: 1656[\r][\n]""
2005/05/13 11:05:21:283 EDT [DEBUG] header - << ""Content-Type: text/html[\r][\n]""
2005/05/13 11:05:21:283 EDT [DEBUG] header - << ""Server: Microsoft-IIS/6.0[\r][\n]""
2005/05/13 11:05:21:283 EDT [DEBUG] header - << ""WWW-Authenticate: Digest
qop=""auth"",algorithm=MD5-sess,nonce=""b2a83a38cd57c501af3ad2c91f189512060524424ffc2b818c9920db15cd247a9d47cf5a789d63c6"",opaque=""1704373a505e74c4ec692978e5c1a539"",charset=utf-8,realm=""Digest""[\r][\n]""
2005/05/13 11:05:21:283 EDT [DEBUG] header - << ""X-Powered-By: ASP.NET[\r][\n]""
2005/05/13 11:05:21:283 EDT [DEBUG] header - << ""Date: Fri, 13 May 2005 15:05:37
GMT[\r][\n]""
2005/05/13 11:05:21:283 EDT [DEBUG] HttpMethodBase - Authorization required
2005/05/13 11:05:21:283 EDT [DEBUG] HttpAuthenticator - Authenticating with the
'Digest' authentication realm at mappoint-css.partners.extranet.microsoft.com
2005/05/13 11:05:21:283 EDT [DEBUG] DigestScheme - Using qop method auth
2005/05/13 11:05:21:283 EDT [DEBUG] HttpMethodBase - HttpMethodBase.execute():
Server demanded authentication credentials, will try again.
2005/05/13 11:05:21:293 EDT [DEBUG] HttpMethodBase - Resorting to protocol
version default close connection policy
2005/05/13 11:05:21:293 EDT [DEBUG] HttpMethodBase - Should NOT close
connection, using HTTP/1.1.
2005/05/13 11:05:21:293 EDT [DEBUG] HttpMethodBase - Execute loop try 2
2005/05/13 11:05:21:293 EDT [DEBUG] header - >> ""GET
/CustomerData-30/CustomerDataService.asmx HTTP/1.1[\r][\n]""
2005/05/13 11:05:21:293 EDT [DEBUG] HttpMethodBase - Request to add Host header
ignored: header already added
2005/05/13 11:05:21:293 EDT [DEBUG] header - >> ""User-Agent: Jakarta
Commons-HttpClient/2.0.2[\r][\n]""
2005/05/13 11:05:21:293 EDT [DEBUG] header - >> ""Host:
mappoint-css.partners.extranet.microsoft.com[\r][\n]""
2005/05/13 11:05:21:293 EDT [DEBUG] header - >> ""Authorization: Digest
username=""domain\user"", realm=""Digest"",
nonce=""b2a83a38cd57c501af3ad2c91f189512060524424ffc2b818c9920db15cd247a9d47cf5a789d63c6"",
uri=""/CustomerData-30/CustomerDataService.asmx"", qop=""auth"",
algorithm=""MD5-sess"", nc=00000001, cnonce=""393a8abf65cd20f85ffdf46a9273b28b"",
response=""854bf54261112caf2e86652276cb2ce6"",
opaque=""1704373a505e74c4ec692978e5c1a539""[\r][\n]""
2005/05/13 11:05:21:293 EDT [DEBUG] header - >> ""[\r][\n]""
2005/05/13 11:05:21:994 EDT [DEBUG] header - << ""HTTP/1.1 200 OK[\r][\n]""HTTP
result: 200

===========================================================

commons-httpclient-3.0-rc2 (does not work):
2005/05/13 11:16:54:881 EDT [DEBUG] DefaultHttpParams - -Set parameter
http.useragent = Jakarta Commons-HttpClient/3.0-rc2
2005/05/13 11:16:54:881 EDT [DEBUG] DefaultHttpParams - -Set parameter
http.protocol.version = HTTP/1.1
2005/05/13 11:16:54:881 EDT [DEBUG] DefaultHttpParams - -Set parameter
http.connection-manager.class = class
org.apache.commons.httpclient.SimpleHttpConnectionManager
2005/05/13 11:16:54:891 EDT [DEBUG] DefaultHttpParams - -Set parameter
http.protocol.cookie-policy = rfc2109
2005/05/13 11:16:54:891 EDT [DEBUG] DefaultHttpParams - -Set parameter
http.protocol.element-charset = US-ASCII
2005/05/13 11:16:54:891 EDT [DEBUG] DefaultHttpParams - -Set parameter
http.protocol.content-charset = ISO-8859-1
2005/05/13 11:16:54:891 EDT [DEBUG] DefaultHttpParams - -Set parameter
http.method.retry-handler =
org.apache.commons.httpclient.DefaultHttpMethodRetryHandler@5048d78c
2005/05/13 11:16:54:891 EDT [DEBUG] DefaultHttpParams - -Set parameter
http.dateparser.patterns = [EEE, dd MMM yyyy HH:mm:ss zzz, EEEE, dd-MMM-yy
HH:mm:ss zzz, EEE MMM d HH:mm:ss yyyy, EEE, dd-MMM-yyyy HH:mm:ss z, EEE,
dd-MMM-yyyy HH-mm-ss z, EEE, dd MMM yy HH:mm:ss z, EEE dd-MMM-yyyy HH:mm:ss z,
EEE dd MMM yyyy HH:mm:ss z, EEE dd-MMM-yyyy HH-mm-ss z, EEE dd-MMM-yy HH:mm:ss
z, EEE dd MMM yy HH:mm:ss z, EEE,dd-MMM-yy HH:mm:ss z, EEE,dd-MMM-yyyy HH:mm:ss
z, EEE, dd-MM-yyyy HH:mm:ss z]
2005/05/13 11:16:54:911 EDT [DEBUG] HttpClient - -Java version: 1.3.1
2005/05/13 11:16:54:911 EDT [DEBUG] HttpClient - -Java vendor: IBM Corporation
2005/05/13 11:16:54:911 EDT [DEBUG] HttpClient - -Java class path: 
2005/05/13 11:16:54:941 EDT [DEBUG] HttpClient - -Operating system name: Windows XP
2005/05/13 11:16:54:941 EDT [DEBUG] HttpClient - -Operating system architecture: x86
2005/05/13 11:16:54:941 EDT [DEBUG] HttpClient - -Operating system version: 5.1
2005/05/13 11:16:54:941 EDT [DEBUG] HttpClient - -SUN 1.2: SUN (DSA
key/parameter generation; DSA signing; SHA-1, MD5 digests; SecureRandom; X.509
certificates; JKS keystore)
2005/05/13 11:16:54:951 EDT [DEBUG] HttpClient - -SunJCE 1.22: SunJCE Provider
(implements DES, Triple DES, Blowfish, PBE, Diffie-Hellman, HMAC-MD5, HMAC-SHA1)
2005/05/13 11:16:54:951 EDT [DEBUG] HttpClient - -SunJSSE 1.0303: Sun JSSE
provider(implements RSA Signatures, PKCS12, SunX509 key/trust factories, SSLv3,
TLSv1)
2005/05/13 11:16:54:961 EDT [DEBUG] HttpConnection - -Open connection to
mappoint-css.partners.extranet.microsoft.com:443
2005/05/13 11:17:00:629 EDT [DEBUG] header - ->> ""GET
/CustomerData-30/CustomerDataService.asmx HTTP/1.1[\r][\n]""
2005/05/13 11:17:00:629 EDT [DEBUG] HttpMethodBase - -Adding Host request header
2005/05/13 11:17:00:639 EDT [DEBUG] header - ->> ""User-Agent: Jakarta
Commons-HttpClient/3.0-rc2[\r][\n]""
2005/05/13 11:17:00:639 EDT [DEBUG] header - ->> ""Host:
mappoint-css.partners.extranet.microsoft.com[\r][\n]""
2005/05/13 11:17:00:639 EDT [DEBUG] header - ->> ""[\r][\n]""
2005/05/13 11:17:00:989 EDT [DEBUG] header - -<< ""HTTP/1.1 401 Unauthorized[\r][\n]""
2005/05/13 11:17:00:999 EDT [DEBUG] header - -<< ""Content-Length: 1656[\r][\n]""
2005/05/13 11:17:00:999 EDT [DEBUG] header - -<< ""Content-Type: text/html[\r][\n]""
2005/05/13 11:17:00:999 EDT [DEBUG] header - -<< ""Server: Microsoft-IIS/6.0[\r][\n]""
2005/05/13 11:17:00:999 EDT [DEBUG] header - -<< ""WWW-Authenticate: Digest
qop=""auth"",algorithm=MD5-sess,nonce=""c66759cace57c5016cf5645c6dee5b649ed29067f652939d6aaf7239310bb333eeb0153783ae445f"",opaque=""e7e259c137b65766c971d6cfc4115789"",charset=utf-8,realm=""Digest""[\r][\n]""
2005/05/13 11:17:00:999 EDT [DEBUG] header - -<< ""X-Powered-By: ASP.NET[\r][\n]""
2005/05/13 11:17:00:999 EDT [DEBUG] header - -<< ""Date: Fri, 13 May 2005
15:16:51 GMT[\r][\n]""
2005/05/13 11:17:00:999 EDT [DEBUG] HttpMethodDirector - -Authorization required
2005/05/13 11:17:01:009 EDT [DEBUG] AuthChallengeProcessor - -Supported
authentication schemes in the order of preference: [ntlm, digest, basic]
2005/05/13 11:17:01:009 EDT [DEBUG] AuthChallengeProcessor - -Challenge for ntlm
authentication scheme not available
2005/05/13 11:17:01:009 EDT [INFO] AuthChallengeProcessor - -digest
authentication scheme selected
2005/05/13 11:17:01:009 EDT [DEBUG] AuthChallengeProcessor - -Using
authentication scheme: digest
2005/05/13 11:17:01:009 EDT [DEBUG] AuthChallengeProcessor - -Authorization
challenge processed
2005/05/13 11:17:01:009 EDT [DEBUG] HttpMethodDirector - -Authentication scope:
DIGEST 'Digest'@mappoint-css.partners.extranet.microsoft.com:443
2005/05/13 11:17:01:009 EDT [DEBUG] HttpMethodDirector - -Retry authentication
2005/05/13 11:17:01:019 EDT [DEBUG] HttpMethodBase - -Resorting to protocol
version default close connection policy
2005/05/13 11:17:01:019 EDT [DEBUG] HttpMethodBase - -Should NOT close
connection, using HTTP/1.1
2005/05/13 11:17:01:019 EDT [DEBUG] HttpConnection - -Connection is locked. 
Call to releaseConnection() ignored.
2005/05/13 11:17:01:019 EDT [DEBUG] HttpMethodDirector - -Authenticating with
DIGEST 'Digest'@mappoint-css.partners.extranet.microsoft.com:443
2005/05/13 11:17:01:019 EDT [DEBUG] HttpMethodParams - -Credential charset not
configured, using HTTP element charset
2005/05/13 11:17:01:019 EDT [DEBUG] DigestScheme - -Using qop method auth
2005/05/13 11:17:01:019 EDT [DEBUG] HttpConnection - -Connection is stale,
closing...
2005/05/13 11:17:01:019 EDT [DEBUG] HttpConnection - -Open connection to
mappoint-css.partners.extranet.microsoft.com:443
2005/05/13 11:17:01:110 EDT [DEBUG] header - ->> ""GET
/CustomerData-30/CustomerDataService.asmx HTTP/1.1[\r][\n]""
2005/05/13 11:17:01:110 EDT [DEBUG] HttpMethodBase - -Adding Host request header
2005/05/13 11:17:01:110 EDT [DEBUG] header - ->> ""User-Agent: Jakarta
Commons-HttpClient/3.0-rc2[\r][\n]""
2005/05/13 11:17:01:110 EDT [DEBUG] header - ->> ""Authorization: Digest
username=""domain\user"", realm=""Digest"",
nonce=""c66759cace57c5016cf5645c6dee5b649ed29067f652939d6aaf7239310bb333eeb0153783ae445f"",
uri=""/CustomerData-30/CustomerDataService.asmx"",
response=""9cab4fcdb2d09f57523aec80d7b51e95"", qop=""auth"", nc=00000001,
cnonce=""b27507ee79c880b2bb565d363598ce07"", algorithm=""MD5-sess"",
opaque=""e7e259c137b65766c971d6cfc4115789""[\r][\n]""
2005/05/13 11:17:01:120 EDT [DEBUG] header - ->> ""Host:
mappoint-css.partners.extranet.microsoft.com[\r][\n]""
2005/05/13 11:17:01:120 EDT [DEBUG] header - ->> ""[\r][\n]""
2005/05/13 11:17:01:540 EDT [DEBUG] header - -<< ""HTTP/1.1 401 Unauthorized[\r][\n]""
2005/05/13 11:17:01:540 EDT [DEBUG] header - -<< ""Content-Length: 1539[\r][\n]""
2005/05/13 11:17:01:540 EDT [DEBUG] header - -<< ""Content-Type: text/html[\r][\n]""
2005/05/13 11:17:01:550 EDT [DEBUG] header - -<< ""Server: Microsoft-IIS/6.0[\r][\n]""
2005/05/13 11:17:01:550 EDT [DEBUG] header - -<< ""WWW-Authenticate: Digest
qop=""auth"",algorithm=MD5-sess,nonce=""3296b0dace57c5012fe314f8c6f8cafd10abc7a61c09484b2be5c7ef19ecb3c080da1f82c3f5a532"",opaque=""87578a7f0871280654aed868cb9497fb"",charset=utf-8,realm=""Digest""[\r][\n]""
2005/05/13 11:17:01:550 EDT [DEBUG] header - -<< ""X-Powered-By: ASP.NET[\r][\n]""
2005/05/13 11:17:01:550 EDT [DEBUG] header - -<< ""Date: Fri, 13 May 2005
15:17:19 GMT[\r][\n]""
2005/05/13 11:17:01:550 EDT [DEBUG] HttpMethodDirector - -Authorization required
2005/05/13 11:17:01:550 EDT [DEBUG] AuthChallengeProcessor - -Using
authentication scheme: digest
2005/05/13 11:17:01:550 EDT [DEBUG] AuthChallengeProcessor - -Authorization
challenge processed
2005/05/13 11:17:01:550 EDT [DEBUG] HttpMethodDirector - -Authentication scope:
DIGEST 'Digest'@mappoint-css.partners.extranet.microsoft.com:443
2005/05/13 11:17:01:550 EDT [DEBUG] HttpMethodDirector - -Credentials required
HTTP result: 401
2005/05/13 11:17:01:550 EDT [DEBUG] HttpMethodDirector - -Credentials provider
not available
2005/05/13 11:17:01:580 EDT [INFO] HttpMethodDirector - -Failure authenticating
with DIGEST 'Digest'@mappoint-css.partners.extranet.microsoft.com:443
2005/05/13 11:17:01:580 EDT [DEBUG] HttpMethodBase - -Buffering response body
2005/05/13 11:17:01:580 EDT [DEBUG] HttpMethodBase - -Resorting to protocol
version default close connection policy
2005/05/13 11:17:01:580 EDT [DEBUG] HttpMethodBase - -Should NOT close
connection, using HTTP/1.1
2005/05/13 11:17:01:580 EDT [DEBUG] HttpConnection - -Releasing connection back
to connection manager."
0,"IOUtils.closeSafely should log suppressed Exceptions in stack trace of original Exception (a new feature of Java 7)I was always against Java 6 support, as it brings no really helpful new features into Lucene. But there are several things that make life easier in coming Java 7 (hopefully on July 28th, 2011). One of those is simplier Exception handling and suppression on Closeable, called ""Try-With-Resources"" (see http://docs.google.com/View?id=ddv8ts74_3fs7483dp, by the way all Lucene classes support these semantics in Java 7 automatically, the cool try-code below would work e.g. for IndexWriter, TokenStreams,...).

We already have this functionality in Lucene since adding the IOUtils.closeSafely() utility (which can be removed when Java 7 is the minimum requirement of Lucene - maybe in 10 years):

{code:java}
try (Closeable a = new ...; Closeable b = new ...) {
  ... use Closeables ...
} catch (Exception e) {
  dosomething;
  throw e;
}
{code}

This code will close a and b in an autogenerated finally block and supress any exception. This is identical to our IOUtils.closeSafely:

{code:java}
Exception priorException = null;
Closeable a,b;
try (Closeable a = new ...; Closeable b = new ...) {
  a = new ...;
  b = new ...
  ... use Closeables ...
} catch (Exception e) {
  priorException = e;
  dosomething;
} finally {
  IOUtils.closeSafely(priorException, a, b);
}
{code}

So this means we have the same functionality without Java 7, but there is one thing that makes logging/debugging much nicer:
The above Java 7 code also adds maybe suppressed Exceptions in those Closeables to the priorException, so when you print the stacktrace, it not only shows the stacktrace of the original Exception, it also prints all Exceptions that were suppressed to throw this Exception (all Closeable.close() failures):

{noformat}
org.apache.lucene.util.TestIOUtils$TestException: BASE-EXCEPTION
    at org.apache.lucene.util.TestIOUtils.testSuppressedExceptions(TestIOUtils.java:61)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:601)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1486)
    at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1404)
    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768)
    Suppressed: java.io.IOException: TEST-IO-EXCEPTION-1
            at org.apache.lucene.util.TestIOUtils$BrokenCloseable.close(TestIOUtils.java:36)
            at org.apache.lucene.util.IOUtils.closeSafely(IOUtils.java:58)
            at org.apache.lucene.util.TestIOUtils.testSuppressedExceptions(TestIOUtils.java:62)
            ... 26 more
    Suppressed: java.io.IOException: TEST-IO-EXCEPTION-2
            at org.apache.lucene.util.TestIOUtils$BrokenCloseable.close(TestIOUtils.java:36)
            at org.apache.lucene.util.IOUtils.closeSafely(IOUtils.java:58)
            at org.apache.lucene.util.TestIOUtils.testSuppressedExceptions(TestIOUtils.java:62)
            ... 26 more
{noformat}

For this in Java 7 a new method was added to Throwable, that allows logging such suppressed Exceptions (it is called automatically by the synthetic bytecode emitted by javac). This patch simply adds this functionality conditionally to IOUtils, so it ""registers"" all suppressed Exceptions, if running on Java 7. This is done by reflection: once it looks for this method in Throwable.class and if found, it invokes it in closeSafely, so the exceptions thrown on Closeable.close() don't get lost.

This makes debugging much easier and logs all problems that may occur.

This patch does *not* change functionality or behaviour, it just adds more nformation to the stack trace in a Java-7-way (similar to the way how Java 1.4 added causes). It works here locally on Java 6 and Java 7, but only Java 7 gets the additional stack traces. For Java 6 nothing changes. Same for Java 5 (if we backport to 3.x).

This would be our first Java 7 improvement (a minor one). Next would be NIO2... - but thats not easy to do with reflection only, so we have to wait 10 years :-)"
0,"FieldCacheImpl cache gets rebuilt every timeFieldCacheImpl uses WeakHashMap to store the cached objects, but since 
there is no other reference to this cache it is getting released every time."
1,"GData TestDateFormater (sic) fails when the Date returned is: Sun, 23 Sep 2007 01:29:06 GMT+00:00TestDateFormater.testFormatDate fails when the Date returned is Sun, 23 Sep 2007 01:29:06 GMT+00:00

The issue lies with the +00:00 at the end of the string.  

The question is, though, is that a valid date for GData?

This is marked as major b/c it is causing nightly builds to fail."
0,"[PATCH] Contribution: A QueryParser which passes wildcard and prefix queries to analyzerLucenes built-in QueryParser class does not analyze prefix nor wildcard queries.
Attached is a subclass which passes these queries to the analyzer as well."
0,"core analyzers should not produce tokens > N (100?) characters in lengthDiscussion that led to this:

  http://www.gossamer-threads.com/lists/lucene/java-dev/56103

I believe nearly any time a token > 100 characters in length is
produced, it's a bug in the analysis that the user is not aware of.

These long tokens cause all sorts of problems, downstream, so it's
best to catch them early at the source.

We can accomplish this by tacking on a LengthFilter onto the chains
for StandardAnalyzer, SimpleAnalyzer, WhitespaceAnalyzer, etc.

Should we do this in 2.3?  I realize this is technically a break in
backwards compatibility, however, I think it must be incredibly rare
that this break would in fact break something real in the application?"
0,FAQ documentCollect the frequently discussed issues from the mailing list and the wiki into an FAQ document.
0,SPI2DAV: setup automated test executiontask copied from JCR-1877
1,FreqProxTermsWriter leaks file handles if exceptions are thrown during flush()FreqProxTermsWriter leaks open file handles if exceptions are thrown during flush. Code needs to be protected by try-finally clauses.
0,"3.x backwards tests are using Version.LUCENE_CURRENT: aren't testing backwards!The 3.x backwards tests are mostly all using Version.LUCENE_CURRENT, therefore they don't always test the behavior as they should.

I added TEST_VERSION_CURRENT = 3.0 to the backwards/LuceneTestCase, and I think we should fix all backwards tests to use TEST_VERSION_CURRENT instead."
0,"[PATCH] UpdateTest has two typosUpdateTest has a typo where it doesn't grab an element out of an array, but uses the array itself to do comparisons. This patch fixes this."
0,"Allow easy extensions of TopDocCollectorTopDocCollector's members and constructor are declared either private or package visible. It makes it hard to extend it as if you want to extend it you can reuse its *hq* and *totatlHits* members, but need to define your own. It also forces you to override getTotalHits() and topDocs().
By changing its members and constructor (the one that accepts a PQ) to protected, we allow users to extend it in order to get a different view of 'top docs' (like TopFieldCollector does), but still enjoy its getTotalHits() and topDocs() method implementations."
0,"jackrabbit-jcr-tests should still be based on Java 1.4The JCR 2.0 TCK needs to be runnable on Java 1.4, so even though we've upgraded to Java 5 as the base platform for Jackrabbit 2.0, the jackrabbit-jcr-tests component needs to still be based on Java 1.4."
1,"NullPointerException in VirtualNodeTypeStateManager.nodeTypeRegisteredI am working on a custom persistence manager which requires an additional node type being registered. For performance reasons, the existence of this node type is verified during PersistenceManager.init.

Unfortunately this does not seem to work as the VirtualNodeTypeStateManager is not prepared to handle this situation at that point in time - the systemSession field seems to still be null."
0,"if a filter can support random access API, we should use itI ran some performance tests, comparing applying a filter via
random-access API instead of current trunk's iterator API.

This was inspired by LUCENE-1476, where we realized deletions should
really be implemented just like a filter, but then in testing found
that switching deletions to iterator was a very sizable performance
hit.

Some notes on the test:

  * Index is first 2M docs of Wikipedia.  Test machine is Mac OS X
    10.5.6, quad core Intel CPU, 6 GB RAM, java 1.6.0_07-b06-153.

  * I test across multiple queries.  1-X means an OR query, eg 1-4
    means 1 OR 2 OR 3 OR 4, whereas +1-4 is an AND query, ie 1 AND 2
    AND 3 AND 4.  ""u s"" means ""united states"" (phrase search).

  * I test with multiple filter densities (0, 1, 2, 5, 10, 25, 75, 90,
    95, 98, 99, 99.99999 (filter is non-null but all bits are set),
    100 (filter=null, control)).

  * Method high means I use random-access filter API in
    IndexSearcher's main loop.  Method low means I use random-access
    filter API down in SegmentTermDocs (just like deleted docs
    today).

  * Baseline (QPS) is current trunk, where filter is applied as iterator up
    ""high"" (ie in IndexSearcher's search loop)."
0,"Add CopyMoveHanlder so that the copy/move behavior can be customized (as this is the case for the IOHandler and PropertyHandler)The IOHandler impls let you define a specific import/export behavior either for specific nodetypes/locations/etc which is just great. this works well while you create or modify a web dav resource but does not work for the copy/move use case.
the attached patch provides a proposal for an additional CopyMoveHandler."
1,"hotspot bug in readvint gives wrong resultsWhen testing the 3.1-RC1 made by Yonik on the PANGAEA (www.pangaea.de) productive system I figured out that suddenly on a large segment (about 5 GiB) some stored fiels suddenly produce a strange deflate decompression problem (CompressionTools) although the stored fields are no longer pre-3.0 compressed. It seems that the header of the stored field is read incorrectly at the buffer boundary in MultiMMapDir and then FieldsReader just incorrectly detects a deflate-compressed field (CompressionTools).

The error occurs reproducible on CheckIndex with MMapDirectory, but not with NIODir or SimpleDir. The FDT file of that segment is 2.6 GiB, on Solaris the chunk size is Integer.MAX_VALUE, so we have 2 MultiMMap IndexInputs.

Robert and me have the index ready as a tar file, we will do tests on our local machines and hopefully solve the bug, maybe introduced by Robert's recent changes to MMap."
0,"Deprecation of autoCommit in 2.4 leads to compile problems, when autoCommit should be falseI am currently changing my code to be most compatible with 2.4. I switched on deprecation warnings and got a warning about the autoCommit parameter in IndexWriter constructors.

My code *should* use autoCommit=false, so I want to use the new semantics. The default of IndexWriter is still autoCommit=true. My problem now: How to disable autoCommit whithout deprecation warnings?

Maybe, the ""old"" constructors, that are deprecated should use autoCommit=true. But there are new constructors with this ""IndexWriter.MaxFieldLength mfl"" in it, that appear new in 2.4 but are deprecated:

IndexWriter(Directory d, boolean autoCommit, Analyzer a, boolean create, IndexDeletionPolicy deletionPolicy, IndexWriter.MaxFieldLength mfl) 
          Deprecated. This will be removed in 3.0, when autoCommit will be hardwired to false. Use IndexWriter(Directory,Analyzer,boolean,IndexDeletionPolicy,MaxFieldLength) instead, and call commit() when needed.

What the hell is meant by this, a new constructor that is deprecated? And the hint is wrong. If I use the other constructor in the warning, I get autoCommit=true.

There is something completely wrong.

It should be clear, which constructors set autoCommit=true, which set it per default to false (perhaps new ones), and the Deprecated text is wrong, if autoCommit does not default to false."
1,"FilterIndexReader does not override all of IndexReader methodsFilterIndexReader does not override all of IndexReader methods. We've hit an error in LUCENE-3573 (and fixed it). So I thought to write a simple test which asserts that FIR overrides all methods of IR (and we can filter our methods that we don't think that it should override). The test is very simple (attached), and it currently fails over these methods:
{code}
getRefCount
incRef
tryIncRef
decRef
reopen
reopen
reopen
reopen
clone
numDeletedDocs
document
setNorm
setNorm
termPositions
deleteDocument
deleteDocuments
undeleteAll
getIndexCommit
getUniqueTermCount
getTermInfosIndexDivisor
{code}

I didn't yet fix anything in FIR -- if you spot a method that you think we should not override and delegate, please comment."
0,"The cluster syncDelay attribute is millisecondsThe repository DTDs document the cluster syncDelay attribute as being the sync interval in seconds, when it really is the interval in milliseconds."
0,"UserManagement: Don't read cached memberships if session has pending (group) changesfor backwards compatibility reasons reading group membership should not access the overall cache in case of pending (group) change.
the current implememation (always reading from cache) caused a regression in test-case we have @day that relied on accurate group
membership information with having unsaved group-member changes."
0,"Occasional JCA test failuresAs discussed in JCR-2870, the JCA packaging tests are occasionally failing with assertion failures in TransientRepository.startRepository(). We haven't seen this before JCR-2870, but it doesn't look like that change could possibly trigger this failure except perhaps by subtly affecting garbage collection. Thus I'm opening this new issue to track this as a separate problem.

Based on my analysis so far it looks likely that this problem has something to do with the ReferenceMap used by TransientRepository to track open sessions. The fact that the problem occurs only occasionally and on just some systems supports the assumption that this is related to some non-deterministic process like garbage collection."
1,"Problems with maxMergeDocs parameterI found two possible problems regarding IndexWriter's maxMergeDocs value. I'm using the following code to test maxMergeDocs:

{code:java} 
  public void testMaxMergeDocs() throws IOException {
    final int maxMergeDocs = 50;
    final int numSegments = 40;
    
    MockRAMDirectory dir = new MockRAMDirectory();
    IndexWriter writer  = new IndexWriter(dir, new WhitespaceAnalyzer(), true);      
    writer.setMergePolicy(new LogDocMergePolicy());
    writer.setMaxMergeDocs(maxMergeDocs);

    Document doc = new Document();
    doc.add(new Field(""field"", ""aaa"", Field.Store.YES, Field.Index.TOKENIZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
    for (int i = 0; i < numSegments * maxMergeDocs; i++) {
      writer.addDocument(doc);
      //writer.flush();      // uncomment to avoid the DocumentsWriter bug
    }
    writer.close();
    
    new SegmentInfos.FindSegmentsFile(dir) {

      protected Object doBody(String segmentFileName) throws CorruptIndexException, IOException {

        SegmentInfos infos = new SegmentInfos();
        infos.read(directory, segmentFileName);
        for (int i = 0; i < infos.size(); i++) {
          assertTrue(infos.info(i).docCount <= maxMergeDocs);
        }
        return null;
      }
    }.run();
  }
{code} 
  
- It seems that DocumentsWriter does not obey the maxMergeDocs parameter. If I don't flush manually, then the index only contains one segment at the end and the test fails.

- If I flush manually after each addDocument() call, then the index contains more segments. But still, there are segments that contain more docs than maxMergeDocs, e. g. 55 vs. 50. The javadoc in IndexWriter says:
{code:java}
   /**
   * Returns the largest number of documents allowed in a
   * single segment.
   *
   * @see #setMaxMergeDocs
   */
  public int getMaxMergeDocs() {
    return getLogDocMergePolicy().getMaxMergeDocs();
  }
{code}"
1,"BindVariable not registered in JCR-SQL2 CONTAINSThe following fails with a ""java.lang.IllegalArgumentException: not a valid variable in this query:""

Query query = qm.createQuery(""SELECT * FROM [my:document] AS document WHERE CONTAINS(document.original, $x)"", Query.JCR_SQL2);
query.bindVariable(""x"", vf.createValue(""moo""));

And query.getBindVariableNames() returns an empty array.

The FullTextSearchExpression _is_ however correctly parsed as a BindVariableValueImpl:
((FullTextSearch) ((QueryObjectModelImpl) query).getConstraint()).getFullTextSearchExpression() instanceof BindVariableValue
"
1,"Enable the use of NoMergePolicy and NoMergeScheduler by BenchmarkBenchmark allows one to set the MP and MS to use, by defining the class name and then use reflection to instantiate them. However NoMP and NoMS are singletons and therefore reflection does not work for them. Easy fix in CreateIndexTask. I'll post a patch soon."
0,SimpleAnalyzer and WhitespaceAnalyzer should have Version ctorsDue to the Changes to CharTokenizer ( LUCENE-2183 ) WhitespaceAnalyzer and SimpleAnalyzer need a Version ctor. Default ctors must be deprecated
0,"Deprecate and replace TestWebapp with the SimpleHttpServer based testing frameworkBasically TestWebapp based testcases test functionality of Tomcat, rather than
that of HttpClient. They tend to get broken with every major release of Tomcat
and have proven more of a burden than any good"
0,"When authentication is invalidated during redirection, proxy authentication also should be invalidatedThis was discovered during use by Lucene Connector Framework, on 3.1.

When a document is fetched through a proxy authenticated with NTLM, and
that document is a redirection (301 or 302), the httpclient fails to
properly use the right proxy credentials on the subsequent document
fetch. This leads to 407 errors on these kinds of documents.

I've attached a proposed patch.
"
1,JcrValueType#typeFromContentType throws IllegalArgumentException for type weak-ref and uri
1,"HttpState cannot differentiate credentials for different hosts with same Realm namesIt seems that one needs a separate HttpState per client per host: from the 
javadocs, if (by coincidence or by design) more than one host uses the same 
realm name, such as ""Private"", then there's an unresolvable conflict, as 
HttpState can only store one set of credentials for a given name...

According to Oleg Kalnichevski, it is plausible just to extend the HttpState 
class with additional methods that would require host to be specified along the
authentication realm when dealing with credentials.

See postings on ""Commons HttpClient Project"" mailing list for more info (dated 
21/03/2003)."
0,"jcr-server: add possibility to PROPFIND the JCR_NODETYPES_CND_LN propertypatch by uwe jaeger see JCR-2454

in didn't include in the resolution of JCR-2454 since i would like to have JCR-2946 fixed as a prerequisite."
0,"BooleanQuery add public method that returns number of clauses this queryBooleanQuery add public method getClausesCount() that returns number of clauses this query.

current ways of getting clauses count are:
1).
 int clausesCount  = booleanQuery.getClauses().length;

or 

"
0,"non mantatory revision property in the Journal configurationAn exception is thrown if property 'revision' is not set. I think it would be great to save the revision file in the repository home dir 
when the property is not set."
1,"Incorrect SegmentInfo.delCount when IndexReader.flush() is usedWhen deleted documents are flushed using IndexReader.flush() the delCount in SegmentInfo is updated based on the current value and SegmentReader.pendingDeleteCount (introduced by LUCENE-1267). It seems that pendingDeleteCount is not reset after the commit, which means after a second flush() or close() of an index reader the delCount in SegmentInfo is incorrect. A subsequent IndexReader.open() call will fail with an error when assertions are enabled. E.g.:

java.lang.AssertionError: delete count mismatch: info=3 vs BitVector=2
	at org.apache.lucene.index.SegmentReader.loadDeletedDocs(SegmentReader.java:405)
[...]"
1,"Inconsistency between Session.getProperty and Node.getProperty for binary valuesthere an inconsistency in the binary handling between the batch-reading facility and those cases where a property is directly
accessed without having accessed the parent node before.

this issue came up with timothee maret running into performance issues when retrieving the length of a binary property:

if the property-entry has been created in the run of a batch-read operation the corresponding property-data object
contains internal values that contain the length of the binary (such as transported with the json response) and only
read the data from the server if the value stream is explicitly requested.
however, if the property is accessed directly (e.g. Session.getProperty or Node.getProperty with a relative path) 
a GET request is made to the corresponding dav resource and the stream is read immediately.

possible solution:

if RepositoryService#getItemInfos(SessionInfo, ItemId) is called with a PropertyId the implementation
should not result in a GET request to the corresponding resource by calling super.getPropertyInfo(sessionInfo, (PropertyId) itemId).
instead it should be consistent with the batch-read and only make a PROPFIND request for the property
length. the returned PropertyInfo object would in that case be identical to the one generated by the batch-read functionality.
"
1,"HostConfiguration.setHost(String) causes NullPointerExceptionCalling setHost(String) on a HostConfiguration object causes a null pointer
exception.

As far as I can tell, this is due to it incorrectly calling the deprecated
setHost(String, String, int, Protocol) method, rather than setHost(String, int,
Protocol)

So:

public synchronized void setHost(final String host) {
  Protocol defaultProtocol = Protocol.getProtocol(""http""); 
  setHost(host, null, defaultProtocol.getDefaultPort(), defaultProtocol);
}

should become :

public synchronized void setHost(final String host) {
    Protocol defaultProtocol = Protocol.getProtocol(""http""); 
    setHost(host, defaultProtocol.getDefaultPort(), defaultProtocol);
}"
0,Change log level for text extractor timeoutCurrently an info message is written to the log when a text extractor job times out. Because this may happen quite frequently this should be changed to log level debug.
0,"Remove jcr-commons dependency from jackrabbit-webdavwhile looking at JCR-2896 i just happen to see that jackrabbit-webdav contains a dependency to jcr-commons.
this was never intended to be and i want to get rid of it again... the webdav library should not have any dependency to JCR."
1,LuceneTestCase#newFSDirectoryImpl misses to set LockFactory if ctor call throws exceptionselckin reported on IRC that if you run ant test -Dtestcase=TestLockFactory -Dtestmethod=testNativeFSLockFactoryPrefix -Dtests.directory=FSDirectory the test fails. Since FSDirectory is an abstract class it can not be instantiated so our code falls back to FSDirector.open. yet we miss to set the given lockFactory though.
0,"fix or deprecate TermsEnum.skipToThis method is a trap: it looks legitimate but it has hideously poor performance (simple linear scan implemented in the TermsEnum base class since none of the concrete impls override it with a more efficient implementation).

The least we should do for 2.9 is deprecate the method with  a strong warning about its performance.

See here for background: http://www.lucidimagination.com/search/document/77dc4f8e893d3cf3/possible_terminfosreader_speedup

And, here for historical context: 

http://www.lucidimagination.com/search/document/88f1b95b404ebf16/remove_termenum_skipto_term_target"
0," The jackrabbit-ocm DTD 1.5 is missing and has to be publish
The jackrabbit-ocm DTD 1.5 is missing and it should be made available for reference on the Jackrabbit web site."
1,"Invalid cookie causing IllegalArgumentExceptionThe bug reported by Oliver Kll <listen at quimby.de> on HttpClient mailing list

<quote>
I'm dealing with a site that serves invalid Cookies in various kind of  
ways. In some cases the Cookie values contain "","" characters, which  
really confuses the Header/Cookie parsers and eventually leads to  
IllegalArgumentExceptions thrown by the Cookie constructor:

java.lang.IllegalArgumentException: Cookie name may not be blank
   at org.apache.commons.httpclient.Cookie.<init>(Cookie.java:142)
   at  
org.apache.commons.httpclient.cookie.CookieSpecBase.parse(CookieSpecBase 
.java:192)
   at  
org.apache.commons.httpclient.cookie.CookieSpecBase.parse(CookieSpecBase 
.java:256)
   at  
org.apache.commons.httpclient.HttpMethodBase.processResponseHeaders(Http 
MethodBase.java:1826)
   at  
org.apache.commons.httpclient.HttpMethodBase.readResponse(HttpMethodBase 
.java:1939)
   at  
org.apache.commons.httpclient.HttpMethodBase.processRequest(HttpMethodBa 
se.java:2631)
   at  
org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java 
:1085)
   at  
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:6 
74)
   at  
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:5 
29)
   at my.code.Test.getHttp(Test.java:114)

What bothers me, is that these IllegalArgumentExceptions are never  
caught in the HttpClient code, making it effectivily impossible to  
handle these responses.

</quote>"
0,"extend ConsistencyChecker API to allow adoption of orphaned nodes to a to-be-specified parent nodeThe optional ConsistencyChecker API on persistence managers allows analyzing and fixing storage inconsistencies. The current fixup code though does not attempt to ""adopt"" orphaned nodes."
0,"contrib.benchmark.quality package improvementsFew fixes and improvements for the search quality benchmark package:
- flush report and logger at the end (otherwise long submission reports might miss last lines).
- add run-tag-name to submission report (API change).
- add control over max-#queries to run (useful at debugging a quality evaluation setup).
- move control over max-docs-to-retrieve from benchmark constructor to a setter method (API change).
- add computation of Mean Reciprocal Rank (MRR) in QualityStats.
- QualityStats fixed to not fail if there are no results to average.
- Add a TREC queries reader adequate for the 1MQ track (track started 2007).

All tests pass, will commit this in 1-2 days if there is no objection.
"
1,"SimpleFSLockFactory ignores error on deleting the lock fileSpinoff from here:

    http://www.gossamer-threads.com/lists/lucene/java-user/54438

The Lock.release for SimpleFSLockFactory ignores the return value of lockFile.delete().  I plan to throw a new LockReleaseFailedException, subclassing from IOException, when this returns false.  This is a very minor change to backwards compatibility because all methods in Lucene that release a lock already throw IOException."
1,"double encoding of URLsIn HttpMethodBase.generateRequestLine(HttpConnection connection, String name, 
String reqPath, String qString, String protocol)

the path is always encoded using URIUtil.encode(reqPath,URIUtil.pathSafe()). 
However, if the path already contains an encoding space, i.e. %20, the % will 
be encoded again, so we get %2520. This behavior is not correct. We shouldn't 
encode any % signs."
0,"Replace NodeReferencesId with NodeIdThe NodeReferencesId class simply wraps a NodeId and forwards all essential method calls to it.

The main (only?) benefit of having NodeReferencesId as a separate class is the ability to distinguish between the overloaded exists() and load() method signatures on PersistenceManager. The downside is the need to instantiate all the NodeReferencesId wrapper objects whenever accessing the references to a node.

I propose to rename the overloaded methods to hasReferencesTo(NodeId) and getReferencesTo(NodeId) and to replace the NodeReferencesId with just the target NodeId wherever used.
"
0,"NodeTypeManagerImpl.hasNodeType should allow unknown prefixesThe current implementation of NodeTypeImpl.hasNodeType(String) throws an exception if the given name uses an unknown prefix. A better alternative would be to just return false, as by definition a node type in an unknown namespace can not exist."
0,"set jcr:encoding when importing files into simple webdav serverattached is a patch that sets the jcr:encoding property when importing files into the simple webdav server. it also strips parameters from the content type before setting the jcr:mimetype property.
"
1,"Workspace.copy(src, dest) throws unexpected RepositoryException (""Invalid path"")when using the davex remoting layer (jcr2spi->spi2davex), 
the following code fragment causes an unexpected RepositoryException:

<snip>
    Node testNode1 = session.getRootNode().addNode(""test"", ""nt:folder"");

    Node copyDestination = testNode1.addNode(""CopyDestination"", ""nt:folder"");
    testNode1.addNode(""CopySource"", ""nt:folder"").addNode(""testCopyCommand"", ""nt:folder"").addNode(""abc"", ""nt:folder"");
    session.save();
    copyDestination.addMixin(""mix:referenceable"");
    session.save();

    session.getWorkspace().copy(""/test/CopySource/testCopyCommand"", ""/test/CopyDestination/testCopyCommand"");
</snip>

==> Caused by: javax.jcr.RepositoryException: Invalid path:/test/CopyDestination//testCopyCommand
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.jackrabbit.spi2dav.ExceptionConverter.generate(ExceptionConverter.java:69)
	at org.apache.jackrabbit.spi2dav.ExceptionConverter.generate(ExceptionConverter.java:51)
	at org.apache.jackrabbit.spi2dav.RepositoryServiceImpl.execute(RepositoryServiceImpl.java:482)
	at org.apache.jackrabbit.spi2dav.RepositoryServiceImpl.copy(RepositoryServiceImpl.java:1307)
	at org.apache.jackrabbit.spi2davex.RepositoryServiceImpl.copy(RepositoryServiceImpl.java:326)
	at org.apache.jackrabbit.jcr2spi.WorkspaceManager$OperationVisitorImpl.visit(WorkspaceManager.java:889)
	at org.apache.jackrabbit.jcr2spi.operation.Copy.accept(Copy.java:48)
	at org.apache.jackrabbit.jcr2spi.WorkspaceManager$OperationVisitorImpl.execute(WorkspaceManager.java:848)
	at org.apache.jackrabbit.jcr2spi.WorkspaceManager$OperationVisitorImpl.access$400(WorkspaceManager.java:793)
	at org.apache.jackrabbit.jcr2spi.WorkspaceManager.execute(WorkspaceManager.java:581)
	at org.apache.jackrabbit.jcr2spi.WorkspaceImpl.copy(WorkspaceImpl.java:149)
	[...]  


however, the following slightly altered code fragment works as expected:


<snip>
    Node testNode1 = session.getRootNode().addNode(""test"", ""nt:folder"");
/*
    Node copyDestination = testNode1.addNode(""CopyDestination"", ""nt:folder"");
    testNode1.addNode(""CopySource"", ""nt:folder"").addNode(""testCopyCommand"", ""nt:folder"").addNode(""abc"", ""nt:folder"");
    session.save();
    copyDestination.addMixin(""mix:referenceable"");
    session.save();
*/
    testNode1.addNode(""CopyDestination"", ""nt:folder"").addMixin(NodeType.MIX_REFERENCEABLE);
    Node n = testNode1.addNode(""CopySource"", ""nt:folder"").addNode(""testCopyCommand"", ""nt:folder"").addNode(""abc"", ""nt:folder"");
    session.save();

    session.getWorkspace().copy(""/test/CopySource/testCopyCommand"", ""/test/CopyDestination/testCopyCommand"");
</snip>
"
0,"[API Doc] Compile new preference architecture and HTTP parameterization guideDocument the new preference architecture based on the hierarchy of HttpParams
collections, as well as available parameters and options"
0,"Provide a JackrabbitNode#setMixins(String[] mixinNames) methodassume the following scenario:

- mixin A declares the mandatory property p
- mixin A' extends from A
- node n has mixin A'
- we'd like to migrate/downgrade node n from mixin A' to A

currently there's no easy way of replacing the assigned mixins.

assigning A first results in a NOP since A would be redundant.
removing A' first removes the mandatory property p.

a new method setMixins(String[]) would allow to migrate
a node from mixin A' to A while preserving 'shared' content.
the semantics of setMixins(String[]) would be similar to
Node.setPrimaryType(String)."
1,"DictionaryCompoundWordTokenFilter does not properly add tokens from the end compound word.Due to an off-by-one error, a subword placed at the end of a compound word will not get a token added to the token stream.


For example (from the unit test in the attached patch):
Dictionary: {""ab"", ""cd"", ""ef""}
Input: ""abcdef""
Created tokens: {""abcdef"", ""ab"", ""cd""}
Expected tokens: {""abcdef"", ""ab"", ""cd"", ""ef""}


Additionally, it could produce tokens that were shorter than the minSubwordSize due to another off-by-one error. For example (again, from the attached patch):


Dictionary: {""abc"", ""d"", ""efg""}
Minimum subword length: 2
Input: ""abcdefg""
Created tokens: {""abcdef"", ""abc"", ""d"", ""efg""}
Expected tokens: {""abcdef"", ""abc"", ""efg""}
"
0,"[PATCH] HTMLParser doesn't parse hexadecimal character referencesI recently inherited a project from an ex-colleague; it uses Lucene and in
particular the HTML Parser.  I've found that she had made an amendment to the
parser to allow it to parse and decode hexadecimal character references, which
we depend on, but had not reported a bug.  If she had, someone might have
pointed out that her correction was wrong ...

I don't seem to be able to attach the (fairly trivial) patch to an initial bug
report (and in any case I've failed to find the instructions for generating a
diff file in the right format, even though I'm sure I've seen it somewhere)."
0,"NodeImpl.checkout() calls save() two timesSimilar to JCR-975, The version related properties on a versionable node that is checked out are saved individually. There is no need to save them individually because checkd in node must not have pending changes and save() can be called safely on the node itself."
1,"ParallelReader fails on deletes and on seeks of previously unused fieldsIn using ParallelReader I've hit two bugs:

1.  ParallelReader.doDelete() and doUndeleteAll() call doDelete() and doUndeleteAll() on the subreaders, but these methods do not set hasChanges.  Thus the changes are lost when the readers are closed.  The fix is to call deleteDocument() and undeleteAll() on the subreaders instead.

2.  ParallelReader discovers the fields in each subindex by using IndexReader.getFieldNames() which only finds fields that have occurred on at least one document.  In general a parallel index is designed with assignments of fields to sub-indexes and term seeks (including searches) may be done on any of those fields, even if no documents in a particular state of the index have yet had an assigned field.  Seeks/searches on fields that have not yet been indexed generated an NPE in ParallelReader's various inner class seek() and next() methods because fieldToReader.get() returns null on the unseen field.  The fix is to extend the add() methods to supply the correct list of fields for each subindex.

Patch that corrects both of these issues attached.
"
0,"benchmark for collationSteven Rowe attached a contrib/benchmark-based benchmark for collation (both jdk and icu) under LUCENE-2084, along with some instructions to run it... 

I think it would be a nice if we could turn this into a committable patch and add it to benchmark.
"
1,"Manually set 'Cookie' & 'Authorization' headers get discardedHttpClient discards all the 'Cookie' & 'Authorization' headers including those
manually set when populating request header collection with automatically
generated headers."
1,"NumericUtils.floatToSortableInt/doubleToSortableLong does not sort certain NaN ranges correctly and NumericRangeQuery produces wrong results for NaNs with half-open rangesThe current implementation of floatToSortableInt does not account for different NaN ranges which may result in NaNs sorted before -Infinity and after +Infinity. The default Java ordering is: all NaNs after Infinity.

A possible fix is to make all NaNs canonic ""quiet NaN"" as in:
{code}
// Canonicalize NaN ranges. I assume this check will be faster here than 
// (v == v) == false on the FPU? We don't distinguish between different
// flavors of NaNs here (see http://en.wikipedia.org/wiki/NaN). I guess
// in Java this doesn't matter much anyway.
if ((v & 0x7fffffff) > 0x7f800000) {
  // Apply the logic below to a canonical ""quiet NaN""
  return 0x7fc00000 ^ 0x80000000;
}
{code}

I don't commit because I don't know how much of the existing stuff relies on this (nobody should be keeping different NaNs  in their indexes, but who knows...)."
0,"Unnecessary null check in EffectiveNodeType.getApplicableChildNodeDef()This is just a trivial thing I noticed this while inspecting the code. getApplicableChildNodeDef() says:

        // try named node definitions first
        ItemDef[] defs = getNamedItemDefs(name);
        if (defs != null) {

but getNamedItemDefs() is currently defined to not return null:

    public ItemDef[] getNamedItemDefs(Name name) {
        List defs = (List) namedItemDefs.get(name);
        if (defs == null || defs.size() == 0) {
            return ItemDef.EMPTY_ARRAY;
        }
        return (ItemDef[]) defs.toArray(new ItemDef[defs.size()]);
    }

I didn't check to see if there were any other unnecessary null checks."
1,"MemoryIndex doesn't call TokenStream.reset() and TokenStream.end()MemoryIndex from contrib/memory does not honor the contract for a consumer of a TokenStream

will work up a patch right quick"
1,"SSL Tunneling does not work with MultiThreadedHttpConnectionManagerThe HttpConnection is released prematurely when doing SSL tunneling with the
MultiThreadedHttpConnectionManager.  The ConnectMethod releases the connection
in responseBodyConsumed() before it can be used by the real method."
1,"Invalid query results when using jcr:like with a case transform function and a pattern not starting with a wildcardIf the repository contains nodes with the following value for the property name :
john
JOhn
joe
Joey

and we run the following query :
//element(*, document)/*[jcr:like(fn:lower-case(@name), 'joh%')]"")
then all the previous nodes will match especially the last 2 nodes.

The reason is the use of two range scans from the lucene term index:
..._name_jOH
..................
..._name_joh_

and

..._name_JOH
..................
..._name_Joh_

The first range will contains ..._name_joe property and the second will contains ..._name_Joey.
But the pattern 'joh%' and so the regexp '.*' because of the range scan will match
the substring values of the properties ('' in the first range and 'y' in the second range).

The solution is to use the full pattern (ie 'joh.*') for matching each properties."
0,"Rename ""Version Managers""currently there is a VersionManager interface and VersionManagerImpl class that operate on the version storage.
new for JSR283, the is a javax.jcr.version.VersionManager and its implementation JcrVersionManager.

in order to avoid confusion, i would like to rename the following classes:

o.a.j.core.version.VersionManager - > o.a.j.core.version.InternalVersionManager
o.a.j.core.version.VersionManagerImpl -> o.a.j.core.version.InternalVersionManagerImpl
o.a.j.core.version.AbstractVersionManager -> o.a.j.core.version.AbstractInternalVersionManager
o.a.j.core.version.XAVersionManager -> o.a.j.core.version.XAInternalVersionManager

o.a.j.core.version.JcrVersionManagerImpl -> o.a.j.core.version.VersionManagerImpl"
0,"Add unsigned packed int impls in oal.utilThere are various places in Lucene that could take advantage of an
efficient packed unsigned int/long impl.  EG the terms dict index in
the standard codec in LUCENE-1458 could subsantially reduce it's RAM
usage.  FieldCache.StringIndex could as well.  And I think ""load into
RAM"" codecs like the one in TestExternalCodecs could use this too.

I'm picturing something very basic like:
{code}
interface PackedUnsignedLongs  {
  long get(long index);
  void set(long index, long value);
}
{code}

Plus maybe an iterator for getting and maybe also for setting.  If it
helps, most of the usages of this inside Lucene will be ""write once""
so eg the set could make that an assumption/requirement.

And a factory somewhere:

{code}
  PackedUnsignedLongs create(int count, long maxValue);
{code}

I think we should simply autogen the code (we can start from the
autogen code in LUCENE-1410), or, if there is an good existing impl
that has a compatible license that'd be great.

I don't have time near-term to do this... so if anyone has the itch,
please jump!
"
0,"RAMDirectory not SerializableThe current implementation of RAMDirectory throws a NotSerializableException when trying to serialize, due to the inner class KeySet of HashMap not being serializable (god knows why)

java.io.NotSerializableException: java.util.HashMap$KeySet
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1081)

Caused by line 43:

private Set fileNames = fileMap.keySet();

EDIT:

while we're at it: same goes for inner class Values 

java.io.NotSerializableException: java.util.HashMap$Values
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1081)

Collection files = fileMap.values();
"
0,"rename KeywordMarkerTokenFilterI would like to rename KeywordMarkerTokenFilter to KeywordMarkerFilter.
We havent released it yet, so its a good time to keep the name brief and consistent."
1,"ClassCastExeption when executing union queriesThe XPathQueryBuilder throws a ClassCastException on line 322 in release 0.9 when executing syntactically valid union queries. An example query would be ""//element(*, nt:file) or //element(*, mix:lockable)"". It appears that in the invocation of the visit method the SimpleNode id indicates a type of JJTROOTDESCENDANTS at a certain point but the data is actually an OrQueryNode and hence the cast to a PathQueryNode fails."
0,"Compressed entities are not being cached properlyorg.apache.http.impl.client.cache.CacheValidityPolicy.contentLengthHeaderMatchesActualLength() returns false for entities decompressed by ContentEncodingHttpClient, because the length of decompressed entity stored in cache will be different from the length specified in the response header.
Consequently, gzipped/deflated entities will never be satisfied from the cache.

Proposed fix: introduce new field in HttpCacheEntry() - actualContentLength, and populate it with the actual content length rigth before the cache entry is stored in the cache. Change the org.apache.http.impl.client.cache.CacheValidityPolicy.contentLengthHeaderMatchesActualLength() method to compare
entry.getResource().length() with entry.getActualContentLength()
"
0,"reorder arguments of Field constructor to be more intuitiveI think Field should take (name, value, type) not (name, type, value) ?

This seems more intuitive and consistent with previous releases

Take this change to some code I had for example:
{noformat}
-    d1.add(new Field(""foo"", ""bar"", Field.Store.YES, Field.Index.ANALYZED));
+    d1.add(new Field(""foo"", TextField.TYPE_STORED, ""bar""));
{noformat}

I think it would be better if it was
{noformat}
document.add(new Field(""foo"", ""bar"", TextField.TYPE_STORED));
{noformat}"
0,MatchAllDocsQuery to return all documentsIt would be nice to have a type of query just return all documents from an index.
0,"Absorb NIOFSDirectory into FSDirectoryI think whether one uses java.io.* vs java.nio.* or eventually
java.nio2.*, or some other means, is an under-the-hood implementation
detail of FSDirectory and doesn't merit a whole separate class.

I think FSDirectory should be the core class one uses when one's index
is in the filesystem.

So, I'd like to deprecate NIOFSDirectory, absorbing it into
FSDirectory, and add a setting ""useNIO"" to FSDirectory.  It should
default to ""true"" for non-Windows OSs, because it gives far better
concurrent performance on all platforms but Windows (due to known Sun
JRE issue http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6265734).
"
0,"Make IndexReader.decRef() call refCount.decrementAndGet instead of getAndDecrementIndexReader.decRef() has this code:

{code}
    final int rc = refCount.getAndDecrement();
    if (rc == 1) {
{code}

I think it will be clearer if it was written like this:

{code}
    final int rc = refCount.decrementAndGet();
    if (rc == 0) {
{code}

It's a very simple change, which makes reading the code (at least IMO) easier. Will post a patch shortly."
0,"DbDataStore: improve error message when init failsWhen initialization of the database data store fails, the error message does not
contain enough data to analyze the problem:

Driver: Oracle JDBC driver / 10.2.0.1.0
could not execute statement, reason: ORA-00902: invalid datatype, state/code: 42000/902
Can not init data store, driver=oracle.jdbc.OracleDriver url=jdbc:oracle:thin:@localhost:1521:orcl user=JACKRABBIT

Additionally the create table statement should be logged, and the table name."
0,"All Analysis Consumers should use reusableTokenStreamWith Analyzer now using TokenStreamComponents, theres no reason for Analysis consumers to use tokenStream() (it just gives bad performance).  Consequently all consumers will be moved over to using reusableTokenStream().  The only challenge here is that reusableTokenStream throws an IOException which many consumers are not rigged to deal with.

Once all consumers have been moved, we can rename reusableTokenStream() back to tokenStream()."
1,"deadlock in TestIndexWriterExceptions    [junit] 2012-01-18 18:18:16
    [junit] Full thread dump Java HotSpot(TM) 64-Bit Server VM (19.1-b02 mixed mode):
    [junit] 
    [junit] ""Indexer 3"" prio=10 tid=0x0000000041b9b800 nid=0x6291 waiting for monitor entry [0x00007f7e8868f000]
    [junit]    java.lang.Thread.State: BLOCKED (on object monitor)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.innerPurge(DocumentsWriterFlushQueue.java:118)
    [junit] 	- waiting to lock <0x00000000e40ff2a8> (a org.apache.lucene.index.DocumentsWriterFlushQueue)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.tryPurge(DocumentsWriterFlushQueue.java:141)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:439)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:317)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:390)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)
    [junit] 
    [junit] ""Indexer 2"" prio=10 tid=0x0000000041b9b000 nid=0x6290 waiting on condition [0x00007f7e8838c000]
    [junit]    java.lang.Thread.State: WAITING (parking)
    [junit] 	at sun.misc.Unsafe.park(Native Method)
    [junit] 	- parking to wait for  <0x00000000e4103100> (a org.apache.lucene.index.DocumentsWriterStallControl$Sync)
    [junit] 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireShared(AbstractQueuedSynchronizer.java:941)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireShared(AbstractQueuedSynchronizer.java:1261)
    [junit] 	at org.apache.lucene.index.DocumentsWriterStallControl.waitIfStalled(DocumentsWriterStallControl.java:115)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushControl.waitIfStalled(DocumentsWriterFlushControl.java:591)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.preUpdate(DocumentsWriter.java:302)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:362)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)
    [junit] 
    [junit] ""Indexer 1"" prio=10 tid=0x0000000042500000 nid=0x628f waiting for monitor entry [0x00007f7e8858e000]
    [junit]    java.lang.Thread.State: BLOCKED (on object monitor)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.addSegment(DocumentsWriterFlushQueue.java:84)
    [junit] 	- waiting to lock <0x00000000e40ff2a8> (a org.apache.lucene.index.DocumentsWriterFlushQueue)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:424)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:317)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:390)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)
    [junit] 
    [junit] ""Indexer 0"" prio=10 tid=0x0000000041508000 nid=0x628d waiting on condition [0x00007f7e8848d000]
    [junit]    java.lang.Thread.State: WAITING (parking)
    [junit] 	at sun.misc.Unsafe.park(Native Method)
    [junit] 	- parking to wait for  <0x00000000e414b408> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    [junit] 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:842)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1178)
    [junit] 	at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)
    [junit] 	at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.forcePurge(DocumentsWriterFlushQueue.java:130)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.addDeletesAndPurge(DocumentsWriterFlushQueue.java:50)
    [junit] 	- locked <0x00000000e40ff2a8> (a org.apache.lucene.index.DocumentsWriterFlushQueue)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.applyAllDeletes(DocumentsWriter.java:179)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:460)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:317)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:390)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)
    [junit] 
    [junit] ""Low Memory Detector"" daemon prio=10 tid=0x00007f7e84025800 nid=0x6003 runnable [0x0000000000000000]
    [junit]    java.lang.Thread.State: RUNNABLE
    [junit] 
    [junit] ""CompilerThread1"" daemon prio=10 tid=0x00007f7e84022800 nid=0x6002 waiting on condition [0x0000000000000000]
    [junit]    java.lang.Thread.State: RUNNABLE
    [junit] 
    [junit] ""CompilerThread0"" daemon prio=10 tid=0x00007f7e8401f800 nid=0x6001 waiting on condition [0x0000000000000000]
    [junit]    java.lang.Thread.State: RUNNABLE
    [junit] 
    [junit] ""Signal Dispatcher"" daemon prio=10 tid=0x00007f7e8401d800 nid=0x6000 waiting on condition [0x0000000000000000]
    [junit]    java.lang.Thread.State: RUNNABLE
    [junit] 
    [junit] ""Finalizer"" daemon prio=10 tid=0x00007f7e84001000 nid=0x5ffa in Object.wait() [0x00007f7e8961b000]
    [junit]    java.lang.Thread.State: WAITING (on object monitor)
    [junit] 	at java.lang.Object.wait(Native Method)
    [junit] 	- waiting on <0x00000000e2ece380> (a java.lang.ref.ReferenceQueue$Lock)
    [junit] 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:118)
    [junit] 	- locked <0x00000000e2ece380> (a java.lang.ref.ReferenceQueue$Lock)
    [junit] 	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:134)
    [junit] 	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)
    [junit] 
    [junit] ""Reference Handler"" daemon prio=10 tid=0x000000004101e000 nid=0x5ff9 in Object.wait() [0x00007f7e8971c000]
    [junit]    java.lang.Thread.State: WAITING (on object monitor)
    [junit] 	at java.lang.Object.wait(Native Method)
    [junit] 	- waiting on <0x00000000e2ece318> (a java.lang.ref.Reference$Lock)
    [junit] 	at java.lang.Object.wait(Object.java:485)
    [junit] 	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)
    [junit] 	- locked <0x00000000e2ece318> (a java.lang.ref.Reference$Lock)
    [junit] 
    [junit] ""main"" prio=10 tid=0x0000000040fb2000 nid=0x5fe2 in Object.wait() [0x00007f7e8ecc1000]
    [junit]    java.lang.Thread.State: WAITING (on object monitor)
    [junit] 	at java.lang.Object.wait(Native Method)
    [junit] 	- waiting on <0x00000000e4105080> (a org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread)
    [junit] 	at java.lang.Thread.join(Thread.java:1186)
    [junit] 	- locked <0x00000000e4105080> (a org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread)
    [junit] 	at java.lang.Thread.join(Thread.java:1239)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:286)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit] 	at java.lang.reflect.Method.invoke(Method.java:597)
    [junit] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit] 	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:528)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit] 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit] 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit] 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)
    [junit] 
    [junit] ""VM Thread"" prio=10 tid=0x0000000041017800 nid=0x5fef runnable 
    [junit] 
    [junit] ""GC task thread#0 (ParallelGC)"" prio=10 tid=0x0000000040fc5000 nid=0x5fe3 runnable 
    [junit] 
    [junit] ""GC task thread#1 (ParallelGC)"" prio=10 tid=0x0000000040fc7000 nid=0x5fe4 runnable 
    [junit] 
    [junit] ""GC task thread#2 (ParallelGC)"" prio=10 tid=0x0000000040fc9000 nid=0x5fe5 runnable 
    [junit] 
    [junit] ""GC task thread#3 (ParallelGC)"" prio=10 tid=0x0000000040fca800 nid=0x5fe6 runnable 
    [junit] 
    [junit] ""GC task thread#4 (ParallelGC)"" prio=10 tid=0x0000000040fcc800 nid=0x5fe7 runnable 
    [junit] 
    [junit] ""GC task thread#5 (ParallelGC)"" prio=10 tid=0x0000000040fce800 nid=0x5fe8 runnable 
    [junit] 
    [junit] ""GC task thread#6 (ParallelGC)"" prio=10 tid=0x0000000040fd0000 nid=0x5fe9 runnable 
    [junit] 
    [junit] ""GC task thread#7 (ParallelGC)"" prio=10 tid=0x0000000040fd2000 nid=0x5fea runnable 
    [junit] 
    [junit] ""VM Periodic Task Thread"" prio=10 tid=0x00007f7e84030000 nid=0x6004 waiting on condition 
    [junit] 
    [junit] JNI global references: 1578
    [junit] 
    [junit] 
    [junit] Found one Java-level deadlock:
    [junit] =============================
    [junit] ""Indexer 3"":
    [junit]   waiting to lock monitor 0x0000000041477498 (object 0x00000000e40ff2a8, a org.apache.lucene.index.DocumentsWriterFlushQueue),
    [junit]   which is held by ""Indexer 0""
    [junit] ""Indexer 0"":
    [junit]   waiting for ownable synchronizer 0x00000000e414b408, (a java.util.concurrent.locks.ReentrantLock$NonfairSync),
    [junit]   which is held by ""Indexer 3""
    [junit] 
    [junit] Java stack information for the threads listed above:
    [junit] ===================================================
    [junit] ""Indexer 3"":
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.innerPurge(DocumentsWriterFlushQueue.java:118)
    [junit] 	- waiting to lock <0x00000000e40ff2a8> (a org.apache.lucene.index.DocumentsWriterFlushQueue)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.tryPurge(DocumentsWriterFlushQueue.java:141)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:439)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:317)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:390)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)
    [junit] ""Indexer 0"":
    [junit] 	at sun.misc.Unsafe.park(Native Method)
    [junit] 	- parking to wait for  <0x00000000e414b408> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
    [junit] 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:811)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:842)
    [junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1178)
    [junit] 	at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:186)
    [junit] 	at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:262)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.forcePurge(DocumentsWriterFlushQueue.java:130)
    [junit] 	at org.apache.lucene.index.DocumentsWriterFlushQueue.addDeletesAndPurge(DocumentsWriterFlushQueue.java:50)
    [junit] 	- locked <0x00000000e40ff2a8> (a org.apache.lucene.index.DocumentsWriterFlushQueue)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.applyAllDeletes(DocumentsWriter.java:179)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:460)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:317)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:390)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1534)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1506)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions$IndexerThread.run(TestIndexWriterExceptions.java:187)
    [junit] 
    [junit] Found 1 deadlock.
    [junit] 
    [junit] Heap
    [junit]  PSYoungGen      total 67136K, used 4647K [0x00000000f5560000, 0x00000000fbc60000, 0x0000000100000000)
    [junit]   eden space 65792K, 5% used [0x00000000f5560000,0x00000000f58a5e10,0x00000000f95a0000)
    [junit]   from space 1344K, 96% used [0x00000000f9740000,0x00000000f98840a0,0x00000000f9890000)
    [junit]   to   space 19840K, 0% used [0x00000000fa900000,0x00000000fa900000,0x00000000fbc60000)
    [junit]  PSOldGen        total 171392K, used 66868K [0x00000000e0000000, 0x00000000ea760000, 0x00000000f5560000)
    [junit]   object space 171392K, 39% used [0x00000000e0000000,0x00000000e414d080,0x00000000ea760000)
    [junit]  PSPermGen       total 21248K, used 14733K [0x00000000dae00000, 0x00000000dc2c0000, 0x00000000e0000000)
    [junit]   object space 21248K, 69% used [0x00000000dae00000,0x00000000dbc635a8,0x00000000dc2c0000)
    [junit] 
"
0,"CompactNodeTypeDefReader could auto-provide default namespace mappings if omittedthe default namespaces for 'jcr', 'nt', 'mix', '' should  be automatically registered during parsing if needed."
0,"Reducing buffer sizes for TermDocs.From java-dev: 
 
On Friday 09 September 2005 00:34, Doug Cutting wrote: 
> Paul Elschot wrote: 
> > I suppose one of these cases are when many terms are used in a query.  
> > Would it be easily possible to make the buffer size for a term iterator 
> > depend on the numbers of documents to be iterated? 
> > Many terms only occur in a few documents, so this could be a  
> > nice win on total buffer size for the many terms case. 
>  
> This would not be too difficult. 
>  
> Look in SegmentTermDocs.java. The buffer may be allocated when the  
> parent's stream is first cloned, but clone() won't allocate a buffer if  
> the source hasn't had a buffer allocated yet, and nothing should perform  
> i/o directly on the parent's freqStream, so in practice a buffer should  
> not be allocated until the first read is performed on the clone. 
 
I tried delaying the buffer allocation in BufferedIndexInput by 
using this clone() method: 
 
 public Object clone() { 
  BufferedIndexInput clone = (BufferedIndexInput)super.clone(); 
  clone.buffer = null; 
  clone.bufferLength = 0; 
  clone.bufferPosition = 0; 
  clone.bufferStart = getFilePointer();  
  return clone; 
 } 
 
With this all term document iterators seem to be empty, no 
query in the test cases gives any results, for example TestDemo 
and TestBoolean2. 
As far as I can see, this delaying should work, but it doesn't and 
I have no idea why. 
 
End of quote from java-dev. 
 
Doug replied that at a glance this clone method looks good. 
Without this delayed buffer allocation, a reduced buffer size 
for TermDocs cannot be implemented easily."
0,"Use parallel arrays instead of PostingList objectsThis is Mike's idea that was discussed in LUCENE-2293 and LUCENE-2324.

In order to avoid having very many long-living PostingList objects in TermsHashPerField we want to switch to parallel arrays.  The termsHash will simply be a int[] which maps each term to dense termIDs.

All data that the PostingList classes currently hold will then we placed in parallel arrays, where the termID is the index into the arrays.  This will avoid the need for object pooling, will remove the overhead of object initialization and garbage collection.  Especially garbage collection should benefit significantly when the JVM runs out of memory, because in such a situation the gc mark times can get very long if there is a big number of long-living objects in memory.

Another benefit could be to build more efficient TermVectors.  We could avoid the need of having to store the term string per document in the TermVector.  Instead we could just store the segment-wide termIDs.  This would reduce the size and also make it easier to implement efficient algorithms that use TermVectors, because no term mapping across documents in a segment would be necessary.  Though this improvement we can make with a separate jira issue."
1,"[patch] QValueFactoryImpl.equals doesn't do compare correctlyequals compares it's uri to it's own uri, as poosed to the other one.

                 // for both the value has not been loaded yet
                 if (!initialized) {
                     if (other.uri != null) {
-                        return uri.equals(uri);
+                        return other.uri.equals(uri);
                     } else {
                         // need to load the binary value in order to be able
                         // to compare the 2 values."
0,"DocumentsWriter.applyDeletes should not create TermDocs or IndexSearcher if not neededDocumentsWriter.applyDeletes(IndexReader, int) always creates TermDocs and IndexSearcher, even if there were no deletes by Term or by Query. The attached patch wraps those creations w/ checks on whether there were any deletes by these two. Additionally, the searcher wasn't closed in a finally block, so I fixed that as well.

I'll attach a patch shortly."
0,Move FilterIterator and SizedIterator from package flat to package iteratorI suggest to move said classes from package org.apache.jackrabbit.commons.flat to package org.apache.jackrabbit.commons.iterator. 
0,"Extend Codec to handle also stored fields and term vectorsCurrently Codec API handles only writing/reading of term-related data, while stored fields data and term frequency vector data writing/reading is handled elsewhere.

I propose to extend the Codec API to handle this data as well."
1,"NTLM authentication failed due to closing of connectionDescription:

When dealing with a NTLM proxy server that sends response back with lines:

14:51:27:750 << HTTP/1.0 407 Proxy Authentication Required
14:51:27:796 << Date: Mon, 14 Apr 2003 19:52:43GMT[\r][\n]
14:51:27:796 << Content-Length: 257[\r][\n]
14:51:27:796 << Content-Type: text/html[\r][\n]
14:51:27:796 << Server: NetCache appliance (NetApp/5.3.1R1)[\r][\n]
14:51:27:796 << Connection: keep-alive[\r][\n]
14:51:27:796 << Proxy-Authenticate: NTLM 
TlRMTVNTUAACAAAABgAGACgAAAAGggEAtOoNy4M0g0EAAAAAAAAAAEdMT0JBTA==[\r][\n]

The httpClient code is using the ""HTTP/1.0"" as clue for closing the connection 
and ignored the ""Connection: keep-alive"".  That caused the NTLM authentication 
to fail as the NTLM requires the response to the challenge to be sent back on 
the same connection.

Proposed Fix:

Our fix is to add a flag inProxyAuthenticationRetry (in HttpMethodBase) to 
indicate that the method is doing proxy authentication retry.  When the flag is 
true, in ""HttpMethodBase.shouldCloseConnection"", check the ""Connection: keep-
alive"" before determining to close the connection."
0,"New utility: Journal walker for journal filesGiven the cluster record access provided by JCR-1789, add a journal walker utiltity that provides descriptive information about the contents of journal records."
1,"FieldCache keeps hard references to readers, doesn't prevent multiple threads from creating same instance"
1,"SQL2 parser: identifiers should be case sensitiveCurrently the SQL2 parser converts the query to uppercase before parsing. However the identifiers should be kept case sensitive.

Instead of converting the query to uppercase, String.equalsIgnoreCase should be used to compare against keywords.
"
1,webapp: troubleshooting.jsp fails
1,"IndexingQueue not checked on initial index creationWith a default value of 100 for extractorBackLogSize and lots of text extractions that time out, the temp folder gets filled with extractor*.tmp files. This is because the IndexingQueue.getFinishedDocuments() is not called during the initial index creation. This is not an issue during regular operation because the method is called periodically from a timer thread."
0,"Remove duplicate code in InternalValueFactoryAfter JCR-2245 has been applied some of the duplicate code in InternaValueFactory can be removed.

Namely:
- create(String, int)
- all other create methods that are non-binary ?"
0,"QueryManagerImpl hardwires supported query languagesQueryManagerImpl hardwires supported query languages. This seems to be sub-optimal, given the fact that Jackrabbit has a pluggable architecture for additional query languages.
"
1,"Lucene RAM Directory doesn't work for Index Size > 8 GBfrom user list - http://www.gossamer-threads.com/lists/lucene/java-user/50982

Problem seems to be casting issues in RAMInputStream.

Line 90:
      bufferStart = BUFFER_SIZE * currentBufferIndex;
both rhs are ints while lhs is long.
so a very large product would first overflow MAX_INT, become negative, and only then (auto) casted to long, but this is too late. 

Line 91: 
     bufferLength = (int) (length - bufferStart);
both rhs are longs while lhs is int.
so the (int) cast result may turn negative and the logic that follows would be wrong.
"
1,"Warning while building DAV:parent-set for root-node resourcethe following warning is generated when calculating DAV:parent-set for the resource representing the root-node:

05.12.2008 11:49:50 *WARN * DavResourceImpl: unable to calculate parent set (DavResourceImpl.java, line 955)
javax.jcr.ItemNotFoundException: root node doesn't have a parent
        at org.apache.jackrabbit.core.NodeImpl.getParent(NodeImpl.java:2078)
        at org.apache.jackrabbit.webdav.simple.DavResourceImpl.getParentElements(DavResourceImpl.java:949)
        at org.apache.jackrabbit.webdav.simple.DavResourceImpl.initProperties(DavResourceImpl.java:393)

this could simply be avoided by slightly modifying the method getParentElements (starting at line 937) and adding a test asserting that node.getParent() is not called for the root node.

"
1,"memory leak in MultiThreadedHttpConnectionManagerMultiThreadedHttpConnectionManager.getConnectionsInPool(hostConfiguration) will create HostConnectionPool entries that will not be cleaned up unless they are later used for communication. This should be changed to not create pools in that method, but rather return 0 for a non-existent pool.
"
0,"Next steps towards flexible indexingIn working on LUCENE-1410 (PFOR compression) I tried to prototype
switching the postings files to use PFOR instead of vInts for
encoding.

But it quickly became difficult.  EG we currently mux the skip data
into the .frq file, which messes up the int blocks.  We inline
payloads with positions which would also mess up the int blocks.
Skipping offsets and TermInfo offsets hardwire the file pointers of
frq & prox files yet I need to change these to block + offset, etc.

Separately this thread also started up, on how to customize how Lucene
stores positional information in the index:

  http://www.gossamer-threads.com/lists/lucene/java-user/66264

So I decided to make a bit more progress towards ""flexible indexing""
by first modularizing/isolating the classes that actually write the
index format.  The idea is to capture the logic of each (terms, freq,
positions/payloads) into separate interfaces and switch the flushing
of a new segment as well as writing the segment during merging to use
the same APIs.
"
0,Allow pseudo properties in query relationThe XPath query parser does not allow using a function name as part of a relation in a query.
0,"Typo in repository.xmlThere's another typo in repository.xml. This is basically the same as JCR-1460, but for the SearchIndex element at the bottom of the repository.xml."
0,"TCK: SetPropertyCalendarTest compares Calendar objectsSetPropertyCalendarTest# testNewCalendarPropertySession
SetPropertyCalendarTest# testModifyCalendarPropertySession
SetPropertyCalendarTest# testNewCalendarPropertyParent
SetPropertyCalendarTest# testModifyCalendarPropertyParent

Tests compare Calendar objects.  Calendar.equals(Object) is a stronger test than JSR-170 specifies for Value.equals(Object), leading to false failures.  For the purpose of these tests, even Value.equals(Object) is too strong an equality test, since some repositories may normalize date/time values across a save/read roundtrip (for example, converting ""Z"" to ""+00:00"", or adding/removing trailing zeros in fractional seconds).

Proposal: compare the getTimeInMillis() values.

--- SetPropertyCalendarTest.java        (revision 422074)
+++ SetPropertyCalendarTest.java        (working copy)
@@ -52,8 +52,8 @@
         testNode.setProperty(propertyName1, c1);
         superuser.save();
         assertEquals(""Setting property with Node.setProperty(String, Calendar) and Session.save() not working"",
-                c1,
-                testNode.getProperty(propertyName1).getDate());
+                c1.getTimeInMillis(),
+                testNode.getProperty(propertyName1).getDate().getTimeInMillis());
     }
  
     /**
@@ -66,8 +66,8 @@
         testNode.setProperty(propertyName1, c2);
         superuser.save();
         assertEquals(""Modifying property with Node.setProperty(String, Calendar) and Session.save() not working"",
-                c2,
-                testNode.getProperty(propertyName1).getDate());
+                c2.getTimeInMillis(),
+                testNode.getProperty(propertyName1).getDate().getTimeInMillis());
     }
  
     /**
@@ -78,8 +78,8 @@
         testNode.setProperty(propertyName1, c1);
         testRootNode.save();
         assertEquals(""Setting property with Node.setProperty(String, Calendar) and parentNode.save() not working"",
-                c1,
-                testNode.getProperty(propertyName1).getDate());
+                c1.getTimeInMillis(),
+                testNode.getProperty(propertyName1).getDate().getTimeInMillis());
     }
  
     /**
@@ -92,8 +92,8 @@
         testNode.setProperty(propertyName1, c2);
         testRootNode.save();
         assertEquals(""Modifying property with Node.setProperty(String, Calendar) and parentNode.save() not working"",
-                c2,
-                testNode.getProperty(propertyName1).getDate());
+                c2.getTimeInMillis(),
+                testNode.getProperty(propertyName1).getDate().getTimeInMillis());
     }
"
0,"Log at debug level rather that info in CacheManagerPlease change org.apache.jackrabbit.core.state.CacheManager#resizeAll to log at DEBUG level rather that INFO.
15:50:01,058 INFO  [CacheManager] resizeAll size=8

--- jackrabbit-core/src/main/java/org/apache/jackrabbit/core/state/CacheManager.java    (revision 565102)
+++ jackrabbit-core/src/main/java/org/apache/jackrabbit/core/state/CacheManager.java    (working copy)
@@ -122,7 +122,7 @@
      * Re-calcualte the maximum memory for each cache, and set the new limits.
      */
     private void resizeAll() {
-        log.info(""resizeAll size="" + caches.size());
+        log.debug(""resizeAll size="" + caches.size());
         // get strong references
         // entries in a weak hash map may disappear any time
         // so can't use size() / keySet() directly

"
1,"max connections per host setting does not workWhen using the MultiThreadedHttpConnectionManager the default maximal
connections per host/port cannot be exceeded (allowed maximum is 2 by default).
Attempts to exceed this by manually setting the max connections using
HttpConnectionManagerParams#setMaxConnectionsPerHost fail. This is caused by a
bug in the MultiThreadedHttpConnectionManager."
1,"More query classes with missing extractTerms()The query classes DerefQuery, RangeQuery and WildcardQuery do not overwrite the method extractTerms()."
0,"Referenced derby library behaves buggy on FreeBSDThe derby library referenced on dependencies page (http://jackrabbit.apache.org/dependencies.html) behaves buggy on FreeBSD. This is the same issue like the one with Magnolia CMS: http://jira.magnolia.info/browse/MAGNOLIA-818. Version derby-10.1.3.1 works fine.
"
0,"Remove unused ""numSlotsFull"" from FieldComparator.setNextReaderThis param is a relic from older optimizations that we've since turned off, and it's quite confusing.  I don't think we need it, and we haven't released the API yet so we're free to remove it now."
0,jcr mapping layer does not expose node move and node copy via PersistenceManager.javaThe PersistenceManagerImpl.java  in jcr-apping layer does not implement move and copy methods for a node.  
1,Scorer.skipTo() doesn't always work if called before next()skipTo() doesn't work for all scorers if called before next().
0,"A replacement for AsciiFoldingFilter that does a more thorough job of removing diacritical marks or non-spacing modifiers.The ISOLatin1AccentFilter takes Unicode characters that have diacritical marks and replaces them with a version of that character with the diacritical mark removed.  For example  becomes e.  However another equally valid way of representing an accented character in Unicode is to have the unaccented character followed by a non-spacing modifier character (like this:    )    The ISOLatin1AccentFilter doesn't handle the accents in decomposed unicode characters at all.    Additionally there are some instances where a word will contain what looks like an accented character, that is actually considered to be a separate unaccented character  such as    but which to make searching easier you want to fold onto the latin1  lookalike  version   L  .   

The UnicodeNormalizationFilter can filter out accents and diacritical marks whether they occur as composed characters or decomposed characters, it can also handle cases where as described above characters that look like they have diacritics (but don't) are to be folded onto the letter that they look like (   -> L )"
0,"Inefficient growth of OpenBitSetHi, I found a potentially serious efficiency problem with OpenBitSet.

One typical (I think) way to build a bit set is to set() the bits one by one -
e.g., have a HitCollector set() the bit for each matching document.
The underlying array of longs needs to grow as more as more bits are set, of
course.

But looking at the code, it appears to me that the array grows very
ineefficiently - in the worst case (when doc ids are sorted, as they would
normally be in the HitCollector case for example), copying the array again
and again for every added bit... The relevant code in OpenBitSet.java is:

  public void set(long index) {
    int wordNum = expandingWordNum(index);
    ...
  }

  protected int expandingWordNum(long index) {
    int wordNum = (int)(index >> 6);
    if (wordNum>=wlen) {
      ensureCapacity(index+1);
    ...
  }
  public void ensureCapacityWords(int numWords) {
    if (bits.length < numWords) {
      long[] newBits = new long[numWords];
      System.arraycopy(bits,0,newBits,0,wlen);
      bits = newBits;
    }
  }

As you can see, if the bits array is not long enough, a new one is
allocated at exactly the right size - and in the worst case it can grow
just one word every time...

Shouldn't the growth be more exponential in nature, e.g., grow to the maximum
of index+1 and twice the existing size?

Alternatively, if the growth is so inefficient, this should be documented,
and it should be recommended to use the variant of the constructor with the
correct initial size (e.g., in the HitCollector case, the number of documents
in the index). and the fastSet() method instead of set().

Thanks,
Nadav.
"
1,"Node.setProperty(String, String) does not convert valueswhen setting the value of a defined property via the Node.setProperty(String, String) method, a ConstraintViolationException is thrown. but the string value should be converted, or a ValueFormatException must be thrown.

"
1,"Long values not properly storedWhen a long value assigned to a property is too big, when restarting the server the value become 0 !! 

The test pass with versions 1.6.4 and 2.0"
0,Performance improvement in OpenBitSetDISI.inPlaceAnd()
0,"Create merge policy that doesn't periodically inadvertently optimizeThe current merge policy, at every maxBufferedDocs *
power-of-mergeFactor docs added, will do a fully cascaded merge, which
is the same as an optimize.

I think this is not good because at that ""optimization poin"", the
particular addDocument call is [surprisingly] very expensive.  While,
amortized over all addDocument calls, the cost is low, the cost is
paid ""up front"" and in a very ""bunched up"" manner.

I think of this as ""pay it forward"": you are paying the full cost of
an optimize right now on the expectation / hope that you will be
adding a great many more docs.  But, if you don't add that many more
docs, then, the amortized cost for your index is in fact far higher
than it should have been.  Better to ""pay as you go"" instead.

So we could make a small change to the policy by only merging the
first mergeFactor segments once we hit 2X the merge factor.  With
mergeFactor=10, when we have created the 20th level 0 (just flushed)
segment, we merge the first 10 into a level 1 segment.  Then on
creating another 10 level 0 segments, we merge the second set of 10
level 0 segments into a level 1 segment, etc.

With this new merge policy, an index that's a bit bigger than a
current ""optimization point"" would then have a lower amortized cost
per document.  Plus the merge cost is less ""bunched up"" and less ""pay
it forward"": instead you pay for what you are actually using.

We can start by creating this merge policy (probably, combined with
with the ""by size not by doc count"" segment level computation from
LUCENE-845) and then later decide whether we should make it the
default merge policy.
"
0,Speed up NodeIndexer.isIndexed() checkThe isIndexed() method is called for every value in a multi-valued property. This may be quite expensive when there are a lot of values.
1,WebDAV: LocatorFactoryImplEx doesn't properly evaluate resource path...... if the item path happens to start with the workspace name.
0,Incorrect link on Apache Jackrabbit Welcome homepageLink to JSR283 in welcome text points to http://jcp.org/en/jsr/detail?id=170 instead of http://jcp.org/en/jsr/detail?id=283
1,"Avoid exceptions during shutting repository down if several PMs/FSs use same DBAccording to docs and forum discussions, it's legal to use same DB for different FileSystems/Persistence Managers. Such configurations seem to work fine, but when repository is stopped, exceptions are produced like following:

SEVERE: Error while closing Version Manager.
java.sql.SQLNonTransientConnectionException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.getMetaData(Unknown Source)
	at org.apache.jackrabbit.core.persistence.db.DerbyPersistenceManager.closeConnection(DerbyPersistenceManager.java:109)
	at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.close(DatabasePersistenceManager.java:261)
	at org.apache.jackrabbit.core.version.VersionManagerImpl.close(VersionManagerImpl.java:201)
	at org.apache.jackrabbit.core.RepositoryImpl.doShutdown(RepositoryImpl.java:1000)
	at org.apache.jackrabbit.core.RepositoryImpl.shutdown(RepositoryImpl.java:948)
	at org.apache.jackrabbit.core.TransientRepository.stopRepository(TransientRepository.java:275)
	at org.apache.jackrabbit.core.TransientRepository.loggedOut(TransientRepository.java:427)
	at org.apache.jackrabbit.core.SessionImpl.notifyLoggedOut(SessionImpl.java:574)
	at org.apache.jackrabbit.core.SessionImpl.logout(SessionImpl.java:1247)
	at org.apache.jackrabbit.core.XASessionImpl.logout(XASessionImpl.java:403)
	at com.blandware.tooling.jcrplugin.ExportMojo.execute(ExportMojo.java:81)
	at org.apache.maven.plugin.DefaultPluginManager.executeMojo(DefaultPluginManager.java:447)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoals(DefaultLifecycleExecutor.java:539)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeStandaloneGoal(DefaultLifecycleExecutor.java:493)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoal(DefaultLifecycleExecutor.java:463)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoalAndHandleFailures(DefaultLifecycleExecutor.java:311)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeTaskSegments(DefaultLifecycleExecutor.java:278)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.execute(DefaultLifecycleExecutor.java:143)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:333)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:126)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:282)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.classworlds.Launcher.launchEnhanced(Launcher.java:315)
	at org.codehaus.classworlds.Launcher.launch(Launcher.java:255)
	at org.codehaus.classworlds.Launcher.mainWithExitCode(Launcher.java:430)
	at org.codehaus.classworlds.Launcher.main(Launcher.java:375)
Caused by: java.sql.SQLException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 35 more
"
0,"Remove or deprecate contrib/similarityClasses under contrib/similarity seem to be duplicates of classes under contrib/queries.
I'd like to remove *.java from contrib/similarity without bothering with deprecation, since the same functionality exists in contrib/queries.
Anyone minds?
"
0,Make it possible to configure Lucene Analyzer for SearchIndexJackrabbit does not support the specification of a Lucene analyzer class (org.apache.lucene.analysis.Analyzer) to be used by a SearchIndex (or other Lucene based indices) declared in an XML configuration file. Custom analyzer is useful for indexing language specific content.
1,"BooleanFilter changed behavior in 3.5, no longer acts as if ""minimum should match"" set to 1The change LUCENE-3446 causes a change in behavior in BooleanFilter. It used to work as if minimum should match clauses is 1 (compared to BQ lingo), but now, if no should clauses match, then the should clauses are ignored, and for example, if there is a must clause, only that one will be used and returned.

For example, a single must clause and should clause, with the should clause not matching anything, should not match anything, but, it will match whatever the must clause matches.

The fix is simple, after iterating over the should clauses, if the aggregated bitset is null, return null."
0,"NumericField should be stored in binary format in index (matching Solr's format)(Spinoff of LUCENE-3001)

Today when writing stored fields we don't record that the field was a NumericField, and so at IndexReader time you get back an ""ordinary"" Field and your number has turned into a string.  See https://issues.apache.org/jira/browse/LUCENE-1701?focusedCommentId=12721972&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-12721972

We have spare bits already in stored fields, so, we should use one to record that the field is numeric, and then encode the numeric field in Solr's more-compact binary format.

A nice side-effect is we fix the long standing issue that you don't get a NumericField back when loading your document."
1,"InternalXAResource.rollback() can be called twice and without prepareduring the prepare phase in the transaction context, each resource is 'prepared'. if one of them fails to prepare, the rest is rolledback, and later all of them are rolledback again. this can cause that:
- a resource that is never prepared is rolled back (which is ok, since is may need to cleanup stuff)
- a resource's rollback() may be called twice (which i don't know, if it's ok)

however, some of the resources are buggy and can't handle neither case correctly."
1,"Exception during IndexWriter.close() prevents release of the write.lockAfter encountering a case of index corruption - see http://issues.apache.org/jira/browse/LUCENE-140 - when the close() method encounters an exception in the flushRamSegments() method, the index write.lock is not released (ie. it is not really closed).

The writelock is only released when the IndexWriter is GC'd and finalize() is called."
0,provide a memcached implementation for HttpCacheThe feature here would be an implementation of the HttpCache interface that stored cache entries in memcached.
1,document field lengths count analyzer synonym overlaysUsing a synonym expansion analyzer to add tokens with zero offset from the substituted token should not extend the length of the field in the document (for scoring purposes)
1,SpanScorer fails when sloppyFreq() returns 0I think we should fix this for 2.4 (now back to 10)?
0,"Unnecessary hasItemState() call in SessionItemStateManagerAt the end of  SessionItemStateManager.getItemState(ItemId) the underlying item state manager is first asked whether it contains the item and then it is retrieved. In case the item state manager does not know the item a NoSuchItemStateException is thrown.

The initial check is unnecessary because getItemState() on the underlying manager will also throw the exception if the item does not exist."
1,"BlockJoinCollector only allows retrieving groups for only one BlockJoinQuerySpinoff from Mark Harwood's email (subject ""BlockJoin concerns"") to
dev list.

It's fine to use multiple nested joins in a single query, and
BlockJoinCollector should let you retrieve the top groups for all of
them.

But currently it always returns null after the first query's groups
have been retrieved, because of a silly bug.
"
1,"NodeReferencesId.equals() is not symetricNodeReferencesId.equals() is not symetric when equality is tested against a NodeId.

Code example:
UUID uuid = UUID.randomUUID();
NodeId id = new NodeId(uuid);
NodeReferencesId refId = new NodeReferencesId(uuid);
id.equals(refId); // will return true
refId.equals(id); // will return false

NodeReferencesId should be decouled from the ItemId hierarchy. The class NodeReferences already does not extend from NodeState which makes perfectly sense. So, the same should apply to the identifier of NodeReferences.

The attached patch to NodeReferencesId also requires minor changes to some of the persistence managers."
0,"Fix PayloadProcessorProvider to no longer use Directory for lookup, instead AtomicReaderThe PayloadProcessorProvider has a broken API, this should be fixed. The current trunk mimics the old behaviour, but not 100%.

The PayloadProcessorProvider API should return a PayloadProcessor based on the AtomicReader instance that gets merged. As AtomicReader do no longer know the directory they are reside (they could be e.g. FilterIndexReaders, MemoryIndexes,...) a selection by Directory is no longer possible.

The current code in Lucene trunk mimics the old behavior by doing an instanceof SegmentReader check and then asking for a DirProvider. If something else is merged in, Payload processing is not supported. This should be changed, the old API could be kept backwards compatible by moving the instanceof check in a ""convenience class"" DirPayloadProcessorProvider, extending PayloadProcessorProvider."
0,Text extraction may congest thread pool in the repositoryText extraction congests the thread pool in the repository when e.g. many PDFs are loaded into the workspace. Tasks submitted by the index merger are delayed because of that and will result in many index segment folders.
0,"Add SimpleText codecInspired by Sahin Buyrukbilen's question here:

  http://www.lucidimagination.com/search/document/b68846e383824653/how_to_export_lucene_index_to_a_simple_text_file#b68846e383824653

I made a simple read/write codec that stores all postings data into a
single text file (_X.pst), looking like this:

{noformat}
field contents
  term file
    doc 0
      pos 5
  term is
    doc 0
      pos 1
  term second
    doc 0
      pos 3
  term test
    doc 0
      pos 4
  term the
    doc 0
      pos 2
  term this
    doc 0
      pos 0
END
{noformat}

The codec is fully funtional -- all Lucene & Solr tests pass with
-Dtests.codec=SimpleText -- but, its performance is obviously poor.

However, it should be useful for debugging, transparency,
understanding just what Lucene stores in its index, etc.  And it's a
quick way to gain some understanding on how a codec works...
"
0,"Separate SegmentReaders (and other atomic readers) from composite IndexReadersWith current trunk, whenever you open an IndexReader on a directory you get back a DirectoryReader which is a composite reader. The interface of IndexReader has now lots of methods that simply throw UOE (in fact more than 50% of all methods that are commonly used ones are unuseable now). This confuses users and makes the API hard to understand.

This issue should split ""atomic readers"" from ""reader collections"" with a separate API. After that, you are no longer able, to get TermsEnum without wrapping from those composite readers. We currently have helper classes for wrapping (SlowMultiReaderWrapper - please rename, the name is really ugly; or Multi*), those should be retrofitted to implement the correct classes (SlowMultiReaderWrapper would be an atomic reader but takes a composite reader as ctor param, maybe it could also simply take a List<AtomicReader>). In my opinion, maybe composite readers could implement some collection APIs and also have the ReaderUtil method directly built in (possibly as a ""view"" in the util.Collection sense). In general composite readers do not really need to look like the previous IndexReaders, they could simply be a ""collection"" of SegmentReaders with some functionality like reopen.

On the other side, atomic readers do not need reopen logic anymore? When a segment changes, you need a new atomic reader? - maybe because of deletions thats not the best idea, but we should investigate. Maybe make the whole reopen logic simplier to use (ast least on the collection reader level).

We should decide about good names, i have no preference at the moment."
0,"Indexing performance tests with realtime branchWe should run indexing performance tests with the DWPT changes and compare to trunk.

We need to test both single-threaded and multi-threaded performance.

NOTE:  flush by RAM isn't implemented just yet, so either we wait with the tests or flush by doc count."
0,"Reduce memory usage of transient nodesWhen adding lots of transient nodes, most of them don't have child nodes because they are leafs. The attached patch initializes NodeState.childNodeEntries with an unmodifiable empty ChildNodeEntries instance and turns it into a modifiable one only when needed.

Running a test with 100k nodes (10 children per node) the memory consumption for child node entries drops from 42MB to 12MB with this patch."
0,"Redesign SPI observationWith the current SPI observation design it may happen that events are lost while the filter for an event listener is changed.

See:
- http://www.nabble.com/SPI-observation%3A-EventFilter-lifecycle-tf4732281.html
- http://people.apache.org/~mreutegg/spi-event/problem.png

My proposal is to introduce a Subscription interface. See attached patch and:
http://people.apache.org/~mreutegg/spi-event/proposal.png"
1,"UserManagement: IndexNodeResolver.findNodes(..... , nonExact) fails to find values containing backslash"
0,"remove IndexSearcher.closeNow that IS is never ""heavy"" (since you have to pass in your own IR), IS.close is truly a no-op... I think we should remove it.

"
0,Query Stats should use the TimeSeries mechanismRefactor the Query Stats to use TimeSeries for the average query duration.
0,Highlighter Documentation updatesVarious places in the Highlighter documentation refer to bytes (i.e. SimpleFragmenter) when it should be chars.  See http://www.gossamer-threads.com/lists/lucene/java-user/56986
1,"DatabaseJournal.checkSchema generates ""Cannot call commit when autocommit=true"" ExceptionWhen I tried to activate clustering with adding the following XML to repository.xml, 

    <Cluster id=""node_1"" syncDelay=""5"">
		<Journal class=""org.apache.jackrabbit.core.journal.DatabaseJournal"">
			<param name=""revision"" value=""${rep.home}/revision""/>
			<param name=""driver"" value=""com.mysql.jdbc.Driver""/>
			<param name=""url"" value=""jdbc:mysql://localhost/jcr""/>
			<param name=""user"" value=""userX""/>
			<param name=""password"" value=""passWordC""/>
			<param name=""schema"" value=""mysql""/>
			<param name=""schemaObjectPrefix"" value=""J_C_""/>
		</Journal>
    </Cluster>

Databse Journal threw the following exception:

....
Caused by: javax.jcr.RepositoryException: Unable to initialize connection.: Unable to initialize connection.
        at org.apache.jackrabbit.core.RepositoryImpl.createClusterNode(RepositoryImpl.java:677)
        at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:276)
        at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:584)
        at org.apache.jackrabbit.core.TransientRepository$2.getRepository(TransientRepository.java:245)
        at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:265)
        at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:333)
        at com.liferay.portal.jcr.jackrabbit.JCRFactoryImpl.createSession(JCRFactoryImpl.java:71)
        at com.liferay.portal.jcr.JCRFactoryUtil.createSession(JCRFactoryUtil.java:53)
        at com.liferay.portal.jcr.JCRFactoryUtil.createSession(JCRFactoryUtil.java:57)
        at com.liferay.documentlibrary.util.IndexerImpl.reIndex(IndexerImpl.java:258)
        ... 17 more
Caused by: org.apache.jackrabbit.core.cluster.ClusterException: Unable to initialize connection.
        at org.apache.jackrabbit.core.cluster.ClusterNode.init(ClusterNode.java:218)
        at org.apache.jackrabbit.core.cluster.ClusterNode.init(ClusterNode.java:189)
        at org.apache.jackrabbit.core.RepositoryImpl.createClusterNode(RepositoryImpl.java:674)
        ... 26 more
Caused by: java.sql.SQLException: Can't call commit when autocommit=true
        at com.mysql.jdbc.Connection.commit(Connection.java:2161)
        at org.apache.jackrabbit.core.journal.DatabaseJournal.checkSchema(DatabaseJournal.java:437)
        at org.apache.jackrabbit.core.journal.DatabaseJournal.init(DatabaseJournal.java:168)
        at org.apache.jackrabbit.core.cluster.ClusterNode.init(ClusterNode.java:213)
        ... 28 more

When I examined the source code of the release jackrabbit 1.3, I saw that the init() method for DatabaseJournal class is:

			...
			Class.forName(driver);
			con = DriverManager.getConnection(url, user, password);
			con.setAutoCommit(true);

			checkSchema();
			prepareStatements();
			...

and just before checkSchema() method's finally block:

			...
			// commit the changes
			con.commit();
			...

So, it seemed normal to see the mentioned exception. I just commented out the commit expression and continued my development. Am I missing something?"
0,"Confusing Javadoc in Searchable.javaIn Searchable.java, the javadoc for maxdoc() is:

  /** Expert: Returns one greater than the largest possible document number.
   * Called by search code to compute term weights.
   * @see org.apache.lucene.index.IndexReader#maxDoc()

The qualification ""expert"" and the statement ""called by search code to compute term weights"" is a bit confusing, It implies that maxdoc() somehow computes weights, which is obviously not true (what it does is explained in the other sentence). Maybe it is used as one factor of the weight, but do we really need to mention this here? "
1,"'100-continue' response times out unexpectedlyEntity encosing methods time out (3 seconds) rather than getting the
100-continue response. Then, after it has send the body, the 100-continue
response is received and returned.

Adding

  method.setUseExpectHeader(false);

seems to fix it.

Platform 1: Jetty server on Windows XP, Sun JDK 1.4.1_01, 
Platform 2: Tomcat-4.1.18 + Turbine on Windows 2000 Pro, Sun JDK 1.3.1
Platform 3: Tomcat-4.1.18 on Linux if the connection is running over stunnel-4.00

Reported by: 
 Simon Roberts <simon.roberts@fifthweb.net>
 Aurelien Pernoud <apernoud@sopragroup.com>
 Ingo Brunberg <ib@fiz-chemie.de>"
0,Remove deprecated methods in PriorityQueue
0,"[PATCH] Avoid checking for TermBuffer in SegmentTermEnum#scanToIt seems that SegmentTermEnum#scanTo is a critical method which is called very often, especially whenever we iterate over a sequence of terms in an index.

When that method is called, the first thing happens is that it checks whether a temporary TermBuffer ""scratch"" has already been initialized.

In fact, this is not necessary. We can simply declare and initialize the ""scratch""-Buffer at the class-level (right now, the initial value is _null_). Java's lazy-loading capabilities allow this without adding any memory footprint for cases where we do not need that buffer.

The attached patch takes care of this. We now save one comparison per term.
In addition to that, the patch renames ""scratch"" to ""scanBuffer"", which aligns with the naming of the other two buffers that are declared in the class."
0,"ItemImpl#validateTransientItems: Incomplete validation of mandatory child itemItemImpl#validateTransientItems iterates over all mandatory child node/property definitions in order to assert that those items have
been created. However, it only checks if an item with the name defined by the mandatory item definition is present and not if that
existing item really has the mandatory definition.

the example i had:
- mandatory single-value property.
- there is the possibility to add residual props
- added a residual property with the name of the mandatory prop but with multiple values

-> changes are saved without exception.
-> the node doesn't have a property with the mandatory definition.

((without having tried it out, i think the same would be possible with child nodes))

suggested fix:
if there is a child item with the mandatory-item-name -> make sure it's definition is mandatory (or the expected one...)
patch will follow.
"
1,"Node's childNodes out of sync after unsuccessful save()If node.save() failes due to an exception in PersistanceManager.store(ChangeLog) a successive call to node.getNodes() will still contain a reference to the node which failed to be persisted.
This is the case even after a call to refresh(false).
You have to restart Jackrabbit in order to get rid of this reference


UseCase in kind of dummy-code:

node.addNode(""new"", ""nt:unstructured"");
node.save();
=> ItemStateException from persistance

node.refresh(false);

Iterator itr = node.getNodes();
while(itr.hasNext()) {
   child = itr.nextNode();
}
=> Exception: ""Failed to build path to ""new""

"
1,"Jackrabbit fails to shutdown properly when tomcat is shutting downThis is the same issue already discudded in http://issues.apache.org/jira/browse/JCR-57

The problem only occurs when Jackrabbit is deployed in the WEB-INF/lib directory of a web application in Tomcat.
During dispose() jackrabbit tries to instantiate a few objects from classes which were not previously loaded by the webapp classloader, but tomcat doesn't allow to load new classes while shutting down.
This causes the repository not to be closed properly, and an annoying set of stack traces are written to the log.

It seems that there are only two classes which are loaded in this situation: org.apache.jackrabbit.core.observation.EventListenerIteratorImpl and org.apache.jackrabbit.core.fs.FileSystemPathUtil. This is the log from the server standard output:

org.apache.catalina.loader.WebappClassLoader loadClass
INFO: Illegal access: this web application instance has been stopped already.  Could not load org.apache.jackrabbit.core.observation.EventListenerIteratorImpl.  The eventual following stack trace is caused by an error thrown for debugging purposes as well as to attempt to terminate the thread which caused the illegal access, and has no functional impact.
[repeaded more times at each shutdown]

org.apache.catalina.loader.WebappClassLoader loadClass
INFO: Illegal access: this web application instance has been stopped already.  Could not load org.apache.jackrabbit.core.fs.FileSystemPathUtil.  The eventual following stack trace is caused by an error thrown for debugging purposes as well as to attempt to terminate the thread which caused the illegal access, and has no functional impact.


A quick fix is to force preloading of classes normally needed only during shutdown, simply adding a static block to caller classes. The following patch makes tomcat happy, causing classes to be loaded by the webapp classloaded when still allowed  (probably not really elegant, but perfectly working...)




Index: org/apache/jackrabbit/core/fs/FileSystemResource.java
===================================================================
--- src\java\org\apache\jackrabbit\core\fs\FileSystemResource.java	(revision 169503)
+++ src\java\org\apache\jackrabbit\core\fs\FileSystemResource.java	(working copy)
@@ -30,6 +30,11 @@
 
     protected final String path;
 
+    static {
+        // preload FileSystemPathUtil to prevent classloader issues during shutdown
+        FileSystemPathUtil.class.hashCode();
+    }
+
     /**
      * Creates a new <code>FileSystemResource</code>
      *
Index: org/apache/jackrabbit/core/observation/ObservationManagerImpl.java
===================================================================
--- src\java\org\apache\jackrabbit\core\observation\ObservationManagerImpl.java	(revision 169503)
+++ src\java\org\apache\jackrabbit\core\observation\ObservationManagerImpl.java	(working copy)
@@ -54,6 +54,11 @@
      */
     private final ObservationManagerFactory obsMgrFactory;
 
+    static {
+        // preload EventListenerIteratorImpl to prevent classloader issues during shutdown
+        EventListenerIteratorImpl.class.hashCode();
+    }
+
     /**
      * Creates an <code>ObservationManager</code> instance.
      *


"
0,Best effort merge if concurrent modifications include changes to mixin typescurrently the NodeStateMerger#merge method immediately aborts if the mixin types of the 2 nodes are not the same.
0,"[PATCH] Make a getter for SortField[] fields in org.apache.lucene.search.SortI'm have my own Collector and I would like to use the Sort object within my
collector, but SortField[] fields; is not accessible outside Lucene's package.
Can you please consider making a public getFields() method in the Sort object so
we can use it in our implementation?"
0,"Disable Usersadd ""disable user"" functionality to prevent an existing user from login into the repository."
1,"JNDI data sources with various PersistenceManager: wrong default valuesWith JCR-1305 Jackrabbit supports creating a connection throug a JNDI Datasource and without configuring user and password. This works for some but not all provided PersistenceManagers. Some of them - like the Oracle-specific BundleDBPersistenceManager - sets default values for user and password if none are provided in the jackrabbit config. This way its impossible to use such PersistenceManagers with the plain JNDI DS.

This concerns the following BundleDbPersistenceManagers: OraclePersistenceManager, DerbyPersistenceManager, H2PersistenceManager.

There also might be other PMs (perhaps some special SimpleDbPersistenceManagers) with similar behaviour."
1,New BufferedIndexOutput optimization fails to update bufferStartNew BufferIndexOutput optimization of writeBytes fails to update bufferStart under some conditions. Test case and fix attached.
1,"search.jsp doesn't handle utf-8 parameters correctly
1.  I  cannot use WebDav client to uploaded a file whose name is in Chinese.  The file name I had is ''.txt'  and the uploaded command by the WebDav client did something like:

  ========= Outbound Message =========
PUT /op/%ED%EF%3A.txt HTTP/1.1
Host: localhost:8080
-----

 The server didn't decode it correctly -- the result is the file name got screwed and the file content was not uploaded.

2. In the default web.war module,  there is search.jsp for rendering the search page. If I type Chinese text in the search box,  search.jsp does not decode the input parameter from ISO-8859-1 to utf-8 and in turn the search engine searches wrong string.

3. The search engine does do search correctly if I hardcode the query  variable in search.jsp or do decoding the query parameter from ISO-885901 to utf-8.

"
0,"Add support for distributed stats(its a bug in a way, since we broke this, temporarily).

There is no way to do this now (distributed IDF, etc) with the new API.

But we should do it right:
* having the sim ask the searcher for docfreq of a term is wasteful and dangerous, 
  usually we have already seek'd to the term and already collected the 'raw' stuff.
* the situation is more than just docfreq, because you should be able to implement
  distributed scoring for all of the new sim models (or your own), that use any
  of Lucene's stats.
"
1,"SSL verification occurs before setSoTimeout, which can lead to hangspartial thread dump:

       at java.net.SocketInputStream.socketRead0(Native Method)
       at java.net.SocketInputStream.read(SocketInputStream.java:129)
       at com.sun.net.ssl.internal.ssl.InputRecord.readFully(InputRecord.java:293)
       at com.sun.net.ssl.internal.ssl.InputRecord.read(InputRecord.java:331)
       at com.sun.net.ssl.internal.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:723)
       - locked <0x00002aaab87d9de0> (a java.lang.Object)
       at com.sun.net.ssl.internal.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1030)
       - locked <0x00002aaab87d9dc0> (a java.lang.Object)
       at com.sun.net.ssl.internal.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1057)
       at com.sun.net.ssl.internal.ssl.SSLSocketImpl.getSession(SSLSocketImpl.java:1757)
       at org.apache.http.conn.ssl.AbstractVerifier.verify(AbstractVerifier.java:87)
       at org.apache.http.conn.ssl.SSLSocketFactory.connectSocket(SSLSocketFactory.java:295)
       at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:131)
       at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:143)
       at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:120)
       at org.apache.http.impl.client.DefaultClientRequestDirector.execute(DefaultClientRequestDirector.java:286)
       at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:452)
       at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:406)
       at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:365)


... this is because in DefaultClientConnectionOperator, prepareSocket (which sets any configured timeouts) isn't called until after SocketFactory.connectSocket. When using SSLSocketFactory, the default behavior is to verify the hostname, which opens a connection, and can block indefinitely.

Simple workaround is to use the AllowAllHostnameVerifier which doesn't do any verification."
0,OCM test are too verboseThe OCM test cases print quite a bit of stuff on standard output which makes the standard build output harder to read. It would be better if the tests either used explicit assertions to verify correct behaviour or at least redirected free-form output to a log file in the target directory.
1,UserManager.getAuthorizable() may fail with InvalidQueryExceptionHappens when the principal name contains an apostrophe.
0,Add basic I/O counters to query handlerThere should be a couple of simple counters that track the number of I/O operations that are performed during a query execution. This will help debug query performance issues.
1,"Read permission on parent node required to access an item's definitionIf a session is granted all permissions on a given item B but lacks permission to read it's parent node A an attempt to
access the definition of B by means of Node.getDefinition or Property.getDefinition will fail with AccessDeniedException.

Similarly, the same session will not be able to modify that item B - e.g. add a child node in case it was a node - since implementation e.g. checks of that
item B isn't protected, which is determined by looking at the definition.

My feeling is, that the item definition should be accessible even if the parent node cannot be read."
1,"Importer drops jcr:xmlcharacters fields after a large jcr:xmlcharacters entryIf you have an XML node as follows:

<a>
[lots and lots of data]
</a>

This should translate into

Node: a
 +- Node: jcr:xmltext
   +- Property: jcr:xmlcharacters = [lots and lots of data]

Instead, the following things happen:
- There is no node jcr:xmltext
- If the node a has child nodes, they also lose the jcr:xmltext node
- Any nodes on the same level after node a also lose the jcr:xmltext node
- Nodes that come after a, but are on a higher level, have correct jcr:xmltext 
nodes

(I've used some 100+k of data, namely a base64 encoded picture)"
1,"UUID compareTo and hashCodeThe current UUID.compareTo implementation is not correct. Usually, 'equals' is used so this is not a big problem, but I need to create an ordered list of UUIDs and for this I need compareTo. The current implementation is based on subtraction, but this doesn't always work. Example:

//long a = 10, b = 20, c = 0;
long a = Long.MAX_VALUE, b = Long.MIN_VALUE, c = 0;
System.out.println((a - b) < 0 ? ""a < b"" : ""a >= b"");
System.out.println((c - a) < 0 ? ""c < a"" : ""c >= a"");
System.out.println((b - c) < 0 ? ""b < c"" : ""b >= c"");

The hashCode implementation is OK, but the multiplication is not required."
0,"Review the use of BaiscHttpParams and HttpProtocolProcessor in HttpClientReview the use of BaiscHttpParams and HttpProtocolProcessor in HttpClient and replace with thread-safe implementations where necessary.

Oleg"
0,"NamespaceRegistryTest.testRegisterNamespace test assumptionsNamespaceRegistryTest.testRegisterNamespace() makes the assumption that it is possible to create arbitrarily nodes inside the root folder.

This is not required to be the case.

Proposal: 

- get the name of the test node from the config, and

- use a property rather than a child node for the test (as far as I can tell, many repositories will not allow node names in namespaces other than the empty one).
"
1,"jcr2spi spi2dav getProperties returns only cached propertiesI'm using JCR through webdav (contrib jcr2spi and spi2dav libraries).
Server is default jcr server (jackrabbit-webapp-1.3.1 on tomcat),
client is 2007/09/28 svn snapshot

I've noticed that Node.getProperty returns only cached properties. 

Sample test case can be found at http://kplab.tuke.sk/svn/kplab/ctm/trunk/test/org/kplab/tsf/CTMTest.java

Note that sometimes properties do get printed, so it may not be so straightforwarding to reproduce this bug.


Simple Example:

// I have nt:file node at kplab/jojo in my repository
...
Session session = repository.login();
Node root  = session.getRootNode();
Node node = root.getNode(""kplab/jojo"");
// node.getProperty(""jcr:content/jcr:data""); // force to load property
from server
dump(node); // simple dump method from (
http://jackrabbit.apache.org/doc/firststeps.html )


dump prints:
/kplab/jojo
/kplab/jojo/jcr:content

But when I uncomment the getProperty line above, it prints:
/kplab/jojo
/kplab/jojo/jcr:content
/kplab/jojo/jcr:content/jcr:lastModified = 2007-09-27T15:52:27.312+02:00
/kplab/jojo/jcr:content/jcr:uuid = 4421ed5a-6200-4918-864e-c58643bc8d4e
/kplab/jojo/jcr:content/jcr:mimeType = text/plain
/kplab/jojo/jcr:content/jcr:data = hura hura
/kplab/jojo/jcr:content/jcr:primaryType = nt:resource

--
Jozef Wagner"
0,"Proxy tunneling/auth with CONNECT for non-HTTP protocolsHttpClient would be even more useful if it supported connections tunneled 
through proxies and proxy authentication for non-HTTP protocols. E.g. Binary 
protocols such as SSH or JXTA-TCP could be tunneled through a web proxy if 
HttpClient provided access to the underlying Socket after the negotiations 
(auth, CONNECT) with the web proxy were complete."
1,"TestIndexWriterExceptions random failure: AIOOBE in ByteBlockPool.allocSliceTestIndexWriterExceptions threw this today, and its reproducable"
0,"Fix CommitIndexTask to also commit IndexReader changesI'm setting up a benchmark for LUCENE-1458, and one limitation I hit is that the CommitIndexTask doesn't commit pending changes in the IndexReader (eg via DeleteByPercent), using a named commit point."
0,"substitute for URLUtils.javaI would like to contribute a substitute for existing URLUtils.java... UrlEncodedUtils.java offer utility methods for dealing with 'urlencoded' data. Main difference with existing class is that parameters are Map <String, List <String>> instead of NameValue pairs and lack of third party dependencies...  It's partially covered with tests which I will further extend to cover all methods of this utility class."
1,"ItemStateMap warnings during node type changesAs reported already in JCR-1105, the ItemStateMap logs warnings when a cached item state is being overwritten. This shouldn't normally happen, but it turns out that virtual item state providers do this when the root of the virtual tree is modified. Probably the most common such situation is when node types are being modified. This case is luckily not troublesome for the virtual tree functionality, but the logged warnings are annoying.

Here's a relevant part of a stack trace where this problem occurs:

        at org.apache.jackrabbit.core.state.ItemStateMap.put(ItemStateMap.java:72)
        at org.apache.jackrabbit.core.virtual.AbstractVISProvider.cache(AbstractVISProvider.java:324)
        at org.apache.jackrabbit.core.virtual.AbstractVISProvider.createNodeState(AbstractVISProvider.java:284)
        at org.apache.jackrabbit.core.nodetype.virtual.VirtualNodeTypeStateProvider.createNodeTypeState(VirtualNodeTypeStateProvider.java:157)
        at org.apache.jackrabbit.core.nodetype.virtual.VirtualNodeTypeStateProvider.createRootNodeState(VirtualNodeTypeStateProvider.java:80)
        at org.apache.jackrabbit.core.virtual.AbstractVISProvider.stateDiscarded(AbstractVISProvider.java:470)
        at org.apache.jackrabbit.core.state.ItemState.notifyStateDiscarded(ItemState.java:226)
        at org.apache.jackrabbit.core.state.ItemState.discard(ItemState.java:370)
        at org.apache.jackrabbit.core.nodetype.virtual.VirtualNodeTypeStateProvider.onNodeTypesRemoved(VirtualNodeTypeStateProvider.java:139)
        at org.apache.jackrabbit.core.nodetype.virtual.VirtualNodeTypeStateManager.nodeTypesUnregistered(VirtualNodeTypeStateManager.java:199)
        at org.apache.jackrabbit.core.nodetype.virtual.VirtualNodeTypeStateManager.nodeTypeReRegistered(VirtualNodeTypeStateManager.java:174)
        at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.notifyReRegistered(NodeTypeRegistry.java:1821)
        at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.reregisterNodeType(NodeTypeRegistry.java:433)
        at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.reregisterNodeType(NodeTypeRegistry.java:364)
        at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:591)
        at org.apache.jackrabbit.commons.cnd.CndImporter.registerNodeTypes(CndImporter.java:118)"
0,BundleDumper to analyze broken bundlesThe BundleReader fails if it can't read a bundle. We should have a tool to analyze broken bundles.
0,"Checks for optional features in test cases are wrongReported by David Sanders:

The TCK for JSR-170 Final Release
(http://jcp.org/aboutJava/communityprocess/final/jsr170/index.html)
checks for level 2 and optional features by comparing
Repository.getDescriptor to null.  According to the
spec and javadoc, getDescriptor must return either
""true"" or ""false"" for the ""capability"" keys.

Example in AbsractJCRest.java:

        // setup custom namespaces
        if
(helper.getRepository().getDescriptor(Repository.LEVEL_2_SUPPORTED)
!= null) {
            NamespaceRegistry nsReg =
superuser.getWorkspace().getNamespaceRegistry();


I think the above if statement should be:
       if
(helper.getRepository().getDescriptor(Repository.LEVEL_2_SUPPORTED)
.equals(""true"")) "
0,"Improve logging of Session.save() to trace back root cause of externally modified nodesCurrently it's very difficult to find the root cause of error like: javax.jcr.InvalidItemStateException: <UUID> has been modified externally.

To better trace back such issues, it would be nice to add DEBUG logging for the Session.save() call."
1,"Exception shouldn't be thrown for unsupported authentication methodCurrently, Authenticator will throw an UnsupportedOperationException for 
unsupported authentication method (like NTLM). This is correct. However,  
HttpMethodBase.execute only catches HttpException, so this 
UnsupportedOperationException is leaked to the user. This is undesirable, 
because user may want a chance to handle such such authentication themself. The 
correct way is to pass the http status code to the user, just like how it 
treats redirect to a different host.

The simple fix is to catch all exceptions in HttpMethodBase.execute when 
calling Authenticator.authenticate."
0,"Move listeners from item state to item state managersClients interested in item state modifications directly subscribe to the item states, which is a very flexible approach. On the other side, it increases the memory consumption of an item state, because every item state holds a collection of its listeners. It further increases complexity, because item state listeners can potentially have a shorter life and might be garbage collected.

Listeners should therefore be moved to their associated item state manager. At the same time, this enables an item state manager to completely remove an item state from its cache and resurrect it at a later time without losing the listeners interested in notifications."
0,Rename IOUtils.close methodsThe closeSafely methods that take a boolean suppressExceptions are dangerous... I've renamed to .close (no suppression) and .closeWhileHandlingException (suppresses all exceptions).
0,"Exception may get lost in WorkspaceManager.OperationVisitorImpl.execute()The method calls Batch.submit() in the finally block. If both the try and finally block throw exceptions, the one from the try block is ignored."
0,"nuke IndexSearcher(directory)IndexSearcher is supposed to be a cheap wrapper around a reader,
but sometimes it is, sometimes it isn't.

I think its confusing tangling of a heavyweight and lightweight
object that it sometimes 'houses' a reader and must close it in that case.
"
1,"ArrayIndexOutOfBoundsException: ConcurrentCacheArrayIndexOutOfBoundsException after several days of uptime.

I'm experiencing some strange ArrayIndexOutOfBoundsExceptions on
 accessing the jackrabbit ConcurrentCache in 2.2.5. in Line 241 during
 shrinkIfNeeded check.

 Caused by: java.lang.ArrayIndexOutOfBoundsException: -14
        at
 org.apache.jackrabbit.core.cache.ConcurrentCache.shrinkIfNeeded(ConcurrentCache.java:241)


I reviewed jackrabbit-code and I'm sure it's caused by that
 AtomicInteger for realizing accessCounter in AbstractCache, which will
 have become negative during increasing over the Integer.MAX_VALUE constant.

         // Semi-random start index to prevent bias against the first
 segments
         int start = (int) getAccessCount() % segments.length;
         for (int i = start; isTooBig(); i = (i + 1) % segments.length) {
             synchronized (segments[i]) {

 ___________________________

 Uncaught Throwable java.lang.ArrayIndexOutOfBoundsException: -7
         at
 org.apache.jackrabbit.core.cache.ConcurrentCache.shrinkIfNeeded(ConcurrentCache.java:241)
         at
 org.apache.jackrabbit.core.cache.ConcurrentCache.put(ConcurrentCache.java:176)
         at
 org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.getBundle(AbstractBundlePersistenceManager.java:657)
         at
 org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.load(AbstractBundlePersistenceManager.java:400)
         at
 org.apache.jackrabbit.core.state.SharedItemStateManager.loadItemState(SharedItemStateManager.java:1819)
         at
 org.apache.jackrabbit.core.state.SharedItemStateManager.getNonVirtualItemState(SharedItemStateManager.java:1739)
         at
 org.apache.jackrabbit.core.state.SharedItemStateManager.getItemState(SharedItemStateManager.java:261)
         at
 org.apache.jackrabbit.core.state.LocalItemStateManager.getNodeState(LocalItemStateManager.java:107)
         at
 org.apache.jackrabbit.core.state.LocalItemStateManager.getItemState(LocalItemStateManager.java:172)
         at
 org.apache.jackrabbit.core.state.XAItemStateManager.getItemState(XAItemStateManager.java:260)
         at
 org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:161)
         at
 org.apache.jackrabbit.core.ItemManager.getItemData(ItemManager.java:370)
         at
 org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:316)
         at
 org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:610)
         at
 org.apache.jackrabbit.core.SessionImpl.getNodeById(SessionImpl.java:493)
         at
 org.apache.jackrabbit.core.SessionImpl.getNodeByIdentifier(SessionImpl.java:1045)
         at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
         at
 sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
         at java.lang.reflect.Method.invoke(Method.java:597)
         at
 org.apache.sling.jcr.base.SessionProxyHandler$SessionProxyInvocationHandler.invoke(SessionProxyHandler.java:109)
         at $Proxy2.getNodeByIdentifier(Unknown Source)
         at
 de.dig.cms.frontend.servlet.helper.ResourceUtil.findResourceById(ResourceUtil.java:44)
         at
 de.dig.cms.frontend.servlet.CMSContentEnrichServletFilter.doFilter(CMSContentEnrichServletFilter.java:194)
         at
 org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60)
         at
 de.dig.cms.frontend.servlet.CacheControlFilter.doFilter(CacheControlFilter.java:120)
         at
 org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60)
         at
 de.dig.cms.cache.impl.WallCacheServletFilter.processCacheableRequest(WallCacheServletFilter.java:244)
         at
 de.dig.cms.cache.impl.WallCacheServletFilter.processCacheableRequestWithLatch(WallCacheServletFilter.java:185)
         at
 de.dig.cms.cache.impl.WallCacheServletFilter.doFilter(WallCacheServletFilter.java:154)
         at
 org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60)
         at
 de.dig.cms.frontend.servletapi.CMSSlingHttpServletRequestFilter.doFilter(CMSSlingHttpServletRequestFilter.java:52)
         at
 org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60)
         at
 org.apache.sling.engine.impl.SlingMainServlet.service(SlingMainServlet.java:313)
         at
 org.apache.sling.engine.impl.SlingMainServlet.service(SlingMainServlet.java:207)
         at
 org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)
         at
 org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:389)
         at
 org.ops4j.pax.web.service.internal.HttpServiceServletHandler.handle(HttpServiceServletHandler.java:64)
         at
 org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
         at
 org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)
         at
 org.ops4j.pax.web.service.internal.HttpServiceContext.handle(HttpServiceContext.java:111)
         at
 org.ops4j.pax.web.service.internal.JettyServerHandlerCollection.handle(JettyServerHandlerCollection.java:64)
         at
 org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
         at org.mortbay.jetty.Server.handle(Server.java:324)
         at
 org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:535)
         at
 org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:865)
         at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:539)
         at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
         at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
         at
 org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)
         at
 org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:520)"
1,"DBFileSystem MySQL DDL not compatible with pre-5.0 versionsThe packaged ddl for mysql index sizes is too large for 4.x versions of MySQL. As the sum-total of the index sizes may only reach 500.

So, 

create unique index ${schemaObjectPrefix}FSENTRY_IDX on ${schemaObjectPrefix}FSENTRY (FSENTRY_PATH(745), FSENTRY_NAME)

will not work. I would suggest shortening the FSENTRY_PATH index value to 245, as FSENTRY_NAME is already set to 255.

create unique index ${schemaObjectPrefix}FSENTRY_IDX on ${schemaObjectPrefix}FSENTRY (FSENTRY_PATH(245), FSENTRY_NAME)"
1,"adding docs with large (binary) fields of 5mb causes OOM regardless of heap sizeas reported by George Washington in a message to java-user@lucene.apache.org with subect ""Storing large text or binary source documents in the index and memory usage"" arround 2006-01-21 there seems to be a problem with adding docs containing really large fields.

I'll attach a test case in a moment, note that (for me) regardless of how big i make my heap size, and regardless of what value I set  MIN_MB to, once it starts trying to make documents of containing 5mb of data, it can only add 9 before it rolls over and dies.

here's the output from the code as i will attach in a moment...

    [junit] Testsuite: org.apache.lucene.document.TestBigBinary
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 78.656 sec

    [junit] ------------- Standard Output ---------------
    [junit] NOTE: directory will not be cleaned up automatically...
    [junit] Dir: /tmp/org.apache.lucene.document.TestBigBinary.97856146.100iters.4mb
    [junit] iters completed: 100
    [junit] totalBytes Allocated: 419430400
    [junit] NOTE: directory will not be cleaned up automatically...
    [junit] Dir: /tmp/org.apache.lucene.document.TestBigBinary.97856146.100iters.5mb
    [junit] iters completed: 9
    [junit] totalBytes Allocated: 52428800
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testBigBinaryFields(org.apache.lucene.document.TestBigBinary):    Caused an ERROR
    [junit] Java heap space
    [junit] java.lang.OutOfMemoryError: Java heap space


    [junit] Test org.apache.lucene.document.TestBigBinary FAILED
"
0,"add numDocs() and maxDoc() methods to IndexWriter; deprecate docCount()Spinoff from here:

  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200804.mbox/%3c405706.11550.qm@web65411.mail.ac4.yahoo.com%3e

I think we should add maxDoc() and numDocs() methods to IndexWriter,
and deprecate docCount() in favor of maxDoc().  To do this I think we
should cache the deletion count of each segment in the segments file.

"
1,"ConcurrentModificationException in WebDAV UPDATEAfter fixing JCR-2750, I started seeing the following exception in the jcr2dav integration tests:

java.util.ConcurrentModificationException: null
	at java.util.LinkedHashMap$LinkedHashIterator.nextEntry(LinkedHashMap.java:365) ~[na:1.5.0_22]
	at java.util.LinkedHashMap$ValueIterator.next(LinkedHashMap.java:380) ~[na:1.5.0_22]
	at java.util.AbstractCollection.toArray(AbstractCollection.java:176) ~[na:1.5.0_22]
	at org.apache.jackrabbit.webdav.MultiStatus.getResponses(MultiStatus.java:122) ~[jackrabbit-webdav-2.2-SNAPSHOT.jar:2.2-SNAPSHOT]
	at org.apache.jackrabbit.webdav.MultiStatus.toXml(MultiStatus.java:151) ~[jackrabbit-webdav-2.2-SNAPSHOT.jar:2.2-SNAPSHOT]
	at org.apache.jackrabbit.webdav.WebdavResponseImpl.sendXmlResponse(WebdavResponseImpl.java:145) ~[jackrabbit-webdav-2.2-SNAPSHOT.jar:2.2-SNAPSHOT]
	at org.apache.jackrabbit.webdav.WebdavResponseImpl.sendMultiStatus(WebdavResponseImpl.java:113) ~[jackrabbit-webdav-2.2-SNAPSHOT.jar:2.2-SNAPSHOT]
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doUpdate(AbstractWebdavServlet.java:1117) ~[jackrabbit-webdav-2.2-SNAPSHOT.jar:2.2-SNAPSHOT]
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(AbstractWebdavServlet.java:327) ~[jackrabbit-webdav-2.2-SNAPSHOT.jar:2.2-SNAPSHOT]
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(AbstractWebdavServlet.java:201) ~[jackrabbit-webdav-2.2-SNAPSHOT.jar:2.2-SNAPSHOT]
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820) ~[servlet-api-2.5-20081211.jar:na]
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511) ~[jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:390) ~[jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.Server.handle(Server.java:326) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:938) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:755) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228) [jetty-6.1.22.jar:6.1.22]
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582) [jetty-util-6.1.22.jar:6.1.22]

Instead of something caused by JCR-2750, it looks like a deeper problem that the JCR_2750 fix just uncovered. As far as I can tell, the ConcurrentModificationException is coming from the AbstractResource.EListener class that may end up concurrently modifying the MultiStatus response while it's being serialized."
0,"Add getVersion method to IndexCommitReturns the equivalent of IndexReader.getVersion for IndexCommit

{code}
public abstract long getVersion();
{code}"
0,"Add deleteAllDocuments() method to IndexWriterIdeally, there would be a deleteAllDocuments() or clear() method on the IndexWriter

This method should have the same performance and characteristics as:
* currentWriter.close()
* currentWriter = new IndexWriter(..., create=true,...)

This would greatly optimize a delete all documents case. Using deleteDocuments(new MatchAllDocsQuery()) could be expensive given a large existing index.

IndexWriter.deleteAllDocuments() should have the same semantics as a commit(), as far as index visibility goes (new IndexReader opening would get the empty index)

I see this was previously asked for in LUCENE-932, however it would be nice to finally see this added such that the IndexWriter would not need to be closed to perform the ""clear"" as this seems to be the general recommendation for working with an IndexWriter now

deleteAllDocuments() method should:
* abort any background merges (they are pointless once a deleteAll has been received)
* write new segments file referencing no segments

This method would remove one of the final reasons i would ever need to close an IndexWriter and reopen a new one 
"
0,Rename package namesRename package names  *graffito* into *jackrabbit*
1,"Jcr-server: Parsing NodeTypeProperty not compliant with definitionCreating a new NodeTypeProperty from an existing DavProperty fails, since assumptions made are not compliant with definition:

a) nodetype name is always enclosed in a 'nodetypename' element
b) nodetype property may be empty, thus contain no 'nodetype' element."
0,"SQL2 joins on empty sets are not efficientIt seems that in the cases where the LEFT side of the join doesn't contain any hits, the QueryEngine in unable to generate an efficient query for the RIGHT side, so it basically select all the possible nodes.
See this discussion as context [0].

Example:
LEFT side has hits, RIGHT side select is fast given some conditions: 
> SQL2 JOIN LEFT SIDE took 18 ms. fetched 145 rows.
> SQL2 JOIN RIGHT SIDE took 67 ms. fetched 0 rows.

LEFT side has no hits, RIGHT select everything
> SQL2 JOIN LEFT SIDE took 8 ms. fetched 0 rows.
> SQL2 JOIN RIGHT SIDE took 845 ms. fetched 13055 rows.
...so it fetches 130k nodes and doesn't keep any of them.


[0] http://jackrabbit.510166.n4.nabble.com/Strange-Search-Performance-problem-with-OR-td4507121.html
"
0,Allow separate control over whether body is stored or analyzedSimple enhancement to DocMaker.
0,[PATCH] add term index interval accessorsIt should be possible for folks to set the index interval used when writing indexes.
0,"If test has methods with @Ignore, we should print out a noticeCurrently these silently pass, but there is usually a reason they are @Ignore 
(sometimes good, sometimes really a TODO we should fix)

In my opinion we should add reasons for all these current @Ignores like Mike did with Test2BTerms.

Example output:
{noformat}
[junit] Testsuite: org.apache.lucene.index.Test2BTerms
[junit] Tests run: 0, Failures: 0, Errors: 0, Time elapsed: 0.184 sec
[junit]
[junit] ------------- Standard Error -----------------
[junit] NOTE: Ignoring test method 'test2BTerms' Takes ~4 hours to run on a fast machine!!  And requires that you don't use PreFlex codec.
[junit] ------------- ---------------- ---------------

...

[junit] Testsuite: org.apache.solr.handler.dataimport.TestMailEntityProcessor
[junit] Tests run: 0, Failures: 0, Errors: 0, Time elapsed: 0.043 sec
[junit]
[junit] ------------- Standard Error -----------------
[junit] NOTE: Ignoring test method 'testConnection'
[junit] NOTE: Ignoring test method 'testRecursion'
[junit] NOTE: Ignoring test method 'testExclude'
[junit] NOTE: Ignoring test method 'testInclude'
[junit] NOTE: Ignoring test method 'testIncludeAndExclude'
[junit] NOTE: Ignoring test method 'testFetchTimeSince'
[junit] ------------- ---------------- ---------------
{noformat}"
0,"Add a LATENT FieldSelectorResultI propose adding LATENT FieldSelectorResult

this would be similar to LAZY_LOAD except that it would NEVER cache the stored value

This will be useful for very large fields that should always go direct to disk (because they will take so much memory)
when caching Documents returned from a Searcher, the large field may be initially requested as LAZY_LOAD, however once someone reads this field, it will then get locked into memory. if this Document (and others like it) are cached, it can start to use a very large amount of memory for these fields

Contract for FieldSelectorResult.LATENT should be that it will always be pulled direct from the IndexInput and never be persisted in memory as part of a Fieldable

I could prepare a patch if desired

"
1,"Exception executing SQL2/JQOM with non-admin sessionConstraints are correctly handled when session does not have access to a node:

Caused by: javax.jcr.ItemNotFoundException: 21232f29-7a57-35a7-8389-4a0e4a801fc3
        at org.apache.jackrabbit.core.SessionImpl.getNodeById(SessionImpl.java:545)
        at org.apache.jackrabbit.core.query.lucene.constraint.NodeLocalNameOperand.getValues(NodeLocalNameOperand.java:44)
        at org.apache.jackrabbit.core.query.lucene.constraint.ComparisonConstraint.evaluate(ComparisonConstraint.java:80)"
1,"after enabling access manager, I can't createNode and setProperty without a node.save in the middleI added my own access manager. after that I can't get the following code working 

Node n = createNewNode(parentNode);
n.setProperty();
parentNode.save();

It seems that setProperty will invoke access control check, but since the new node is not in the repository yet, my access manager implementation won't be able to grant permission. I also tried to use hierachyManager to get the path of the new node, it also returned null. 


"
0,"Make getAttribute(Class attClass) Genericorg.apache.lucene.util.AttributeSource

current:
public Attribute getAttribute(Class attClass) {
    final Attribute att = (Attribute) this.attributes.get(attClass);
    if (att == null) {
      throw new IllegalArgumentException(""This AttributeSource does not have the attribute '"" + attClass.getName() + ""'."");
    }
    return att;
}
sample usage:
TermAttribute termAtt = (TermAttribute)ts.getAttribute(TermAttribute.class)


my improvment:
@SuppressWarnings(""unchecked"")
	public <T> T getAttribute2(Class<? extends Attribute> attClass) {
    final T att = (T) this.attributes.get(attClass);
    if (att == null) {
      throw new IllegalArgumentException(""This AttributeSource does not have the attribute '"" + attClass.getName() + ""'."");
    }
    return att;
 }
sample usage:
TermAttribute termAtt = ts.getAttribute(TermAttribute.class)"
0,"MANIFEST.MF cleanup (main jar and luci customizations)there are several problems with the MANIFEST.MF file used in the core jar, and some inconsistencies in th luci jar:

Lucli's build.xml has an own ""jar"" target and does not use the jar target from common-build.xml. The result
is that the MANIFEST.MF file is not consistent and the META-INF dir does not contain LICENSE.TXT and NOTICE.TXT.

Is there a reason why lucli behaves different in this regard? If not I think we should fix this."
1,"NullPointerException in ClassDescriptorIndex: /Users/cziegeler/Developer/workspaces/default/jackrabbit/contrib/jackrabbit-jcr-mapping/jcr-mapping/src/main/java/org/apache/jackrabbit/ocm/mapper/model/ClassDescriptor.java
===================================================================
--- /Users/cziegeler/Developer/workspaces/default/jackrabbit/contrib/jackrabbit-jcr-mapping/jcr-mapping/src/main/java/org/apache/jackrabbit/ocm/mapper/model/ClassDescriptor.java	(revision 579109)
If a class descriptor (for whatever reason) does not have a jcr type, a npe is thrown in ClassDescriptor.
The following patch solves this issue:

+++ contrib/jackrabbit-jcr-mapping/jcr-mapping/src/main/java/org/apache/jackrabbit/ocm/mapper/model/ClassDescriptor.java	(working copy)
@@ -468,7 +468,7 @@
         while (iterator.hasNext()) {
             ClassDescriptor descendantClassDescriptor = (ClassDescriptor) iterator.next();
   
-            if (descendantClassDescriptor.getJcrType().equals(nodeType)) {
+            if (nodeType.equals(descendantClassDescriptor.getJcrType())) {
                 return descendantClassDescriptor;
             }
   
"
1,"Identifier paths for inexistent items throw exceptionThe following fails with a RepositoryException but it should rather return false:

session.itemExists(""["" + UUID.randomUUID() + ""]"")"
1,"FastVectorHighlighter: small fragCharSize can cause StringIndexOutOfBoundsException If fragCharSize is smaller than Query string, StringIndexOutOfBoundsException is thrown."
1,"InstantiatedTermEnum#skipTo(Term) throws ArrayIndexOutOfBoundsException on empty index{code}
java.lang.ArrayIndexOutOfBoundsException: 0
	at org.apache.lucene.store.instantiated.InstantiatedTermEnum.skipTo(InstantiatedTermEnum.java:105)
	at org.apache.lucene.store.instantiated.TestEmptyIndex.termEnumTest(TestEmptyIndex.java:73)
	at org.apache.lucene.store.instantiated.TestEmptyIndex.testTermEnum(TestEmptyIndex.java:54)
{code}"
0,"request.abort() should interrupt thread waiting for a connectionCalls to HttpRequestBase.abort() will not unblock a thread that is still waiting for a connection and therefore has no ConnectionReleaseTrigger yet.
"
0,Expose namespace registry via workspace instead via session in spi2jcrspi2jcr/SessionInfoImpl.getNamespaceResolver() returns the namespace registry through the current session of the wrapped repository. Since session scoped namespace remapping is not visible to the SPI I think the method should return the namespace registry through the current workspace. 
0,EasySimilarity to interpret document length as float
0,"Java 1.4 compile error in EclipseAs reported by Ate Douma in JCR-804, the revision 520841 introduced code that causes the Eclipse compiler to fail in Java 1.4 compliance mode. However, the same code compiles with the Sun JDK 1.4.

The problem is a enclosing class reference that an anonymous innner class instantiated in the constructor of a named inner class contains. Apparently (and understandably), in Eclipse 1.4 complier the instance references are not yet available when evaluating the super() arguments in the constructor."
1,"SegmentMerger doesn't set payload bit in new optimized codeIn the new optimized code in SegmentMerger the payload bit is not set correctly
in the merged segment. This means that we loose all payloads during a merge!

The Payloads unit test doesn't catch this. Now that we have the new
DocumentsWriter we buffer much more docs by default then before. This means
that the test cases can't assume anymore that the DocsWriter flushes after 10
docs by default. TestPayloads however falsely assumed this, which means that no
merges happen anymore in TestPayloads. We should check whether there are
other testcases that rely on this.

The fixes for TestPayloads and SegmentMerger are very simple, I'll attach a patch
soon."
0,"Handle virtual hosts, relative urls, multi-homingNeed to be able to open a socket to one ipaddress (or hostname) and then include
a virtual hostname in the Host header. Use InetAddress class perhaps."
1,"Cookie.java blowing up on cookies from ""country code"" domainsThe following exception is thrown from Cookie.java when receiving a cookie from
a ""country code"" domain such as amazon.ca.

     [java] INFO: Cookie.parse(): Rejecting set cookie header
""session-id=702-1613649-9326458; path=/; domain=
.amazon.ca; expires=Tuesday, 29-Oct-2002 08:00:00 GMT,
session-id-time=1035878400; path=/; domain=.amazon.ca;
expires=Tuesday, 29-Oct-2002 08:00:00 GMT"" because ""session-id"" has an illegal
domain attribute ("".amazon.ca"")
 for the given domain ""www.amazon.ca"".  It violoates the Netscape cookie
specification for non-special TLDs.
     [java] Oct 22, 2002 9:32:37 AM org.apache.commons.httpclient.HttpMethodBase
processResponseHeaders
     [java] SEVERE: Exception processing response headers
     [java] org.apache.commons.httpclient.HttpException: Bad Set-Cookie header:
session-id=702-1613649-9326458
; path=/; domain=.amazon.ca; expires=Tuesday, 29-Oct-2002 08:00:00 GMT,
session-id-time=1035878400; path=/; do
main=.amazon.ca; expires=Tuesday, 29-Oct-2002 08:00:00 GMT Illegal domain
attribute .amazon.ca
     [java]     at org.apache.commons.httpclient.Cookie.parse(Cookie.java:944)
     [java]     at
org.apache.commons.httpclient.HttpMethodBase.processResponseHeaders(HttpMethodBase.java:141
9)
     [java]     at
org.apache.commons.httpclient.HttpMethodBase.readResponse(HttpMethodBase.java:1504)
     [java]     at
org.apache.commons.httpclient.HttpMethodBase.processRequest(HttpMethodBase.java:2128)
     [java]     at
org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java:790)
     [java]     at
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:442)


The http response that caused this exception is below.

HTTP/1.1 302 Found
Date: Tue, 22 Oct 2002 13:30:11 GMT
Server: Stronghold/2.4.2 Apache/1.3.6 C2NetEU/2412 (Unix)
Set-Cookie: session-id=702-8591055-5561622; path=/; domain=.amazon.ca;
expires=Tuesday, 29-Oct-2002 08:00:00 GMT
Set-Cookie: session-id-time=1035878400; path=/; domain=.amazon.ca;
expires=Tuesday, 29-Oct-2002 08:00:00 GMT
Location: http://www.amazon.ca/exec/obidos/tg/browse/-/915398/702-8591055-5561622
Connection: close
Transfer-Encoding: chunked
Content-Type: text/html

I've seen this problem with other .ca domains so this isn't a problem unique to
amazon.ca.

My guess would be that the problem is on line 929 of Cookie.java:

int domainParts = new StringTokenizer(cookie.getDomain(), ""."").countTokens();

Where domainParts would be 2 for a domain like "".amazon.ca"" instead of the 3
that the code is expecting.  I'm not that familiar with the cookie spec so I
could be completely wrong ;-)

The results above were done with the Oct 20/2002 gump build."
0,"DbClusterTest failure due to network configurationAs reported by Serge, the DbClusterTest case fails when run with certain network configuration.

Thomas already suggested a fix:

### Eclipse Workspace Patch 1.0
#P jackrabbit-core
Index: src/test/java/org/apache/jackrabbit/core/cluster/DbClusterTest.java
===================================================================
--- 
src/test/java/org/apache/jackrabbit/core/cluster/DbClusterTest.java (revisi
on 1067983)
+++ 
src/test/java/org/apache/jackrabbit/core/cluster/DbClusterTest.java (workin
g copy)
@@ -37,9 +37,9 @@
     public void setUp() throws Exception {
         deleteAll();
         server1 = Server.createTcpServer(""-tcpPort"", ""9001"", ""-baseDir"",
-                ""./target/dbClusterTest/db1"").start();
+                ""./target/dbClusterTest/db1"", ""-tcpAllowOthers"").start();
         server2 = Server.createTcpServer(""-tcpPort"", ""9002"", ""-baseDir"",
-                ""./target/dbClusterTest/db2"").start();
+                ""./target/dbClusterTest/db2"", ""-tcpAllowOthers"").start();
         FileUtils.copyFile(
                 new
File(""./src/test/resources/org/apache/jackrabbit/core/cluster/repository-h2
.xml""),
                 new File(""./target/dbClusterTest/node1/repository.xml""));
"
0,"Jcr-Server: missing-auth-mapping init parameter should have option for GuestCredential loginthe missing-auth-mapping parameter of the servlets contained in jcr-server is defined as follows:

             <param-value>anonymous:anonymous</param-value>
             <description>
                 Defines how a missing authorization header should be handled.
                 1) If this init-param is missing, a 401 response is generated.
                    This is suiteable for clients (eg. webdav clients) for which
                    sending a proper authorization header is not possible if the
                    server never sent a 401.
                 2) If this init-param is present with an empty value,
                    null-credentials are returned, thus forcing an null login
                    on the repository.
                 3) If this init-param has a 'user:password' value, the respective
                    simple credentials are generated.
             </description>

JCR 2.0 introduces GuestCredentials used to obtain a ""anonymous"" session.

Therefore we should probably extend/modify the missing-auth-param in a way that
allows to distinguish between

- null-login
- guest login

in case of missing authorization header."
1,"setProperty(""name"", new Value[0], PropertyType.LONG) loses property typeAdding an empty multivalued property with a specific non-STRING type to an unstructured node (i.e. one with an UNDEFINED multivalued property definition) creates an empty multivalued property of type STRING.

In some cases keeping the explicit type information is important, so we should avoid losing it."
0,"Allow benchmark tasks from alternative packagesRelax current limitation of all tasks in same package - that of PerfTask.
Add a property ""alt.tasks.packages"" - its value are comma separated full package names.
If the task class is not found in the default package, an attempt is made to load it from the alternate packages specified in that property."
0,Stack traces from failed tests are messed up on ANT 1.7.x
0,"Remove unnecessary TestAll classes in jcr-commonsThe module jackrabbit-jcr-commons uses the default test configuration, which means the TestAll test suites are not necessary. They actually cause all tests to be executed twice."
0,"FIXME in src/test/org/apache/lucene/IndexTest.javaIndex: src/test/org/apache/lucene/IndexTest.java
===============================================================
====
--- src/test/org/apache/lucene/IndexTest.java   (revision 155945)
+++ src/test/org/apache/lucene/IndexTest.java   (working copy)
@@ -27,8 +27,7 @@   
public static void main(String[] args) {
     try {
       Date start = new Date();
-      // FIXME: OG: what's with this hard-coded dirs??
-      IndexWriter writer = new IndexWriter(""F:\\test"", new SimpleAnalyzer(),
+      IndexWriter writer = new IndexWriter(File.createTempFile(""luceneTest"",""idx""), new 
SimpleAnalyzer(),
                                           true);
        writer.setMergeFactor(20);"
0,"Allow setting arbitrary objects on PerfRunDataPerfRunData is used as the intermediary objects between PerfRunTasks. Just like we can set IndexReader/Writer on it, it will be good if it allows setting other arbitrary objects that are e.g. created by one task and used by another.

A recent example is the enhancement to the benchmark package following the addition of the facet module. We had to add TaxoReader/Writer.

The proposal is to add a HashMap<String, Object> that custom PerfTasks can set()/get(). I do not propose to move IR/IW/TR/TW etc. into that map. If however people think that we should, I can do that as well."
0,"Some small javadocs/extra import fixesTwo things that Uwe Schindler caught, plus fixes for javadoc warnings in core.  I plan to commit to trunk & 2.4."
0,Jcr2spiRepositoryFactory: make class loading more robustCurrently Jcr2spiRepositoryFactory loads a RepositoryServiceFactory from the context class loader. In an OSGi environment this fails with a ClassNotFoundException. I suggest to fall back to the class loader of a the class (Jcr2spiRepositoryFactory) in this case. 
0,"Workspace{Copy|Move}VersionableTest assumptions on versioningThese test cases assume that an ancestor of a versioned node can be made versioned. This may not be true for all JCR compliant stores.

There should be a way to skip the test when it can not be executed.

One obvious approach would be to throw a NotExecutableException when the attempt to enable versioning on the parent fails. However this has the drawback that it can mask configuration errors.

Thoughts?
"
0,"ShingleFilter: don't output all-filler shingles/unigrams; also, convert from TermAttribute to CharTermAttributeWhen the input token stream to ShingleFilter has position increments greater than one, filler tokens are inserted for each position for which there is no token in the input token stream.  As a result, unigrams (if configured) and shingles can be filler-only.  Filler-only output tokens make no sense - these should be removed.

Also, because TermAttribute has been deprecated in favor of CharTermAttribute, the patch will also convert TermAttribute usages to CharTermAttribute in ShingleFilter."
0,"GetMethod.getResponseBodyAsStream() .available() could return content-lengthIt would be nice if the InputStream returned from
GetMethod.getResponseBodyAsStream() could override the available()
method to return the content-length of the requested URL.  This would
make things like ProgressMonitorInputStream useful for monitoring the
progress of a download.  Here is a code snippet:


/**
 * supply a hard-coded value for available() method.
 */
class FixedInputStream extends FilterInputStream {
  private int contentLength;

  public FixedInputStream(InputStream is,
              int contentLength) {
    super(is);
    this.contentLength = contentLength;
  }

  public int available() throws IOException {
    return contentLength;
  }
} 



Also, somewhat related to this request, could
GetMethod.getResponseContentLength() must be made public?  Is there a
good reason for it to be protected?  I had to extend GetMethod and
implement a public getResponseContentLength() in order to feed that
value to my FixedInputStream.

Thanks for your time."
0,"Credentials ignored if realm specified in preemptive authenticationWhen you specifiy credentials for a specific realm using preemptive 
authentication, the credentials are ignored during the first try (error 401 
back).

...
HttpClient client = new HttpClient(manager);
client.getState().setCredentials(""myRealm"",""myHost"",
			new UsernamePasswordCredentials(
				""user"",""password""));
client.getState().setAuthenticationPreemptive(true); 
...

""myRealm"" will be ignored in HttpState's matchCredentials() private method 
because during preemptive authentication, it is called with a null realm:

 private static Credentials matchCredentials(HashMap map, String realm, String 
host) {
        HttpAuthRealm entry = new HttpAuthRealm(host, realm);
	// no possible match here, map only contains the version with the realm
        Credentials creds = (Credentials) map.get(entry);
        if (creds == null && host != null && realm != null) {
            entry = new HttpAuthRealm(host, null);
            creds = (Credentials) map.get(entry);
            if (creds == null) {
                entry = new HttpAuthRealm(null, realm);
                creds = (Credentials) map.get(entry);
            }
        }
        if (creds == null) {
            creds = (Credentials) map.get(DEFAULT_AUTH_REALM);
        }
        return creds;
    } 

This is quite logical since the realm comes from the server and you don't 
contact the server first during preemptive authentication.

But, it should not be possible to set a realm when using preemptive mode, or at 
least it should not be silently ignored.

The current workaround is to set the realm to null in setCredential(), no 
elegant but works.

Regards,

Philippe"
0,"Move Kuromoji to analysis.ja and introduce Japanese* namingLucene/Solr 3.6 and 4.0 will get out-of-the-box Japanese language support through {{KuromojiAnalyzer}}, {{KuromojiTokenizer}} and various other filters.  These filters currently live in {{org.apache.lucene.analysis.kuromoji}}.

I'm proposing that we move Kuromoji to a new Japanese package {{org.apache.lucene.analysis.ja}} in line with how other languages are organized.  As part of this, I also think we should rename {{KuromojiAnalyzer}} to {{JapaneseAnalyzer}}, etc. to further align naming to our conventions by making it very clear that these analyzers are for Japanese.  (As much as I like the name ""Kuromoji"", I think ""Japanese"" is more fitting.)

A potential issue I see with this that I'd like to raise and get feedback on, is that end-users in Japan and elsewhere who use lucene-gosen could have issues after an upgrade since lucene-gosen is in fact releasing its analyzers under the {{org.apache.lucene.analysis.ja}} namespace (and we'd have a name clash).

I believe users should have the freedom to choose whichever Japanese analyzer, filter, etc. they'd like to use, and I don't want to propose a name change that just creates unnecessary problems for users, but I think the naming proposed above is most fitting for a Lucene/Solr release.
"
0,"full text search tests use incorrect character for escaping phrasesThe query test cases use single quotes to escape phrases. The grammar in 6.6.5.2 however requires double quotes.
"
1,"Change value of 'Expect' header in org.apache.http.client.methods.HttpPostsee original report at http://code.google.com/p/android/issues/detail?id=7208.

i'm going to apply the obvious patch to Android:

diff --git a/src/org/apache/http/params/CoreProtocolPNames.java b/src/org/apache/http/params/CoreProtocolPNames.java
index a42c5de..a0a726d 100644
--- a/src/org/apache/http/params/CoreProtocolPNames.java
+++ b/src/org/apache/http/params/CoreProtocolPNames.java
@@ -94,8 +94,8 @@ public interface CoreProtocolPNames {
 
     /**
      * <p>
-     * Activates 'Expect: 100-Continue' handshake for the 
-     * entity enclosing methods. The purpose of the 'Expect: 100-Continue'
+     * Activates 'Expect: 100-continue' handshake for the
+     * entity enclosing methods. The purpose of the 'Expect: 100-continue'
      * handshake to allow a client that is sending a request message with 
      * a request body to determine if the origin server is willing to 
      * accept the request (based on the request headers) before the client
diff --git a/src/org/apache/http/protocol/HTTP.java b/src/org/apache/http/protocol/HTTP.java
index de76ca6..9223955 100644
--- a/src/org/apache/http/protocol/HTTP.java
+++ b/src/org/apache/http/protocol/HTTP.java
@@ -60,7 +60,7 @@ public final class HTTP {
     public static final String SERVER_HEADER = ""Server"";
     
     /** HTTP expectations */
-    public static final String EXPECT_CONTINUE = ""100-Continue"";
+    public static final String EXPECT_CONTINUE = ""100-continue"";
 
     /** HTTP connection control */
     public static final String CONN_CLOSE = ""Close"";
"
0,"Improper EOL for text files on Windowsversion 2.0-rc3. Improper EOL for text files on Windows for README.txt and
LICENSE.txt, docs/*.txt. The files display as one loooong line in Notepad."
0,"JCR2SPI: several performance improvements pointed out by FindbugsFindBug report:

M P Bx: Method org.apache.jackrabbit.jcr2spi.nodetype.BitsetENTCacheImpl.getBitNumber(QName) invokes inefficient Integer(int) constructor; use Integer.valueOf(int) instead	src/main/java/org/apache/jackrabbit/jcr2spi/nodetype	BitsetENTCacheImpl.java	line 177	1190981544656	1666284
M P Bx: Method org.apache.jackrabbit.jcr2spi.query.RowIteratorImpl$RowImpl.getValue(String) invokes inefficient Integer(int) constructor; use Integer.valueOf(int) instead	src/main/java/org/apache/jackrabbit/jcr2spi/query	RowIteratorImpl.java	line 247	1190981544671	1666292
M P Bx: Method org.apache.jackrabbit.jcr2spi.WorkspaceManager.onEventReceived(EventBundle[], InternalEventListener[]) invokes inefficient Integer(int) constructor; use Integer.valueOf(int) instead	src/main/java/org/apache/jackrabbit/jcr2spi	WorkspaceManager.java	line 616	1190981544640	1666279
M P WMI: Method org.apache.jackrabbit.jcr2spi.name.NamespaceCache.syncNamespaces(Map) makes inefficient use of keySet iterator instead of entrySet iterator	src/main/java/org/apache/jackrabbit/jcr2spi/name	NamespaceCache.java	line 193	1190981544656	1666283
M P WMI: Method org.apache.jackrabbit.jcr2spi.nodetype.NodeTypeRegistryImpl.internalRegister(Map) makes inefficient use of keySet iterator instead of entrySet iterator	src/main/java/org/apache/jackrabbit/jcr2spi/nodetype	NodeTypeRegistryImpl.java	line 524	1190981544656	1666285
M P WMI: Method org.apache.jackrabbit.jcr2spi.observation.ObservationManagerImpl.onEvent(EventBundle) makes inefficient use of keySet iterator instead of entrySet iterator	src/main/java/org/apache/jackrabbit/jcr2spi/observation	ObservationManagerImpl.java	line 189	1190981544656	1666286
M P WMI: Method org.apache.jackrabbit.jcr2spi.state.NodeState.persisted(ChangeLog) makes inefficient use of keySet iterator instead of entrySet iterator	src/main/java/org/apache/jackrabbit/jcr2spi/state	NodeState.java	line 275	1190981544671	1666297
"
1,"MatchAllScorer calculateDocFilter() bugIn MatchAllScorer.calculateDocFilter(), When you have just two nodes, with different properties, like ""myprop"" and ""myprop2"", and you have an xpath String xpath = ""//*[@myprop], you get both nodes back (to be precise, you'll get every node that has a property that startswith ""myprop"")


You can reproduce it by changing the SimpleQueryTest.testIsNotNull() a little:

Change 

bar.setProperty(""text"", ""the quick brown fox jumps over the lazy dog.""); 

to

bar.setProperty(""mytextwhichstartswithmytext"", ""the quick brown fox jumps over the lazy dog."");

Now the test with xpath = ""//*[@jcr:primaryType='nt:unstructured' and @mytext]""; fails because 2 results. I did test for the trunk and tag 1.3.1 and both have the same problem. I have attached MatchAllScorer.java.patch in this mail, or should I create a JIRA issue for this? 

Furthermore I would like to discuss a different implementation for the MatchAllScorer, because IMHO the current calculateDocFilter() becomes slow pretty fast (see bottom email the code part i am referring to: if you have 100.000 docs with ""mytext"" property, and you query  [@mytext] the loop below is executed at least 100.000 times). I think it might be out of scope for the user-list, or is the user-list the place to discuss something like this? 

-----------------------------------------------------------------------

TermEnum terms = reader.terms(new Term(FieldNames.PROPERTIES, field));
        try {
            TermDocs docs = reader.termDocs();
            try {
                while (terms.term() != null
                        && terms.term().field() == FieldNames.PROPERTIES
                        && terms.term().text().startsWith(field)) {
                    docs.seek(terms);
                    while (docs.next()) {
                        docFilter.set(docs.doc());
                    }
                    terms.next();
                }
            } finally {
                docs.close();
            }
        } finally {
            terms.close();
        }

-----------------------------------------------------------------------

"
1,"JCR-Server: IllegalArgumentException when retrieving DateHeaderissue reported by martin perez:

Here is one exception. If I access to any repository through WebDAV using a
web browser (http://localhost:8080/webapp/repository/default the first time
goes well, but if I refresh the page then I get the following exception:

GRAVE: Servlet.service() para servlet Webdav lanz excepcin
java.lang.IllegalArgumentException: mar, 29 nov 2005 22:45:48 CET
    at org.apache.catalina.connector.Request.getDateHeader(Request.java
:1791)
    at org.apache.catalina.connector.RequestFacade.getDateHeader(
RequestFacade.java:630)
    at org.apache.jackrabbit.webdav.WebdavRequestImpl.getDateHeader(
WebdavRequestImpl.java:724)
    at org.apache.jackrabbit.server.AbstractWebdavServlet.spoolResource(
AbstractWebdavServlet.java:363)
    at org.apache.jackrabbit.server.AbstractWebdavServlet.doGet(
AbstractWebdavServlet.java:344)
    at org.apache.jackrabbit.j2ee.SimpleWebdavServlet.execute(
SimpleWebdavServlet.java:191)
    at org.apache.jackrabbit.server.AbstractWebdavServlet.service(
AbstractWebdavServlet.java:170)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(
ApplicationFilterChain.java:252)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(
ApplicationFilterChain.java:173)
    at org.apache.catalina.core.StandardWrapperValve.invoke(
StandardWrapperValve.java:213)
    at org.apache.catalina.core.StandardContextValve.invoke(
StandardContextValve.java:178)
    at org.apache.catalina.core.StandardHostValve.invoke(
StandardHostValve.java:126)
    at org.apache.catalina.valves.ErrorReportValve.invoke(
ErrorReportValve.java:105)
    at org.apache.catalina.core.StandardEngineValve.invoke(
StandardEngineValve.java:107)
    at org.apache.catalina.connector.CoyoteAdapter.service(
CoyoteAdapter.java:148)
    at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java
:868)
    at
org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.processConnection
(Http11BaseProtocol.java:663)
    at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(
PoolTcpEndpoint.java:527)
    at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(
LeaderFollowerWorkerThread.java:80)
    at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(
ThreadPool.java:684)
    at java.lang.Thread.run(Thread.java:595)"
0,"Remove jdk 1.4 restriction for jcr-testsThis restriction only exist because these tests form the TCK for JSR-283 which needed to support JDK 1.4. If maintenance on the JSR-283 TCK is needed, it can happen in a previous branch (2.3?).

"
0,"WebDAV BIND supportI'm tempted to work on implementing the WebDAV BIND protocol, as currently defined in http://greenbytes.de/tech/webdav/draft-ietf-webdav-bind-20.html.

This issue can be used to collect design proposals and track progress.

1) DAV:resource-id live property

This can be implemented in terms of the JCR UUID. However, we need to turn this one into a URI for WebDAV. If the JCR UUID happens to *really* use UUID syntax, we *could* use urn:uuid. Otherwise, it would probably useful to mint an HTTP URI, served by the WebDAV servlet. (note that the latter has the disadvantage that moving a node to a different server will affect its resource-id, in case that other server allows importing UUIDs).

2) REBIND and UNBIND methods

Same as MOVE and DELETE, with the excpetion of marshalling.

3) DAV:parent-set property

Either trivial (when node isn't shared), or needs to use the JCR 2.0 shared set functionality.

4) BIND method

Either trivial (when shareable nodes aren't supported), or needs to use the JCR 2.0 shared set functionality.

5) Cycle detection in depth:infinity requests

TBD :-)
"
0,CanAddChildNodeCallWithNodeTypeTest.testDefinedAndLegalType() may fail if protected child node definition is pickedIf the utility NodeTypeUtil.locateChildNodeDef() picks a protected child node definition the test case will fail because it is not allowed add a protected child node.
1,[PATCH] Fix possible Null Ptr exception in ConnectionFactorycode will throw npe if driver string is null - patch fixes this.
0,"Allow QP subclasses to support Wildcard Queries with leading ""*""It would be usefull for some users if the logic that prevents QueryParser from creating WIldcardQueries with leading wildcard characters (""?"" or ""*"") be moved from the grammer into the base implimentation of getWildcardQuery so that it may be overridden in subclasses without needing to modifiy the grammer directly.
"
0,"Contribution: Efficient Sorting of DateField/DateTools Encoded Timestamp Long ValuesHello Tim,

As promised, the sort functionality for ""long"" values is included in the
attached files.

patchTestSort.txt contains the diff info. for my modifications to the
TestSort.java class

org.apache.lucene.search.ZIP contains the three new class files for
efficient sorting of ""long"" field values and of encoded timestamp
field values as ""long"" values.

Let me know if you have any questions.

Regards,
Rus"
0,"convert automaton to char[] based processing and TermRef / TermsEnum apiThe automaton processing is currently done with String, mostly because TermEnum is based on String.
it is easy to change the processing to work with char[], since behind the scenes this is used anyway.

in general I think we should make sure char[] based processing is exposed in the automaton pkg anyway, for things like pattern-based tokenizers and such.
"
1,"Decompounders based on CompoundWordTokenFilterBase cannot be used with custom attributesThe CompoundWordTokenFilterBase.setToken method will call clearAttributes() and then will reset only the default Token attributes (term, position, flags, etc) resulting in any custom attributes losing their value. Commenting out clearAttributes() seems to do the trick, but will fail the TestCompoundWordTokenFilter tests.."
1,"QueryImpl result offSet must be considered after security class grant the item.ackrabbit version is 1.4 (jackrabbit-core - 1.4.5).
I use searches with result limit and offset but it is working some wrong for my case.
Lets suppose the total of nodes that will return with the search:

NAME      - GRANTACCESS -   OFFSET
node1     -     true                    - 0
node2     -     false                  -  1
node3     -     true                    - 2
node4     -     true                    - 3
node5     -     false                  -  4

My page must have 2 records, so first I do a count for the search and get size of 3 records (after filtered by my security class invoked automatically by jackrabbit), so I have 2 pages to show to the user. The first page must return 2 records, of course, and the second must return 1 record.

In the first search I do set:
QueryImp.setOffset(0);
QueryImpl.setLimit(2);

So, I get the nodes 1 and 3, thats correct.

In the second same search (for second page), I do set:
QueryImp.setOffset(2);
QueryImpl.setLimit(2);

This way I pretend to get two records, starting from the record nro 3, which would be only the node4.
But, the result I got is node3 (again) and node4, because the offset worked not according to the grantacess (provided by the security class), but according to the sequence of the raw result.

This offset have to start in the correct position, counting only the granted nodes returned by the security class.
Hope this make sense for you.

Thanks.
Helio."
1,"ClassCastException when registering custom node by XML fileWhen trying to register node type from XML file using following code:

		JackrabbitNodeTypeManager nodeTypeManager = (JackrabbitNodeTypeManager)workspace.getNodeTypeManager();
		for(Resource resource : nodeDefinitions){
			System.out.println(""** registering node:""+resource);
			nodeTypeManager.registerNodeTypes(resource.getInputStream(), JackrabbitNodeTypeManager.TEXT_XML);
		}

we receive such surprise:

Caused by: java.lang.ClassCastException: com.sun.org.apache.xerces.internal.dom.DeferredDocumentImpl
	at org.apache.jackrabbit.core.util.DOMWalker.iterateElements(DOMWalker.java:215)
	at org.apache.jackrabbit.core.nodetype.xml.NodeTypeReader.getNodeTypeDefs(NodeTypeReader.java:121)
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:257)
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:499)
	at pl.codeservice.jcr.JcrCustomNodeRegister.registerNodes(JcrCustomNodeRegister.java:41)
	at pl.codeservice.jcr.JcrCustomNodeRegister.init(JcrCustomNodeRegister.java:27)
	...


Registering nodes by .cnd files works fine."
0,"""reproduce with"" on test failure isn't right if you manually overrided anythingIf you run a test with eg -Dtests.codec=SimpleText...

If it fails, the ""reproduce with"" fails to include that manual override (-Dtests.codec=SimpleText), ie it only includes the seed / test class / test method.  So it won't actually reproduce the fail, in general.

We just need to fix the ""reproduce with"" to add any manual overrides...."
1,"Infinite loop on NTLM authenticationI got an infinite loop on NTLM authentication if the authentication failed (bad credentials).

The state FAILED of the NTLM sheme is never catched in the method authenticate of the class HttpAuthenticator (line 123).
I fix temporatily this bug by adding a case for the protocol state HANDSHAKE."
1,"make spell checker test case work againSee attached path which makes the spellchecker test case work again. The problem without the patch is that consecutive calls to indexDictionary() will create a spelling index with duplicate words. Does anybody see a problem with this patch? I see that the spellchecker code is now used in Solr, isn't it? I didn't have time to test this patch inside Solr.

Also see http://issues.apache.org/jira/browse/LUCENE-632, but the null check is included in this patch so the NPE described there cannot happen anymore.
"
1,"AbstractHttpClient.determineTarget does not recognize target host correctlyI am trying to execute an HttpGet with the following URI:
""http://www.foo.foo/doSomething.html?url=http://www.bar.bar/doSomethingElse.html""

This leads to UnknownHostException

Going through the internal code, the problem seems to be in the AbstractHttpClient.determineTarget method:
            String ssp = requestURI.getSchemeSpecificPart();
            ssp = ssp.substring(2, ssp.length()); //remove ""//"" prefix
            int end = ssp.indexOf(':') > 0 ? ssp.indexOf(':') :
                    ssp.indexOf('/') > 0 ? ssp.indexOf('/') :
                    ssp.indexOf('?') > 0 ? ssp.indexOf('?') : ssp.length();
            String host = ssp.substring(0, end);

This code sets the target host to ""www.foo.foo/doSomething.html?url=http"" instead of ""www.foo.foo"". This obviously breaks the execution not far down the line... DefaultClientConnectionOperator.resolveHostname throws an UnknownHostException.

FWIW the AbstractHttpClient.determineTarget method actually has access to the request URI object, which correctly states that the host is ""www.foo.foo"".

So why does it try to extract the host from the scheme specific part anyway?

I hope this is useful... and if there is any workaround please let me know, as I'm stuck on this one.

Marco"
0,"add infrastructure for longer running nightly test casesI'm spinning this out of LUCENE-2762...

The patch there adds initial infrastructure for tests to pull documents from a line file, and adds a longish running test case using that line file to test NRT.

I'd like to see some tests run on more substantial indices based on real data... so this is just a start."
0,"bad test assumptions in org.apache.jackrabbit.test.api.lockThese tests make a lot of assumptions that may not be true for a compliant repository, such as:

- ability to add nodes without specifiying the node type
- assumption that ordering and same name siblings are supported
- assumption that addMixin(lockable) is required on newly added nodes

Furthermore, some repositories may not support shallow locks on leaf nodes. That's not compliant, but failure to do so should not abort a test that tests something else.
"
0,"Provide Date Header Util MethodsHello,

It would be really nice to have util methods that help with setting date
headers.  For instance, like the servlet spec's setDateHeader() method.  This
allows for the client of HttpClient to not have to deal with formatting the date
into the correct string.  There is a parseDate method that turns a String into a
Date.  It would be nice to have the opposite method.

Thanks!
Seth"
0,"allow cache to be configured as a non-shared (private) cacheCurrently the CachingHttpClient only behaves as a shared cache, which is a safe and conservative assumption. However, in some settings, it would be appropriate to be able to configure the CachingHttpClient as a non-shared cache, which would make more responses cacheable, including:
* responses to requests with Authorization headers
* responses with 'Cache-Control: private'
* ability to serve stale responses when invalidation fails for 'Cache-Control: proxy-revalidate'
"
0,"Developer documentationProvide more example code in CVS and give a clear link on the website.  A
walkthrough of the API.  Documenntation suitable for new users."
1,"Error on query initialization - intermittentAbout 1 in ten times, I get the error as shown in the stack trace below. This happens when I run test, or when I start the app. The only way to resolve (when testing) seems to be to blow away the repository. 

It always happens at the point the query manager is accessed (triggering the query subsystem to start up). It DOES NOT cause an exception to be thrown back to the caller, I just noticed it in the logs. Basically the queries return NO data at all (and show up as test failures of course). 

In each case when I startup the system/test, if the repository exists I use it, and (for tests) clean it by deleting the root node of the user content, and then starting again, otherwise there is nothing that exciting.

Please let me know if more info is needed.


ERROR 05-03 15:54:39,386 (LazyQueryResultImpl.java:getResults:266)  -Exception while executing query:
java.io.IOException : No such file or directory
    at java.io.UnixFileSystem.createFileExclusively(Native Method)
    at java.io.File.createNewFile(File.java:850)
    at org.apache.jackrabbit.core.query.lucene.FSDirectory$1.obtain( FSDirectory.java:119)
    at org.apache.lucene.store.Lock.obtain(Lock.java:51)
    at org.apache.lucene.store.Lock$With.run(Lock.java:98)
    at org.apache.lucene.index.IndexReader.open(IndexReader.java:141)
    at org.apache.lucene.index.IndexReader.open(IndexReader.java:136)
    at org.apache.jackrabbit.core.query.lucene.AbstractIndex.getReadOnlyIndexReader(AbstractIndex.java:191)
    at org.apache.jackrabbit.core.query.lucene.MultiIndex.getIndexReader (MultiIndex.java:616)
    at org.apache.jackrabbit.core.query.lucene.SearchIndex.executeQuery(SearchIndex.java:384)
    at org.apache.jackrabbit.core.query.lucene.LazyQueryResultImpl.executeQuery(LazyQueryResultImpl.java :204)
    at org.apache.jackrabbit.core.query.lucene.LazyQueryResultImpl.getResults(LazyQueryResultImpl.java:244)
    at org.apache.jackrabbit.core.query.lucene.LazyQueryResultImpl.<init>(LazyQueryResultImpl.java :161)
    at org.apache.jackrabbit.core.query.lucene.QueryImpl.execute(QueryImpl.java:164)
    at org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:142)"
1,"Possible rare thread hazard in IW.commitI was seeing a very rare intermittent failure in TestIndexWriter.testCommitThreadSafety.

The issue happens if one thread calls commit while another is flushing, and is exacerbated at high flush rates (eg maxBufferedDocs=2).  The thread doing commit will first flush, and then it syncs the files.  However in between those two, if other threads manage to add enough docs and trigger another flush, a 2nd new segment can sneak into the SegmentInfos before we sync.

This is normally harmless, in that it just means the commit includes a few more docs that had been added by other threads, so it's fine. But, it can mean that a committed segment references the still-open doc store files.  Our tests now catch this (I changed MockDirWrapper to throw an exception in this case), and so testCommitThreadSafety can fail with this exception.  If you hardwire the maxBufferedDocs to 2 it happens quite often.

It's not clear this is really a problem in real apps vs just our anal MockDirWrapper but I think we should fix it..."
1,"DocMakers setup for the ""docs.dir"" property fails when passing an absolute path.setConfig in TrecDocMaker assumes docs.dir is a relative path. Therefore it create new File(workDir, docs.dir). However, if docs.dir is an absolute path, this works incorrectly and results in No txt files in dataDir exception."
0,"rename optimize to a less cool-sounding nameI think users see the name optimize and feel they must do this, because who wants a suboptimal system? but this probably just results in wasted time and resources.

maybe rename to collapseSegments or something?"
0,"Improve read/write concurrencyI'd like to set up a few performance tests to help identify our worst bottlenecks for various kinds of concurrent read-only and read-write access patterns.

Once identified, I'm hoping to fix at least some of those bottlenecks."
0,Make Jackrabbit repository DTD easier to extendIt would be nice if a downstream project could easily extend the Jackrabbit repository configuration format by adding new configuration elements under the <Repository> root. Currently that can only be done by customizing a copy of the entire DTD. It would be better if the relevant parts of the DTD could simply be included via an external parameter entity into a downstream DTD.
0,"Deprecate / Remove DutchAnalyzer.setStemDictionaryDutchAnalyzer.setStemDictionary(File) prevents reuse of TokenStreams (and also uses a File which isn't ideal).  It should be deprecated in 3x, removed in trunk."
0,"Add test to check maven artifacts and their pomsAs release manager it is hard to find out if the maven artifacts work correct. It would be good to have an ant task that executes maven with a .pom file that requires all contrib/core artifacts (or one for each contrib) that ""downloads"" the artifacts from the local dist/maven folder and builds that test project. This would require maven to execute the build script. Also it should pass the ${version} ANT property to this pom.xml"
0,expose PM for versioning manager so that the consistency check can be run from test casesWe need to be able to run the PM consistency checks for the versioning store as well.
0,"RAMInputStream and RAMOutputStream without further bufferingFrom java-dev, Doug's reply of 12 Sep 2005 
on Delaying buffer allocation in BufferedIndexInput: 
 
Paul Elschot wrote: 
... 
> I noticed that RAMIndexInput extends BufferedIndexInput. 
> It has all data in buffers already, so why is there another 
> layer of buffering? 
 
No good reason: it's historical. 
 
To avoid this either: (a) the BufferedIndexInput API would need to be  
modified to permit subclasses to supply the buffer; or (b)  
RAMInputStream could subclass IndexInput directly, using its own  
buffers.  The latter would probably be simpler. 
 
End of quote. 
 
I made version (b) of RAMInputStream. 
Using this RAMInputStream, TestTermVectorsReader failed as the only 
failing test."
0,Improve Memory Consumption for merging DocValues SortedBytes variantsCurrently SortedBytes are loaded into memory during merge which could be a potential trap. Instead of loading them into Heap memory we can merge those sorted values with much smaller memory and without loading all values into ram.
0,"Releasing a connection is unconfirmedWhen a connection is attempted to be released using
HttpConnection.releaseConnection(), it is unclear whether this is actually done.
The implementation for the method is as follows in 3.0-beta1:

    /**
     * Release the connection.
     */
    public void releaseConnection() {
        LOG.trace(""enter HttpConnection.releaseConnection()"");
        if (locked) {
            LOG.debug(""Connection is locked.  Call to releaseConnection() ignore
        } else if (httpConnectionManager != null) {
            LOG.debug(""Releasing connection back to connection manager."");
            httpConnectionManager.releaseConnection(this);
        } else {
            LOG.warn(""HttpConnectionManager is null.  Connection cannot be relea
        }
    }

Silently ignoring a request (to release the connection, in this case) is hardly
ever the right thing to do, in my opinion. Instead, I suggest the method
indicates whether the connection was actually closed or not.

I see at least 2 alternatives:

1) throw an exception to indicate the connection could not be released;
2) return a flag indicating whether the connection could actually be released."
1,"BeanConfig may incorrectly throw ConfigurationExceptionWith the changes from JCR-1462 the BeanConfig.newInstance() may throw a ConfigurationException if the bean does not support a configuration parameter that is configured.

There may be cases where the check in newInstance() yields an unsupported property even though there is a bean property present with the given key. Because the implementation uses 'map.get(key) == null'  as a check for a property name the method will throw if the key exists but the value is null.

The implementation should rather use 'map.containsKey(key)'."
1,"getScheme() and getPort() return wrong defaults for HttpsURLgetScheme(), if called on an instance of HttpsURL, wrongly returns http instead
of https. That's because dynamic data binding doesn't work for final static
fields (see DEFAULT_SCHEME)."
0,"Add maven-eclipse-plugin properties to project.xml for easier configuration in IDE- add the maven.eclipse.resources.addtoclasspath=true property to project.properties (make the eclipse plugin create source dirs also for resources). 

- add the <eclipse.dependency>true</eclipse.dependency> property to all the jackrabbit internal dependencies in all the POMs (all the dependencies with ""jackrabbit"" groupId) so internal dependencies becomes project dependencies in eclipse."
0,"Changes.html not explicitly included in releaseNone of the release related ant targets explicitly call cahnges-to-html ... this seems like an oversight.  (currently it's only called as part of the nightly target)

"
0,Deprecate RepositoryService.getNodeInfo methodI suggest to deprecate RepositoryService.getNodeInfo in favor of RepositoryService.getItemInfos. The former method is not called from jsr2spi anymore and thus not required. 
0,"Remove synchronization in CompoundFileReaderCurrently there is what seems to be unnecessary synchronization in CompoundFileReader.  This is solved by cloning the base IndexInput.  Synchronization in low level IO classes creates lock contention on highly multi threaded Lucene installations, so much so that in many cases the CPU utilization never reaches the maximum without using something like ParallelMultiSearcher."
0,"Add support for tablespaces to Oracle related classesWhen a user account for an Oracle database has no or a temporary default tablespace, then the appropriate database schemas cannot be created. This is an issue for at least the following packages:
- o.a.j.core.persistence.bundle
- o.a.j.core.persistence.db
- o.a.j.core.fs.db
"
0,"Add support for terms in BytesRef format to Term, TermQuery, TermRangeQuery & Co.It would be good to directly allow BytesRefs in TermQuery and TermRangeQuery (as both queries convert the strings to BytesRef internally). For NumericRange support in Solr it will be needed to support numerics as ByteRef in single-term queries.

When this will be added, don't forget to change TestNumericRangeQueryXX to use the BytesRef ctor of TRQ."
1,Deadlock in IndexWriterIf autoCommit == true a merge usually triggers a commit. A commit (prepareCommit) can trigger a merge vi the flush method. There is a synchronization mechanism for commit (commitLock) and a separate synchronization mechanism for merging (ConcurrentMergeScheduler.wait). If one thread holds the commitLock monitor and another one holds the ConcurrentMergeScheduler monitor we have a deadlock.
1,"QueryStat getPopularQueries doesn't set the proper positionEmbarrassing copy/paste error. I was updating the wrong array and the position info was never returned. 

This made any jmx client to fail with: 
at javax.management.openmbean.TabularDataSupport.checkValueAndIndex(TabularDataSupport.java:871) 
at javax.management.openmbean.TabularDataSupport.internalPut(TabularDataSupport.java:331) 
at javax.management.openmbean.TabularDataSupport.put(TabularDataSupport.java:323) 
at org.apache.jackrabbit.core.jmx.QueryStatManager.asTabularData(QueryStatManager.java:103)"
0,"NodeTest.testAddNodeConstraintViolationExceptionUndefinedNodeType relies on addNode(name, ""nt:base"")NodeTest.testAddNodeConstraintViolationExceptionUndefinedNodeType() relies on the ability to create a new node of type ""nt:base"", which isn't something repositories are required to support.

Proposal: make the node type name for this test case configurable.
"
1,"NPE Thrown when two Cluster Nodes are hitting the same underlying database.I've created a test that creates two repositories with clustering enabled that are backed by the same database.  Using the following workflow causes a NullPointerException to be thrown.

The workflow I'm using is:
The root node is versioned.
ClusterNode1 creates a versioned child node named ""foo"".
The test waits to make sure the syncDelay has passed so ClusterNode2 will notice the newly created node.
ClusterNode2 retrieves the ""foo"" child node and removes it.
The test waits for the change ClusterNode1 to sync with that change.
ClusterNode1 tries to create another new node however a NullPointerException is thrown when the it tries to checkout the rootNode.

java.lang.NullPointerException: null values not allowed
	at org.apache.commons.collections.map.AbstractReferenceMap.put(AbstractReferenceMap.java:251)
	at org.apache.jackrabbit.core.version.VersionManagerImpl.getItem(VersionManagerImpl.java:280)
	at org.apache.jackrabbit.core.version.XAVersionManager.getItem(XAVersionManager.java:334)
	at org.apache.jackrabbit.core.version.AbstractVersionManager.getVersion(AbstractVersionManager.java:87)
	at org.apache.jackrabbit.core.NodeImpl.getBaseVersion(NodeImpl.java:3198)
	at org.apache.jackrabbit.core.NodeImpl.checkout(NodeImpl.java:2991)
	at com.cerner.system.configuration.repository.jcr.SimpleJackrabbitConflictTest.testNullPointerExceptionThrown(SimpleJackrabbitConflictTest.java:96)"
1,"CartesianPolyFilterBuilder doesn't properly account for which tiers actually exist in the index In the CartesianShapeFilterBuilder, there is logic that determines the ""best fit"" tier to create the Filter against.  However, it does not account for which fields actually exist in the index when doing so.  For instance, if you index tiers 1 through 10, but then choose a very small radius to restrict the space to, it will likely choose a tier like 15 or 16, which of course does not exist."
1,"TestBytesRefHash#testCompact is brokenTestBytesRefHash#testCompact fails when run with ant test -Dtestcase=TestBytesRefHash -Dtestmethod=testCompact -Dtests.seed=-7961072421643387492:5612141247152835360
{noformat}

    [junit] Testsuite: org.apache.lucene.util.TestBytesRefHash
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.454 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestBytesRefHash -Dtestmethod=testCompact -Dtests.seed=-7961072421643387492:5612141247152835360
    [junit] NOTE: test params are: codec=PreFlex, locale=et, timezone=Pacific/Tahiti
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestBytesRefHash]
    [junit] NOTE: Linux 2.6.35-28-generic amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=12,threads=1,free=363421800,total=379322368
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testCompact(org.apache.lucene.util.TestBytesRefHash):	Caused an ERROR
    [junit] bitIndex < 0: -27
    [junit] java.lang.IndexOutOfBoundsException: bitIndex < 0: -27
    [junit] 	at java.util.BitSet.set(BitSet.java:262)
    [junit] 	at org.apache.lucene.util.TestBytesRefHash.testCompact(TestBytesRefHash.java:146)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1260)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1189)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.util.TestBytesRefHash FAILED
{noformat}

the test expects that _TestUtil.randomRealisticUnicodeString(random, 1000); will never return the same string.

I will upload a patch soon."
1,"IdleConnectionHandler can leave closed connections in a inconsistent stateIdleConnectionHandler when shutting down 'stale' connection does not update the state of AbstractPoolEntry thus causing an inconsistency between the state of the connection (closed) and that of the pool entry (still assumed open). The problem is mitigated by the fact that the pooling manager usually evicts closed connections almost immediately. There is a small window of time in the ThreadSafeClientConnManager#closeIdleConnection method, at which a connection can be closed by the IdleConnectionHandler and immediately leased from the pool by another thread in an inconsistent state before the main thread gets a chance to re-acquire the pool lock and clean out closed connections.

For 4.0.x the problem can be worked around by retaining the pool lock for the entire span of the #closeIdleConnection. For the 4.1 branch a better solution should be devised. This probably means a complete rewrite or deprecation of IdleConnectionHandler."
1,"iterative removal of same-name sibling nodes might under certain circumstances throw unexpected exceptionscode fragment to reproduce the issue:

            // setup test
            if (root.hasNode(""tmp"")) {
                root.getNode(""tmp"").remove();
                session.save();
            }
            Node tmp = root.addNode(""tmp"");
            for (int i = 0; i < 4; i++) {
                Node a = tmp.addNode(""a"");
                System.out.println(""added "" + a.getPath());
            }
            session.save();

            // iterative removal of same name sibling child nodes
            NodeIterator ni = tmp.getNodes();
            while (ni.hasNext()) {
                Node n = ni.nextNode();
                System.out.println(""removing "" + n.getPath());
                n.remove();
                tmp.save();
            }

console output:

added /tmp/a
added /tmp/a[2]
added /tmp/a[3]
added /tmp/a[4]
removing /tmp/a
removing /tmp/a
removing /
javax.jcr.RepositoryException: /: cannot remove root node
	at org.apache.jackrabbit.core.ItemImpl.internalRemove(ItemImpl.java:766)
	at org.apache.jackrabbit.core.ItemImpl.remove(ItemImpl.java:997)
	at org.apache.jackrabbit.core.Test.main(Test.java:141)


note that the msg of the exception is misleading: the above code did never try to remove
the root node. 

the exception is caused by a bug in CachingHierarchyManager which fails to update
the cache correctly.

btw: if you comment the first logging stmt, i.e. 

                //System.out.println(""added "" + a.getPath());

the problem doesn't occur anymore."
0,"WorkspaceRestoreTest extends RestoreTestWorkspaceRestoreTest extends RestoreTest in order to re-use variables and setUp/tearDown code. 

On the other hand, this causes all tests from RestoreTest that aren't overriden by WorkspaceRestoreTest to be run twice.

Proposal: decouple the classes, copying over the interesting parts from RestoreTest into WorkspaceRestoreTest.
"
0,"Manage Lucene FieldCaches per index segmentJackrabbit uses an IndexSearcher which searches on a single IndexReader which is most likely to be an instance of CachingMultiReader. On every search that does sorting or range queries a FieldCache is populated and associated with this instance of a CachingMultiReader. On successive queries which operate on this CachingMultiReader you will get a tremendous speedup for queries which can reuse  those associated FieldCache instances.
The problem is that Jackrabbit creates a new CachingMultiReader _everytime_ one of the underlying indexes are modified. This means if you just change _one_ item in the repository you will need to rebuild all those FieldCaches because the existing FieldCaches are associated with the old instance of CachingMultiReader.
This does not only lead to slow search response times for queries which contains range queries or are sorted by a field but also leads to massive memory consumption (depending on the size of your indexes) because there might be multiple instances of CachingMultiReaders in use if you have a scenario where a lot of queries and item modifications are executed concurrently.
The goal is to keep those FieldCaches as long as possible."
1,"PdfTextFilter may leave parsed document open in case of errorsIn case of errors in a parsed PDF document jackrabbit may fail to properly close the parsed document. PDFBox will write a stack trace to system out at finalize to warn agains this.

this is the resulting log:

WARN org.apache.jackrabbit.core.query.LazyReader LazyReader.java(read:82) 20.02.2007 15:42:50 exception initializing reader org.apache.jackrabbit.core.query.PdfTextFilter$1: java.io.IOException: Error: Expected hex number, actual=' 2'
java.lang.Throwable: Warning: You did not close the PDF Document
   at org.pdfbox.cos.COSDocument.finalize(COSDocument.java:384)
   at java.lang.ref.Finalizer.invokeFinalizeMethod(Native Method)
   at java.lang.ref.Finalizer.runFinalizer(Finalizer.java:83)
   at java.lang.ref.Finalizer.access$100(Finalizer.java:14)
   at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:160)


this may happens because the parse() method at

parser = new PDFParser(new BufferedInputStream(in));
parser.parse();

immediately creates a document, but it can throw an exception while processing the file.
PdfTextFilter should check if parser still holds a document and close it appropriately.

"
1,"Errors during concurrent session import of nodes with same UUIDs21.08.2009 16:22:14 *ERROR* [Executor 0] ConnectionRecoveryManager: could not execute statement, reason: The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique index identified by 'SQL090821042140130' defined on 'DEFAULT_BUNDLE'., state/code: 23505/20000 (ConnectionRecoveryManager.java, line 453)
21.08.2009 16:22:14 *ERROR* [Executor 0] BundleDbPersistenceManager: failed to write bundle: 6c292772-349e-42b3-8255-7729615c67de (BundleDbPersistenceManager.java, line 1212)
ERROR 23505: The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique index identified by 'SQL090821042140130' defined on 'DEFAULT_BUNDLE'.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.IndexChanger.insertAndCheckDups(Unknown Source)
	at org.apache.derby.impl.sql.execute.IndexChanger.doInsert(Unknown Source)
	at org.apache.derby.impl.sql.execute.IndexChanger.insert(Unknown Source)
	at org.apache.derby.impl.sql.execute.IndexSetChanger.insert(Unknown Source)
	at org.apache.derby.impl.sql.execute.RowChangerImpl.insertRow(Unknown Source)
	at org.apache.derby.impl.sql.execute.InsertResultSet.normalInsertCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.InsertResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.execute(Unknown Source)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmtInternal(ConnectionRecoveryManager.java:371)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmtInternal(ConnectionRecoveryManager.java:298)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmt(ConnectionRecoveryManager.java:261)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmt(ConnectionRecoveryManager.java:239)
	at org.apache.jackrabbit.core.persistence.bundle.BundleDbPersistenceManager.storeBundle(BundleDbPersistenceManager.java:1209)
	at org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.putBundle(AbstractBundlePersistenceManager.java:709)
	at org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.storeInternal(AbstractBundlePersistenceManager.java:651)
	at org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.store(AbstractBundlePersistenceManager.java:527)
	at org.apache.jackrabbit.core.persistence.bundle.BundleDbPersistenceManager.store(BundleDbPersistenceManager.java:563)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:724)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:1101)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:351)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:354)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:326)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1098)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:925)
	at org.apache.jackrabbit.core.ConcurrentImportTest$1.execute(ConcurrentImportTest.java:73)
	at org.apache.jackrabbit.core.AbstractConcurrencyTest$Executor.run(AbstractConcurrencyTest.java:209)
	at java.lang.Thread.run(Thread.java:637)
"
0,"Sep codec should store less in terms dictI'm working on improving Lucene's performance with int block codecs
(FOR/PFOR), but in early perf testing I found that these codecs cause
a big perf hit to those MTQs that need to scan many terms but don't
end up accepting many of those terms (eg fuzzy, wildcard, regexp).

This is because sep codec stores much more in the terms dict, since
each file is separate, ie seek points for each of doc, frq, pos, pyl,
skp files.

So I'd like to shift these seek points to instead be stored in the doc
file, except for the doc seek point itself.  Since a given query will
always need to seek to the doc file, this does not add an extra seek.
But it saves tons of vInt decodes for the next/seke intensive MTQs...
"
0,"Jenkins trunk tests (nightly only) fail quite often with OOM in Automaton/FST testsThe nightly Job Lucene-trunk quite often fails with OOM (in several methods, not always in the same test):

Example from last night (this time a huge Automaton):

{noformat}
[junit] java.lang.OutOfMemoryError: Java heap space
[junit] Dumping heap to /home/hudson/hudson-slave/workspace/Lucene-trunk/heapdumps/java_pid38855.hprof ...
[junit] Heap dump file created [86965954 bytes in 1.186 secs]
[junit] Testsuite: org.apache.lucene.index.TestTermsEnum
[junit] Testcase: testIntersectRandom(org.apache.lucene.index.TestTermsEnum):	Caused an ERROR
[junit] Java heap space
[junit] java.lang.OutOfMemoryError: Java heap space
[junit] 	at org.apache.lucene.util.automaton.RunAutomaton.<init>(RunAutomaton.java:128)
[junit] 	at org.apache.lucene.util.automaton.ByteRunAutomaton.<init>(ByteRunAutomaton.java:28)
[junit] 	at org.apache.lucene.util.automaton.CompiledAutomaton.<init>(CompiledAutomaton.java:134)
[junit] 	at org.apache.lucene.index.TestTermsEnum.testIntersectRandom(TestTermsEnum.java:266)
[junit] 	at org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)
[junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)
[junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)
[junit] 
[junit] 
[junit] Tests run: 6, Failures: 0, Errors: 1, Time elapsed: 11.699 sec
{noformat}

Other traces:

{noformat}
[junit] Testsuite: org.apache.lucene.util.fst.TestFSTs
[junit] Testcase: testRealTerms(org.apache.lucene.util.fst.TestFSTs):	Caused an ERROR
[junit] Java heap space
[junit] java.lang.OutOfMemoryError: Java heap space
[junit] 	at org.apache.lucene.util.ArrayUtil.grow(ArrayUtil.java:338)
[junit] 	at org.apache.lucene.util.fst.FST$BytesWriter.writeBytes(FST.java:927)
[junit] 	at org.apache.lucene.util.fst.ByteSequenceOutputs.write(ByteSequenceOutputs.java:113)
[junit] 	at org.apache.lucene.util.fst.ByteSequenceOutputs.write(ByteSequenceOutputs.java:32)
[junit] 	at org.apache.lucene.util.fst.FST.addNode(FST.java:451)
[junit] 	at org.apache.lucene.util.fst.NodeHash.add(NodeHash.java:122)
[junit] 	at org.apache.lucene.util.fst.Builder.compileNode(Builder.java:180)
[junit] 	at org.apache.lucene.util.fst.Builder.finish(Builder.java:495)
[junit] 	at org.apache.lucene.index.codecs.memory.MemoryCodec$TermsWriter.finish(MemoryCodec.java:232)
[junit] 	at org.apache.lucene.index.FreqProxTermsWriterPerField.flush(FreqProxTermsWriterPerField.java:414)
[junit] 	at org.apache.lucene.index.FreqProxTermsWriter.flush(FreqProxTermsWriter.java:92)
[junit] 	at org.apache.lucene.index.TermsHash.flush(TermsHash.java:117)
[junit] 	at org.apache.lucene.index.DocInverter.flush(DocInverter.java:80)
[junit] 	at org.apache.lucene.index.DocFieldProcessor.flush(DocFieldProcessor.java:78)
[junit] 	at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:472)
[junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:420)
[junit] 	at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:568)
[junit] 	at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:366)
[junit] 	at org.apache.lucene.index.IndexReader.open(IndexReader.java:317)
[junit] 	at org.apache.lucene.util.fst.TestFSTs.testRealTerms(TestFSTs.java:1034)
[junit] 	at org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)
{noformat}

or:

{noformat}
[junit] Testsuite: org.apache.lucene.util.automaton.TestCompiledAutomaton
[junit] Testcase: testRandom(org.apache.lucene.util.automaton.TestCompiledAutomaton):	Caused an ERROR
[junit] Java heap space
[junit] java.lang.OutOfMemoryError: Java heap space
[junit] 	at org.apache.lucene.util.automaton.RunAutomaton.<init>(RunAutomaton.java:128)
[junit] 	at org.apache.lucene.util.automaton.ByteRunAutomaton.<init>(ByteRunAutomaton.java:28)
[junit] 	at org.apache.lucene.util.automaton.CompiledAutomaton.<init>(CompiledAutomaton.java:134)
[junit] 	at org.apache.lucene.util.automaton.TestCompiledAutomaton.build(TestCompiledAutomaton.java:39)
[junit] 	at org.apache.lucene.util.automaton.TestCompiledAutomaton.testTerms(TestCompiledAutomaton.java:55)
[junit] 	at org.apache.lucene.util.automaton.TestCompiledAutomaton.testRandom(TestCompiledAutomaton.java:101)
[junit] 	at org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)
[junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)
[junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)
{noformat}

Almost every nightly test fails, history: [https://builds.apache.org/job/Lucene-trunk]

We should maybe raise the max heap space or reduce doc counts/..."
0,"Tests fail with NoClassDefFoundError: org/w3c/dom/ranges/DocumentRangeThe nekohtml dependency in jackrabbit-text-extractors brings in a transitive xerces 2.4.0 dependency without the extra XML API classes required by Xerces. This causes test failures in jackrabbit-core and jackrabbit-jca because the Xerces dependency included in the test classpath overrides the default XML parser. Then, when the test cases try to parse XML documents, the missing XML API dependency causes a NoClassDefFoundError."
1,"RTF text extractor fails on Java 1.4 in some environmentsI've seen the RTF text extractor fail with the following errors with Java 1.4 on Unix platforms. Both are platform issues, but Jackrabbit should be prepared for such cases and for example just log a warning and return an empty text stream when encountering these errors.

java.lang.UnsatisfiedLinkError: /home/jukka/bin/java/j2sdk1.4.2_18/jre/lib/i386/libawt.so: libXp.so.6: cannot open shared object file: No such file or directory
        at java.lang.ClassLoader$NativeLibrary.load(Native Method)
        at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1586)
        at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1503)
        at java.lang.Runtime.loadLibrary0(Runtime.java:788)
        at java.lang.System.loadLibrary(System.java:834)
        at sun.security.action.LoadLibraryAction.run(LoadLibraryAction.java:50)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.awt.NativeLibLoader.loadLibraries(NativeLibLoader.java:38)
        at sun.awt.DebugHelper.<clinit>(DebugHelper.java:29)
        at java.awt.EventQueue.<clinit>(EventQueue.java:83)
        at javax.swing.SwingUtilities.isEventDispatchThread(SwingUtilities.java:1238)
        at javax.swing.text.StyleContext.reclaim(StyleContext.java:419)
        at javax.swing.text.StyleContext.addAttribute(StyleContext.java:276)
        at javax.swing.text.StyleContext$NamedStyle.addAttribute(StyleContext.java:1468)
        at javax.swing.text.StyleContext$NamedStyle.setName(StyleContext.java:1278)
        at javax.swing.text.StyleContext$NamedStyle.<init>(StyleContext.java:1226)
        at javax.swing.text.StyleContext.addStyle(StyleContext.java:88)
        at javax.swing.text.StyleContext.<init>(StyleContext.java:68)
        at javax.swing.text.DefaultStyledDocument.<init>(DefaultStyledDocument.java:88)
        at org.apache.jackrabbit.extractor.RTFTextExtractor.extractText(RTFTextExtractor.java:60)
        at org.apache.jackrabbit.extractor.RTFTextExtractorTest.testExtractor(RTFTextExtractorTest.java:35)

java.lang.InternalError: Can't connect to X11 window server using ':0.0' as the value of the DISPLAY variable.
	at sun.awt.X11GraphicsEnvironment.initDisplay(Native Method)
	at sun.awt.X11GraphicsEnvironment.<clinit>(X11GraphicsEnvironment.java:134)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:141)
	at java.awt.GraphicsEnvironment.getLocalGraphicsEnvironment(GraphicsEnvironment.java:62)
	at sun.awt.motif.MToolkit.<clinit>(MToolkit.java:81)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:141)
	at java.awt.Toolkit$2.run(Toolkit.java:748)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.awt.Toolkit.getDefaultToolkit(Toolkit.java:739)
	at java.awt.Toolkit.getEventQueue(Toolkit.java:1519)
	at java.awt.EventQueue.isDispatchThread(EventQueue.java:676)
	at javax.swing.SwingUtilities.isEventDispatchThread(SwingUtilities.java:1238)
	at javax.swing.text.StyleContext.reclaim(StyleContext.java:419)
	at javax.swing.text.StyleContext.addAttribute(StyleContext.java:276)
	at javax.swing.text.StyleContext$NamedStyle.addAttribute(StyleContext.java:1468)
	at javax.swing.text.StyleContext$NamedStyle.setName(StyleContext.java:1278)
	at javax.swing.text.StyleContext$NamedStyle.<init>(StyleContext.java:1226)
	at javax.swing.text.StyleContext.addStyle(StyleContext.java:88)
	at javax.swing.text.StyleContext.<init>(StyleContext.java:68)
	at javax.swing.text.DefaultStyledDocument.<init>(DefaultStyledDocument.java:88)
	at org.apache.jackrabbit.extractor.RTFTextExtractor.extractText(RTFTextExtractor.java:60)
	at org.apache.jackrabbit.extractor.RTFTextExtractorTest.testExtractor(RTFTextExtractorTest.java:35)

"
0,"clean up serialization in the codebaseWe removed contrib/remote, but forgot to cleanup serialization hell everywhere.

this is no longer needed, never really worked (e.g. across versions), and slows 
development (e.g. i wasted a long time debugging stupid serialization of 
Similarity.idfExplain when trying to make a patch for the scoring system).
"
0,Some unit tests are not well configuredSome unit tests used for the annotation support are not well defined. There are inherited from DigesterTestBase instead of AnnotationTestBase
1,"WriteLineDocTask incorrectly normalizes fieldsWriteLineDocTask normalizes the body, title and date fields by replacing any ""\t"" with a space. However, if any one of them contains newlines, LineDocMaker will fail, since the first line read will include some of the text, however the second line, which it now expects to be a new document, will include other parts of the text.

I don't know how we didn't hit it so far. Maybe the wikipedia text doesn't have such lines, however when I ran over the TREC collection I hit a lot of those.

I will attach a patch shortly."
0,"Provide more Example Code- better project samples showing how to use HttpClient in a variety of ways. 
There is already a src/examples directory which is excellent.  Its in the right
place and should be build with a full compile, if only to know how any API
changes may effect example code, and that we will be required to keep them
current.
- make sure it uses the 2.0 API and no depricated methods!"
0,Improved join query performanceOur current implementation of SQL2 join queries does not perform very well on pretty much any non-trivial data set.
0,"Re-add SorterTemplate and use it to provide fast ArraySorting and replace BytesRefHash sortingThis patch adds back an optimized and rewritten SorterTemplate back to Lucene (removed after release of 3.0). It is of use for several components:

- Automaton: Automaton needs to sort States and other things. Using Arrays.sort() is slow, because it clones internally to ensure stable search. This component is much faster. This patch adds Arrays.sort() replacements in ArrayUtil that work with natural order or using a Comparator<?>. You can choose between quickSort and mergeSort.
- BytesRefHash uses another QuickSort algorithm without insertionSort for very short ord arrays. This class uses SorterTemplate to provide the same with insertionSort fallback in a very elegant way. Ideally this class can be used everywhere, where the sort algorithm needs to be separated from the underlying data and you can implement a swap() and compare() function (that get slot numbers instead of real values). This also applies to Solr (Yonik?).

SorterTemplate provides quickSort and mergeSort algorithms. Internally for short arrays, it automatically chooses insertionSort (like JDK's Arrays). The quickSort algorith was copied modified from old BytesRefHash. This new class only shares MergeSort with the original CGLIB SorterTemplate, which is no longer maintained."
0,Encapsulate RepositoryHelper fieldThis is a first step towards a test suite that will run tests with multiple threads concurrently.
1,"MultipartPost closes input streamThis is something of a collection of issues that are all interrelated.

1. MultipartPost calls close on the outputstream it retrieved from 
HttpConnection which causes an exception to be thrown later on.  This call 
should be replaced with a call to flush().

2. The MultipartPost classes do not have any logging in them.  We should add 
trace statements at a minimum.

3. new FilePart(String, File) throws a null pointer exception.

4. The tests in TestPartsNoHost are broken.

I'll attach patches for these fixes in a moment, broken down as much as 
possible."
1,"CookieSpecBase.domainMatch() leaks cookies to 3rd party domainsThe change committed for #32833
<http://issues.apache.org/bugzilla/show_bug.cgi?id=32833> is buggy; it doesn't
match browser behavior and in fact leaks cookies to third party domains. 

To see, try the following:

CookieSpecBase cspec = new CookieSpecBase();
Cookie cookie = new Cookie("".hotmail.com"",""foo"",""bar"",""/"",Integer.MAX_VALUE,false);
cspec.match(""iwanttostealcookiesfromhotmail.com"",80,""/"",false,cookie);

It will return true. Testing in Firefox1.0.4 and IE6 show no such similar
leakage for similar cases. (Indeed, it'd be a headline-making privacy bug if
they were to do this.)

Those browsers do, in my limited testing, behave as desired by the filer of
#32833: a cookie of domain value '.mydomain.com' will be returned to exact host
'mydomain.com' (. However, the fix that was suggested was overbroad.

I suggest instead for CookieSpecBase.domainMatch():

    public boolean domainMatch(final String host, final String domain) {
// BUGGY: matches a '.service.com' cookie to hosts like 'enemyofservice.com'
//        return host.endsWith(domain)
//            || (domain.startsWith(""."") && host.endsWith(domain.substring(1)));
// BETTER: RFC2109, plus matches a '.service.com' cookie to exact host 'service.com'
        return host.equals(domain)
            || (domain.startsWith(""."") 
                    && (host.endsWith(domain)
                            || host.equals(domain.substring(1))));
    }"
1,"Jackrabbit's lucene based query implementation does not check property constraints on the root node.XPath queries of the kind ""/jcr:root[<any property constraint>]"" apparently always match."
0,"Selective disabling of checks in ItemValidatorI would like to be able to selectively disable checks in ItemValidator in the scope of an operation performed through methods of the SessionState class. Doing so would provide simple means for internally modifying (for example) protected items. Currently such modifications must be done 'manually' on the item state level. This approach is very error prone and not very DRY.

With my upcoming patch in place, setting a protected property would look like this:

final Node parent = ...
final Value value = ...
SessionState sessionState = sessionContext.getSessionState();

Property property = sessionState.performUnchecked(new SessionOperation<Property>() {
    public Property perform(SessionContext context) throws RepositoryException {
        return parent.setProperty(""foo"", value);
    }
}, ItemValidator.CHECK_CONSTRAINTS);

That is, users need to have access to the session context in order to disable checks which makes this only usable from inside Jackrabbit. "
0,"Provide means for exception handling for QueryNodeVisitor implementationsCurrently the methods of QueryNodeVisitor do not declare any exceptions. Even though the query tree might be syntactically correct, an implementation might reach a point where it cannot continue (i.e. if it does not support one of the optional query features). For such cases there are currently two solution: 1. throw an unchecked exception or 2. communicate the error state through the visitor using the data object passed along. 
While I don't like 2. it is still an option. For 1. I'm not sure if this is the right way to go. It might be better to actually throw a checked exception. I therefore created a patch which declares RepositoryException on all visit methods of QueryNodeVisitor. Although the necessary changes in classes using QueryNodeVisitor are trivial, there are quite many of them. 

Any opinions on checked exception with probably breaking (trivially) existing code vs. using not checked exceptions?


"
0,"Small speedups to DocumentsWriterSome small fixes that I found while profiling indexing Wikipedia,
mainly using our own quickSort instead of Arrays.sort.

Testing first 200K docs of Wikipedia shows a speedup from 274.6
seconds to 270.2 seconds.

I'll commit in a day or two."
0,Avoid docFreq calls for non-fulltext queriesLooking up the document frequency for a query terms costs I/O and is only useful for fulltext queries (-> the document frequency has an influence on the relevance of a result node). Simple constraints like: @foo = 'bar' may return a constant value for docFreq.
1,"impl.conn.Wire uses String.getBytes() which depends on the default charsetimpl.conn.Wire uses String.getBytes() which depends on the default charset

The methods 
public void output(final String s)
and
public void input(final String s)

could probably be recoded to avoid this problem, as the output routine uses a StringBuilder."
0,"revise Scorer visitor APICurrently there is an (expert) API in scorer to allow you to take a scorer, and visit its children etc (e.g. booleanquery).

I think we should improve this, so its general to all queries.

The current issues are:
* the current api is hardcoded for booleanquery's Occurs, but we should support other types of children (e.g. disjunctionmax)
* it can be difficult to use the API, because of the amount of generics and the visitor callback API. 
* the current API enforces a DFS traversal when you might prefer BFS instead.
"
0,"Remove deprecated classes in jackrabbit-coresimilar to JCR-2109 i think i would be favorable to get rid of stuff that has been marked deprecated for 1.x releases.
"
0,AbstractQueryTest.evaluateResultOrder() should fail if workspace does not contain enough contentThe method AbstractQueryTest.evaluateResultOrder() currently throws a NotExecutableException if the workspace contains less than two nodes that can be used for ordering. It should rather fail with an error message telling that the workspace does not contain sufficient content to run the test.
0,"[PATCH] fixes for gcj target.I've modified the Makefile so that it compiles with GCJ-4.0.

This involved fixing the CORE_OBJ macro to match the generated jar file as well
as excluding FieldCacheImpl from being used from its .java source (GCJ has
problems with anonymous inner classes, I guess).

Also, I changed the behaviour of FieldInfos.fieldInfo(int). It depended on
catching IndexOutOfBoundsException exception. I've modified it to test the
bounds first, returning -1 in that case. This helps with gcj since we build with
-fno-bounds-check.

I compiled with;

GCJ=gcj-4.0 GCJH=gcjh-4.0 GPLUSPLUS=g++-4.0 ant clean gcj

patch to follow."
0,"Make FieldSelector usable from Searchable Seems reasonable that you would want to be able to specify a FieldSelector from Searchable because many systems do not use IndexSearcher (where you can get a Reader), but instead use Searchable or Searcher so that Searchers and MultiSearchers can be used in a polymorphic manner."
1,"BasicOperations.concatenate creates invariantsI started writing a test for LUCENE-2716, and i found a problem with BasicOperations.concatenate(Automaton, Automaton):
it creates automata with invariant representation (which should never happen, unless you manipulate states/transitions manually).

strangely enough the BasicOperations.concatenate(List<Automaton>) does not have this problem.
"
1,"AccessControlProvider#getEffectivePolicies for a set of principals does not include repo-level acas of JCR-2774 the resource based ac implementation allows to edit permissions for repository level operations.
however, ACLProvider#getEffectivePolicies(Set<Principal>, CompiledPermissions) does not include the repo level AC
in the result set due to a missing test for regular acl OR repo-level acl."
0,"Default namespaces in JackrabbitNodeTypeManager.registerNodetypesIt would be nice if it wasn't necessary to always specify all the namespaces in node type definition files passed to JackrabbitNodeTypeManager.registerNodeTypes(). The node type parsers should by default use the persistent namespace mappings, but allow custom mappigns to be specified in the parsed node type definition files."
0,"URI uses  sun.security.action.GetPropertyActionURI uses a sun.* class but should not.  Use of this class should be removed.

Reported my Mark Wilcox"
0,"Allow to disable referential integrity checking for workspaceSome operations like clone, remove operating on huge subtree of nodes requires a lot of memory. To copy, clone, remove subtree all nodes are loaded into transient spaces. It allows such operations to be transactional, from other side it requires a lot of heap size and this memory size is directly dependent on the size of subtree (number of nodes). In result of this in some cases it is impossible to make such operations in one step. In our environment sometimes 1 GB of java heap is not enough to succesfully clone subtree  from one workspace to another.

You can always clone (copy, remove) tree in chunks, but if you have references between subtrees such approach fails. Possibilty of temporary disabling referential integrity checking for experienced JCR user could be very usefull then.

Another use case is to allow to clone selected subtrees of the whole structure between worskpaces. In our application we need to clone only some selected subtrees from one workspace to another. But we can not do that because of existing references. We need to clone the whol estructure first, then remove all unwanted nodes, which is really time expensive and memory consuming.
"
0,"GData Server MileStone 1 RevisionSome Improvements to the GData Server.
CRUD actions for Entries implemented / tested 
StorageComponent storing entries / feeds / users
Dynamic Feed elements like links added.
Decoupled all server components (storage / ReqeustHandler etc) using lookup service

Added some JavaDoc "
1,"Repository holds onto Session instance after logout
After a call to Session.logout the Repository instance's activeSession map still holds a reference to the session. This causes a problem when trying to unlock nodes locked by another session, the addLockToken method rejects the lock token.

Looking at the code in Session.logout, it tries to notify SessionListeners about the logout but Repository, which implements the SessionListener interface and will remove a session on logout, doesn't register with the Session to receive the logout notification.
"
0,"JcrRemotingServlet should interpolate system properties in the home init-paramFor deployment scenarios where the same Jackrabbit WAR file is deployed multiple times on the same server with the same current working directory, it is useful to have the home init-param support system property interpolation."
0,"New Jackrabbit site skinSome time ago Michael Eppelheimer from Day created a new skin for the Jackrabbit web site, and I've now streamlined it a bit and integrated it with the Maven site build mechanism.

The templates should be easy to adapt also for Confluence when we get around to that migration."
1,"Cannot move a first-level nodeGiven /nodeA,
session.move(""/nodeA"", ""/nodeB"")

throws this exception:

javax.jcr.PathNotFoundException: no ancestor at degree 1: {}
	at org.apache.jackrabbit.spi.commons.name.PathFactoryImpl$PathImpl.getAncestor(PathFactoryImpl.java:481)
	at org.apache.jackrabbit.core.retention.RetentionRegistryImpl.hasEffectiveRetention(RetentionRegistryImpl.java:291)
	at org.apache.jackrabbit.core.ItemValidator.hasRetention(ItemValidator.java:426)
	at org.apache.jackrabbit.core.ItemValidator.checkCondition(ItemValidator.java:328)
	at org.apache.jackrabbit.core.ItemValidator.checkRemove(ItemValidator.java:281)
	at org.apache.jackrabbit.core.SessionImpl.move(SessionImpl.java:1075)
	at org.apache.jackrabbit.core.MoveAtRootTest.testMoveAtRoot(MoveAtRootTest.java:54)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:456)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)

"
0,"TermVectorMapper.setDocumentNumber()Passes down the index of the document whose term vector is currently beeing mapped, once for each top level call to a term vector reader.  

See http://www.nabble.com/Allowing-IOExceptions-in-TermVectorMapper--tf4687704.html#a13397341"
1,"jackrabbit wrongly think nodetype is changed on nodetype re-registrationWhen trying node type re-registration with jackrabbit 2.0, it wrongly detects a nodetype as having changed, with non-trivial changes. Example nodetype definition;

[nen:profile] > mix:referenceable mixin orderable
- nen:dn (string)
- nen:cn (string)
- * (string)
+ * multiple

Exception on nodetype re-registration;

javax.jcr.RepositoryException: The following nodetype change contains
non-trivial changes.Up until now only trivial changes are supported.
(see javadoc for org.apache.jackrabbit.core.nodetype.NodeTypeDefDiff):
org.apache.jackrabbit.core.nodetype.NodeTypeDefDiff[
       nodeTypeName={http://netenviron.com/nen/1.0}profile,
       mixinFlagDiff=NONE,
       supertypesDiff=NONE,
       propertyDifferences=[
               org.apache.jackrabbit.core.nodetype.NodeTypeDefDiff$PropDefDiff[itemName={http://netenviron.com/nen/1.0}dn,
type=TRIVIAL, operation=MODIFIED],
               org.apache.jackrabbit.core.nodetype.NodeTypeDefDiff$PropDefDiff[itemName={http://netenviron.com/nen/1.0}cn,
type=TRIVIAL, operation=MODIFIED],
               org.apache.jackrabbit.core.nodetype.NodeTypeDefDiff$PropDefDiff[itemName={}*,
type=TRIVIAL, operation=MODIFIED]
       ],
       childNodeDifferences=[
               org.apache.jackrabbit.core.nodetype.NodeTypeDefDiff$ChildNodeDefDiff[itemName={}*,
type=MAJOR, operation=REMOVED],
               org.apache.jackrabbit.core.nodetype.NodeTypeDefDiff$ChildNodeDefDiff[itemName={}*,
type=TRIVIAL, operation=ADDED]
       ]
]

       at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.reregisterNodeType(NodeTypeRegistry.java:442)
       at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.reregisterNodeType(NodeTypeRegistry.java:363)
       at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:589)
       at org.apache.jackrabbit.commons.cnd.CndImporter.registerNodeTypes(CndImporter.java:118)
       at com.netenviron.content.manager.SessionManager.checkRepositorySchema(SessionManager.java:355)
       at com.netenviron.content.manager.SessionManager.afterPropertiesSet(SessionManager.java:199)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1288)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1257)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:438)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory$1.run(AbstractAutowireCapableBeanFactory.java:383)
       at java.security.AccessController.doPrivileged(Native Method)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:353)
       at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:245)
       at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:169)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:242)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:164)
       at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:269)
       at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:104)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1172)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:940)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:437)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory$1.run(AbstractAutowireCapableBeanFactory.java:383)
       at java.security.AccessController.doPrivileged(Native Method)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:353)
       at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:245)
       at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:169)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:242)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:164)
       at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:269)
       at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:104)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1172)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:940)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:437)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory$1.run(AbstractAutowireCapableBeanFactory.java:383)
       at java.security.AccessController.doPrivileged(Native Method)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:353)
       at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:245)
       at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:169)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:242)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:164)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getTypeForFactoryBean(AbstractBeanFactory.java:1223)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.getTypeForFactoryBean(AbstractAutowireCapableBeanFactory.java:582)
       at org.springframework.beans.factory.support.AbstractBeanFactory.isTypeMatch(AbstractBeanFactory.java:438)
       at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanNamesForType(DefaultListableBeanFactory.java:214)
       at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanNamesForType(DefaultListableBeanFactory.java:189)
       at org.springframework.beans.factory.BeanFactoryUtils.beanNamesForTypeIncludingAncestors(BeanFactoryUtils.java:143)
       at org.springframework.beans.factory.support.DefaultListableBeanFactory.findAutowireCandidates(DefaultListableBeanFactory.java:614)
       at org.springframework.beans.factory.support.DefaultListableBeanFactory.resolveDependency(DefaultListableBeanFactory.java:572)
       at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor$AutowiredMethodElement.inject(AutowiredAnnotationBeanPostProcessor.java:496)
       at org.springframework.beans.factory.annotation.InjectionMetadata.injectMethods(InjectionMetadata.java:87)
       at org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor.postProcessPropertyValues(AutowiredAnnotationBeanPostProcessor.java:250)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:928)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:437)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory$1.run(AbstractAutowireCapableBeanFactory.java:383)
       at java.security.AccessController.doPrivileged(Native Method)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:353)
       at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:245)
       at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:169)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:242)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:164)
       at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveReference(BeanDefinitionValueResolver.java:269)
       at org.springframework.beans.factory.support.BeanDefinitionValueResolver.resolveValueIfNecessary(BeanDefinitionValueResolver.java:104)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1172)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:940)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:437)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory$1.run(AbstractAutowireCapableBeanFactory.java:383)
       at java.security.AccessController.doPrivileged(Native Method)
       at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:353)
       at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:245)
       at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:169)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:242)
       at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:164)
       at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeansOfType(DefaultListableBeanFactory.java:299)
       at org.springframework.context.support.AbstractApplicationContext.getBeansOfType(AbstractApplicationContext.java:955)
       at org.springframework.context.support.AbstractApplicationContext.registerListeners(AbstractApplicationContext.java:712)
       at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:366)
       at org.springframework.web.context.ContextLoader.createWebApplicationContext(ContextLoader.java:261)
       at org.springframework.web.context.ContextLoader.initWebApplicationContext(ContextLoader.java:199)
       at org.springframework.web.context.ContextLoaderListener.contextInitialized(ContextLoaderListener.java:45)
       at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:3972)
       at org.apache.catalina.core.StandardContext.start(StandardContext.java:4467)
       at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:791)
       at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:771)
       at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:526)
       at org.apache.catalina.startup.HostConfig.deployWAR(HostConfig.java:905)
       at org.apache.catalina.startup.HostConfig.deployWARs(HostConfig.java:740)
       at org.apache.catalina.startup.HostConfig.deployApps(HostConfig.java:500)
       at org.apache.catalina.startup.HostConfig.check(HostConfig.java:1345)
       at org.apache.catalina.startup.HostConfig.lifecycleEvent(HostConfig.java:303)
       at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:119)
       at org.apache.catalina.core.ContainerBase.backgroundProcess(ContainerBase.java:1337)
       at org.apache.catalina.core.ContainerBase$ContainerBackgroundProcessor.processChildren(ContainerBase.java:1601)
       at org.apache.catalina.core.ContainerBase$ContainerBackgroundProcessor.processChildren(ContainerBase.java:1610)
       at org.apache.catalina.core.ContainerBase$ContainerBackgroundProcessor.run(ContainerBase.java:1590)
       at java.lang.Thread.run(Thread.java:637)
"
0,Exclude JavaCC-generated code from static analysisThe JavaCC-generated code we have in spi-commons should be excluded from static analysis done by tools like Sonar.
1,"CachingHttpClient returns a 411 respones when executing a POST (HttpPost) request The CachingHttpClient validates requests prior executing them, by calling RequestProtocolCompliance.requestIsFatallyNonCompliant(..).

When executing an HttpPost, this method considers the request is invalid because it does not contain (yet) a content-length header. Indeed, I observed that this header is generated at the time the DefaultHttpClient fires the request.

NB: i'm using the Cache 4.1-alpha2 plugged over the HttpClient 4.0.1-final. I can't use the latest version for both because I need to rely on a stable version if there's any. I would be curious to know if we get the same behaviour in 4.1...

Anyway, I would see two fixes for that issue:
- make HttpPost set the content-length at the time the entity is set,
- or remove the validation step on the CachingHttpClient side.
"
0,HttpClient javadocs need improving.. patch coming.
1,NPE in ConsolidatingChangeLog for id base NodeIdConsolidatingChangeLog does not guard guard against null in the path value of a NodeId. 
0,"optimize fuzzytermsenum per-segmentwe can make fuzzyquery about 3% faster by not creating DFA(s) for each segment.

creating the DFAs is still somewhat heavy: i can address this here too, but this is easy."
0,"Documentation for tii and tis files seems to be out of sync with codeThe documentation on the .tii file in fileformats.xml seems to be out of sync
with the actual code in TermInfosReader.java.

Specifically, the docs for the TermInfosIndex file seems to leave out several
fields that are read from the file in the readIndex() method (well, specifically
they're read in by the SegmentTermEnum constructor, but you get the idea)."
0,"Documentation improvements for 1.9 releaseI've poked arround the 1.9-rc1 builds and noticed a few simple documentation things that could be cleaned up, a patch will follow that...

1) Adds some additional info to the README.txt
2) Updates the version info in queryparsersyntax.xml and fileformats.html, and advises people 
     with older versions how to find the correct documentation for their version
3) Builds javadocs for all of the contrib modules (the list was incomplete)"
0,"Create org.apache.jackrabbit.core.idI'd like to create a separate package for the identifier interfaces and classes in jackrabbit-core. Currently all the identifiers are in org.apache.jackrabbit.core, which makes almost all the other packages have dependencies to o.a.j.core and causes trouble for various package-level code quality and dependency analysis tools.

For now the package would contain the ItemId, NodeId, and PropertyId classes."
0,"Move tika-parsers dependency to deployment packagesAs discussed on the mailing list, it would be better if the tika-parsers dependency (and all the parser libraries it pulls in transitively) was included in our deployment packages but not directly in jackrabbit-core. This would make it easier for people to set up custom lightweight deployments with no or only partial full text extraction functionality.

To do this we'll first need to wait for Tika 0.9, as we currently have a custom PDFParser class in jackrabbit-core as a workaround to a problem in Tika 0.8.

At the same time we should do a more thorough review of the transitive parser dependencies we include. At least the rome and bouncycastle libraries were flagged as potentially unnecessary."
0,"TermScorer caches values unnecessarilyTermScorer aggressively caches the doc and freq of 32 documents at a time for each term scored.  When querying for a lot of terms, this causes a lot of garbage to be created that's unnecessary.  The SegmentTermDocs from which it retrieves its information doesn't have any optimizations for bulk loading, and it's unnecessary.

In addition, it has a SCORE_CACHE, that's of limited benefit.  It's caching the result of a sqrt that should be placed in DefaultSimilarity, and if you're only scoring a few documents that contain those terms, there's no need to precalculate the SQRT, especially on modern VMs.

Enclosed is a patch that replaces TermScorer with a version that does not cache the docs or feqs.  In the case of a lot of queries, that saves 196 bytes/term, the unnecessary disk IO, and extra SQRTs which adds up."
1,"Suggested Patches to MultiPhraseQuery and QueryTermExtractor (for use with HighLighter)I encountered a problem with the Highlighter, where it was not recognizing MultiPhraseQuery.
To fix this, I developed the following two patches:

=====================================================
1. Addition to org.apache.lucene.search.MultiPhraseQuery:

Add the following method:

/** Returns the set of terms in this phrase. */
public Term[] getTerms() {
  ArrayList allTerms = new ArrayList();
  Iterator iterator = termArrays.iterator();
  while (iterator.hasNext()) {
    Term[] terms = (Term[])iterator.next();
    for (int i = 0, n = terms.length; i < n; ++i) {
      allTerms.add(terms[i]);
    }
  }
  return (Term[])allTerms.toArray(new Term[0]);
}

=====================================================
2. Patch to org.apache.lucene.search.highlight.QueryTermExtractor:

a) Add the following import:
import org.apache.lucene.search.MultiPhraseQuery;

b) Add the following code to the end of the getTerms(...) method:
      else  if(query instanceof MultiPhraseQuery)
              getTermsFromMultiPhraseQuery((MultiPhraseQuery) query, terms, fieldName);
  }

c) Add the following method:
 private static final void getTermsFromMultiPhraseQuery(MultiPhraseQuery query, HashSet terms, String fieldName)
 {
   Term[] queryTerms = query.getTerms();
   int i;

   for (i = 0; i < queryTerms.length; i++)
   {
       if((fieldName==null)||(queryTerms[i].field()==fieldName))
       {
           terms.add(new WeightedTerm(query.getBoost(),queryTerms[i].text()));
       }
   }
 }


=====================================================

Can the team update the repository?

Thanks
Michael Harhen "
0,Remaining contrib testcases should use Version based ctors instead of deprecated onesMany testcases in contrib use deprecated ctors for WhitespaceTokenizer / Analyzer etc.
1,"Group#getMembers may list inherited members multiple timesi just happen to detect the following regression that seems to be introduces quite a while ago:

Group#getMembers is defined to return all group members including those inherited by another group being member of that group.

Example:
User t
Group a : t is declared member
Group b : t is declared member
Group c : a, b are declared members

The expected result of Group.getMembers was: a, b and t.

What is currently happening is that t is included twice in the returned iterator.
Quickly testing on jackrabbit 2.0 revealed that this used to work before...

I didn't carefully check when that bug has been introduced but the the refactoring of the membership
collections seems to be a possible culprit.

"
1,"Handle URIs with path component nullHttpClient does not handle URIs with path component null (e.g. http://google.com) the same as path component '/'. This results e.g. in a ProtocolException ""The server failed to respond with a valid HTTP response""."
1,Session holds LockToken after removeLockToken in XA Environment
0,Remove deprecated Field.Store.COMPRESSAlso remove FieldForMerge and related code.
1,"HttpOptions.getAllowedMethods expects single Allow headerIn client.methods.HttpOptions.getAllowMethods(), a single Allow header is parsed to obtain the result. Since the value is a comma-separated list, servers can optionally return the values in multiple headers. HttpMethod.getHeaders(name) should be used instead of .getFirstHeader(name).
"
0,Remove remaining deprecations from indexer package
1,"Node.getPath() will corrupt the sessionWhen calling Node.getPath() anytime, no mather if its before or after save, and when deleting nodes, the internal reference points to the wrong nodes. 
The attached test will always fail with a javax.jcr.RepositoryException: /: cannot remove root node. 
We have seen other configurations where a node suddenly behaves as the another node that has references and throw a reference exception, and yet other configurations where the node we though we deleted still exists, and another node has now disappeared.

I do not know what causes the bug,a good bet is perhaps the CachingHierarchyManager?. It was not present in Jackrabbit 1.0.1, but was introduced in 1.1.

Have also tested the latest release: 1.2.2, and the bug is still present there.
"
1,"BundleFsPersistenceManager has no property called: minBlobSize2008-04-03 16:48:51,ERROR,org.apache.jackrabbit.core.RepositoryImpl,Thread-237 failed to start Repository: Cannot instantiate persistence manager org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager: The bean of type: org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager has no property called: minBlobSize
javax.jcr.RepositoryException: Cannot instantiate persistence manager org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager: The bean of type: org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager has no property called: minBlobSize: The bean of type: org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager has no property called: minBlobSize
	at org.apache.jackrabbit.core.RepositoryImpl.createPersistenceManager(RepositoryImpl.java:1176)
	at org.apache.jackrabbit.core.RepositoryImpl.createVersionManager(RepositoryImpl.java:390)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:294)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:557)
	at pps.jcr.util.RepositoryManager.createRepository(RepositoryManager.java:117)
	at pps.jcr.util.RepositoryManager.startRepository(RepositoryManager.java:43)
	at pps.jcr.ejb.session.JcrUtilFacade.startRepository(JcrUtilFacade.java:62)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.enterprise.security.application.EJBSecurityManager.runMethod(EJBSecurityManager.java:1067)
	at com.sun.enterprise.security.SecurityUtil.invoke(SecurityUtil.java:176)
	at com.sun.ejb.containers.BaseContainer.invokeTargetBeanMethod(BaseContainer.java:2895)
	at com.sun.ejb.containers.BaseContainer.intercept(BaseContainer.java:3986)
	at com.sun.ejb.containers.EJBLocalObjectInvocationHandler.invoke(EJBLocalObjectInvocationHandler.java:197)
	at com.sun.ejb.containers.EJBLocalObjectInvocationHandlerDelegate.invoke(EJBLocalObjectInvocationHandlerDelegate.java:127)
	at $Proxy181.startRepository(Unknown Source)
	at pps.jcr.web.JcrLifecycleListener.contextInitialized(JcrLifecycleListener.java:55)
	at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:4523)
	at org.apache.catalina.core.StandardContext.start(StandardContext.java:5184)
	at com.sun.enterprise.web.WebModule.start(WebModule.java:326)
	at org.apache.catalina.core.ContainerBase.addChildInternal(ContainerBase.java:973)
	at org.apache.catalina.core.ContainerBase.addChild(ContainerBase.java:957)
	at org.apache.catalina.core.StandardHost.addChild(StandardHost.java:688)
	at com.sun.enterprise.web.WebContainer.loadWebModule(WebContainer.java:1584)
	at com.sun.enterprise.web.WebContainer.loadWebModule(WebContainer.java:1222)
	at com.sun.enterprise.web.WebContainer.loadJ2EEApplicationWebModules(WebContainer.java:1147)
	at com.sun.enterprise.server.TomcatApplicationLoader.doLoad(TomcatApplicationLoader.java:141)
	at com.sun.enterprise.server.AbstractLoader.load(AbstractLoader.java:244)
	at com.sun.enterprise.server.ApplicationManager.applicationDeployed(ApplicationManager.java:336)
	at com.sun.enterprise.server.ApplicationManager.applicationDeployed(ApplicationManager.java:210)
	at com.sun.enterprise.server.ApplicationManager.applicationDeployed(ApplicationManager.java:645)
	at com.sun.enterprise.admin.event.AdminEventMulticaster.invokeApplicationDeployEventListener(AdminEventMulticaster.java:928)
	at com.sun.enterprise.admin.event.AdminEventMulticaster.handleApplicationDeployEvent(AdminEventMulticaster.java:912)
	at com.sun.enterprise.admin.event.AdminEventMulticaster.processEvent(AdminEventMulticaster.java:461)
	at com.sun.enterprise.admin.event.AdminEventMulticaster.multicastEvent(AdminEventMulticaster.java:176)
	at com.sun.enterprise.admin.server.core.DeploymentNotificationHelper.multicastEvent(DeploymentNotificationHelper.java:308)
	at com.sun.enterprise.deployment.phasing.DeploymentServiceUtils.multicastEvent(DeploymentServiceUtils.java:226)
	at com.sun.enterprise.deployment.phasing.ServerDeploymentTarget.sendStartEvent(ServerDeploymentTarget.java:298)
	at com.sun.enterprise.deployment.phasing.ApplicationStartPhase.runPhase(ApplicationStartPhase.java:132)
	at com.sun.enterprise.deployment.phasing.DeploymentPhase.executePhase(DeploymentPhase.java:108)
	at com.sun.enterprise.deployment.phasing.PEDeploymentService.executePhases(PEDeploymentService.java:919)
	at com.sun.enterprise.deployment.phasing.PEDeploymentService.start(PEDeploymentService.java:591)
	at com.sun.enterprise.deployment.phasing.PEDeploymentService.start(PEDeploymentService.java:635)
	at com.sun.enterprise.admin.mbeans.ApplicationsConfigMBean.start(ApplicationsConfigMBean.java:744)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.enterprise.admin.MBeanHelper.invokeOperationInBean(MBeanHelper.java:375)
	at com.sun.enterprise.admin.MBeanHelper.invokeOperationInBean(MBeanHelper.java:358)
	at com.sun.enterprise.admin.config.BaseConfigMBean.invoke(BaseConfigMBean.java:464)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
	at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.enterprise.admin.util.proxy.ProxyClass.invoke(ProxyClass.java:90)
	at $Proxy1.invoke(Unknown Source)
	at com.sun.enterprise.admin.server.core.jmx.SunoneInterceptor.invoke(SunoneInterceptor.java:304)
	at com.sun.enterprise.interceptor.DynamicInterceptor.invoke(DynamicInterceptor.java:174)
	at com.sun.enterprise.deployment.client.DeploymentClientUtils.startApplication(DeploymentClientUtils.java:145)
	at com.sun.enterprise.deployment.client.DeployAction.run(DeployAction.java:537)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.IllegalArgumentException: The bean of type: org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager has no property called: minBlobSize
	at org.apache.commons.collections.BeanMap.put(BeanMap.java:367)
	at org.apache.jackrabbit.core.config.BeanConfig.newInstance(BeanConfig.java:109)
	at org.apache.jackrabbit.core.RepositoryImpl.createPersistenceManager(RepositoryImpl.java:1171)
	... 64 more

<?xml version=""1.0""?>
<Repository>
    <DataStore class=""org.apache.jackrabbit.core.data.FileDataStore"">
        <param name=""path"" value=""${rep.home}/datastore""/>
        <param name=""minRecordLength"" value=""100""/>
    </DataStore>    
    <FileSystem class=""org.apache.jackrabbit.core.fs.local.LocalFileSystem"">
        <param name=""path"" value=""${rep.home}""/>
    </FileSystem>    
    <Security appName=""Jackrabbit"">
        <AccessManager class=""org.apache.jackrabbit.core.security.SimpleAccessManager"" />
        <LoginModule class=""org.apache.jackrabbit.core.security.SimpleLoginModule"">
            <param name=""anonymousId"" value=""anonymous"" />
        </LoginModule>
    </Security>
    <Workspaces rootPath=""${rep.home}/workspaces"" defaultWorkspace=""default"" />
    <Workspace name=""${wsp.name}"">
        <FileSystem class=""org.apache.jackrabbit.core.fs.local.LocalFileSystem"">
            <param name=""path"" value=""${rep.home}/${wsp.name}""/>
        </FileSystem>            
        <!--  <PersistenceManager class=""org.apache.jackrabbit.core.persistence.obj.ObjectPersistenceManager""/>  -->
        <PersistenceManager class=""org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager"">
            <param name=""bundleCacheSize"" value=""8""/> 
            <param name=""blobFSBlockSize"" value=""0""/> 
            <param name=""minBlobSize"" value=""4096""/> 
            <param name=""errorHandling"" value=""""/>             
        </PersistenceManager>        
        <SearchIndex class=""org.apache.jackrabbit.core.query.lucene.SearchIndex"">
            <param name=""path"" value=""${wsp.home}/index""/>    
            <param name=""textFilterClasses"" value=""
                   org.apache.jackrabbit.extractor.MsExcelTextExtractor,
                   org.apache.jackrabbit.extractor.MsPowerPointTextExtractor,
                   org.apache.jackrabbit.extractor.MsWordTextExtractor,
                   org.apache.jackrabbit.extractor.PdfTextExtractor,
                   org.apache.jackrabbit.extractor.PlainTextExtractor,
                   org.apache.jackrabbit.extractor.HTMLTextExtractor,
                   org.apache.jackrabbit.extractor.XMLTextExtractor,
                   org.apache.jackrabbit.extractor.RTFTextExtractor,
            org.apache.jackrabbit.extractor.OpenOfficeTextExtractor""/>
        </SearchIndex>
    </Workspace>
    <Versioning rootPath=""${rep.home}/version"">
        <FileSystem class=""org.apache.jackrabbit.core.fs.local.LocalFileSystem"">
            <param name=""path"" value=""${rep.home}/version""/>
        </FileSystem>                
        <!-- <PersistenceManager class=""org.apache.jackrabbit.core.persistence.obj.ObjectPersistenceManager""/> -->
        <PersistenceManager class=""org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager"">
            <param name=""bundleCacheSize"" value=""8""/> 
            <param name=""blobFSBlockSize"" value=""0""/> 
            <param name=""minBlobSize"" value=""4096""/> 
            <param name=""errorHandling"" value=""""/>             
        </PersistenceManager>            
    </Versioning>
    <SearchIndex class=""org.apache.jackrabbit.core.query.lucene.SearchIndex"">
        <param name=""path"" value=""${rep.home}/index""/>    
        <param name=""textFilterClasses"" value=""
               org.apache.jackrabbit.extractor.MsExcelTextExtractor,
               org.apache.jackrabbit.extractor.MsPowerPointTextExtractor,
               org.apache.jackrabbit.extractor.MsWordTextExtractor,
               org.apache.jackrabbit.extractor.PdfTextExtractor,
               org.apache.jackrabbit.extractor.PlainTextExtractor,
               org.apache.jackrabbit.extractor.HTMLTextExtractor,
               org.apache.jackrabbit.extractor.XMLTextExtractor,
               org.apache.jackrabbit.extractor.RTFTextExtractor,
        org.apache.jackrabbit.extractor.OpenOfficeTextExtractor""/>
    </SearchIndex>    
</Repository>
"
0,"Behavior on hard power shutdownWhen indexing a large number of documents, upon a hard power failure  (e.g. pull the power cord), the index seems to get corrupted. We start a Java application as an Windows Service, and feed it documents. In some cases (after an index size of 1.7GB, with 30-40 index segment .cfs files) , the following is observed.

The 'segments' file contains only zeros. Its size is 265 bytes - all bytes are zeros.
The 'deleted' file also contains only zeros. Its size is 85 bytes - all bytes are zeros.

Before corruption, the segments file and deleted file appear to be correct. After this corruption, the index is corrupted and lost.

This is a problem observed in Lucene 1.4.3. We are not able to upgrade our customer deployments to 1.9 or later version, but would be happy to back-port a patch, if the patch is small enough and if this problem is already solved.
"
0,Remove deprecated methods in CompoundTokenFilters
1,"Token authentication parameters are not loaded from JAAS configuration.token based authentication can be disabled and expiration time set in the login module config.
however, this only works with local auth context but  not when using a jaas configuration."
0,"Add method getID to interface ItemInfoItemInfo is the base for NodeInfo and PropertyInfo both of which declare a method getId with return type NodeId and PropertyId, respectively. With Java 1.5. it is now possible to override a method with a covariant return type. I thus propose to introduce a method getId on ItemInfo with return type ItemId which is the common base type of NodeId and PropertyId."
0,"remove RoutedRequest from ClientRequestDirector interfaceRemove the RoutedRequest from ClientRequestDirector.execute, pass the request and route/target separately."
1,"Session.import() failes to resolve propert property definition in some casesSome Properties get assigned the wrong definiton when imported via SysView XML.

The selecteion of the definition failes under the following condition:
The nodetype contains a multi-valued property and a single-valued
residual property.
If the data to be imported than contains only one value for the multivalued property, it will be created with the residual definition.
A later access to this propertie's values will fail with an ValueFormatException.

Example:
Node-Type
 - Property
  - name: myapp:name
  - mulitple: true
 - Property
  - name: *
  - multible: false

Sysview:
<sv:node sv:name=""somenode"">
  <sv:property sv:name=""jcr:primaryType"" sv:type=""Name"">
   <sv:value>myapp:sampleNt</sv:value> 
  </sv:property>
 
  <sv:property sv:name=""myapp:name"" sv:type=""String"">
   <sv:value>At least I could have multi values</sv:value> 
  </sv:property>
</sv:node>

=> The ""mayapp:name"" will be imported into the residule property."
0,"Permit using different tablespaces for tables and indexes with OracleOracleFileSystem, OraclePersistenceManager and OracleDatabaseJournal already provide a tableSpace parameter to customize the DDL, but the same tablespace is used for both tables and indexes. It is common place to use distinct tablespaces for these. Jackrabbit could provide support for this."
0,Spellchecker uses default IW mergefactor/ramMB settings of 300/10These settings seem odd - I'd like to investigate what makes most sense here.
1,"jcr2spi NodeEntryImpl.getPath() blows stack due to getIndex() calling itselfThe jcr2spi NodeEntryImpl class contains logic that causes getIndex() to call itself.

Calling code:

    Session sess = repo.login(creds);
    Node inboxNode = sess.getRootNode().getNode(""Inbox"");
    inboxNode.getPath(); <== blows stack

Tracing reveals:

    1. NodeEntryImpl.getPath() ultimately calls getIndex()
    2. getIndex() calls NodeState.getDefinition()
    3. which calls ItemDefinitionProviderImpl.getQNodeDefinition(...)
    4. which catches a RepositoryException then calls NodeEntryImpl.getWorkspaceId()
    5. which calls NodeEntryImpl.getWorkspaceIndex()
    6. which calls getIndex() (back to step 2, ad infinitum)

Configuration:
    1. A configuration is loaded specifying in-memory persist manager
    2. Config is wrapped in TransientRepository
    3. that's wrapped in spi2jcr's RepositoryService using default BatchReadConfig
    4. a jcr2spi provider is instantiated that directly couples to spi2jcr
    5. Node in question is created as follows:

    Session sess = repo.login(creds);
    sess.getRootNode().addNode(""Inbox"", ""nt:folder"");
    sess.save();

I guess that's about it.
David"
0,"Exclude system index for queries that restrict the result set to nodetypes not availble in the ""jcr:system"" subtreeWe already have code that is able to decide whether the system index needs to be included in a search or not (see JCR-967). If I execute a query like ""my:app//element(*, my:doc)"" this will only search the workspace index. Unfortunately this is slower than ""//element(*, my:doc)"", since the first query can not be optimized as the second. In our case both queries return the same result set because we use application specific node types. Even though the second query includes the system index it is still faster than the first one. But it could be even faster because it doesn't need to search the system index because nodes with the application specific node type can't be added to the ""jcr:system""-tree and are therefore are added never to the system index (am I right?)."
0,"DateUtils should cache SimpleDateFormatDateUtils create a SimpleDateFormat for each invocation of #formatDate and #parseDate. This can be optimized if SimpleDateFormat instances are cached. Since SimpleDateFormat is not threadsafe, the cache must be threadlocal."
0,"Database Data Store: clean up the codeThere is some unnecessary code in the DbDataStore that should be removed.
Also, some more tests should be added."
0,"Investigate ways to compile the refactored jcr-mapping for Java 1.4The last refactoring of the jcr-mapping project included the annotation based mapping description into the main code based thus requiring compilation with Java 5 or higher.

There are still some use cases, which require Java 1.4. The goal is to investigate, whether it would be possible to define a build profile in the pom, which compiles for 1.4 by ignoring the annotation classes."
1,"CMS fails to cleanly stop threadsWhen you close IW, it waits for (or aborts and then waits for) all running merges.

However, it's wait criteria is wrong -- it waits for the threads to be done w/ their merges, not for the threads to actually die.

CMS already has a sync() method, to wait for running threads, which we can call from CMS.close.  However it has a thread hazard because a MergeThread removes itself from mergeThreads before it actually exits.  So sync() is able to return even while a merge thread is still running.

This was uncovered by LUCENE-2819 on the test case TestCustomScoreQuery.testCustomExternalQuery, though I expect other test cases would show it."
1,"ArrayIndexOutOfBounds Exception on invalid content-lengthIf the server returns an invalid (not parsable to int) content legnth the method
protected int getResponseContentLength() in HttpMethodBase walks off the
end of the Header[] array and throws the ArrayIndexOutOfBoundsException.

The loop at line 687 in HttpMethodBase.java:

   for (int i = headers.length - 1; i >= 0; i++) {

starts at the end of the array, but uses ++ intead of -- and so walks off the
end of the array on the next line if the header is invalid.  If the header is
valid the return statement in the try block succeeds so there is no error.

The fix is simply to change the line to be

   for (int i = headers.length -1; i>=0; i--) {"
0,"GroupImp#getMembers and #getDeclaredMembers should return RangeIteratorfor those cases where the total amount of members can easily be detected/calculated the
implementations of Group#getMembers() and #getDeclaredMembersOf() should return a RangeIterator.

so far i found that Group#declaredMembers() can be easily adjusted for those cases where
the group members are stored in a multivalued property."
0,"Update copyright years in READMEs and NOTICEsThe README.txt files of Jackrabbit components contain copyright lines like this:

    Collective work: Copyright 2007 The Apache Software Foundation.

The year should be updated."
0,"TermVectorAccessor, transparent vector space access This class visits TermVectorMapper and populates it with information transparent by either passing it down to the default terms cache (documents indexed with Field.TermVector) or by resolving the inverted index."
0,"Genericize DirectIOLinuxDir -> UnixDirToday DirectIOLinuxDir is tricky/dangerous to use, because you only want to use it for indexWriter and not IndexReader (searching).  It's a trap.

But, once we do LUCENE-2793, we can make it fully general purpose because then a single native Dir impl can be used.

I'd also like to make it generic to other Unices, if we can, so that it becomes UnixDirectory."
0,new CachingNamespaceResolver introduces dependency from commons-jackrabbit to commons-collectionsnew CachingNamespaceResolver introduces dependency from commons-jackrabbit to commons-collections which is undesried
0,"pass liveDocs Bits down in scorercontext, instead of Weights pulling from the reader Spinoff from LUCENE-1536, this would allow filters to work in a more flexible way (besides just cleaning up)"
1,"Inconsistencies if ""everyone"" Group is created by User Managementcurrently the 'everyone' principal used to define ACEs that apply for all regular users in the repository is hardcoded in the
principal management. this leads to inconsistencies if a group (or user) is created within the user management that has a principal 
name 'everyone'.


"
0,"Scorer.explain is deprecated but abstract, should have impl that throws UnsupportedOperationExceptionSuggest having Scorer implement explain to throw UnsupportedOperationException

right now, i have to implement this method (because its abstract), and javac yells at me for overriding a deprecated method

if the following implementation is in Scorer, i can remove my ""empty"" implementations of explain from my Scorers
{code}
  /** Returns an explanation of the score for a document.
   * <br>When this method is used, the {@link #next()}, {@link #skipTo(int)} and
   * {@link #score(HitCollector)} methods should not be used.
   * @param doc The document number for the explanation.
   *
   * @deprecated Please use {@link IndexSearcher#explain}
   * or {@link Weight#explain} instead.
   */
  public Explanation explain(int doc) throws IOException {
    throw new UnsupportedOperationException();
  }
{code}

best i figure, this shouldn't break back compat (people already have to recompile anyway) (2.9 definitely not binary compatible with 2.4)
"
1,"Test case failure for testConnTimeout[java] There was 1 failure:
     [java] 1)
testConnTimeout(org.apache.commons.httpclient.TestHttpConnection)junit.framework.AssertionFailedError:
Should have timed out
     [java]     at
org.apache.commons.httpclient.TestHttpConnection.testConnTimeout(TestHttpConnection.java:118)
     [java]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
     [java]     at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
     [java]     at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)

This test has been failing for some time.  It is run with the test-local ant target."
0,"Cutover remaining usage of pre-flex APIsA number of places still use the pre-flex APIs.

This is actually healthy, since it gives us ongoing testing of the back compat emulation layer.

But we should at some point cut them all over to flex.  Latest we can do this is 4.0, but I'm not sure we should do them all for 3.1... still marking this as 3.1 to ""remind us"" :)"
0,"Allow to override LockManager creationCurrently, Repository.getLockManager() internally creates a new lock manager if needed.

Jackrabbit should provide an extension point so that a JCR repository that extends it can create a custom lock manager."
0,[patch] remove bogus testcode checks an int to see if it's bigger than MAX_VALUE which is impossible -- removed.
0,"need to add a default constructor for CookieThe Cookie class doesn't have a default (no argument) constructor. This is 
causing problem for some framework which supports marshalling and unmarshalling 
of data types. e.g. a SOAP implementation may need to do this to transfer it 
between the SOAP server and SOAP client. It would be nice to add a default 
constructor, as it won't break anything, follows JavaBean convention, and 
potentially save user some trouble in the future."
1,SearcherManager misses to close IR if manager is closed during reopenif we close SM while there is a thread calling maybReopen() and swapSearcher throws already closed exception we miss to close the searcher / reader.
1,"Mixins as supertypes do not appear to be queryableWhen creating custom nodetypes that contain mixins as the supertype, nodes of the custom type do not appear to be queryable when using statements of the form: //element(*, mixin). Attached are a relatively simple JUnit test and compact type definition that seem to illustrate the problem."
1,"Evict fixed NodePropBundle from cacheThe BundleDbPersistenceManager only stores back fixed NodePropBundles in checkConsistency() but does not invalidate the cache, which may potentially contain a cached version of a NodePropBundle."
1,"SystemSessions created for GarbageCollector are not logged out ofI have a simple garbage collection task that runs periodically. After upgrading to 1.5.5 it started logging a warning shortly after each run:

2009-05-09 03:44:45,480 WARN [org.apache.jackrabbit.core.SessionImpl] - <Unclosed session detected. The session was opened here: >
java.lang.Exception: Stack Trace
	at org.apache.jackrabbit.core.SessionImpl.<init>(SessionImpl.java:239)
	at org.apache.jackrabbit.core.SystemSession.<init>(SystemSession.java:76)
	at org.apache.jackrabbit.core.SystemSession.create(SystemSession.java:64)
	at org.apache.jackrabbit.core.SessionImpl.createDataStoreGarbageCollector(SessionImpl.java:649)

So it's not my session, but an internally created SystemSession.


Code I'm using:
            getTemplate().execute(new JcrCallback()
            {
                public Object doInJcr(Session session)
                    throws IOException, RepositoryException {
                    SessionImpl sessionImpl = (SessionImpl)session;
                    GarbageCollector gc = sessionImpl.createDataStoreGarbageCollector();
                    gc.scan();
                    gc.stopScan();
                    gc.deleteUnused();
                    return null;
                }
            }, true);
"
0,"RFE: Make Credentials SerializableI've been working on upgrading the HtmlUnit library to use HttpClient 4, and I've realized that we could eliminate some hackish internal code if Credentials instances were Serializable. I don't really see a downside, and this would be a huge convenience for us.

The change would involve making the org.apache.http.auth.Credentials interface extend Serializable, and having org.apache.http.auth.BasicUserPrincipal and org.apache.http.auth.NTUserPrincipal implement Serializable (plus serialVersionUIDs where appropriate, I guess)."
1,"spi2dav: Observation's user data not property handledorg.apache.jackrabbit.test.api.observation#GetUserDataTest still fail in the setup jcr2spi - spi2dav(ex) - jcr-server.

"
0,"Expose FilteredTermsEnum from MTQ MTQ#getEnum() is protected and in order to access it you need to be in the o.a.l.search package. 

here is a relevant snipped from the mailing list discussion

{noformat}
getEnum() is protected so it is intended to be called *only* by subclasses (that's the idea behind protected methods). They are also accessible by other classes from the same package, but that's more a Java bug than a feature. The problem with MTQ is that RewriteMethod is a separate *class* and *not a subclass* of MTQ, so the method cannot be called (it can because of the ""java bug"" called from same package). So theoretically it has to be public otherwise you cannot call getEnum().

Another cleaner fix would be to add a protected final method to RewriteMethod that calls this method from MTQ. So anything subclassing RewriteMethod can get the enum from inside the RewriteMethod class which is the ""correct"" way to handle it. Delegating to MTQ is then ""internal"".
{noformat}"
0,"[PATCH] Refactoring of SpanScorerRefactored some common code in next() and skipTo(). 
Removed dependency on score value for next() and skipTo(). 
Passes all current tests at just about the same speed 
as the current version. Added minimal javadoc. 
 
Iirc, there has been some discussion on the dependency of next() 
and skipTo() on the score value, but I don't remember the conclusion. 
In case that dependency should stay in, it can be adapted 
in the refactored code."
0,"TCK: NamespaceRegistryTest#testRegisterNamespace doesn't remove node in new namespaceThe test creates a node in the new namespace, but doesn't remove it.  This prevents tearDown from unregistering the namespace.

Proposal: the test should remove the new node before returning.

--- NamespaceRegistryTest.java  (revision 422074)
+++ NamespaceRegistryTest.java  (working copy)
@@ -138,6 +138,10 @@
  
         testRootNode.addNode(namespacePrefix + "":root"");
         testRootNode.save();
+
+        // Need to remove it here, otherwise teardown can't unregister the NS.
+        testRootNode.getNode(namespacePrefix + "":root"").remove();
+        testRootNode.save();
     }
"
0,"Authorizable#getProperty and #setProperty should deal with relativePath Authorizable#getProperty and #setProperty defines the property to be identified by a name.

The JCR item based implementation could easily deal with relative paths instead and also retrieve or write properties below child
nodes of the rep:Authorizable node."
1,"add workaround for jre breakiterator bugson some inputs, the java breakiterator support will internally crash.

for example: ant test -Dtestcase=TestThaiAnalyzer -Dtestmethod=testRandomStrings -Dtests.seed=-8005471002120855329:-2517344653287596566 -Dtests.multiplier=3"
1,"SimpleSpanFragmenter can create very short fragmentsLine 74 of SimpleSpanFragmenter returns true when the current token is the start of a hit on a span or phrase, thus starting a new fragment. Two problems occur:

- The previous fragment may be very short, but if it contains a hit it will be combined with the new fragment later so this disappears.
- If the token is close to a natural fragment boundary the new fragment will end up very short; possibly even as short as just the span or phrase itself. This is the result of creating a new fragment without incrementing currentNumFrags.

To fix, remove or comment out line 74. The result is that fragments average to the fragment size unless a span or phrase hit is towards the end of the fragment - that fragment is made larger and the following fragment shorter to accommodate the hit."
1,"IndexReader.setNorms is no op if one of the field instances omits normsIf I add two documents to an index w/ same field, and one of them omit norms, then IndexReader.setNorms is no-op. I'll attach a patch w/ test case"
1,"maxFieldLength actual limit is 1 greater than expected value.
// Prepare document.
Document document = new Document();
document.add(new Field(""name"",
            ""pattern oriented software architecture"", Store.NO,
            Index.TOKENIZED, TermVector.WITH_POSITIONS_OFFSETS));

// Set max field length to 2.
indexWriter.setMaxFieldLength(2);

// Add document into index.
indexWriter.addDocument(document, new StandardAnalyzer());

// Create a query.
QueryParser queryParser = new QueryParser(""name"", new StandardAnalyzer());
Query query = queryParser.parse(""software"");

// Search the 3rd term.
Hits hits = indexSearcher.search(query);

Assert.assertEquals(0, hits.length());
// failed. Actual hits.length() == 1, but expect 0."
0,"Remove/deprecate IndexReader.undeleteAllThis API is rather dangerous in that it's ""best effort"" since it can only un-delete docs that have not yet been merged away, or, dropped (as of LUCENE-2010).

Given that it exposes impl details of how Lucene prunes deleted docs, I think we should remove this API.

Are there legitimate use cases....?"
1,"Trouble undeploying jackrabbit-webapp from TomcatWhen testing jackrabbit-webapp for the 1.4 release, I again came across this issue that I've occasionally seen also before, but never qualified enough for a bug report.

The Jackrabbit webapp would deploy without problems, but when I undeploy the webapp Tomcat fails to remove the Derby jar in WEB-INF/lib (I have unpackWARs enabled). This causes problems especially when I have autoDeploy enabled, as Tomcat then deploys the skeleton webapp right after undeployment, and the only way to really get rid of the webapp is to shutdown Tomcat and to manually remove the webapp on the file system.

I suspect that this problem is related to Derby jar being somehow referenced even after the webapp is undeployed, causing Windows to prevent the jar file from being removed.

Unless someone has some bright idea on how to resolve this, I'll consider this a known issue in Jackrabbit 1.4."
0,"JavaCC grammar generation to ${maven.build.dir}/generated-srcCurrently the JavaCC grammars in src/grammar/{xpath,sql} are processed into Java source files in src/java/org/apache/jacrabbit/core/query/{xpath,sql} where we also have normal version controlled source files. This leads to the need to maintain special svn:ignore properties and also the more general issue of mixing manually written and automatically generated source files. Because of this the ""maven clean"" command does not (at the moment) truly restore your source tree to a ""fresh checkout"" state.

I'm proposing (as a wish, you are free to disagree) that the JavaCC grammars be generated into Java files within the Maven build directory. The attached patch modifies the javacc maven goals to generate files into ${maven.build.dir}/generated-src. The modified prepare-filesystem goal also adds the generated source path ${maven.build.dir}/generated-src/main/java into the maven compile set so that the generated sources are included in the normal builds.

PS. There are a couple of JavaCC generated files that have been intentionally modified for Jackrabbit. The ant:delete commands at the end of the jacrabbit:generate-*-parser goals specifically remove these generated files. It would however be nicer if custom modifications would not be needed."
1,"Cluster: Node type register/unregister deadlockA deadlock can occur when two cluster nodes concurrently register or unregister node types.

Reason: 

NodeTypeRegistry.registerNodeTypes is synchronized, and calls eventChannel.registered(ntDefs), which calls AbstractJournal.lockAndSync(), which tries to lock AbstractJournal.rwLock.

On the other hand, AbstractJournal.sync() locks AbstractJournal.rwLock, then calls NodeTypeRecord.process, which calls NodeTypeRegistry.unregisterNodeTypes, which is also synchronized.

Possible solutions: Either 

- NodeTypeRegistry doesn't synchronize on the object when calling a eventChannel method,

- or NodeTypeRegistry locks AbstractJournal.rwLock before synchronizing.

There might be other solutions."
0,"Basic tool for checking & repairing an indexThis has been requested a number of times on the mailing lists.  Most
recently here:

  http://www.gossamer-threads.com/lists/lucene/java-user/53474

I think we should provide a basic tool out of the box.
"
1,"insufficient privilegesHI,
In Jackrabbit  DBStore, On the fly its creating some tables in DB .  But, In our Dev environment we do not have permission for creating tables on the fly. So, I manually inserted all the dll (tables & indexes) before the application start. Although I'm getting the following exception while running application. 

Attached repository.xml.

Below the log.

[11/23/09 11:32:04:405 EST] 0000003a SystemOut     O WARN > org.apache.jackrabbit.core.config.ConfigurationErrorHandler[WebContainer : 3]: Warning parsing the configuration at line 4 using system id file:/usr/local/web/fda/WAS/61x/svdw0047v61fda/installedApps/afda21Network001/osa_registry.ear/osa_registry.war/WEB-INF/cfg/repository.xml: org.xml.sax.SAXParseException: Document root element ""Repository"", must match DOCTYPE root ""null"".
[11/23/09 11:32:04:407 EST] 0000003a SystemOut     O WARN > org.apache.jackrabbit.core.config.ConfigurationErrorHandler[WebContainer : 3]: Warning parsing the configuration at line 4 using system id file:/usr/local/web/fda/WAS/61x/svdw0047v61fda/installedApps/afda21Network001/osa_registry.ear/osa_registry.war/WEB-INF/cfg/repository.xml: org.xml.sax.SAXParseException: Document is invalid: no grammar found.
[11/23/09 11:32:04:977 EST] 0000003a SystemOut     O INFO > org.apache.jackrabbit.core.RepositoryImpl[WebContainer : 3]: Starting repository...
[11/23/09 11:32:05:474 EST] 0000003a SystemOut     O ERROR> org.apache.jackrabbit.core.fs.db.DatabaseFileSystem[WebContainer : 3]: failed to initialize file system
java.sql.SQLException: ORA-01031: insufficient privileges

	at oracle.jdbc.driver.DatabaseError.throwSqlException(DatabaseError.java:112)
	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:331)
	at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:288)
	at oracle.jdbc.driver.T4C8Oall.receive(T4C8Oall.java:745)
	at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:210)
	at oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:961)
	at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1190)
	at oracle.jdbc.driver.OracleStatement.executeUpdateInternal(OracleStatement.java:1657)
	at oracle.jdbc.driver.OracleStatement.executeUpdate(OracleStatement.java:1626)
	at org.apache.jackrabbit.core.fs.db.OracleFileSystem.checkSchema(OracleFileSystem.java:211)
	at org.apache.jackrabbit.core.fs.db.DatabaseFileSystem.init(DatabaseFileSystem.java:190)
	at org.apache.jackrabbit.core.fs.db.OracleFileSystem.init(OracleFileSystem.java:137)
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$2.getFileSystem(RepositoryConfigurationParser.java:762)
	at org.apache.jackrabbit.core.config.RepositoryConfig.getFileSystem(RepositoryConfig.java:666)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:262)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:621)
	at org.apache.jackrabbit.core.jndi.BindableRepository.createRepository(BindableRepository.java:140)
	at org.apache.jackrabbit.core.jndi.BindableRepository.init(BindableRepository.java:116)
	at org.apache.jackrabbit.core.jndi.BindableRepository.<init>(BindableRepository.java:105)
	at org.apache.jackrabbit.core.jndi.BindableRepositoryFactory.getObjectInstance(BindableRepositoryFactory.java:51)
	at org.apache.jackrabbit.core.jndi.RegistryHelper.registerRepository(RegistryHelper.java:74)
	at com.ssc.soareg.jackrabbit.ContentRepository.<clinit>(ContentRepository.java:71)
	at com.ssc.soareg.jaxr.registry.client.infomodel.ServiceImpl.<init>(ServiceImpl.java:177)
	at com.ssc.soareg.governance.client.SOALifeCycleManagerImpl.saveBusinessServices(SOALifeCycleManagerImpl.java:259)
	at com.ssc.soareg.registry.server.UploadServlet.service(UploadServlet.java:473)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:856)
	at com.ibm.ws.webcontainer.servlet.ServletWrapper.service(ServletWrapper.java:1068)
	at com.ibm.ws.webcontainer.servlet.ServletWrapper.handleRequest(ServletWrapper.java:543)
	at com.ibm.ws.wswebcontainer.servlet.ServletWrapper.handleRequest(ServletWrapper.java:478)
	at com.ibm.ws.webcontainer.webapp.WebApp.handleRequest(WebApp.java:3357)
	at com.ibm.ws.webcontainer.webapp.WebGroup.handleRequest(WebGroup.java:267)
	at com.ibm.ws.webcontainer.WebContainer.handleRequest(WebContainer.java:811)
	at com.ibm.ws.wswebcontainer.WebContainer.handleRequest(WebContainer.java:1455)
	at com.ibm.ws.webcontainer.channel.WCChannelLink.ready(WCChannelLink.java:115)
	at com.ibm.ws.http.channel.inbound.impl.HttpInboundLink.handleDiscrimination(HttpInboundLink.java:454)
	at com.ibm.ws.http.channel.inbound.impl.HttpInboundLink.handleNewInformation(HttpInboundLink.java:383)
	at com.ibm.ws.http.channel.inbound.impl.HttpICLReadCallback.complete(HttpICLReadCallback.java:102)
	at com.ibm.ws.tcp.channel.impl.AioReadCompletionListener.futureCompleted(AioReadCompletionListener.java:165)
	at com.ibm.io.async.AbstractAsyncFuture.invokeCallback(AbstractAsyncFuture.java:217)
	at com.ibm.io.async.AsyncChannelFuture.fireCompletionActions(AsyncChannelFuture.java:161)
	at com.ibm.io.async.AsyncFuture.completed(AsyncFuture.java:136)
	at com.ibm.io.async.ResultHandler.complete(ResultHandler.java:195)
	at com.ibm.io.async.ResultHandler.runEventProcessingLoop(ResultHandler.java:784)
	at com.ibm.io.async.ResultHandler$2.run(ResultHandler.java:873)
	at com.ibm.ws.util.ThreadPool$Worker.run(ThreadPool.java:1473)
"
0,Fix javadocs after deprecation removalThere are a lot of @links in Javadocs to methods/classes that no longer exist. javadoc target prints tons of warnings. We should fix that.
0,Cut over SpanQuery#getSpans to AtomicReaderContextFollowup from LUCENE-2831 - SpanQuery#getSpans(IR) seems to be the last remaining artifact that doesn't enforce per-segments context while it should really work on AtomicReaderContext (SpanQuery#getSpans(AtomicReaderContext) instead of a naked IR.
0,"Some improvements to CMSWhile running optimize on a large index, I've noticed several things that got me to read CMS code more carefully, and find these issues:

* CMS may hold onto a merge if maxMergeCount is hit. That results in the MergeThreads taking merges from the IndexWriter until they are exhausted, and only then that blocked merge will run. I think it's unnecessary that that merge will be blocked.

* CMS sorts merges by segments size, doc-based and not bytes-based. Since the default MP is LogByteSizeMP, and I hardly believe people care about doc-based size segments anymore, I think we should switch the default impl. There are two ways to make it extensible, if we want:
** Have an overridable member/method in CMS that you can extend and override - easy.
** Have OneMerge be comparable and let the MP determine the order (e.g. by bytes, docs, calibrate deletes etc.). Better, but will need to tap into several places in the code, so more risky and complicated.

On the go, I'd like to add some documentation to CMS - it's not very easy to read and follow.

I'll work on a patch."
0,"First cut at column-stride fields (index values storage)I created an initial basic impl for storing ""index values"" (ie
column-stride value storage).  This is still a work in progress... but
the approach looks compelling.  I'm posting my current status/patch
here to get feedback/iterate, etc.

The code is standalone now, and lives under new package
oal.index.values (plus some util changes, refactorings) -- I have yet
to integrate into Lucene so eg you can mark that a given Field's value
should be stored into the index values, sorting will use these values
instead of field cache, etc.

It handles 3 types of values:

  * Six variants of byte[] per doc, all combinations of fixed vs
    variable length, and stored either ""straight"" (good for eg a
    ""title"" field), ""deref"" (good when many docs share the same value,
    but you won't do any sorting) or ""sorted"".

  * Integers (variable bit precision used as necessary, ie this can
    store byte/short/int/long, and all precisions in between)

  * Floats (4 or 8 byte precision)

String fields are stored as the UTF8 byte[].  This patch adds a
BytesRef, which does the same thing as flex's TermRef (we should merge
them).

This patch also adds basic initial impl of PackedInts (LUCENE-1990);
we can swap that out if/when we get a better impl.

This storage is dense (like field cache), so it's appropriate when the
field occurs in all/most docs.  It's just like field cache, except the
reading API is a get() method invocation, per document.

Next step is to do basic integration with Lucene, and then compare
sort performance of this vs field cache.

For the ""sort by String value"" case, I think RAM usage & GC load of
this index values API should be much better than field caache, since
it does not create object per document (instead shares big long[] and
byte[] across all docs), and because the values are stored in RAM as
their UTF8 bytes.

There are abstract Writer/Reader classes.  The current reader impls
are entirely RAM resident (like field cache), but the API is (I think)
agnostic, ie, one could make an MMAP impl instead.

I think this is the first baby step towards LUCENE-1231.  Ie, it
cannot yet update values, and the reading API is fully random-access
by docID (like field cache), not like a posting list, though I
do think we should add an iterator() api (to return flex's DocsEnum)
-- eg I think this would be a good way to track avg doc/field length
for BM25/lnu.ltc scoring.
"
0,"[CONTRIB] SSL authenticating protocol socket factoryHere's the long promised SSL client/server authenticating socket factory. This
socket factory can be used to enforce client/server authentication during the
SSL context negotiation. Let me know what you think. Please, please someone
proof-read the accompanying javadocs and let me know if the text is comprehensible 

I have also tweaked EasySSLProtocolSocketFactory a little

The patch is against HTTPCLIENT_2_0_BRANCH

Oleg"
0,IndexWriter.deleteDocuments bugIndexWriter.deleteDocuments() fails random testing
0,"Promote solr's PrefixFilter into Java Lucene's coreSolr's PrefixFilter class is not specific to Solr and seems to be of interest to core lucene users (PyLucene in this case).
Promoting it into the Lucene core would be helpful."
1,"problems with IR's readerFinishedListenerThere are two major problems:
1. The listener api does not really apply all indexreaders. for example segmentreaders dont fire it on close, only segmentcorereaders. this is wrong, a segmentcorereader is *not* an indexreader. Furthermore, if you register it on a top-level reader you get events for anything under the reader tree (sometimes, unless they are segmentreaders as mentioned above, where it doesnt work correctly at all).
2. Furthermore your listener is 'passed along' in a viral fashion from clone() and reopen(). This means for example, if you are trying to listen to readers in NRT search you are just accumulating reader listeners, all potentially keeping references to old indexreaders (because, in order to deal with #1 your listener must 'keep' a reference to the IR it was registered on, so it can check if thats *really* the one).

We should discuss how to fix #1. 

I will create a patch for #2 shortly and commit it, its just plain wrong.
"
1,"httpClient failed to reconnect after keep-alive connection timed outDescription:

When using httpClient with https tunnelling througha proxy server, after keep-
alive connection timed out on server side.  The httpClient code was unable to 
establish the connection again.

Cause:

The HttpMethodBase.processRequest's retry loop retries the connection without 
going through the ""CONNECT"" request to the proxy server.  Our proxy server 
returns 407 error code.  In case of tunnelling connection, proper reconnect 
should be done by first doing the ""CONNECT"" sequence to get authenticated 
throught the proxy.

Temp fix and Work around:

We implemented some work around to do the retry from the application layer.  In 
order to detect the situation, we have to rely on the error message contained 
in the HttpRecoverableException.  We are checking the text ""Connection aborted 
by peer: socket write error"".  We also have to modify the HttpMethodBase code 
to throw the HttpRecoverableException out to the application."
1,"ConstraintSplitter.getSelectorNames doesn't support FullTextSearch constraintsThe constraint type FullTextSearch is missing in the tested types in org.apache.jackrabbit.core.query.lucene.join.ConstraintSplitter.getSelectorNames(Constraint) method. Submitting a QOM query with a full-text constraint throws a javax.jcr.UnsupportedRepositoryOperationException, while the repository reports supporting such queries : session.getRepository().getDescriptorValue(Repository.QUERY_FULL_TEXT_SEARCH_SUPPORTED).getBoolean() returns TRUE.

Typical stack trace :

javax.jcr.UnsupportedRepositoryOperationException: Unknown constraint type: CONTAINS(r.[jcr:title], 'REGA -APA')
	org.apache.jackrabbit.core.query.lucene.join.ConstraintSplitter.getSelectorNames(ConstraintSplitter.java:177)
	org.apache.jackrabbit.core.query.lucene.join.ConstraintSplitter.getSelectorNames(ConstraintSplitter.java:195)
	org.apache.jackrabbit.core.query.lucene.join.ConstraintSplitter.getSelectorNames(ConstraintSplitter.java:157)
	org.apache.jackrabbit.core.query.lucene.join.ConstraintSplitter.split(ConstraintSplitter.java:106)
	org.apache.jackrabbit.core.query.lucene.join.ConstraintSplitter.split(ConstraintSplitter.java:104)
	org.apache.jackrabbit.core.query.lucene.join.ConstraintSplitter.<init>(ConstraintSplitter.java:80)
	org.apache.jackrabbit.core.query.lucene.join.QueryEngine.execute(QueryEngine.java:162)
	org.apache.jackrabbit.core.query.lucene.join.QueryEngine.execute(QueryEngine.java:147)
	org.apache.jackrabbit.core.query.QueryObjectModelImpl.execute(QueryObjectModelImpl.java:114)"
0,"Sandbox remaining contrib queriesIn LUCENE-3271, I moved the 'good' queries from the queries contrib to new destinations (primarily the queries module).  The remnants now need to find their home.  As suggested in LUCENE-3271, these classes are not bad per se, just odd.  So lets create a sandbox contrib that they and other 'odd' contrib classes can go to.  We can then decide their fate at another time."
1,"Startup fails if clustered jackrabbit is upgrade from 1.4.4 to 1.5This is closely related to JCR-1087

The call to checkLocalRevisionSchema() is too late because preapreStatements() already uses the LOCAL_REVISIONS table.

checkLocalRevisionSchema() should be called in checkSchema()"
0,"Rename Field.Index.UN_TOKENIZED/TOKENIZED/NO_NORMSThere is confusion about these current Field options and I think we
should rename them, deprecating the old names in 2.4/2.9 and removing
them in 3.0.  How about this:

{code}
TOKENIZED --> ANALYZED
UN_TOKENIZED --> NOT_ANALYZED
NO_NORMS --> NOT_ANALYZED_NO_NORMS
{code}

Should we also add ANALYZED_NO_NORMS?

Spinoff from here:

    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200808.mbox/%3C48a3076a.2679420a.1c53.ffffa5c4%40mx.google.com%3E
    "
0,"Unclosed threads in JackrabbitThe Tomcat integration test added in JCR-2831 shows the following warnings about Jackrabbit threads that remain in place even after the repository has been closed:

08-Dec-2010 12:14:58 org.apache.catalina.loader.WebappClassLoader clearReferencesThreads
SEVERE: The web application [] appears to have started a thread named [Timer-1] but has failed to stop it. This is very likely to create a memory leak.
08-Dec-2010 12:14:58 org.apache.catalina.loader.WebappClassLoader clearReferencesThreads
SEVERE: The web application [] appears to have started a thread named [DynamicPooledExecutor] but has failed to stop it. This is very likely to create a memory leak.
08-Dec-2010 12:14:58 org.apache.catalina.loader.WebappClassLoader clearReferencesThreads
SEVERE: The web application [] appears to have started a thread named [Timer-2] but has failed to stop it. This is very likely to create a memory leak.

It would be best to close all such background threads even if they are singleton daemon threads and thus unlikely to cause trouble when left unattended."
0,"Field tokenStream should be usable with stored fields.Field.tokenStream should be usable for indexing even for stored values.  Useful for many types of pre-analyzed values (text/numbers, etc)
http://search.lucidimagination.com/search/document/902bad4eae20bdb8/field_tokenstreamvalue"
0,OpenDocument files missing in mimetypes.propertiesThe mime-types from Oasis OpenDocument are missing from mimetypes.properties file. 
1,"NullPointerException in CompoundFileReaderHello,

we have got a NullPointerException in the Lucene-class CompoundFileReader:

java.lang.NullPointerException
        at
org.apache.lucene.index.CompoundFileReader.<init>(CompoundFileReader.java:94)
        at org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:97)
        at org.apache.lucene.index.IndexWriter.mergeSegments(IndexWriter.java:466)
        at
org.apache.lucene.index.IndexWriter.flushRamSegments(IndexWriter.java:426)
        at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:236)

Lucene has been working fine for some days, until this NullPointerException
has occured which has corrupted the complete index.

The reason for this NullPointerException is the following Code 
in Lucenes source file CompoundFileReader.java:

    public CompoundFileReader(Directory dir, String name)
    throws IOException
    {
        boolean success = false;
        ...

        try {
            stream = dir.openFile(name);

            // read the directory and init files
            ...

            success = true;

        } finally {
            if (! success) {
                try {
                    stream.close();
                } catch (IOException e) { }
            }
        }
    }

If the IO-method-call ""dir.openFile()"" throws an IOExeption,
then the variable ""stream"" remains its null value.
The statement ""stream.close()"" in the finally clause will then cause a
NullPointerException.

I would suggest that you change the code from:
    stream.close();
to:
    if ( stream != null ) {
        stream.close();
    }

There are a lot of reasons why an IO-operation like ""dir.openFile()""
could throw an IOException.
I cannot guarantee that such an IO exception will never occur again.
Therefore it is better to handle such an IO exception correctly.

This issue is similar to bug# 29774, except that I recommand an easy way
to solve this problem."
0,"BooleanScorer.nextDoc should also delegate to sub-scorer's bulk scoring methodBooleanScorer uses the bulk score methods of its sub scorers, asking them to score each chunk of 2048 docs.

However, its .nextDoc fails to do this, instead manually walking through the sub's docs (calling .nextDoc()), which is slower (though this'd be tiny in practice).

As far as I can tell it should delegate to the bulk scorer just like it does in its bulk scorer method."
0,"JSR 283 lifecycle managementJSR 283 specifies a simple lifecycle management mechanism for nodes, and as the reference implementation we should implement that feature. We also need to implement the related TCK tests.

This feature introduces a few new API methods, but during Jackrabbit 1.x we can just introduce them as custom jsr-283 API extensions.
"
0,Allow use of compact DocIdSet in CachingWrapperFilterExtends CachingWrapperFilter with a protected method to determine the DocIdSet to be cached.
1,"DocViewSAXEventGenerator produces invalid SAX streamISO9075.encode() is called twice in DocViewSAXEventGenerator.leaving(), which produces invalid endElement events.

Faulty block of code (note the encode method called twice):

        // encode node name to make sure it's a valid xml name
        name = ISO9075.encode(name);
        // element name
        String elemName;
        if (node.getDepth() == 0) {
            // root node needs a name
            elemName = jcrRoot;
        } else {
            // encode node name to make sure it's a valid xml name
            elemName = ISO9075.encode(name);
        }"
0,"test granularity for calendar (date) propertiesThere are repositories out there that do support properties of type Date, but not Calendar (the main difference being that Calendar also captures the time zone). Also, some repositories may not be able to store timestamps with millisecond resolution.

Although both these restrictions make a repository non-compliant, it would be useful for the tests to test these aspects as separate issues. Thus I propose to simplify the existing tests so that they just compare timestamps (factoring out the time zone), and do not require resolution finer than 1s. These two aspects then should be tested in a separate test case (thinking of it, they currently may not test sub-second resolution, in which case I propose to leave things as they are with respect to this).

"
0,"BalancedSegmentMergePolicy, contributed from the Zoie project for realtime indexingWritten by Yasuhiro Matsuda for Zoie realtime indexing system used to handle high update rates to avoid large segment merges.
Detailed write-up is at:

http://code.google.com/p/zoie/wiki/ZoieMergePolicy
"
0,"add ASCIIFoldingFilter and deprecate ISOLatin1AccentFilterThe ISOLatin1AccentFilter is removing accents from accented characters in the ISO Latin 1 character set.
It does what it does and there is no bug with it.

It would be nicer, though, if there was a more comprehensive version of this code that included not just ISO-Latin-1 (ISO-8859-1) but the entire Latin 1 and Latin Extended A unicode blocks.
See: http://en.wikipedia.org/wiki/Latin-1_Supplement_unicode_block
See: http://en.wikipedia.org/wiki/Latin_Extended-A_unicode_block

That way, all languages using roman characters are covered.
A new class, ISOLatinAccentFilter is attached. It is intended to supercede ISOLatin1AccentFilter which should get deprecated."
1,QPropertyDefinitionImpl.equals() is implemented incorrectly 
1,"CachingIndexReader: NullPointerException initializing parents cacheUsing the jackrabbit-core-1.4.9 (after upgrading from jackrabbot-core-1.4.6), the following exception is logged. The code where the exception happens was introduced in JCR-1884 and is first included in the 1.4.9 core release.

10.03.2009 18:56:25 *WARN * CachingIndexReader: Error initializing parents cache. (CachingIndexReader.java, line 310)
java.lang.NullPointerException
    at org.apache.jackrabbit.core.query.lucene.CachingIndexReader$CacheInitializer$2.collect(CachingIndexReader.java:362)
    at org.apache.jackrabbit.core.query.lucene.CachingIndexReader$CacheInitializer.collectTermDocs(CachingIndexReader.java:426)
    at org.apache.jackrabbit.core.query.lucene.CachingIndexReader$CacheInitializer.initializeParents(CachingIndexReader.java:356)
    at org.apache.jackrabbit.core.query.lucene.CachingIndexReader$CacheInitializer.run(CachingIndexReader.java:306)
    at org.apache.jackrabbit.core.query.lucene.CachingIndexReader.<init>(CachingIndexReader.java:109)
    at org.apache.jackrabbit.core.query.lucene.AbstractIndex.getReadOnlyIndexReader(AbstractIndex.java:276)
    at org.apache.jackrabbit.core.query.lucene.MultiIndex.getIndexReader(MultiIndex.java:731)
    at org.apache.jackrabbit.core.query.lucene.MultiIndex.<init>(MultiIndex.java:303)
    at org.apache.jackrabbit.core.query.lucene.SearchIndex.doInit(SearchIndex.java:454)
    at com.day.crx.query.lucene.LuceneHandler.doInit(LuceneHandler.java:93)
    at org.apache.jackrabbit.core.query.AbstractQueryHandler.init(AbstractQueryHandler.java:53)
    at org.apache.jackrabbit.core.SearchManager.initializeQueryHandler(SearchManager.java:583)
    at org.apache.jackrabbit.core.SearchManager.<init>(SearchManager.java:265)
    at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getSearchManager(RepositoryImpl.java:1600)
    at org.apache.jackrabbit.core.RepositoryImpl.initWorkspace(RepositoryImpl.java:606)
    at org.apache.jackrabbit.core.RepositoryImpl.getWorkspaceInfo(RepositoryImpl.java:718)
    at com.day.crx.core.CRXRepositoryImpl.login(CRXRepositoryImpl.java:964)
    at org.apache.sling.jcr.base.internal.SessionPool.acquireSession(SessionPool.java:268)
    at org.apache.sling.jcr.base.internal.SessionPoolManager.login(SessionPoolManager.java:99)
    at org.apache.sling.jcr.base.AbstractSlingRepository.login(AbstractSlingRepository.java:240)
    at org.apache.sling.jcr.base.AbstractSlingRepository.loginAdministrative(AbstractSlingRepository.java:206)
    at org.apache.sling.jcr.base.AbstractSlingRepository.pingAndCheck(AbstractSlingRepository.java:506)
    at org.apache.sling.jcr.base.AbstractSlingRepository.startRepository(AbstractSlingRepository.java:810)
    at org.apache.sling.jcr.base.AbstractSlingRepository.activate(AbstractSlingRepository.java:629)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.felix.scr.impl.ImmediateComponentManager.createImplementationObject(ImmediateComponentManager.java:226)
    at org.apache.felix.scr.impl.ImmediateComponentManager.createComponent(ImmediateComponentManager.java:133)
    at org.apache.felix.scr.impl.AbstractComponentManager.activateInternal(AbstractComponentManager.java:476)
    at org.apache.felix.scr.impl.AbstractComponentManager.enableInternal(AbstractComponentManager.java:398)
    at org.apache.felix.scr.impl.AbstractComponentManager.access$000(AbstractComponentManager.java:36)
    at org.apache.felix.scr.impl.AbstractComponentManager$1.run(AbstractComponentManager.java:99)
    at org.apache.felix.scr.impl.ComponentActorThread.run(ComponentActorThread.java:85)
10.03.2009 18:56:31 *INFO * SearchIndex: Index initialized: /u01/media/u01/crxlocal/workspaces/dailymail-prod/index Version: 2 (SearchIndex.java, line 492)
"
1,"ArrayIndexOutOfBoundsException when using MultiFieldQueryParserWe get the following exception:

Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: -1
        at java.util.Vector.elementAt(Vector.java:434)
        at org.apache.lucene.queryParser.QueryParser.addClause(QueryParser.java:181)
        at org.apache.lucene.queryParser.QueryParser.Query(QueryParser.java:529)
        at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:108)
        at org.apache.lucene.queryParser.QueryParser.parse(QueryParser.java:87)
        at
org.apache.lucene.queryParser.MultiFieldQueryParser.parse(MultiFieldQueryParser.java:77)
        at idx.Mquery.main(Mquery.java:64)


We are using a query with 'AND' like 'bla AND blo' on 5 fields.
One of the fields has a Tokenizer which returns no token
at all on this query, and this together with the AND
triggers the exception."
0,"Degrade gracefully when reading invalid date valuesAs noted in JCR-1996, it is possible for an old version of Jackrabbit to store date invalid date values in the repository. Currently such values cause exceptions when the repository attempts to read them. A better approach would be to automatically detect such dates and map them instead to string values to avoid losing any information. A client could then access the information as a string through the normal JCR API, and would only get a ValueFormatException when trying to read the value as a date, i.e. using the getDate() method."
0,"Integrate IndexReader with IndexWriter The current problem is an IndexReader and IndexWriter cannot be open
at the same time and perform updates as they both require a write
lock to the index. While methods such as IW.deleteDocuments enables
deleting from IW, methods such as IR.deleteDocument(int doc) and
norms updating are not available from IW. This limits the
capabilities of performing updates to the index dynamically or in
realtime without closing the IW and opening an IR, deleting or
updating norms, flushing, then opening the IW again, a process which
can be detrimental to realtime updates. 

This patch will expose an IndexWriter.getReader method that returns
the currently flushed state of the index as a class that implements
IndexReader. The new IR implementation will differ from existing IR
implementations such as MultiSegmentReader in that flushing will
synchronize updates with IW in part by sharing the write lock. All
methods of IR will be usable including reopen and clone. 
"
0,"TokenFilter should implement reset()TokenFilter maintains a private member of TokenStream.
It should implement reset() and call its member TokenStream's reset() method. Otherwise, that TokenStream never gets reset.
Patch applied."
0,"Publish source/javadoc jar files to the Maven repositoryIt would be really nice if HttpComponents (Core and Client) published jar files to the Maven repository for not just the bytecode, but also for the source and javadoc (done by defining a ""classifier"" attribute of ""javadoc"" or ""source"" for the jar when publishing with Maven).

Having these in the Maven repo allows an IDE (like Eclipse) to auto-download and attach the source/javadoc to the HttpComponent jar files - meaning developers will then see the API documentation automatically in their IDE.  This also greatly aids debugging if one needs to step through HttpComponent code, and placing the source in the hands of more developers also means you might see more patches coming back."
1,"If you ""flush by RAM usage"" then IndexWriter may over-mergeI think a good way to maximize performance of Lucene's indexing for a
given amount of RAM is to flush (writer.flush()) the added documents
whenever the RAM usage (writer.ramSizeInBytes()) has crossed the max
RAM you can afford.

But, this can confuse the merge policy and cause over-merging, unless
you set maxBufferedDocs properly.

This is because the merge policy looks at the current maxBufferedDocs
to figure out which segments are level 0 (first flushed) or level 1
(merged from <mergeFactor> level 0 segments).

I'm not sure how to fix this.  Maybe we can look at net size (bytes)
of a segment and ""infer"" level from this?  Still we would have to be
resilient to the application suddenly increasing the RAM allowed.

The good news is to workaround this bug I think you just need to
ensure that your maxBufferedDocs is less than mergeFactor *
typical-number-of-docs-flushed.
"
0,"make sure no static loggers are usedReview all loggers used in the component, make sure they are stored in non-static attributes only.
http://wiki.apache.org/jakarta-commons/Logging/StaticLog
"
1,"Contrib RMI: NotSerializableExceptionorg.apache.jackrabbit.rmi.client.RemoteRepositoryException:

error unmarshalling return; nested exception is:.java.io.WriteAbortedException: writing aborted; java.io.NotSerializableException: javax.jcr.NameValue

"
0,"common interface for HttpRoute and RouteTrackerClasses HttpRoute and RouteTracker have many identical getters. There should be a common interface, for example RouteInfo, to define these getters and a toRoute() method that returns an unmodifiable representation. Some portions of the API may then accept the interface instead of the specific class HttpRoute.
"
1,"Optimize runs forever if you keep deleting docs at the same timeBecause we ""cascade"" merges for an optimize... if you also delete documents while the merges are running, then the merge policy will see the resulting single segment as still not optimized (since it has pending deletes) and do a single-segment merge, and will repeat indefinitely (as long as your app keeps deleting docs)."
0,A faster JFlex-based replacement for StandardAnalyzerJFlex (http://www.jflex.de/) can be used to generate a faster (up to several times) replacement for StandardAnalyzer. Will add a patch and a simple benchmark code in a while.
1,"finalize()-methods of FSDirectory.FSIndexInput and FSDirectory.FSIndexOutput try to close already closed fileHi all,

I found a small problem in FSDirectory: The finalize()-methods of FSDirectory.FSIndexInput and FSDirectory.FSIndexOutput try to close the underlying file. This is not a problem unless the file has been closed before by calling the close() method. If it has been closed before, the finalize method throws an IOException saying that the file is already closed. Usually this IOException would go unnoticed, because the GarbageCollector, which calls finalize(), just eats it. However, if I use the Eclipse debugger the execution of my code will always be suspended when this exception is thrown.

Even though this exception probably won't cause problems during normal execution of Lucene, the code becomes cleaner if we apply this small patch. Might this IOException also have a performance impact, if it is thrown very frequently?

I attached the patch which applies cleanly on the current svn HEAD. All testcases pass and I verfied with the Eclipse debugger that the IOException is not longer thrown."
1,"Query for name literal without namespace failsQuery for a name literal without a namespace fails. 

Example:
//*[@foo = 'bla']

should return nodes with foo property that contain the String value 'bla' or the Name value 'bla' (no namespace). Only nodes with String value 'bla' are returned."
0,"Automaton Query/Filter (scalable regex)Attached is a patch for an AutomatonQuery/Filter (name can change if its not suitable).

Whereas the out-of-box contrib RegexQuery is nice, I have some very large indexes (100M+ unique tokens) where queries are quite slow, 2 minutes, etc. Additionally all of the existing RegexQuery implementations in Lucene are really slow if there is no constant prefix. This implementation does not depend upon constant prefix, and runs the same query in 640ms.

Some use cases I envision:
 1. lexicography/etc on large text corpora
 2. looking for things such as urls where the prefix is not constant (http:// or ftp://)

The Filter uses the BRICS package (http://www.brics.dk/automaton/) to convert regular expressions into a DFA. Then, the filter ""enumerates"" terms in a special way, by using the underlying state machine. Here is my short description from the comments:

     The algorithm here is pretty basic. Enumerate terms but instead of a binary accept/reject do:
      
     1. Look at the portion that is OK (did not enter a reject state in the DFA)
     2. Generate the next possible String and seek to that.

the Query simply wraps the filter with ConstantScoreQuery.

I did not include the automaton.jar inside the patch but it can be downloaded from http://www.brics.dk/automaton/ and is BSD-licensed."
1,"Query dump failed with deep query treeWith a big query (more than 400 OR operands) the query dump failed.
The query dump is made at QueryImpl.execute (line 136)

It failed because of the constant PADDING at QueryTreeDump.visit(line 85).
The constant PADDING is a 255 character array, but in my program it would need it to be bigger.
I think putting it to 65535 would not be a problem : it would only take a little bit more memory.

This is the top of the stacktract for info:
java.lang.ArrayIndexOutOfBoundsException
	at java.lang.System.arraycopy(Native Method)
	at java.lang.StringBuffer.append(StringBuffer.java:499)
	at org.apache.jackrabbit.core.query.QueryTreeDump.visit(QueryTreeDump.java:85)
	at org.apache.jackrabbit.core.query.OrQueryNode.accept(OrQueryNode.java:50)
	at org.apache.jackrabbit.core.query.QueryTreeDump.traverse(QueryTreeDump.java:263)
                     ...

This is not critical because I can avoid the dump by unactivating debug logs.
"
1,"ConcurrentModificationException during logoutWe regularly get the following exception:

java.util.ConcurrentModificationException
        at org.apache.commons.collections.map.AbstractReferenceMap$ReferenceEntrySetIterator.checkMod(AbstractReferenceMap.java:761)
        at org.apache.commons.collections.map.AbstractReferenceMap$ReferenceEntrySetIterator.hasNext(AbstractReferenceMap.java:735)
        at java.util.Collections$UnmodifiableCollection$1.hasNext(Collections.java:1009)
        at java.util.Collections$UnmodifiableCollection$1.hasNext(Collections.java:1009)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.dispose(LocalItemStateManager.java:341)
        at org.apache.jackrabbit.core.WorkspaceImpl.dispose(WorkspaceImpl.java:170)
        at org.apache.jackrabbit.core.SessionImpl.logout(SessionImpl.java:1225)
        at org.apache.jackrabbit.core.XASessionImpl.logout(XASessionImpl.java:379)

Two causes for this exception have been identified:

 (Taken from an email to the dev-list from Marcel Reutegger):
> - session A reads some items I
> - session B transiently removes items in I
> - session A logs out and starts to iterate over I in  LocalItemStateManager (LISM)
> - session B saves changes and removed items are evicted from A's LISM
> - session A gets concurrent modification exception

Another scenario is the following:
- Session A gets the iterator of the values of (the primary cache of) an ItemStateReferenceCache in LocalItemStateManager.dispose.
- Session B then does something that triggers the CacheManager.
- The CacheManager then calls resizeAll, and evicts some items from the secondary cache of the ItemStateReferenceCache of which the LocalItemStateManager has a values iterator.
- The garbage collector then runs and evicts the removed items also from the primary cache, which effectively modifies the set over which is iterated.

Regards,

Martijn Hendriks"
0,"Upgrade to SLF4J 1.3Version 1.1 of the SLF4J logging facade was recently released. It contains no functional improvements that we'd need, but is split to a separate slf4j-api library and a set of backend-specic logging adapters. This would allow us to avoid exposing log4j as a transitive dependency for projects that depend on Jackrabbit."
0,"extend LevenshteinAutomata to support transpositions as primitive editsThis would be a nice improvement for spell correction: currently a transposition counts as 2 edits,
which means users of DirectSpellChecker must use larger values of n (e.g. 2 instead of 1) and 
larger priority queue sizes, plus some sort of re-ranking with another distance measure for good results.

Instead if we can integrate ""chapter 7"" of http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.652 
then you can just build an alternative DFA where a transposition is only a single edit 
(http://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)

According to the benchmarks in the original paper, the performance for LevT looks to be very similar to Lev.

Support for this is now in moman (https://bitbucket.org/jpbarrette/moman/) thanks to Jean-Philippe 
Barrette-LaPierre.
"
0,"IfHeader: Incorrect test for parsing keyword ""not"" in parseIfListThe test for the ""not"" keyword tests the last character as

     (not !='t' || not != 'T')

which always yields true (not cannot be t and T at the same time). The correct test would be

     (not !='t' && not != 'T')"
0,"build.xml: result of ""dist-src"" should support ""build-contrib""Currently the packed src distribution would fail to run ""ant build-contrib"".
It would be much nicer if that work.
In fact, would be nicer if you could even ""re-pack"" with it.

For now I marked this for 2.1, although I am not yet sure if this is a stopper."
1,NodeDefinitionTemplateImpl.setDefaultPrimaryTypeName(null) throws exceptionexpect to clear the name.
1,"handling of expanded-form jcr names by node type *Template classes ItemDefinitionTemplate treats the name as opque string, instead of a JCR Name.

Example: when setting the name to

  ""{http://example.org/}foo""

then getName() needs to return

  ""bar:foo""

which the prefix ""bar"" being mapped to the namesapce ""http://example.org/""."
1,"MultipartPostMethod Holding File Stream Open?From: ""Daniel Walsh"" <daniel.walsh13@verizon.net>
Date: Tue Feb 25, 2003  8:05:49 PM US/Eastern
To: ""Commons HttpClient Project"" <commons-httpclient-dev@jakarta.apache.org>
Subject: MultipartPostMethod Holding File Stream Open?
Reply-To: ""Commons HttpClient Project"" <commons-httpclient-dev@jakarta.apache.org>

I'm using a MultipartPostMethod to upload a file to a servlet:

File file = new File(strUrl);

HttpClient client = new HttpClient();
HostConfiguration hostConfig = new HostConfiguration();
MultipartPostMethod mpPost = new MultipartPostMethod();

 hostConfig.setHost(someURL.getHost(), someURL.getPort(), someURL.getProtocol());
client.setConnectionTimeout(30000);
client.setHostConfiguration(hostConfig);

mpPost.addParameter(""someName"", ""someValue"");
mpPost.addParameter(file.getName(), file);

mpPost.setPath(strPath);
client.executeMethod(mpPost);

String confirmUpload = tpPost.getResponseBodyAsString();
mpPost.releaseConnection();

file.delete();  // this is being blocked.

After the upload, I would like to delete the file off of my disk.  Using other
methods of uploading the file (in particular a PutMethod), I was able to then
delete the file after the upload.  Now that I am using the MultipartPostMethod
obj for the upload, I am unable to delete the file (the return value is false,
and there is no SecurityException being thrown - no SecurityManager even set as
of this point either).

So, I guess my question is whether there is a call to the MultipartPostMethod
obj that I'm overlooking that would release it's connection (I'm sure that it is
opening an InputStream of some sort to read the file contents, in order to form
the HTTP message) to the file - so that I can then have unimpeded access to it
for other operations?"
1,TCK: check for wrong repository descriptor. should be versioning instead of locking... at least according to the comment.
0,"concurrent read-only access to a sessionEven though the JCR specification does not make a statement about Sessions shared across a number of threads I think it would be great for many applications if we could state that sharing a read-only session is supported by Jackrabbit.
On many occasions in the mailing lists we stated that there should not be an issue with sharing a read-only session, however I think it has never been thoroughly tested or even specified as a ""design goal"".

If we can come to an agreement that this is desirable I think it would be great to start including testcases to validate that behaviour and update the documentation respectively."
0,"Add the ability to disable inheriting ancestor ACLsThe current ACL implementation will walk the tree from the item being accessed, up to the root, collecting ACL entries for all the ancestors. With this system, there is no easy way to restrict access to subnodes except by adding DENY entries to negate the entries inherited from the parent nodes.

I'd like to request a way to turn this behavior off either at a node level or global level.

The place where recursion is happening is in org.apache.jackrabbit.core.security.authorization.acl.ACLProvider$Entries.collectEntries(NodeImpl node). Inside this method, it could perhaps check a global parameter or the existence of property of the ACL policy node to determine whether to recurse up the tree."
0,Add read acessor for user data to SessionInfoImplAdd method SessionInfoImpl.getUserData() to retrieve the user data set through SessionInfoImpl.setUserData(String)
0,"Some improvements to _TestUtil and its usageI've started this issue because I've noticed that _TestUtil.getRandomMultiplier() is called from many loops' condition check, sometimes hundreds and thousands of times. Each time it does Integer.parseInt after calling System.getProperty. This really can become a constant IMO, either in LuceneTestCase(J4) or _TestUtil, as it's not expected to change while tests are running ...

I then reviewed the class and spotted some more things that I think can be fixed/improved:
# getTestCodec() can become a constant as well
# arrayToString is marked deprecated. I've checked an no one calls them, so I'll delete them. This is a 4.0 code branch + a test-only class. No need to deprecate anything.
# getTempDir calls new Random(), instead of newRandom() in LuceneTestCaseJ4, which means that if something fails, we won't know the random seed used ...
#* In that regard, we might want to output all the classes that obtained a static seed in reportAdditionalFailures(), instead of just the class that ran the test.
# rmDir(String) can be removed IMO, and leave only rmDir(File)
# I suggest we include some recursion in rmDir(File) to handle the deletion of nested directories.
#* Also, it does not check whether the dir deletion itself succeeds (but it does so for the files). This can bite us on Windows, if some test did not close things properly.

I'll work out a patch."
0,"Spellchecker doesn't need to store ngramsThe spellchecker in contrib stores the ngrams although this doesn't seem to be necessary. This patch changes that, I will commit it unless someone objects. This improves indexing speed and index size. Some numbers on a small test I did:

Input of the original index: 2200 text files, index size 5.3 MB, indexing took 17 seconds

Spell index before patch: about 60.000 documents, index size 13 MB, indexing took 62 seconds
Spell index after patch: about 60.000 documents, index size 6.3 MB, indexing took 52 seconds

BTW, the test case fails even before this patch. I'll probaby submit another issue about how to fix that.
"
0,"Collapse Common module into Lucene core utilIt was suggested by Robert in [http://markmail.org/message/wbfuzfamtn2qdvii] that we should try to limit the dependency graph between modules and where there is something 'common' it should probably go into lucene core.  Given that I haven't added anything to this module except the MutableValue classes, I'm going to collapse them into the util package, remove the module, and correct the dependencies."
0,"Performance fix, when deserializing large jcr:binary in ValueHelper.deserialize()While profiling import of large PDF files into Magnolia 3.6.3 (which uses Jackrabbit 1.4 as JCR repository) we had found that there is large CPU time spent on:

""http-8080-4"" daemon prio=6 tid=0x5569fc00 nid=0x6ec runnable [0x5712d000..0x5712fb14]
   java.lang.Thread.State: RUNNABLE
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:260)
	at org.apache.jackrabbit.util.Base64.decode(Base64.java:269)
	at org.apache.jackrabbit.util.Base64.decode(Base64.java:184)
	at org.apache.jackrabbit.value.ValueHelper.deserialize(ValueHelper.java:759)
	at org.apache.jackrabbit.core.xml.BufferedStringValue.getValue(BufferedStringValue.java:258)
	at org.apache.jackrabbit.core.xml.PropInfo.apply(PropInfo.java:132)

Looking into source code of Base64.decode it became obvious, that it writes each 1to3byte chunk into unbuffered FileOutputStream (thus calling OS kernel many times to write just few bytes) which causes lot of CPU usage without disk usage.


Provided fix is quite trivial - just wrap FileOutputStream into BufferedOutputStream.
"
0,"If IndexWriter hits OutOfMemoryError it should not commitWhile progress has been made making IndexWriter robust to OOME, I
think there is still a real risk that an OOME at a bad time could put
IndexWriter into a bad state such that if close() is called and
somehow it succeeds without hitting another OOME, it risks
introducing messing up the index.

I'd like to detect if OOME has been hit in any of the methods that
alter IW's state, and if so, do not commit changes to the index.  If
close is called after hitting OOME, I think writer should instead
abort.

Attached patch just adds try/catch clauses to catch OOME, note that
it was hit, and re-throw it.  Then, sync() refuses to commit a new
segments_N if OOME was hit, and close instead calls abort when OOME
was hit.  All tests pass.  I plan to commit in a day or two."
0,"gcj ant target doesn't work on windowsIn order to fix it I made two changes, both really simple.

First I added to org/apache/lucene/store/GCJIndexInput.cc some code to use windows memory-mapped I/O instead than unix mmap().

Then I had to rearrange the link order in the Makefile in order to avoid unresolved symbol errors. Also to build repeatedly I had to instruct make to ignore the return code for the mkdir command as on windows it fails if the directory already exists.

I'm attaching two patches corresponding to the changes; please note that with the patches applied, the gcj target still works on linux. Both patches apply cleanly to the current svn head."
0,"Handling sub-domain cookies.I noticed a difference in behaviour between httpclient and most common browsers 
(IE/Mozilla). If a web site sets a cookie for ""beta.gamma.com"", this cookie is 
not sent in requests to ""alpha.beta.gamma.com"". 
  I am not sure what the cookie specs say, but Mozilla, IE and even 
HTTP::Cookies module in LWP seem to behave differently from HttpClient. 
HttpClient seems to rely on the leading dot in the domain name 
(like "".beta.gamma.com"")."
0,"Documentation bug.  The 2.4.1 query parser syntax wiki page says it is for 1.9This page:

http://lucene.apache.org/java/2_4_1/queryparsersyntax.html

says this:
.bq
This page provides the Query Parser syntax in Lucene 1.9. If you are using a different version of Lucene, please consult the copy of docs/queryparsersyntax.html that was distributed with the version you are using. 

This is misleading on a doc page for 2.4.1"
0,"cache should not generate stale responses to requests explicitly requesting first-hand or fresh onesThe current implementation will serve a stale response in the case that it has a stale cache entry but revalidation with the origin fails. However, the RFC says we SHOULD NOT do this if the client explicitly requested a first-hand or fresh response (via no-cache, max-age, max-stale, or min-fresh).
"
0,"Maintain norms in a single file .nrmNon-compound indexes are ~10% faster at indexing, and perform 50% IO activity comparing to compound indexes. But their file descriptors foot print is much higher. 

By maintaining all field norms in a single .nrm file, we can bound the number of files used by non compound indexes, and possibly allow more applications to use this format.

More details on the motivation for this in: http://www.nabble.com/potential-indexing-perormance-improvement-for-compound-index---cut-IO---have-more-files-though-tf2826909.html (in particular http://www.nabble.com/Re%3A-potential-indexing-perormance-improvement-for-compound-index---cut-IO---have-more-files-though-p7910403.html).
"
0,"Change QueryParser to use ConstantScoreRangeQuery in preference to RangeQuery by defaultChange to QueryParser to default to using new ConstantScoreRangeQuery in preference to RangeQuery
for range queries. This implementation is generally preferable because it 
a) Runs faster 
b) Does not have the scarcity of range terms unduly influence score 
c) avoids any ""TooManyBooleanClauses"" exception.

However, if applications really need to use the old-fashioned RangeQuery and the above
points are not required then the  ""useOldRangeQuery"" property can be used to revert to old behaviour.

The patch includes extra Junit tests for this flag and all other Junit tests pass"
1,"SQL2 Join with OR clause still has some issuesThere are still some issues with Joins that have OR clauses in them. I changed the test, so that it reflects the changes"
1,"SetCookie / DateParser failing to parse non-standard date formatI'm receiving the following expiration date in SetCookie which DateParser 
doesn't handle:

expires=Sat,19-Apr-03 04:28:07 GMT

The lack of a space between ',' and '19' is causing the problem. Is it possible 
to add the following lines to DatePattern?

""EEE,dd-MMM-yy HH:mm:ss z""
""EEE,dd-MMM-yyyy HH:mm:ss z"""
0,"DatabaseJournal needs connection reestablishment logicThe DB based file system and persistence manager implementations have logic for connection reestablishment in case the DB server bounces while the repository is running, but the DB based journal implementation doesn't."
0,"Jcr-Server: useful output upon GET to root- and workspace-resourcesin contrast to the resources representing JCR Node or Properties, the 'root' resource and the resources representing the
workspaces present in the repository don't provide any output when accessed in a browser.

-> add some very simple listing for those 2 resource types."
0,"Update copyright year to 2009Our normal license headers don't contain copyright years, but the NOTICE.txt files do. We should update the year to 2009 where appropriate.
"
1,"Stale connection check does not work with IBM JSSE/JREOS: Windows/AIX
JRE: IBM JRE 1.4.1
JSSE: IBM's implementation (SSLite?)
HttpClient Library: 2.0.2 release

My code enabled connection pooling feature to gain performance improvement in 
the SSL Handshake area. The code works perfectly on Sun JRE 1.4.2 with a think 
time of 60seconds between requests, but the same code fails on IBM JRE. On IBM 
JRE, the code fails to detech stale connections, thus causing down the stream 
setSoTimeout() call to fail.

Further debugging into the library code revealed difference in the way the 
HTTPConnection.isStale() behaves. With in that method, particularly, the 
inputStream.isAvailable() method returns 0 with Sun JRE but -1 with IBM JRE.

I made a small code change to HttpConnection.isStale() method by moving the try
{}finally{} block outside of the if(inputStream.isAvailable()==0) check in the 
following code and BINGO, everything started working on IBM JVMs. It did not 
break anything on Suns JVM.

============== CODE BEGIN
    protected boolean isStale() {
    	LOG.debug(""##SUBBA## HttpConnection.isStale() got called. soTimeout="" 
+ soTimeout);
        boolean isStale = true;
        if (isOpen) {
        	LOG.debug(""##SUBBA## HttpConnection.isStale() got called. 
isOpen="" + isOpen);        	
            // the connection is open, but now we have to see if we can read it
            // assume the connection is not stale.
            isStale = false;

                try {         
                    if (inputStream.available() == 0) {		// ALWAYS 
RETURNS -1 on IBM JVM  0 on SUN
                    	
		  // try {		// SUBBA  MOVED OUTSIDE IF
	                	socket.setSoTimeout(1);
	                  	LOG.debug(""##SUBBA## HttpConnection.isStale() 
got called. setSoTimeout(1)"");                    	
	                    
	                    inputStream.mark(1);
	                    int byteRead = inputStream.read();
	                	LOG.debug(""##SUBBA## HttpConnection.isStale() 
got called. bytesRead="" + byteRead);                    	
	                    
	                    if (byteRead == -1) {
	                    	LOG.debug(""##SUBBA## HttpConnection.isStale() 
got called. SETTING isStale to TRUE HERE"");                    	
	                    	
	                        // again - if the socket is reporting all data 
read,
	                        // probably stale
	                        isStale = true;
	                    } else {
	                        inputStream.reset();
	                    }
		    // SUBBA  MOVED OUTSIDE IF
                //} finally {
                //	LOG.debug(""##SUBBA## HttpConnection.isStale() got 
called. finally block - BEGIN "" + soTimeout);                    	
                //    socket.setSoTimeout(soTimeout);
                //	LOG.debug(""##SUBBA## HttpConnection.isStale() got 
called. finally block - DONE"");                        
               // }

	
                    }                        
                } finally {
                	LOG.debug(""##SUBBA## HttpConnection.isStale() got 
called. finally block - BEGIN "" + soTimeout);                    	
                    socket.setSoTimeout(soTimeout);
                	LOG.debug(""##SUBBA## HttpConnection.isStale() got 
called. finally block - DONE"");                        
                }
.....
.....
.....
========================== CODE END


I've attached logs captured before and after the change on both the JRE's for 
your review:

==================================
IBMs LOG (after change):
==================================
<Jun 10, 2005 1:26:55 PM EDT> <DEBUG> 
<apache.commons.httpclient.MultiThreadedHttpConnectionManager:700> <Getting 
free connection, hostConfig=HostConfiguration
[host=uatservices30.ilab.fnfismd.com, protocol=https:443, port=443]>
<Jun 10, 2005 1:26:55 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:492> <##SUBBA## 
HttpConnection.isStale() got called. soTimeout=0>
<Jun 10, 2005 1:26:55 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:495> <##SUBBA## 
HttpConnection.isStale() got called. isOpen=true>
<Jun 10, 2005 1:26:55 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:500> <##SUBBA## 
HttpConnection.isStale() got called. [class 
java.io.BufferedInputStream].available=-1>
<Jun 10, 2005 1:26:55 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:523> <##SUBBA## 
HttpConnection.isStale() got called. finally block - BEGIN 0>
<Jun 10, 2005 1:26:55 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:532> <An error occurred while 
reading from the socket, is appears to be stale>
java.net.SocketException: Socket is closed
	at java.net.Socket.setSoTimeout(Socket.java:927)
	at com.ibm.sslite.bf.setSoTimeout(Unknown Source)
	at com.ibm.jsse.bg.setSoTimeout(Unknown Source)
	at org.apache.commons.httpclient.HttpConnection.isStale
(HttpConnection.java:524)
	at org.apache.commons.httpclient.HttpConnection.isOpen
(HttpConnection.java:436)
	at 
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnection
Adapter.isOpen(MultiThreadedHttpConnectionManager.java:1122)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:626)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:497)
	at 
com.touchpoint.pia.services.transactions.msp.ApacheHttpClient.invokeRequest
(ApacheHttpClient.java:69)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequest
(MsWSManager.java:86)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequestWithPaylo
ad(MsWSManager.java:114)
	at com.touchpoint.pia.services.transactions.msp.MsWSManager.main
(MsWSManager.java:179)
<Jun 10, 2005 1:26:55 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:540> <##SUBBA## 
HttpConnection.isStale() return=true>
<Jun 10, 2005 1:26:55 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:437> <Connection is stale, 
closing...>

==================================
IBMs LOG (before change):
==================================
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.MultiThreadedHttpConnectionManager:666> <enter 
HttpConnectionManager.ConnectionPool.getHostPool(HostConfiguration)>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.MultiThreadedHttpConnectionManager:700> <Getting 
free connection, hostConfig=HostConfiguration
[host=uatservices30.ilab.fnfismd.com, protocol=https:443, port=443]>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:492> <##SUBBA## 
HttpConnection.isStale() got called.>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:495> <##SUBBA## 
HttpConnection.isStale() got called. isOpen=true>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:500> <##SUBBA## 
HttpConnection.isStale() got called. [class 
java.io.BufferedInputStream].available=-1>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:538> <##SUBBA## 
HttpConnection.isStale() return=false>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:599> <HttpConnection.setSoTimeout(0)>
<Jun 10, 2005 1:07:29 PM EDT> <WARN> 
<apache.commons.httpclient.HttpConnection:607> <##SUBBA## Socket Exception>
java.net.SocketException: Socket is closed
	at java.net.Socket.setSoTimeout(Socket.java:927)
	at com.ibm.sslite.bf.setSoTimeout(Unknown Source)
	at com.ibm.jsse.bg.setSoTimeout(Unknown Source)
	at org.apache.commons.httpclient.HttpConnection.setSoTimeout
(HttpConnection.java:603)
	at 
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnection
Adapter.setSoTimeout(MultiThreadedHttpConnectionManager.java:1296)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:633)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:497)
	at 
com.touchpoint.pia.services.transactions.msp.ApacheHttpClient.invokeRequest
(ApacheHttpClient.java:69)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequest
(MsWSManager.java:86)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequestWithPaylo
ad(MsWSManager.java:114)
	at com.touchpoint.pia.services.transactions.msp.MsWSManager.main
(MsWSManager.java:179)
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:1151> <enter 
HttpConnection.releaseConnection()>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.MultiThreadedHttpConnectionManager:513> <enter 
HttpConnectionManager.releaseConnection(HttpConnection)>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.MultiThreadedHttpConnectionManager:791> <Freeing 
connection, hostConfig=HostConfiguration[host=uatservices30.ilab.fnfismd.com, 
protocol=https:443, port=443]>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.MultiThreadedHttpConnectionManager:666> <enter 
HttpConnectionManager.ConnectionPool.getHostPool(HostConfiguration)>
<Jun 10, 2005 1:07:29 PM EDT> <DEBUG> 
<apache.commons.httpclient.MultiThreadedHttpConnectionManager:774> <Notifying 
no-one, there are no waiting threads>
java.net.SocketException: Socket is closed
	at java.net.Socket.setSoTimeout(Socket.java:927)
	at com.ibm.sslite.bf.setSoTimeout(Unknown Source)
	at com.ibm.jsse.bg.setSoTimeout(Unknown Source)
	at org.apache.commons.httpclient.HttpConnection.setSoTimeout
(HttpConnection.java:603)
	at 
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnection
Adapter.setSoTimeout(MultiThreadedHttpConnectionManager.java:1296)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:633)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:497)
	at 
com.touchpoint.pia.services.transactions.msp.ApacheHttpClient.invokeRequest
(ApacheHttpClient.java:69)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequest
(MsWSManager.java:86)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequestWithPaylo
ad(MsWSManager.java:114)
	at com.touchpoint.pia.services.transactions.msp.MsWSManager.main
(MsWSManager.java:179)
java.net.SocketException: Socket is closed
	at java.net.Socket.setSoTimeout(Socket.java:927)
	at com.ibm.sslite.bf.setSoTimeout(Unknown Source)
	at com.ibm.jsse.bg.setSoTimeout(Unknown Source)
	at org.apache.commons.httpclient.HttpConnection.setSoTimeout
(HttpConnection.java:603)
	at 
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnection
Adapter.setSoTimeout(MultiThreadedHttpConnectionManager.java:1296)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:633)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:497)
	at 
com.touchpoint.pia.services.transactions.msp.ApacheHttpClient.invokeRequest
(ApacheHttpClient.java:69)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequest
(MsWSManager.java:86)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequestWithPaylo
ad(MsWSManager.java:114)
	at com.touchpoint.pia.services.transactions.msp.MsWSManager.main
(MsWSManager.java:179)
Exception in thread ""main"" java.net.SocketException: Socket is closed
	at java.net.Socket.setSoTimeout(Socket.java:927)
	at com.ibm.sslite.bf.setSoTimeout(Unknown Source)
	at com.ibm.jsse.bg.setSoTimeout(Unknown Source)
	at org.apache.commons.httpclient.HttpConnection.setSoTimeout
(HttpConnection.java:603)
	at 
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnection
Adapter.setSoTimeout(MultiThreadedHttpConnectionManager.java:1296)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:633)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:497)
	at 
com.touchpoint.pia.services.transactions.msp.ApacheHttpClient.invokeRequest
(ApacheHttpClient.java:69)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequest
(MsWSManager.java:86)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequestWithPaylo
ad(MsWSManager.java:114)
	at com.touchpoint.pia.services.transactions.msp.MsWSManager.main
(MsWSManager.java:179)


============================================
**SUNs LOG (after change = before change):
============================================
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.MultiThreadedHttpConnectionManager:700> <Getting 
free connection, hostConfig=HostConfiguration
[host=uatservices30.ilab.fnfismd.com, protocol=https:443, port=443]>
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:492> <##SUBBA## 
HttpConnection.isStale() got called. soTimeout=0>
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:495> <##SUBBA## 
HttpConnection.isStale() got called. isOpen=true>
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:500> <##SUBBA## 
HttpConnection.isStale() got called. [class 
java.io.BufferedInputStream].available=0>
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:506> <##SUBBA## 
HttpConnection.isStale() got called. setSoTimeout(1)>
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:510> <##SUBBA## 
HttpConnection.isStale() got called. bytesRead=-1>
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:513> <##SUBBA## 
HttpConnection.isStale() got called. SETTING isStale to TRUE HERE>
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:523> <##SUBBA## 
HttpConnection.isStale() got called. finally block - BEGIN 0>
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:532> <An error occurred while 
reading from the socket, is appears to be stale>
java.net.SocketException: Socket Closed
	at java.net.PlainSocketImpl.setOption(PlainSocketImpl.java:177)
	at java.net.Socket.setSoTimeout(Socket.java:924)
	at com.sun.net.ssl.internal.ssl.SSLSocketImpl.setSoTimeout(DashoA12275)
	at org.apache.commons.httpclient.HttpConnection.isStale
(HttpConnection.java:524)
	at org.apache.commons.httpclient.HttpConnection.isOpen
(HttpConnection.java:436)
	at 
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnection
Adapter.isOpen(MultiThreadedHttpConnectionManager.java:1122)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:626)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:497)
	at 
com.touchpoint.pia.services.transactions.msp.ApacheHttpClient.invokeRequest
(ApacheHttpClient.java:69)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequest
(MsWSManager.java:86)
	at 
com.touchpoint.pia.services.transactions.msp.MsWSManager.invokeRequestWithPaylo
ad(MsWSManager.java:114)
	at com.touchpoint.pia.services.transactions.msp.MsWSManager.main
(MsWSManager.java:179)
<Jun 10, 2005 1:25:12 PM EDT> <DEBUG> 
<apache.commons.httpclient.HttpConnection:540> <##SUBBA## 
HttpConnection.isStale() return=true>"
1,MemoryFileSystem.deleteFolder deletes all folders starting with this name MemoryFileSystem.deleteFolder deletes not only the folder requested but all folders that start with the given name.
0,"Confusing code lineLine 81 of TermScorer:

      if (!(target > docs[pointer])) {

Could be replaced with the more readable:

      if (docs[pointer] >= target) {

Sorry for nit picking!"
1,"highlighter problems with overlapping tokensThe lucene highlighter has problems when tokens that overlap are generated.

For example, if analysis of iPod generates the tokens ""i"", ""pod"", ""ipod"" (with pod and ipod in the same position),
then the highlighter will output this as iipod, regardless of if any of those tokens are highlighted.

Discovered via http://issues.apache.org/jira/browse/SOLR-24
"
0,"[PATCH] FuzzyTermEnum optimization and refactorI took a look at it to see if it could be improved.  I saw speed improvements of
20% - 40% by making a couple changes.  

The patch is here: http://www.hagerfamily.com/patches/FuzzyTermEnumOptimizePatch.txt

The Patch is based on the HEAD of the CVS tree as of Oct 22, 2004.

What Changed?

Since the word was discarded if the edit distance for the word was
above a certain threshold, I updated the distance algorithm to abort
if at any time during the calculation it is determined that the best
possible outcome of the edit distance algorithm is above this
threshold.  The source code has a great explanation.

I also reduced the amount of floating point math, reduced the amount
of potential space the array takes in its first dimension, removed the
potential divide by 0 error when one term is an empty string, and
fixed a bug where an IllegalArgumentException was thrown if the class
was somehow initialized wrong, instead of looking at the arguments.

The behavior is almost identical.  The exception is that similarity is
set to 0.0 when it is guaranteed to be below the minimum similarity.

Results

I saw the biggest improvement from longer words, which makes a sense.
My long word was ""bridgetown"" and I saw a 60% improvement on this.
The biggest improvement are for words that are farthest away from the
median length of the words in the index.  Short words (1-3 characters)
saw a 30% improvement.  Medium words saw a 10% improvement (5-7
characters).  These improvements are with the prefix set to 0."
0,"upgrade contrib/ant's tidy.jarcontrib/ant uses a Tidy.jar that also includes classes in org.w3c.dom, org.xml.sax, etc.

This is no problem if you are an ant user, but if you are an IDE user you need to carefully configure the order of your classpath or things will not compile, as these will override the ones in the Solr libs, for example.

The solution is to upgrade the tidy.jar to the newest one that only includes org.w3c.tidy and doesn't cause these problems."
0,It's not possible to register event listeners that filters on mixin supertypesThe current implementation of blocks() in EventFilter does not check if the given EventState has a mixin that is derived from one of the given node types.
0,"misleading contrib/tck-webapp/...RepositoryServlet javadocIn org.apache.jackrabbit.tck.j2ee.RepositoryServlet it says ""...puts the reference into the application context"" but according to the behavior it should say ""...puts the reference into a class variable"". In a j2ee environment the application context refers to the ServletConext instance bounded to the web application.
"
1,"Duplicate attribute in BeanDescriptor and CollectionDescriptor2 different attributes are used in BeanDescriptor and CollectionDescriptor to store the jcr type (jcrType and jcrNodeType). JcrNodeType can be removed. 
This imply modifications in  the DTD, the pm implementation and the different mapping xml files used for the unit tests. 

Furthermore, it should be nice to use the same name (jcrType)  in all descriptors.  There is a lot of confusion across the different descriptors. sometime jcrType is used, sometime jcrNodeType is used. I propose to use only jcrType with the following purpose : 

* In the ClassDescriptor, it  is used to store the primary node type of the class. 
* In the FieldDescriptor, it  is used to store the property type. 
* In the BeanDescriptor  it is used to defined the child node type. 
* In the CollectionDescriptor , it is used to defined the child node type. "
0,separate java code from .jj fileIt would make development easier to move most of the java code out from the .jj file and into a real java file.
0,"terminology: source uses ""protected  property"" for something that only only indirectly has to do with that termDocumentation and method names (DavProperty) use ""protected"" as pseudonym for ""return upon PROPFIND/allprop"". This isn't really accurate, because the live properties defined in RFC2518/4918 *are* protected, but are returned with PROPFIND/allprop nevertheless.

Proposal: update documentation and method names to say something like ""visibleInAllprop"".
"
0,"Architecture Diagrams needed for Lucene, Solr and Nutch"
0,"Minor refactoring to IndexFileNameFilterIndexFileNameFilter looks like it's designed to be a singleton, however its constructor is public and its singleton member is package visible. The proposed patch changes the constructor and member to private. Since it already has a static getFilter() method, and no code in Lucene references those two, I don't think it creates any problems from an API perspective."
1,"DecompressingEntity not calling close on InputStream retrieved by getContentThe method DecompressingEntity.writeTo(OutputStream outstream) does not close the InputStream retrieved by getContent().
According to the documentation of HttpEntity.writeTo:
IMPORTANT: Please note all entity implementations must ensure that
all allocated resources are properly deallocated when this method
returns.

-> imho this is not satisfied in DecompressingEntity.writeTo "
0,Evaluate if membershipcache (JCR-2703) obsoletes the cache in DefaultPrincipalProvider
1,"Deadlock on version operations in a clustered environmentVersion operations in a cluster may end up in a deadlock: a write operation in the version store will acquire the version manager's write lock (N1.VW) and subsequently the cluster journal's write lock (N1.JW). Another cluster node's write operation in some workspace will acquire the journal's write lock (N2.JW) and first process the journal record log: if some of these changes concern the version store, the version manager's read lock (N2.VR) has to be acquired in order to deliver them. If the first cluster node reaches N1.VW, and the second reaches N2.JW, we have a deadlock. The same scenario takes place when the second cluster node synchronizes to the latest journal changes and reaches N2.JR, when the first cluster node is in N1.VW."
0,"Make ""boolean readOnly"" a required arg to IndexReader.openMost apps don't need read/write IndexReader, and, a readOnly
IndexReader has better concurrent performance.

I'd love to simply default readOnly to true, and you'd have to specify
""false"" if you want a read/write reader (I think that's the natural
default), but I think that'd break too many back-compat cases.

So the workaround is to make the parameter explicit, in 2.9.

I think even for IndexSearcher's methods that open an IndexReader
under the hood, we should also make the parameter explicit.
"
0,Test must not fail if mixin cannot be addedNearly all mixin types are optional and a test must not fail if a mixin cannot be added. It should rather throw a NotExecutableException. Some tests still try to add a mixin without checking whether an implementation supports it.
1,"InvalidItemStateException when attempting concurrent, non conflicting writesI'm having some problems doing concurrent addition of nodes to a parent node in Jackrabbit.  I've attached a simple test which starts up a bunch of threads which add nodes to a parent node concurrently. If I add in locks I can get this to work, however according to the mailing list this should work without locks. However, the test always fails with this:

javax.jcr.InvalidItemStateException: Item cannot be saved because it has been modified externally: node /testParent
	at org.apache.jackrabbit.core.ItemImpl.getTransientStates(ItemImpl.java:281)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:939)
	at org.mule.galaxy.impl.JackrabbitConcurrentWriteTest$1.run(JackrabbitConcurrentWriteTest.java:71)

I'm using Jackrabbit 1.6.1. Here is my (verbose) node type:

  <nodeType name=""galaxy:noSiblings"" 
    isMixin=""false"" 
    hasOrderableChildNodes=""false""
    primaryItemName="""">
    <propertyDefinition name=""*"" requiredType=""undefined"" onParentVersion=""COPY"" />
    <propertyDefinition name=""*"" requiredType=""undefined"" onParentVersion=""COPY"" multiple=""true""/>
    <childNodeDefinition name=""*"" defaultPrimaryType=""nt:unstructured"" onParentVersion=""COPY"" sameNameSiblings=""false"" />
    <supertypes>
        <supertype>nt:base</supertype>
        <supertype>mix:referenceable</supertype>
        <supertype>mix:lockable</supertype>
    </supertypes>
  </nodeType>

And my test:    

package org.mule.galaxy.impl;

import java.io.File;
import java.io.IOException;
import java.io.InputStream;
import java.util.ArrayList;
import java.util.List;
import java.util.UUID;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.TimeUnit;

import javax.jcr.LoginException;
import javax.jcr.Node;
import javax.jcr.Repository;
import javax.jcr.RepositoryException;
import javax.jcr.Session;
import javax.jcr.SimpleCredentials;

import junit.framework.TestCase;

import org.apache.commons.io.FileUtils;
import org.apache.jackrabbit.api.JackrabbitNodeTypeManager;
import org.apache.jackrabbit.core.RepositoryImpl;
import org.apache.jackrabbit.core.TransientRepository;
import org.apache.jackrabbit.core.config.RepositoryConfig;

public class JackrabbitConcurrentWriteTest extends TestCase {
    
    private Repository repository;
    private Session session;
    private String parentUUID;
    private boolean continueLoop = true;
    
    public void setUp() throws Exception {
        FileUtils.deleteDirectory(new File(""repository""));
        File repoDir = new File(""repository"");
        repoDir.mkdirs();
        RepositoryConfig config = RepositoryConfig.create(new File(""src/test/resources/META-INF/jackrabbit-repo-test.xml""), repoDir);
        repository = RepositoryImpl.create(config);
        session = createSession();
        
        createCustomNodeTypes(session);
        
        parentUUID = session.getRootNode().addNode(""testParent"", ""galaxy:noSiblings"").getUUID();
        session.save();
        session.logout();
    }

    private Session createSession() throws LoginException, RepositoryException {
        return repository.login(new SimpleCredentials(""username"", ""password"".toCharArray()));
    }
    
    public void testConcurrency() throws Exception {
        final List<Exception> exceptions = new ArrayList<Exception>();
        int threadCount = 20;
        final CountDownLatch latch = new CountDownLatch(threadCount);
        
        for (int i = 0; i < threadCount; i++) {
            Thread thread = new Thread() {

                @Override
                public void run() {
                    try {
                        while (continueLoop) {
                            Session session = createSession();
                            try {
                                Node node = session.getNodeByUUID(parentUUID);
                                node.addNode(UUID.randomUUID().toString());
                                node.save();
                                session.save();
                            } finally {
                                session.logout();
                            }   
                        }
                    } catch (RepositoryException e) {
                        exceptions.add(e);
                        continueLoop = false;
                    }
                    latch.countDown();
                }
                
            };
            thread.start();
        }
        
        latch.await(10, TimeUnit.SECONDS);
        continueLoop = false;
        
        for (Exception e : exceptions) {
            e.printStackTrace();
        }
        assertEquals(0, exceptions.size());
    }
    
    public void createCustomNodeTypes(Session session) throws RepositoryException, IOException {
        // Get the JackrabbitNodeTypeManager from the Workspace.
        // Note that it must be cast from the generic JCR NodeTypeManager to
        // the Jackrabbit-specific implementation.
        // (see: http://jackrabbit.apache.org/node-types.html)
        JackrabbitNodeTypeManager manager = (JackrabbitNodeTypeManager) session.getWorkspace().getNodeTypeManager();

        // Register the custom node types defined in the CND file
        InputStream is = Thread.currentThread().getContextClassLoader()
                .getResourceAsStream(""org/mule/galaxy/impl/jcr/nodeTypes.xml"");
        manager.registerNodeTypes(is, JackrabbitNodeTypeManager.TEXT_XML);
    }
}"
0,"Create ""quick start"" developer bundles for model 1,2,3 deploymentPlease create ""quick start"" developer bundles for deployment models 1,2,3 with all dependance libs and application startup code. Some kind of ""Hello, world"" app on models 1,2,3.
Tomcat and jetty bundles would be nice."
1,"WebDAV/DaveX Servlets susceptible to CSRF AttacksBoth the WebDAV and the remoting (DaveX) servlets are susceptible to CSRF attacks.

"
1,"ALLOW_CIRCULAR_REDIRECTS has no effect if references include query stringALLOW_CIRCULAR_REDIRECTS parameter in HttpClientParameters has no effect if
circular reference contains in URL parameters."
0,Wrap SegmentInfos in public class Wrap SegmentInfos in a public class so that subclasses of MergePolicy do not need to be in the org.apache.lucene.index package.  
0,"LineDocSource should assign stable IDs; docdate field should be NumericFieldSome small enhancements when indexing docs from a line doc source:

  * Assign docid by line number instead of by number-of-docs-indexed;
    this makes the resulting ID stable when using multiple threads

  * The docdate field is now indexed as a String (possible created
    through DateTools).  I added two numeric fields: one that indexes
    .getTime() (= long msec) and another that indexes seconds since
    the day started.  This gives us two numeric fields to play
    with...
"
0,"thread pool implementation of parallel queriesThis component is a replacement for ParallelMultiQuery that runs a thread pool
with queue instead of starting threads for every query execution (so its
performance is better)."
1,"RamUsageEstimator.NUM_BYTES_ARRAY_HEADER and other constants are incorrectRamUsageEstimator.NUM_BYTES_ARRAY_HEADER is computed like that: NUM_BYTES_OBJECT_HEADER + NUM_BYTES_INT + NUM_BYTES_OBJECT_REF. The NUM_BYTES_OBJECT_REF part should not be included, at least not according to this page: http://www.javamex.com/tutorials/memory/array_memory_usage.shtml

{quote}
A single-dimension array is a single object. As expected, the array has the usual object header. However, this object head is 12 bytes to accommodate a four-byte array length. Then comes the actual array data which, as you might expect, consists of the number of elements multiplied by the number of bytes required for one element, depending on its type. The memory usage for one element is 4 bytes for an object reference ...
{quote}

While on it, I wrote a sizeOf(String) impl, and I wonder how do people feel about including such helper methods in RUE, as static, stateless, methods? It's not perfect, there's some room for improvement I'm sure, here it is:

{code}
	/**
	 * Computes the approximate size of a String object. Note that if this object
	 * is also referenced by another object, you should add
	 * {@link RamUsageEstimator#NUM_BYTES_OBJECT_REF} to the result of this
	 * method.
	 */
	public static int sizeOf(String str) {
		return 2 * str.length() + 6 // chars + additional safeness for arrays alignment
				+ 3 * RamUsageEstimator.NUM_BYTES_INT // String maintains 3 integers
				+ RamUsageEstimator.NUM_BYTES_ARRAY_HEADER // char[] array
				+ RamUsageEstimator.NUM_BYTES_OBJECT_HEADER; // String object
	}
{code}

If people are not against it, I'd like to also add sizeOf(int[] / byte[] / long[] / double[] ... and String[])."
1,"Do not increment revison while target workspace is not initializedIt can happen that a workspace is not initialized at all during a syncronize should be performed.
This can happen when a cluster member performs a re-index of a workspace. The changelog should not be ignored it
should be processed after the workspace is initailized."
0,"[PATCH] small fixes to the new scoring.html docThis is an awesome initiative.  We need more docs that cleanly explain the inner workings of Lucene in general... thanks Grant & Steve & others!

I have a few small initial proposed fixes, largely just adding some more description around the components of the formula.  But also a couple typos, another link out to Wikipedia, a missing closing ), etc.  I've only made it through the ""Understanding the Scoring Formula"" section so far."
0,ItemSaveOperation should not swallow stacktraceWhen a StaleItemStateException is thrown the stacktrace is swallowed. This makes it much harder to figure out what went wrong from the logs.
0,"MultiStatus response for PROPPATCH (copied from JCR-175)Rob Owen commented on JCR-175:
--------------------------------------------------

doPropPatch in AbstractWebdavServlet still needs to send back a multistatus (207) response even in the successful case.

I didn't see a way to collect the success/failure status for each property, but instead created a multistatus response and added a propstat (SC_OK) for each of the properties in the setProperties and removeProperties. This allowed a WebDAV client, which expected a multistatus response from PROPPATCH, to work correctly with jcr-server. In the more general case the actual property status code will need to be used ."
1,"NPE in TestNRTThreadsI hit this when while(1)ing this test... I think it's because the logic on when we ask the SegmentReader to load stored fields is off...

{noformat}
*** Thread: Lucene Merge Thread #1 ***
org.apache.lucene.index.MergePolicy$MergeException:
java.lang.NullPointerException
       at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:507)
       at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:472)
Caused by: java.lang.NullPointerException
       at org.apache.lucene.index.SegmentReader$FieldsReaderLocal.initialValue(SegmentReader.java:245)
       at org.apache.lucene.index.SegmentReader$FieldsReaderLocal.initialValue(SegmentReader.java:242)
       at org.apache.lucene.util.CloseableThreadLocal.get(CloseableThreadLocal.java:68)
       at org.apache.lucene.index.SegmentReader.getFieldsReader(SegmentReader.java:749)
       at org.apache.lucene.index.SegmentReader.document(SegmentReader.java:838)
       at org.apache.lucene.index.IndexReader.document(IndexReader.java:951)
       at org.apache.lucene.index.TestNRTThreads$1.warm(TestNRTThreads.java:86)
       at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3311)
       at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2875)
       at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:379)
       at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:447)
{noformat}
"
1,"Lazy Field Loading has edge case bug causing read past EOFWhile trying to run some benchmarking of Lazy filed loading, i discovered there seems to be an edge case when accessing the last field of the last doc of an index.

the problem seems to only happen when the doc has been accessed after at least one other doc.

i have not tried to dig into the code to find the root cause, testcase to follow..."
0,"Automatic staging of the non-Maven release artefactsCurrently the Jackrabbit release process includes the following manual steps in addition to the standard Maven release plugin invocations:

<script>
VERSION=x.y.z  # Release version number

# Prepare the release directory
mkdir target/$VERSION

# Copy the main release artifacts created in the release:perform stage
cp target/checkout/RELEASE-NOTES.txt target/$VERSION
cp target/checkout/target/jackrabbit-$VERSION-src.zip* target/$VERSION
cp target/checkout/jackrabbit-webapp/target/jackrabbit-webapp-$VERSION.war* target/$VERSION
cp target/checkout/jackrabbit-jca/target/jackrabbit-jca-$VERSION.rar* target/$VERSION
cp target/checkout/jackrabbit-standalone/target/jackrabbit-standalone-$VERSION.jar* target/$VERSION

# Add MD5 and SHA1 checksums
for BINARY in target/$VERSION/*.zip target/$VERSION/*ar; do
  openssl md5 < $BINARY > $BINARY.md5
  openssl sha1 < $BINARY > $BINARY.sha
done

# Upload the release directory to people.apache.org
scp -r target/$VERSION people.apache.org:public_html/jackrabbit/$VERSION
</script>

I'd like to automate these steps."
0,extensibility patch for DavResourceImplattached is a very simple patch that allows subclasses of DavResourceImpl to access the Node represented by the dav resource.
1,"BQ provides an explanation on a non-matchPlug in seed -6336594106867842617L into TestExplanations then run TestSimpleExplanationsOfNonMatches and you'll hit this:
{noformat}
    [junit] Testsuite: org.apache.lucene.search.TestSimpleExplanationsOfNonMatches
    [junit] Testcase: testBQ2(org.apache.lucene.search.TestSimpleExplanationsOfNonMatches):	FAILED
    [junit] Explanation of [[+yy +w3]] for #0 doesn't indicate non-match: 0.08778467 = (MATCH) product of:
    [junit]   0.17556934 = (MATCH) sum of:
    [junit]     0.17556934 = (MATCH) weight(field:w3 in 0), product of:
    [junit]       0.5165708 = queryWeight(field:w3), product of:
    [junit]         0.7768564 = idf(docFreq=4, maxDocs=4)
    [junit]         0.6649502 = queryNorm
    [junit]       0.33987468 = (MATCH) fieldWeight(field:w3 in 0), product of:
    [junit]         1.0 = tf(termFreq(field:w3)=1)
    [junit]         0.7768564 = idf(docFreq=4, maxDocs=4)
    [junit]         0.4375 = fieldNorm(field=field, doc=0)
    [junit]   0.5 = coord(1/2)
    [junit]  expected:<0.0> but was:<0.08778467>
    [junit] junit.framework.AssertionFailedError: Explanation of [[+yy +w3]] for #0 doesn't indicate non-match: 0.08778467 = (MATCH) product of:
    [junit]   0.17556934 = (MATCH) sum of:
    [junit]     0.17556934 = (MATCH) weight(field:w3 in 0), product of:
    [junit]       0.5165708 = queryWeight(field:w3), product of:
    [junit]         0.7768564 = idf(docFreq=4, maxDocs=4)
    [junit]         0.6649502 = queryNorm
    [junit]       0.33987468 = (MATCH) fieldWeight(field:w3 in 0), product of:
    [junit]         1.0 = tf(termFreq(field:w3)=1)
    [junit]         0.7768564 = idf(docFreq=4, maxDocs=4)
    [junit]         0.4375 = fieldNorm(field=field, doc=0)
    [junit]   0.5 = coord(1/2)
    [junit]  expected:<0.0> but was:<0.08778467>
    [junit] 	at org.apache.lucene.search.CheckHits.checkNoMatchExplanations(CheckHits.java:60)
    [junit] 	at org.apache.lucene.search.TestSimpleExplanationsOfNonMatches.qtest(TestSimpleExplanationsOfNonMatches.java:36)
    [junit] 	at org.apache.lucene.search.TestExplanations.qtest(TestExplanations.java:101)
    [junit] 	at org.apache.lucene.search.TestSimpleExplanations.testBQ2(TestSimpleExplanations.java:235)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:397)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.run(LuceneTestCase.java:389)
{noformat}

The bug is real -- BQ's explain method fails to properly enforce required clauses when the sub-scorer is null.  Thank you random testing!
"
1,"ArrayHits does not end properly when skipTo doesn't find documentIf skipTo(target) does not find a document that that has a higher value than the target, it falls out of the loop and calls next() possibly returning a previously found document. The patch makes sure that -1 is returned in this case, otherwise confusing results might occur.

Index: src/main/java/org/apache/jackrabbit/core/query/lucene/hits/ArrayHits.java
===================================================================
--- src/main/java/org/apache/jackrabbit/core/query/lucene/hits/ArrayHits.java	(revision 608900)
+++ src/main/java/org/apache/jackrabbit/core/query/lucene/hits/ArrayHits.java	(working copy)
@@ -87,9 +87,9 @@
             int nextDocValue = hits[i];
             if (nextDocValue >= target) {
                 index = i;
-                break;
+                return next();
             }
         }
-        return next();
+        return -1;
     }
 }
"
0,"optimization in sending requestBy doing network sniffering, I noticed that httpclient send the request line 
and headers as separate packages. All other httpclient, including IE, Netscape, 
Java's URLConnection all send it as one package. This can be easily fixed using 
some buffering in HttpMethodBase."
1,"Node.addNode(String, String) doesn't prevent use of mixin types as primary type"
0,"shutdown of MultiThreadedHttpConnectionManager- declare 'shutdown' attributes volatile
- interrupt cleanup thread to avoid polling
- don't use iterator on WeakHashMap, ConcurrentModificationException
  might be triggered by garbage collection

patch follows
"
0,"Remove oal.util.MapBackedSet (Java 6 offsers Collections.newSetFromMap())Easy search and replace job. In 3.x we still need the class, as Java 5 does not have Collections.newSetFromMap()."
0,"lock token validityThere are several minor issues in the mapping between JCR lock tokens and WebDAV lock tokens:

1) WebDAV lock tokens are supposed to use URI syntax (such as opaquelocktoken: or urn:uuid:)

2) The server currently computes lock tokens for session-scoped locks based on the node id; these are not valid JCR lock tokens though and cause exceptions when they are re-added when they appear in a Lock-Token header or an If header. This will likely cause requests to fail that use both types of locks (yes, maybe academic but should be fixed anyway)

Proposal:

a) Map lock tokens to oqaquelocktoken URIs, using a constant UUID plus a postfix encoding the original lock token
b) Use a syntax that allows to distinguish between tokens for open-scoped locks or session-scoped locks, so that we do not try to add the latter type to the Session (alternatively, handle exceptions doing so gracefully)"
0,"Various places do map lookups in loop instead of using entrySet iteratorVarious places loop over a keyset iterator and do a map look up each time thru the loop, I plan to convert these places to use an entryset iterator to avoid this."
0,"Make IndexReader really read-only in Lucene 4.0As we change API completely in Lucene 4.0 we are also free to remove read-write access and commits from IndexReader. This code is so hairy and buggy (as investigated by Robert and Mike today) when you work on SegmentReader level but forget to flush in the DirectoryReader, so its better to really make IndexReaders readonly.

Currently with IndexReader you can do things like:
- delete/undelete Documents -> Can be done by with IndexWriter, too (using deleteByQuery)
- change norms -> this is a bad idea in general, but when we remove norms at all and replace by DocValues this is obsolete already. Changing DocValues should also be done using IndexWriter in trunk (once it is ready)"
1,"The class from cotrub directory org.apache.lucene.index.IndexSplitter creates a non correct indexWhen using the method IndexSplitter.split(File destDir, String[] segs) from the Lucene cotrib directory (contrib/misc/src/java/org/apache/lucene/index) it creates an index with segments descriptor file with wrong data. Namely wrong is the number representing the name of segment that would be created next in this index.
If some of the segments of the index already has this name this results either to impossibility to create new segment or in crating of an corrupted segment."
0,"User definable default headers supportProvide the ability to set default headers to be sent on every request.  Should
be used whenever an object is created or recycled.  Needs to be user
configurable."
0,"httpMethod.abort neededThis is the problem : I use the httpclient to fire many requests. At some point 
of time, the server has queued up requests. So certain requests are waiting for 
response. Now when I call httpMethod.releaseConnection, the request should stop 
waiting for the response and the connection should be closed. However, this 
does not happen. The request is only given up after it has timed out."
0,Jcr-Server: remove jcr depedency from dav-library
0,"Reduce log level in MultiIndex for deleting obsolete indexThe MultiIndex class issues a logging message (with info level) that the obsolete index cannot be deleted (quite often).
As the segments are deleted later (with a retry) and this ""warning"" can be ignored (http://dev.day.com/kb/content/wiki/kb/Crx/Troubleshooting/UnableToDeleteObsoleteIndex.html ), it would be nice to reduce the logging level to debug. People, who are maintaining projects, are not aware of Jackarabbit details and are sometimes scared about this ""warning"" :-)

Thank you in advance!

Kind regards
Sergiy"
0,"Add automatic default configurationWe should provide a simple way to start a Jackrabbit repository with default configuration. The current First Hops document exposes too much configuration details to be really friendly to first-time users.

I'd like to provide a default TransientRepository constructor that looks for ""repository.xml"" as the configuration file and ""repository"" as the repository home directory. If either of these does not exist, it is automatically created using default settings. This way the repository setup would boil down to:

    Repository repository = new TransientRepository();

As an added feature I'm planning to support system properties ""org.apache.jackrabbit.repository.conf"" and ""org.apache.jackrabbit.repository.home"" for overriding the defaults.

This improvement would make it easier to write and set up ""Hello, World!"" -type applications, thus helping interested people to try out Jackrabbit. This feature will also make it easier to provide a standard template for test classes that exhibit some error condition. Like this:

    import javax.jcr.*;
    import org.apache.jackrabbit.core.TransientRepository;
    public Example {
        public static void main(String[] args) {
            try {
                Repository repository = new TransientRepository();
                Session session = repository.login();
                try {
                    // YOUR CODE HERE
                } finally {
                    session.close();
                }
            } catch (Exception e) {
                e.printStackTRace();
            }
        }
    }

I'm targetting this for inclusion in 1.0 as it affects none of the existing code and it will probably be very helpful for the expected number of new users we are going to see after 1.0 is out."
1,"registration of new namespace does not respect existing session mappingsconsider the following (starting with a default namespace registry):

// remap nt namespace
Session.setNamespacePrefix(""foobar"", ""http://www.jcp.org/jcr/nt/1.0"");

// create new namespace
NamespaceRegistry.registerNamespace(""foobar"", ""http://www.foo.org/bar/1.0"");

now the session used above that remapped the nt namespace has an ambigous namespace mapping:
foobar --> ""http://www.jcp.org/jcr/nt/1.0""
""http://www.jcp.org/jcr/nt/1.0"" --> foobar
""http://www.foo.org/bar/1.0"" --> foobar

i.e. the new foobar namespace is hidden for this session. either the registration should not work, or an automatic prefix is to be defined in all local session mappings.

"
0,"Make ObjectIterator implement RangeIterator interfaceCurrently, it's not possible to skip a part of results returned in the form of ObjectIterator (for example, to implement db-like pagination feature with offset/max parameters).

It would be great if ObjectIterator implement RangeIterator interface, and it's trivial enough since underlying NodeIterator implements this interface."
0,"org.apache.jackrabbit.core.query.lucene.SearchIndex with in-memory lucene indexIf I'm not wrong, there is actually no way to configure SearchIndex in order to use a memory only lucene index.

Since you can configure a repository using a org.apache.jackrabbit.core.state.mem.InMemPersistenceManager, it makes sense that also search index offers a similar configuration.
MultiIndex and PersistentIndex now always use a org.apache.lucene.store.FSDirectory, they should be refactored in order to allow a switching to a org.apache.lucene.store.RAMDirectory for this."
0,Rename some remaining tests for new IndexReader class hierarchy
0,"Various small improvements to contrib/benchmarkI've worked out a few small improvements to contrib/benchmark:

  * Refactored the common code in Open/CreateIndexTask that sets the
    configuration for the IndexWriter.  This also fixes a bug in
    OpenIndexTasks that prevented you from disabling flushing by RAM.

  * Added a new config property for LineDocMaker:

      doc.reuse.fields=true|false

    which turns on/off reusing of Field/Document by LineDocMaker.
    This lets us measure performance impact of sharing Field/Document
    vs not, and also turn it off when necessary (eg if you have your
    own consumer that uses private threads).

  * Added merge.scheduler & merge.policy config options.

  * Added param for OptimizeTask, which expects an int and calls
    optimize(maxNumSegments) with that param.

  * Added param for CloseIndex(true|false) -- if you pass false that
    means close the index, aborting any running merges
"
0,"Use separate index for jcr:system treeCurrently each workspace index also includes index data of repository wide data (e.g. version nodes under jcr:system). There are several drawbacks with this approach:

- indexing is duplicated and does not scale when using a lot of workspaces
- workspaces cannot be 'put to sleep' when they are not actively used.

The repository should have an additional index for system data, which includes: versioning and nodetype representation in content. Basically data under /jcr:system.

Queries issued on a workspace will then use two index to execute the query: the workspace index and the system index."
0,"DefaultHttpRequestRetryHandler is not handling PUT as an idempotent method for retries, though RFC2616 section 9.1.2 clearly defines it to be one.See attached patch file for a fix:

Fix treats PUT requests as idempotent, marking them to be retried when their enclosed HttpEntity is either null or repeatable.

"
1,"BooleanScorer2 does not compile with ecjBooleanScorer2, derived from scorer, has two inner classes both derived, ultimately, from Scorer.
As such they all define doc() or inherit it.
ecj produces an error when doc() is called from score in the inner classes in the methods
        countingDisjunctionSumScorer
    and
        countingConjunctionSumScorer

The error message is:
    The method doc is defined in an inherited type and in an enclosing scope.

The affected lines are: 160, 161, 178, and 179


I have run the junit test TestBoolean2 (as well as all the others) with
        doc()
    changed to
        BooleanScorer2.this.doc()
    and also to:
        this.doc();
The result was that the tests passed for both.

I added debug statements to all the doc methods and the score methods in the affected classes, but I could not determine what it should be.
"
0,"Implement TokenStream.end() in contrib TokenStreamsSee LUCENE-1448. Mike's patch there already contains the necessary fixes.

I'll attach a patch here as soon as LUCENE-1460 is committed."
1,3.1 fileformats out of dateThe 3.1 fileformats is missing the change from LUCENE-2811
1,"session.exportDocumentView() does not work with jaxb 2.1.x  UnmarshallerHandlerI tried to update my project from Jackrabbit 1.4 to 1.5 and found following error, that is critical for my app.
Project uses Import/Export features of JCR and JAXB to map XML from JCR to java objects.

exportDocumentView() works with streams when I call it like this:

              Unmarshaller umr = getUnmarshaller();
        ...
                fo = new FileOutputStream(""/tmp/export-node.xml"");
                jcrs.exportDocumentView(path,fo , false, false);
                fi = new FileInputStream(""/tmp/export-node.xml"");
                umr.unmarshal(new InputSource(fi));    

But it does not work when I call it using SAX event handler:

            UnmarshallerHandler ctxh = umr.getUnmarshallerHandler();
             jcrs.exportDocumentView(path, ctxh, false, false);

giving following exception:

java.lang.NullPointerException
        at org.xml.sax.helpers.AttributesImpl.getIndex(AttributesImpl.java:203)
        at com.sun.xml.bind.v2.runtime.unmarshaller.InterningXmlVisitor$AttributesImpl.getIndex(InterningXmlVisitor.java:112)
        at com.sun.xml.bind.v2.runtime.unmarshaller.XsiNilLoader.selectLoader(XsiNilLoader.java:62)
        at com.sun.xml.bind.v2.runtime.unmarshaller.ProxyLoader.startElement(ProxyLoader.java:53)
        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext._startElement(UnmarshallingContext.java:449)
        at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallingContext.startElement(UnmarshallingContext.java:427)
        at com.sun.xml.bind.v2.runtime.unmarshaller.InterningXmlVisitor.startElement(InterningXmlVisitor.java:71)
        at com.sun.xml.bind.v2.runtime.unmarshaller.SAXConnector.startElement(SAXConnector.java:137)
        at org.apache.jackrabbit.commons.xml.Exporter.startElement(Exporter.java:438)
        at org.apache.jackrabbit.commons.xml.DocumentViewExporter.exportNode(DocumentViewExporter.java:76)
        at org.apache.jackrabbit.commons.xml.Exporter.exportNode(Exporter.java:298)
        at org.apache.jackrabbit.commons.xml.Exporter.exportNodes(Exporter.java:214)
        at org.apache.jackrabbit.commons.xml.DocumentViewExporter.exportNode(DocumentViewExporter.java:77)
        at org.apache.jackrabbit.commons.xml.Exporter.exportNode(Exporter.java:298)
        at org.apache.jackrabbit.commons.xml.Exporter.exportNodes(Exporter.java:214)
        at org.apache.jackrabbit.commons.xml.DocumentViewExporter.exportNode(DocumentViewExporter.java:77)
        at org.apache.jackrabbit.commons.xml.Exporter.exportNode(Exporter.java:298)
        at org.apache.jackrabbit.commons.xml.Exporter.export(Exporter.java:144)
        at org.apache.jackrabbit.commons.AbstractSession.export(AbstractSession.java:461)
        at org.apache.jackrabbit.commons.AbstractSession.exportDocumentView(AbstractSession.java:241)
        at ua.org.dg.semaril.helpers.AbstractTypeResolver.getContent(AbstractTypeResolver.java:31

Version 1.4. works fine.

Jukka, please check your changes to  org.apache.jackrabbit.commons.xml.Exporter.
"
1,"EdgeNGramTokenFilter stops on tokens smaller then minimum gram size.If a token is encountered in the stream that is shorter in length than the min gram size, the filter will stop processing the token stream.

Working up a unit test now, but may be a few days before I can provide it. Wanted to get it in the system."
0,"make max size of CachingEntryCollector's cache configurableTo assist in analyzing the bottleneck it would be good if it was easy to change the max size, currently hard-wired to 5000. Suggest to be pragmatic and do that through a system property."
0,"When we move to java 1.5 in 3.0 we should replace all Interger, Long, etc construction with .valueOf-128 to 128 are guaranteed to be cached and using valueOf in that case is 3.5 times faster than using contructor"
0,"Missing derby dependencythe derby dependency is missing in the OCM subproject. So, the unit tests cannot be executed. "
0,"Jcr-Server: Avoid xml parsing if request body is missingOriginally reported by Brian.

"
0,"Add configuration path to SynonymProviderThis is an enhancement to the SynonymProvider, which will be included in the 1.4 release. The current interface only has a getSynonyms() method but does not allow to initialize the SynonymProvider. It should be possible to initialize the provider with a configuration file. The configuration may then include the synonym definitions or a pointer to a location where the synonyms are defined. The configuration will be implementation dependent.

In addition there should be a simple default implementation in jackrabbit-core. The wordnet-synonyms in the sandbox are only of limited use and must be built manually."
0,Highlighter dist jar includes memory binary class filesMark Harwood sent me a note about this issue noticed by a colleague. Previous releases have the memory class files in the Highlighter distribution jar. The Highlighter uses the same contrib dependency method that the xml query parser does - the problem doesn't manifest there because of the alphabetical order of build though. Fix is to not inheritAll when launching the ant task to build memory contrib.
0,"Logging into a repository with a big version history takes a long timeWenn a SessionImpl instance is created, the VersionManager.getVirtualItemStateProvider method is called. This method - amongst other things - loads the complete (!) version history into memory and walks through it to do some mapping.

Besides taking a long time (near 1 minute just to get the version history through PersistentVersionManager.getVersionHistories()) mapping the version histories ultimately results in an ""OutOfMemoryError"".

Currently there are 768 version histories and this is only a very small fraction of the expected final number of version histories in my application"
0,"Per thread DocumentsWriters that write their own private segmentsSee LUCENE-2293 for motivation and more details.

I'm copying here Mike's summary he posted on 2293:

Change the approach for how we buffer in RAM to a more isolated
approach, whereby IW has N fully independent RAM segments
in-process and when a doc needs to be indexed it's added to one of
them. Each segment would also write its own doc stores and
""normal"" segment merging (not the inefficient merge we now do on
flush) would merge them. This should be a good simplification in
the chain (eg maybe we can remove the *PerThread classes). The
segments can flush independently, letting us make much better
concurrent use of IO & CPU."
0,"TestTimeLimitedCollector  shuold not fail if the testing machine happens to be slowThe test fails on Hudson about once a month, like this:

{quote}
   [junit] Testcase: testTimeoutNotGreedy(org.apache.lucene.search.TestTimeLimitedCollector):  FAILED
   [junit] lastDoc=21 ,&& allowed=799 ,&& elapsed=900 >= 886 = ( 2*resolution +  TIME_ALLOWED + SLOW_DOWN = 2*20 + 799 + 47)
   [junit] junit.framework.AssertionFailedError: lastDoc=21 ,&& allowed=799 ,&& elapsed=900 >= 886 = ( 2*resolution +  TIME_ALLOWED + SLOW_DOWN = 2*20 + 799 + 47)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestTimeout(TestTimeLimitedCollector.java:189)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.testTimeoutNotGreedy(TestTimeLimitedCollector.java:150)
   [junit]
   [junit]
   [junit] Test org.apache.lucene.search.TestTimeLimitedCollector FAILED
{quote}

Modify the test to just print a warning in this case - but still verify that there was an early termination.
"
0,"Improve contrib/benchmark for testing near-real-time search performanceIt's not easy to test NRT performance right now w/ contrib/benchmark.
I've made some initial fixes to improve this:

  * Added new '&', that can follow any task within a serial sequence,
    to ""background"" the task (just like a shell).  The test runs in
    the BG, and then at the end of all serial tasks, any still running
    BG tasks are stopped & joined.

  * Added WaitTask that simply waits; useful for controlling how long
    the BG'd tasks get to run.

  * Added RollbackIndex task, which is real handy for using a given
    index for an NRT test, doing a bunch of updates, then reverting it
    all so your next run uses the same starting index

  * Fixed the existing NearRealTimeReaderTask to simply periodically
    open the new reader (previously it was also running a fixed
    search), and removed its own threading (since & can do that
    now). It periodically wakes up, opens the new reader, and swaps it
    into the PerfRunData, at the schedule you specify.  I switched all
    usage of PerfRunData's get/setIndexReader APIs to use ref
    counting.

With these changes you can now make some very simple but powerful
algs, eg:

{code}
OpenIndex
{
  NearRealtimeReader(0.5) &
  # Warm
  Search
  { ""Index1"" AddDoc > : * : 100/sec &
  [ { ""Search"" Search > : * ] : 4 &
  Wait(30.0)
}
CloseReader
RollbackIndex
RepSumByName
{code}

This alg first opens the IndexWriter, then spawns the BG thread to
reopen the NRT reader twice per second, does one warming Search (in
the FG), spans a new thread to index documents at the rate of 100 per
second, then spawns 4 search threads that do as many searches as they
can.  We then wait for 30 seconds, then stop all the threads, revert
the index, and report.

The patch is a work in progress -- it generally works, but there're a
few nocommits, and, we may want to improve reporting (though I think
that's a separate issue).
"
0,"update NOTICE.txtFrom the java-dev discussion, NOTICE.txt should be up-to-date.

One thing I know, is that the persian stopwords file (analyzers/fa) came from the same source as the arabic stopwords file, and is BSD-licensed. 

There might be others (I think ICU has already been added)"
1,"Removal of first version throws javax.jcr.ReferentialIntegrityExceptionA ReferentialIntegrityException occurs when I delete the first version succeeding the root version. Deleting other versions works fine. Here is the stack:

javax.jcr.ReferentialIntegrityException: Unable to remove version. At least once referenced.
        at org.apache.jackrabbit.core.version.InternalVersionHistoryImpl.removeVersion(InternalVersionHistoryImpl.java:379)
        at org.apache.jackrabbit.core.version.InternalVersionManagerBase.internalRemoveVersion(InternalVersionManagerBase.java:684)
        at org.apache.jackrabbit.core.version.InternalVersionManagerImpl$5.run(InternalVersionManagerImpl.java:495)
        at org.apache.jackrabbit.core.version.InternalVersionManagerImpl$DynamicESCFactory.doSourced(InternalVersionManagerImpl.java:760)
        at org.apache.jackrabbit.core.version.InternalVersionManagerImpl.removeVersion(InternalVersionManagerImpl.java:493)
        at org.apache.jackrabbit.core.version.InternalXAVersionManager.removeVersion(InternalXAVersionManager.java:264)
        at org.apache.jackrabbit.core.version.VersionHistoryImpl.removeVersion(VersionHistoryImpl.java:253)

The code is simple:

VersionHistory vh = session.getWorkspace().getVersionManager().getVersionHistory(path);
vh.removeVersion(version); // where version is the first version succeeding the root version

"
0,"HTTP Client doesn't support multipart/related content-typeIt is not possible to sent data easely as a multipart/related content-type (as 
discribed in rfc 2387) using Http Client."
0,"Redesign NodeInfo.getReferences()The method returns an array of PropertyIds. When there are lots of references this may become an problem. As with any other return value that potentially is large we should return an iterator.

I suggest to redesign the handling of references in line with recent discussions how child infos are handled.

- A NodeInfo implementation must either return the complete list of PropertyIds or null if it does not want to return the PropertyIds at that time.
- Introduce a new method: Iterator<PropertyId> RepositoryService.getReferences(SessionInfo, NodeId)

This has the following advantages:

- loading of references can be delayed until it is really needed
- large collections of references can be streamed through the SPI"
0,"Provide access to cluster recordsCluster records are read/written inside o.a.j.core.cluster.ClusterNode in private methods. In order to support tools such as a journal walker that would display human readable descriptions of cluster records, these inner workings should be made public. "
1,"XMLTextExtractor returns an empty reader when encoding is unsupportedXMLTextExtractor is failing to index xml files.  Searching for content in xml files is not coming back with results.

On the extractText(InputStream stream, String type, String encoding) method, the encoding is coming in as an empty string, and it throws an exception at line 62 (reader.parse(source)).

modifying the following statement fixes the problem:
before:  if (encoding != null) {
after:  if (encoding != null && !encoding.equals("""")) {"
1,occasional MergeException while indexingTestStressIndexing2.testMultiConfig occasionally hits merge exceptions
0,"Analyzer for LatvianLess aggressive form of Kreslins' phd thesis: A stemming algorithm for Latvian.
"
1,"Deadlock: IndexWriter.addIndexes(IndexReader[])A deadlock issue occurs under the following circumstances
- IndexWriter.autoCommit == true
- IndexWriter.directory contains multiple segments
- IndexWriter.AddIndex(IndexReader[]) is invoked

I put together a JUnit test that recreates the deadlock, which I've attached.  It is the first test method, 'testAddIndexByIndexReader()'.

In a nutshell, here is what happens:

        // 1) AddIndexes(IndexReader[]) acquires the write lock,
        // then begins optimization of destination index (this is
        // prior to adding any external segments).
        //
        // 2) Main thread starts a ConcurrentMergeScheduler.MergeThread
        // to merge the 2 segments.
        //
        // 3) Merging thread tries to acquire the read lock at
        // IndexWriter.blockAddIndexes(boolean) in
        // IndexWriter.StartCommit(), but cannot as...
        //
        // 4) Main thread still holds the write lock, and is
        // waiting for the IndexWriter.runningMerges data structure
        // to be devoid of merges with their optimize flag
        // set (IndexWriter.optimizeMergesPending()).
"
0,"NTLM class registers Sun JCE implementation by defaultCurrently the NTLM class attempts to load and register the Sun JCE implementation unless a 
System property is set to indicate a different JCE to use.  We should remove this entirely and leave 
the installation and configuration of the JCE to the application rather than trying to do it ourselves 
as this could cause problems with other implementations of JCE.  I'll attach an initial patch for this 
in a moment, with a patch for the documentation in the morning.  (Writing docs at 1am is never a 
good idea.)"
0,FileRevision should have a flag to control whether to sync the file on every write.FileRevision class syncs the underlying revision.log file it uses on every write which could be a performance problem. Add a boolean flag to control whether to sync the file on every write.
0,"Access cluster node idI need to know the cluster node id in my application. I didn't find any other way than to cast to org.apache.jackrabbit.core.RepositoryImpl : ((RepositoryImpl) session.getRepository()).getConfig().getClusterConfig().getId()

I would appreciate it if I could get to this using the system property ClusterNode.SYSTEM_PROPERTY_NODE_ID."
0,"Documentation Error for FilteredTermEnumAs pointed out in 
http://nagoya.apache.org/eyebrowse/ReadMsg?listName=lucene-user@jakarta.apache.org&msgNo=11034
the documentation of FilteredTermEnum.term() is wrong:
it says 
'Returns the current Term in the enumeration. Initially invalid, valid after
next() called for the first time.'
but the implementation of the constructors of the two derived classes
(FuzzyTermEnum and WildcardTermEnum) already initializes the object to point to
the first match. So calling next() before accessing terms will leave out the
first match.

So I suggest to replace the second sentance by something like
'Returns null if no Term matches or all terms have been enumerated.'
(I checked that for WildcardTermEnum only).
Further one might add some note to the docs of the constructors of FuzzyTermEnum
and WildcardTermEnum that they will point to the first element of the
enumeration (if any)."
0,"Refactoring of the Persistence Managerscurrently the persistence managers reside in:
 org.apache.jackrabbit.core.state
 org.apache.jackrabbit.core.state.db
 org.apache.jackrabbit.core.state.mem
 org.apache.jackrabbit.core.state.obj
 org.apache.jackrabbit.core.state.xml
 (org.apache.jackrabbit.core.state.util)

there are also a lot of other classes that deal with states (eg:
SharedItemStateManager) in the state package that do not relate to
pms.

i would like to move all persistencemanagers and pm related stuff to:

 org.apache.jackrabbit.core.persistence

I'd keep the current classes as deprecated subclasses within
jackrabbit-core.jar until Jackrabbit 2.0. There may (?) be people who
are extending the existing classes, so I'd avoid breaking binary
compatibility there even though we've never promised to actually honor
compatiblity within o.a.j.core."
1,"SegmentTermEnum.next() doesn't maintain prevBuffer at endWhen you're iterating a SegmentTermEnum and you go past the end of the docs, you end up with a state where the nextBuffer = null and the prevBuffer is the penultimate term, not the last term.  This patch fixes it.  (It's also required for my Prefetching bug [LUCENE-506])

Index: java/org/apache/lucene/index/SegmentTermEnum.java
===================================================================
--- java/org/apache/lucene/index/SegmentTermEnum.java	(revision 382121)
+++ java/org/apache/lucene/index/SegmentTermEnum.java	(working copy)
@@ -109,6 +109,7 @@
   /** Increments the enumeration to the next element.  True if one exists.*/
   public final boolean next() throws IOException {
     if (position++ >= size - 1) {
+      prevBuffer.set(termBuffer);
       termBuffer.reset();
       return false;
     }
"
0,Add toString() methods to QOM tree classesHaving the QOM tree classes render themselves to SQL2 in their toString() methods would make debugging the QOM code quite a bit easier.
0,"PostgreSQL support in clustering moduleThere is no ddl file for PostgreSQL in clustering module, so I'm attaching here the one we are using in our project. Hope it helps."
0,"ant generate-maven-artifacts target broken for contribWhen executing 'ant generate-maven-artifacts' from a pristine checkout of branch_3x/lucene or trunk/lucene the following error is encountered:

{code}
dist-maven:
     [copy] Copying 1 file to /home/drew/lucene/branch_3x/lucene/build/contrib/analyzers/common
[artifact:install-provider] Installing provider: org.apache.maven.wagon:wagon-ssh:jar:1.0-beta-2:runtime
[artifact:pom] An error has occurred while processing the Maven artifact tasks.
[artifact:pom]  Diagnosis:
[artifact:pom] 
[artifact:pom] Unable to initialize POM pom.xml.template: Cannot find parent: org.apache.lucene:lucene-contrib for project: org.apache.lucene:lucene-analyzers:jar:3.1-SNAPSHOT for project org.apache.lucene:lucene-analyzers:jar:3.1-SNAPSHOT
[artifact:pom] Unable to download the artifact from any repository
{code}


The contrib portion of the ant build is executed in a subant task which does not pick up the pom definitions for lucene-parent and lucene-contrib from the main build.xml, so the lucene-parent and lucene-controb poms must be loaded specifically as a part of the contrib build using the artifact:pom task.
"
0,"Typo in NodeTypeRegistryIt seems a little typo has been introduced in the NodeTypeRegistry, as illustrated in this stack trace : 

Caused by: javax.jcr.RepositoryException: internal error: invalid resource: nodetypes/custom_nodetypes.xml
	at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.<init>(NodeTypeRegistry.java:703) ~[jackrabbit-core-2.2-SNAPSHOT.jar:2.2-SNAPSHOT]
	at org.apache.jackrabbit.core.RepositoryImpl.createNodeTypeRegistry(RepositoryImpl.java:422) ~[jackrabbit-core-2.2-SNAPSHOT.jar:na]
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:294) ~[jackrabbit-core-2.2-SNAPSHOT.jar:na]

This happens when using a DbFileSystem for the root filesystem. This didn't cause a problem in 2.1.1

The patch attached to this ticket correct the issue."
0,"Fix NOTICE files to match consensus from legal teamAs discussed in LEGAL-62 and the related legal-discuss@ threads, the Jackrabbit NOTICE files currently contain information that doesn't really need to be there. A simple ""Copyright (c) ..."" statement is not a required attribution notice."
0,"Improved background text extractionAs recently discussed on the mailing list (see http://markmail.org/message/syt7lc2guzapt7la), the current approach to text extraction in background threads doesn't work that well especially with the Tika-based extractors that support streamed parsing of many document types.

Also, we currently *all* of the extracted text streams are buffered into Strings before being passed into the Lucene index. It would be good if we could somehow get back to passing just Readers to Lucene."
0,"[PATCH] HttpClient#getHost & HttpClient#getPort methods are misleadingHttpClient#getHost & HttpClient#getPort methods are misleading, accompanied by
obsolete, factually wrong javadocs and as such should be deprecated.

Oleg"
1,"ClusterNode not properly shutdown when repository has shutdownSometimes when the repository is shutdown the ClusterNode is not shutdown and it therefore tries to update records or access a closed Journal file.  The setup that generated the exception is I have 3 VMs each with a Repository that are all connected to the same database.  In the below stack trace one of the repositories is being shutdown however the ClusterNode thread is also trying to update the repository journal at the same time.  Below is a copy of the stack trace.

[4/23/08 9:58:52:496 CDT] 00000061 SystemOut     O 89811653 [WebContainer : 2] INFO  org.apache.jackrabbit.core.RepositoryImpl  - Shutting down repository...
[4/23/08 9:58:52:511 CDT] 0000054c SystemOut     O 89811621 [ClusterNode-b06e4fe7-a602-4a93-b106-e0834046ae0f] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7174
[4/23/08 9:58:52:527 CDT] 00000061 SystemOut     O 89811684 [WebContainer : 2] INFO  org.apache.jackrabbit.core.RepositoryImpl  - shutting down workspace 'default'...
[4/23/08 9:58:52:574 CDT] 00000061 SystemOut     O 89811715 [WebContainer : 2] INFO  org.apache.jackrabbit.core.observation.ObservationDispatcher  - Notification of EventListeners stopped.
[4/23/08 9:58:53:058 CDT] 00000061 SystemOut     O 89812215 [WebContainer : 2] INFO  org.apache.jackrabbit.core.RepositoryImpl  - workspace 'default' has been shutdown
[4/23/08 9:58:53:308 CDT] 00000308 SystemOut     O 91641048 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7165
[4/23/08 9:58:53:324 CDT] 00000308 SystemOut     O 91641064 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7166
[4/23/08 9:58:53:324 CDT] 00000308 SystemOut     O 91641064 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7167
[4/23/08 9:58:53:339 CDT] 00000308 SystemOut     O 91641079 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7168
[4/23/08 9:58:53:339 CDT] 00000308 SystemOut     O 91641079 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7169
[4/23/08 9:58:53:355 CDT] 00000308 SystemOut     O 91641095 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7170
[4/23/08 9:58:53:371 CDT] 00000308 SystemOut     O 91641111 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7171
[4/23/08 9:58:53:386 CDT] 00000308 SystemOut     O 91641126 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7172
[4/23/08 9:58:53:417 CDT] 00000308 SystemOut     O 91641157 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7173
[4/23/08 9:58:53:433 CDT] 00000308 SystemOut     O 91641173 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7174
[4/23/08 9:58:53:433 CDT] 00000308 SystemOut     O 91641173 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7175
[4/23/08 9:58:53:496 CDT] 00000308 SystemOut     O 91641236 [ClusterNode-e609e8a6-320e-44ea-be0f-ab8c5cb89662] INFO  org.apache.jackrabbit.core.journal.AbstractJournal  - Synchronized to revision: 7175
[4/23/08 9:58:54:292 CDT] 00000131 SystemOut     O 89171473 [ClusterNode-4930503b-ab33-4444-999e-c87fb3681bf7] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7173
[4/23/08 9:58:54:308 CDT] 00000131 SystemOut     O 89171504 [ClusterNode-4930503b-ab33-4444-999e-c87fb3681bf7] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7174
[4/23/08 9:58:54:308 CDT] 00000131 SystemOut     O 89171504 [ClusterNode-4930503b-ab33-4444-999e-c87fb3681bf7] INFO  org.apache.jackrabbit.core.cluster.ClusterNode  - Processing revision: 7175
[4/23/08 9:58:54:386 CDT] 00000131 SystemOut     O 89171582 [ClusterNode-4930503b-ab33-4444-999e-c87fb3681bf7] INFO  org.apache.jackrabbit.core.journal.AbstractJournal  - Synchronized to revision: 7175
[4/23/08 9:58:55:417 CDT] 00000061 SystemOut     O 89814574 [WebContainer : 2] INFO  org.apache.jackrabbit.core.RepositoryImpl  - Repository has been shutdown
[4/23/08 9:58:56:089 CDT] 0000054c SystemOut     O 89815199 [ClusterNode-b06e4fe7-a602-4a93-b106-e0834046ae0f] ERROR org.apache.jackrabbit.core.cluster.ClusterNode  - Unable to read revision '7174'.
org.apache.jackrabbit.core.journal.JournalException: I/O error while reading string.
	at org.apache.jackrabbit.core.journal.ReadRecord.readString(ReadRecord.java:169)
	at org.apache.jackrabbit.core.cluster.ClusterNode.consume(ClusterNode.java:979)
	at org.apache.jackrabbit.core.journal.AbstractJournal.doSync(AbstractJournal.java:198)
	at org.apache.jackrabbit.core.journal.AbstractJournal.sync(AbstractJournal.java:173)
	at org.apache.jackrabbit.core.cluster.ClusterNode.sync(ClusterNode.java:303)
	at org.apache.jackrabbit.core.cluster.ClusterNode.run(ClusterNode.java:274)
	at java.lang.Thread.run(Thread.java:797)
Caused by: 
java.io.IOException: Closed Connection
	at oracle.jdbc.driver.DatabaseError.SQLToIOException(DatabaseError.java:517)
	at oracle.jdbc.driver.OracleBlobInputStream.needBytes(OracleBlobInputStream.java:187)
	at oracle.jdbc.driver.OracleBufferedStream.readInternal(OracleBufferedStream.java:130)
	at oracle.jdbc.driver.OracleBufferedStream.read(OracleBufferedStream.java:108)
	at java.io.DataInputStream.readBoolean(DataInputStream.java:246)
	at org.apache.jackrabbit.core.journal.ReadRecord.readString(ReadRecord.java:161)
	... 6 more
[4/23/08 9:58:56:261 CDT] 0000054c SystemOut     O 89815355 [ClusterNode-b06e4fe7-a602-4a93-b106-e0834046ae0f] ERROR org.apache.jackrabbit.core.journal.DatabaseJournal  - Error while moving to next record.
java.sql.SQLException: Closed Connection: next
	at oracle.jdbc.driver.DatabaseError.throwSqlException(DatabaseError.java:112)
	at oracle.jdbc.driver.DatabaseError.throwSqlException(DatabaseError.java:146)
	at oracle.jdbc.driver.OracleResultSetImpl.next(OracleResultSetImpl.java:181)
	at org.apache.jackrabbit.core.journal.DatabaseRecordIterator.fetchRecord(DatabaseRecordIterator.java:136)
	at org.apache.jackrabbit.core.journal.DatabaseRecordIterator.hasNext(DatabaseRecordIterator.java:85)
	at org.apache.jackrabbit.core.journal.AbstractJournal.doSync(AbstractJournal.java:190)
	at org.apache.jackrabbit.core.journal.AbstractJournal.sync(AbstractJournal.java:173)
	at org.apache.jackrabbit.core.cluster.ClusterNode.sync(ClusterNode.java:303)
	at org.apache.jackrabbit.core.cluster.ClusterNode.run(ClusterNode.java:274)
	at java.lang.Thread.run(Thread.java:797)
[4/23/08 9:58:56:402 CDT] 0000054c SystemOut     O 89815418 [ClusterNode-b06e4fe7-a602-4a93-b106-e0834046ae0f] WARN  org.apache.jackrabbit.core.cluster.ClusterNode  - Unable to set current revision to 7174.
org.apache.jackrabbit.core.journal.JournalException: Revision file closed.
	at org.apache.jackrabbit.core.journal.FileRevision.set(FileRevision.java:100)
	at org.apache.jackrabbit.core.cluster.ClusterNode.setRevision(ClusterNode.java:1073)
	at org.apache.jackrabbit.core.journal.AbstractJournal.doSync(AbstractJournal.java:211)
	at org.apache.jackrabbit.core.journal.AbstractJournal.sync(AbstractJournal.java:173)
	at org.apache.jackrabbit.core.cluster.ClusterNode.sync(ClusterNode.java:303)
	at org.apache.jackrabbit.core.cluster.ClusterNode.run(ClusterNode.java:274)
	at java.lang.Thread.run(Thread.java:797)

"
0,"Consistency check/fix doesn't work with PSQL persistence managerPSQL doesn't save blobs directly into table row, instead saves a link there and puts the binary stream somewhere else. The general consistency check method in BundleDBPersistenceManager doesn't take this into account.
I've fixed this by changing getBytes(Blob) method in BundleDbPersistenceManager to getBytes(ResultSet) and overriding it for PSQL."
0,"First Steps document is outdatedAs reported by Manoj Prasad on the development mailing list, the code and configuration shown on the First Steps document [1] is no longer up to date with the latest Jackrabbit sources. The differences are:

   * the Versioning element needs to be added to the repository configuration file
   * the output values printed by the examples have changes (log messages, new node types, etc.)
   * multiple values (especially jcr:mixinTypes properties) are not handled correctly

The document should be updated.

[1] http://incubator.apache.org/jackrabbit/firststeps.html"
1,"DirectoryTaxonomyWriter can lose the INDEX_CREATE_TIME property, causing DirTaxoReader.refresh() to falsely succeed (or fail)DirTaxoWriter sets createTime to null after it put it in the commit data once. But that's wrong because if one calls commit(Map<>) twice, the second time doesn't record the creation time. Also, in the ctor, if an index exists and OpenMode is not CREATE, the creation time property is not read.

I wrote a couple of unit tests that assert this, and modified DirTaxoWriter to always record the creation time (in every commit) -- that's the only safe way.

Will upload a patch shortly."
0,"Jackrabbit performance test suiteI'd like to set up a multi-version performance test suite inside jackrabbit-core/src/test/performance, similar to the compatibility test suite we added in JCR-2631. This performance test suite would produce comparable performance numbers for a number of simple benchmark tests across different Jackrabbit versions, including the latest snapshot.

"
0,"Improved log message: include pathThe cluster logs a message for each appended operation. The log message is currently the revision number. A more interesting log message would be the user name, and the path of the change (the most specific path if the change contains multiple nodes)."
0,"cache module should handle out-of-order validations properly and unconditionally refreshThere is a protocol recommendation that when we attempt to revalidate a cache entry, but we receive a response that has a Date header that's actually *older* than that of our current entry, we SHOULD revalidate again unconditionally with either max-age=0 or no-cache (since some upstream cache would appear to be out-of-date).

http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.2.6

"
0,"JSR 283: Identifier based event filteringJSR 283 PFD states:

Only events whose associated parent node has one of the identifiers in the uuid String array will be received. If this parameter is null then no identifier-related restriction is placed on events received. Note that specifying an empty array instead of null results in no nodes being listened to. The uuid is used for backwards compatibility with JCR 1.0.
"
0,"javacc-maven-plugin version in jackrabbit-core pom fileHi, I noticed that the pom.xml file of the jackrabbit-core project needs to specify version ""2.1"" for the javacc-maven-plugin because if it takes the 2.2-SNAPSHOT it won't compile. I put the 2.1 version and it worked fine.

<plugin>
        <groupId>org.codehaus.mojo</groupId>
        <artifactId>javacc-maven-plugin</artifactId>
        <version>2.1</version>
        <executions>


Im working with revision 529712 [April 17, 2007, 15:05 EST]"
1,"When node is created and locked in same transaction, exception is thrownFollowing code fails when executed inside an XA transaction:

Node n = session.getRootNode().addNode(""n"");
n.addMixin(""mix:lockable"");
session.save();
Lock lock = n.lock(false, false);

Stacktrace is

Caused by: javax.transaction.xa.XAException
	at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:155)
	at org.apache.jackrabbit.core.XASessionImpl.commit(XASessionImpl.java:337)
	at org.apache.jackrabbit.jca.TransactionBoundXAResource.commit(TransactionBoundXAResource.java:39)
	at org.apache.geronimo.transaction.manager.WrapperNamedXAResource.commit(WrapperNamedXAResource.java:47)
	at org.apache.geronimo.transaction.manager.TransactionImpl.commit(TransactionImpl.java:301)
	... 32 more
Caused by: org.apache.jackrabbit.core.TransactionException: Unable to update.
	at org.apache.jackrabbit.core.lock.XAEnvironment.prepare(XAEnvironment.java:275)
	at org.apache.jackrabbit.core.lock.XALockManager.prepare(XALockManager.java:245)
	at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:138)
	... 36 more
Caused by: javax.jcr.ItemNotFoundException: failed to build path of 48fb59d8-ac77-4b9f-8b53-9f2492dca5e5: 48fb59d8-ac77-4b9f-8b53-9f2492dca5e5: 48fb59d8-ac77-4b9f-8b53-9f2492dca5e5
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:407)
	at org.apache.jackrabbit.core.CachingHierarchyManager.getPath(CachingHierarchyManager.java:272)
	at org.apache.jackrabbit.core.lock.LockManagerImpl.getPath(LockManagerImpl.java:651)
	at org.apache.jackrabbit.core.lock.LockManagerImpl.internalLock(LockManagerImpl.java:276)
	at org.apache.jackrabbit.core.lock.XAEnvironment$LockInfo.update(XAEnvironment.java:409)
	at org.apache.jackrabbit.core.lock.XAEnvironment.prepare(XAEnvironment.java:273)
	... 38 more
Caused by: org.apache.jackrabbit.core.state.NoSuchItemStateException: 48fb59d8-ac77-4b9f-8b53-9f2492dca5e5
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:189)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getItemState(HierarchyManagerImpl.java:188)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:402)
	... 43 more

"
0,"default behaviour of useExpectHeaderI suggest to set the ExpectContinueMethod.setUseExpectHeader per default to
false or to arrange that per default it is not used. We lost an awfull lot of
time in a project in which we used MultipartPostMethod via a proxy. Everthing
worked fine in dev, however as soon as we started to use the proxy in production
or testing environment we had severe problems. We lost several manday looking
for the problem, including sniffing and logging op the proxy. It ended up to be
the useexpectheader which was true per default. Putting in on false ended our
problems...

In my opinion it is a bit hard to make something default behaviour if the
javadoc warns : <snip>
handshake should be used with caution, as it may cause problems with HTTP
servers and proxies that do not support HTTP/1.1 protocol.
</snip>

regards
dirkp"
0,"[Maven] Migrate checkstyle.properties to XMLNewer Checkstyle versions (used with latest Maven builds) can not use the old
properties configuration file. They work with XML configuration files.
checkstyle.properties must be migrated in order to use a current version of Maven."
0,"Create OSGi Bundles from jackrabbit-webdav and jackrabbit-jcr-server librariesPropose to generate bundles from the jackrabbit-webdav (exporting everything) and jackrabbit-jcr-server (exporting nothing) libraries. In addition a new class is added to the jackrabbit-jcr-server library, which in case of deployment in an OSGi framework will register a Servlet with the OSGi HttpService to expose the JcrRemotingServlet."
1,"[PATCH] ClassDescriptor.hasIdField uses faulty logichasIdField tries to compare a FieldDescriptor to an empty string, which doesn't make sense, here:

     public boolean hasIdField() {
        return (this.idFieldDescriptor != null && ! this.idFieldDescriptor.equals(""""));
     }


i'm assuming it should be

       return (this.idFieldDescriptor != null && this.idFieldDescriptor.isId());

patch does this

"
1,"IndexDeletionPolicy.delete behaves incorrectly when deleting latest generation I have been looking to provide the ability to rollback committed transactions and encountered some issues.
I appreciate IndexDeletionPolicy's main motivation is to handle cleaning away OLD commit points but it does not explicitly state that it can or cannot be used to clean NEW commit points.

If this is not supported then the documentation should ideally state this. If the intention is to support this behaviour then read on .......

There seem to be 2 issues so far:
1) The first attempt to call IndexCommit.delete on the latest commit point fails to remove any contents. The subsequent call succeeds however
2) Deleting the latest commit point fails to update the segments.gen file to point to segments_N-1. New IndexReaders that are opened are then misdirected to open segments_N which has been deleted

Junit test to follow...

"
0,"add Galician analyzerAdds analyzer for Galician, based upon [""Regras do lematizador para o galego""|http://bvg.udc.es/recursos_lingua/stemming.jsp] , and a set of stopwords created in the usual fashion.

This is really just an adaptation of the Portuguese [RSLP|http://www.inf.ufrgs.br/~viviane/rslp/index.htm], so I added that too, and modified our existing hand-coded RSLP-S (RSLP's plural-only step) to just be a plural-only flow of RSLP.
"
0,"RepositoryStatistics should be more flexibleRight now, Jackrabbit reports TimeSeries for things like BUNDLE_READ_COUNTER, BUNDLE_WRITE_COUNTER, etc. but there is no way to extend Jackrabbit and report TimeSeries for additional properties. That's because the type of TimeSeries are defined in RepositoryStatistics class as Type enum. Enums in Java cannot be extended which limits to TimeSeries to the Types defined in RepositoryStatistics. 

I suggest that RepositoryStatistics is improved to allow additional TimeSeries. One approach is to define an additional RepositoryStatistics#getType(String) method. "
1,"SSLSocketFactory.connectSocket(...) possible NPE    public Socket connectSocket(
            final Socket sock,
            final InetSocketAddress remoteAddress,
            final InetSocketAddress localAddress,
            final HttpParams params) throws IOException, UnknownHostException, ConnectTimeoutException {
...

        SSLSocket sslsock = (SSLSocket) (sock != null ? sock : createSocket()); // ==> sock may be null
        if (localAddress != null) {
            sock.setReuseAddress(HttpConnectionParams.getSoReuseaddr(params)); // ==> NPE if sock is null
            sslsock.bind(localAddress);
        }

Should sock.setReuseAddress be sslsock.setReuseAddress?
"
0,Add path encoding to ISO9075The utility class ISO9075 only allows you to encode and decode names. It should also have methods that allow you to pass a path. This is useful when a XPath query is created with a path constraint based on e.g. a Node.getPath().
0,"don't reuse byte[] in IndexInput/Output for read/writeStringIndexInput now holds a private ""byte[] bytes"", which it re-uses for reading strings.  Likewise, IndexOutput holds a UTF8Result (which holds ""byte[] bytes""), re-used for writing strings.

These are both dangerous, since on reading or writing immense strings, we never free this storage.

We don't use read/writeString in very perf sensitive parts of the code, so, I think we should not reuse the byte[] at all.

I think this is likely the cause of the recent ""IndexWriter and memory usage"" thread, started by Ross Woolf on java-user@."
0,"UUIDDocId should check IndexReader using equals()The method UUIDDocId.getDocumentNumber(IndexReader) tests the passed index reader using its object identity.

This is a left over when there was one index per workspace and no system index. When the system index was introduced each query execution will create a new CombinedIndexReader covering the workspace index and the system index. The method should now use the equals() method to test the passed IndexReader."
0,"ASCIIFoldingFilter: expose folding logic + small improvements to ISOLatin1AccentFilterThis patch adds a couple of non-ascii chars to ISOLatin1AccentFilter (namely: left & right single quotation marks, en dash, em dash) which we very frequently encounter in our projects. I know that this class is now deprecated; this improvement is for legacy code that hasn't migrated yet.

It also enables easy access to the ascii folding technique use in ASCIIFoldingFilter for potential re-use in non-Lucene-related code."
0,Remove Searcher from Weight#explainExplain needs to calculate corpus wide stats in a way that is consistent with MultiSearcher.
0,"[PATCH] queryParser.setOperator(int) should be made typesafeThere are AND and DEFAULT_OPERATOR_AND in QueryParser, so calling 
setOperator(QueryParser.AND) looks okay and compiles, but it's not correct. 
I'll attach a patch that uses a typesafe enum to avoid this problem. As 
there's also a getOperator method I had to change the name of the new method 
to get/setDefaultOperator. I don't like that, but it seems to be the only way 
to avoid compile errors for people who switch to a new version of Lucene. 
 
Okay to commit?"
0,Allow for wildcard restriction in resource-based ACEs
0,"add suggester that uses shortest path/wFST instead of bucketsCurrently the FST suggester (really an FSA) quantizes weights into buckets (e.g. single byte) and puts them in front of the word.
This makes it fast, but you lose granularity in your suggestions.

Lately the question was raised, if you build lucene's FST with positiveintoutputs, does it behave the same as a tropical semiring wFST?

In other words, after completing the word, we instead traverse min(output) at each node to find the 'shortest path' to the 
best suggestion (with the highest score).

This means we wouldnt need to quantize weights at all and it might make some operations (e.g. adding fuzzy matching etc) a lot easier."
0,"some jcr-client tests fail if a server runs on localhost:80 This is caused by RepositoryFactoryImplTest.testGetSpi2davRepository() and probably RepositoryFactoryImplTest.testGetSpi2davexRepository(). 

To fix this the tests should set up the required webDav servers for the test runs. Currently the tests just try to connect to localhost:80 expecting the connection to fail in a specific way. "
1,"SegmentMerger should assert .del and .s* files are not passed to createCompoundFileSpinoff from LUCENE-3126. SegmentMerger.createCompoundFile does not document that it should not receive files that are not included in the .cfs, such as .del and .s* (separate norms). Today, that method is called from code which ensures that, but we should:
# Add some documentation to clarify that.
# Add some asserts so that if a test (or other code, running w/ -ea) does that, we catch it.

Will post a patch soon"
0,"Create UAX29URLEmailAnalyzer: a standard analyzer that recognizes URLs and emailsThis Analyzer should contain the same components as StandardAnalyzer, except for the tokenizer, which should be UAX29URLEmailTokenizer instead of StandardTokenizer."
0,"auto close idle connectionsThis has been mentioned several times on the mailing list (most recently here:
http://nagoya.apache.org/eyebrowse/ReadMsg?listName=commons-httpclient-dev@jakarta.apache.org&msgNo=5191
)
It is desirable for the http client to close it's connection after some
configurable idle time. Failing to do so causes the server (and every TCP
resource in between) to keep the socket open and possibly run out of resources
under load.

The HTTP 1.1 RFC has this to say under section 8.1.4:
Servers will usually have some time-out value beyond which they will
   no longer maintain an inactive connection. Proxy servers might make
   this a higher value since it is likely that the client will be making
   more connections through the same server. The use of persistent
   connections places no requirements on the length (or existence) of
   this time-out for either the client or the server.

   When a client or server wishes to time-out it SHOULD issue a graceful
   close on the transport connection. Clients and servers SHOULD both
   constantly watch for the other side of the transport close, and
   respond to it as appropriate. If a client or server does not detect
   the other side's close promptly it could cause unnecessary resource
   drain on the network.

The first sentence of the 2nd paragraph is interesting: how is the client
supposed to do a ""graceful close""? Does it simply mean closing the socket?
One possiblity may be to issue a HTTP/OPTIONS * request with a Connection:close
header."
1,"NativeFSLockFactory.makeLock(...).isLocked() does not workIndexWriter.isLocked() or IndexReader.isLocked() do not work with NativeFSLockFactory.

The problem is, that the method NativeFSLock.isLocked() just checks if the same lock instance was locked before (lock != null). If the LockFactory created a new lock instance, this always returns false, even if its locked."
1,"DateValue.equals() relies on Calendar.equals()JSR170 states regarding Date values:
""The text format of dates must follow the following ISO 8601:2000-compliant format"".

While DateValue.valueOf(String) and DateValue.getString() both rely on the functionality provided by the org.apache.jackrabbit.util.ISO8601, DateValue.equals() compares the equality of the internal Calendar object (DateValue line 89). This may return false even if the Iso-format of both values are equal.

In other words: Creating a new DateValue using the ValueFactory from the String representation of an existing DateValue will return an object, that is not equal to the original DateValue. The reason for this is, that the String does not contain all infomation, that is used during Calendar.equals.

regards
angela

"
1,"FastVectorHighlighter: AIOOBE occurs if one PhraseQuery is contained by another PhraseQueryI'm very sorry but this is another one. If q=""a b c d"" OR ""b c"", then ArrayIndexOutOfBoundsException occurs in FieldQuery.checkOverlap(). I'm working on this and fix with test case soon to be posted.
Thank you for your patient!
"
0,"Implement ""point in time"" searching without relying on filesystem semanticsThis was touched on in recent discussion on dev list:

  http://www.gossamer-threads.com/lists/lucene/java-dev/41700#41700

and then more recently on the user list:

  http://www.gossamer-threads.com/lists/lucene/java-user/42088

Lucene's ""point in time"" searching currently relies on how the
underlying storage handles deletion files that are held open for
reading.

This is highly variable across filesystems.  For example, UNIX-like
filesystems usually do ""close on last delete"", and Windows filesystem
typically refuses to delete a file open for reading (so Lucene retries
later).  But NFS just removes the file out from under the reader, and
for that reason ""point in time"" searching doesn't work on NFS
(see LUCENE-673 ).

With the lockless commits changes (LUCENE-701 ), it's quite simple to
re-implement ""point in time searching"" so as to not rely on filesystem
semantics: we can just keep more than the last segments_N file (as
well as all files they reference).

This is also in keeping with the design goal of ""rely on as little as
possible from the filesystem"".  EG with lockless we no longer re-use
filenames (don't rely on filesystem cache being coherent) and we no
longer use file renaming (because on Windows it can fails).  This
would be another step of not relying on semantics of ""deleting open
files"".  The less we require from filesystem the more portable Lucene
will be!

Where it gets interesting is what ""policy"" we would then use for
removing segments_N files.  The policy now is ""remove all but the last
one"".  I think we would keep this policy as the default.  Then you
could imagine other policies:

  * Keep past N day's worth

  * Keep the last N

  * Keep only those in active use by a reader somewhere (note: tricky
    how to reliably figure this out when readers have crashed, etc.)

  * Keep those ""marked"" as rollback points by some transaction, or
    marked explicitly as a ""snaphshot"".

  * Or, roll your own: the ""policy"" would be an interface or abstract
    class and you could make your own implementation.

I think for this issue we could just create the framework
(interface/abstract class for ""policy"" and invoke it from
IndexFileDeleter) and then implement the current policy (delete all
but most recent segments_N) as the default policy.

In separate issue(s) we could then create the above more interesting
policies.

I think there are some important advantages to doing this:

  * ""Point in time"" searching would work on NFS (it doesn't now
    because NFS doesn't do ""delete on last close""; see LUCENE-673 )
    and any other Directory implementations that don't work
    currently.

  * Transactional semantics become a possibility: you can set a
    snapshot, do a bunch of stuff to your index, and then rollback to
    the snapshot at a later time.

  * If a reader crashes or machine gets rebooted, etc, it could choose
    to re-open the snapshot it had previously been using, whereas now
    the reader must always switch to the last commit point.

  * Searchers could search the same snapshot for follow-on actions.
    Meaning, user does search, then next page, drill down (Solr),
    drill up, etc.  These are each separate trips to the server and if
    searcher has been re-opened, user can get inconsistent results (=
    lost trust).  But with, one series of search interactions could
    explicitly stay on the snapshot it had started with.

"
1,"InstantiatedIndexReader throws NullPointerException in norms() when used with a MultiReader
When using InstantiatedIndexReader under a MultiReader where the other Reader contains documents, a NullPointerException is thrown here;

 public void norms(String field, byte[] bytes, int offset) throws IOException {
    byte[] norms = getIndex().getNormsByFieldNameAndDocumentNumber().get(field);
    System.arraycopy(norms, 0, bytes, offset, norms.length);
  }

the 'norms' variable is null. Performing the copy only when norms is not null does work, though I'm sure it's not the right fix.

java.lang.NullPointerException
	at org.apache.lucene.store.instantiated.InstantiatedIndexReader.norms(InstantiatedIndexReader.java:297)
	at org.apache.lucene.index.MultiReader.norms(MultiReader.java:273)
	at org.apache.lucene.search.TermQuery$TermWeight.scorer(TermQuery.java:70)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:131)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:112)
	at org.apache.lucene.search.Searcher.search(Searcher.java:136)
	at org.apache.lucene.search.Searcher.search(Searcher.java:146)
	at org.apache.lucene.store.instantiated.TestWithMultiReader.test(TestWithMultiReader.java:41)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:164)
	at junit.framework.TestCase.runBare(TestCase.java:130)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:120)
	at junit.framework.TestSuite.runTest(TestSuite.java:230)
	at junit.framework.TestSuite.run(TestSuite.java:225)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)

"
0,"MultiThreadedConnectionManager should provide a shutdownMultiThreadedConnectionManager should provide a shutdown() method to release 
all its resources, it is currently using daemon threads that cannot be stopped 
and HTTP connections that cannot be released.
This is annoying when the pool of connection is created within a web 
application that is undeployed and re-deployed (i.e. the JVM is not restarted) 
consuming resources on local and remote servers."
0,"the spi2dav sandbox project should be put into a common release cycleRemoting JSR170 calls requires obviously both server and client sides.  The server is available for download, as JCR WebDAV Server in both http://www.apache.org/dyn/closer.cgi/jackrabbit/binaries/jackrabbit-jcr-server-1.4.1.jar and http://www.apache.org/dyn/closer.cgi/jackrabbit/binaries/jackrabbit-webapp-1.4.war.  However, the client is just casually mentioned as ""can be found in the Jackrabbit sandbox.""  This issue is to request that the SPI2DAV client code (especially a ResponseFactory that returns a JCR2SPI Repository implementation) be available for download as well.

Furthermore, please make the RepositoryFactory implements javax.naming.spi.ObjectFactory so that only configuration (vs. Java coding) is needed in order to use it.  This is how org.apache.jackrabbit.rmi.client.ClientRepositoryFactory works."
1,Null paths break the compare methodThe compare method cannot handle the path being null
1,"Syns2Index failsRunning Syns2Index fails with a
java.lang.IllegalArgumentException: maxBufferedDocs must at least be 2 when enabled exception.
at org.apache.lucene.index.IndexWriter.setMaxBufferedDocs(IndexWriter.java:883)
at org.apache.lucene.wordnet.Syns2Index.index(Syns2Index.java:249)
at org.apache.lucene.wordnet.Syns2Index.main(Syns2Index.java:208)

The code is here
		// blindly up these parameters for speed
		writer.setMergeFactor( writer.getMergeFactor() * 2);
		writer.setMaxBufferedDocs( writer.getMaxBufferedDocs() * 2);

It looks like getMaxBufferedDocs used to return 10, and now it returns -1, not sure when that started happening.

My suggestion would be to just remove these three lines.  Since speed has already improved vastly, there isn't a need to speed things up.

To run this, Syns2Index requires two args.  The first is the location of the wn_s.pl file, and the second is the directory to create the index in."
0,Make DefaultSecurityManager the default security manager (instead of SimpleSecurityManager)JCR-2164 made DefaultSecurityManager the default security manager for test runs. However the repository.xml included in jackrabbit core and the one for cluster tests still refer to the SimpleSecurityManager. For consistency reasons I think it makes sense to change these places to DefaultSecurityManager.
1,"IndexWriter.addIndexes results in java.lang.OutOfMemoryErrorI'm re-opening a bug I logged previously. My previous bug report has 
disappeared. 

Issue: IndexWriter.addIndexes results in java.lang.OutOfMemoryError for large 
merges.

Until this writing, I've been merging successfully only through repetition, 
i.e. I keep repeating merges until a success. As my index size has grown, my 
success rate has steadily declined. I've reached the point where merges now 
fail 100% of the time. I can't merge.

My tests indicate the threshold is ~30GB on P4/800MB VM with 6 indexes. I have 
repeated my tests on many different machines (not machine dependent). I have 
repeated my test using local and attached storage devices (not storage 
dependent).

For what its worth, I believe the exception occurs entirely during the optimize 
process which is called implicitly after the merge. I say this because each 
time it appears the correct amount of bytes are written to the new index. Is it 
possible to decouple the merge and optimize processes?


The code snippet follows. I can send you the class file and 120GB data set. Let 
me know how you want it.

>>>>> code sample >>>>>

Directory[] sources = new Directory[paths.length];
...

Directory dest = FSDirectory.getDirectory( path, true);
IndexWriter writer = new IndexWriter( dest, new TermAnalyzer( 
StopWords.SEARCH_MAP), true);

writer.addIndexes( sources);
writer.close();"
1,"VirtualNodeTypeStateProvider creates PropertyState with type != value(s).getTypeVirtualNodeTypeStateProvider creates the item states for the in content representation of the node type definitions and in case of jcr:defaultValues hard codes the type of the property state (thus the jcr property) to PropertyType.STRING.

the nt-definition of nt:propertyDefinition however states that jcr:defaultValues doesn't have a required type, thus the type should rather be determined based on the values themselves.

the current behaviour leads situations where

Property.getType != Property.getValues()[0].getType()

which from my point of view is a bug."
0,"TCK: SetPropertyAssumeTypeTest doesn't allow ValueFormatException upon type conversion failureSetPropertyAssumeTypeTest# testValuesConstraintVioloationExceptionBecauseOfInvalidTypeParameter

This test should allow an implementation to throw ValueFormatException.  In Section 7.1.5, the Javadoc for setProperty(String, Value[] int) states: ""If the property type of the supplied Value objects is different from that specified, then a best-effort conversion is attempted. If the conversion fails, a ValueFormatException is thrown.""

Proposal: catch and consume ValueFormatException.

--- SetPropertyAssumeTypeTest.java      (revision 422074)
+++ SetPropertyAssumeTypeTest.java      (working copy)
@@ -28,6 +28,7 @@
 import javax.jcr.PropertyType;
 import javax.jcr.RepositoryException;
 import javax.jcr.Property;
+import javax.jcr.ValueFormatException;
 import java.util.Calendar;
 import java.util.Date;
  
@@ -525,6 +526,9 @@
         catch (ConstraintViolationException e) {
             // success
         }
+        catch (ValueFormatException e) {
+            // success
+        }
     }
"
0,"FastVectorHighlighter - expose FieldFragList.fragInfo for user-customizable FragmentsBuilderNeeded to build a custom highlightable snippet - snippet should start with the sentence containing the first match, then continue for 250 characters.

So created a custom FragmentsBuilder extending SimpleFragmentsBuilder and overriding the createFragments(IndexReader reader, int docId, String fieldName, FieldFragList fieldFragList) method - unit test containing the code is attached to the JIRA.

To get this to work, needed to expose (make public) the FieldFragList.fragInfo member variable. This is currently package private, so only FragmentsBuilder implementations within the lucene-highlighter o.a.l.s.vectorhighlight package (such as SimpleFragmentsBuilder) can access it. Since I am just using the lucene-highlighter.jar as an external dependency to my application, the simplest way to access FieldFragList.fragInfo in my class was to make it public.
"
0,"Create a Codec to work with streaming and append-only filesystemsSince early 2.x times Lucene used a skip/seek/write trick to patch the length of the terms dict into a place near the start of the output data file. This however made it impossible to use Lucene with append-only filesystems such as HDFS.

In the post-flex trunk the following code in StandardTermsDictWriter initiates this:
{code}
    // Count indexed fields up front
    CodecUtil.writeHeader(out, CODEC_NAME, VERSION_CURRENT); 

    out.writeLong(0);                             // leave space for end index pointer
{code}
and completes this in close():
{code}
      out.seek(CodecUtil.headerLength(CODEC_NAME));
      out.writeLong(dirStart);
{code}

I propose to change this layout so that this pointer is stored simply at the end of the file. It's always 8 bytes long, and we known the final length of the file from Directory, so it's a single additional seek(length - 8) to read it, which is not much considering the benefits."
0,"rename expungeDeletesSimilar to optimize(), expungeDeletes() has a misleading name.

We already had problems with this on the user list because TieredMergePolicy
didn't 'expunge' all their deletes.

Also I think expunge is the wrong word, because expunge makes it seem
like you just wrangle up the deletes and kick them out of the party and
that it should be fast.



"
0,"querystring still not set in Url*Method constructorsThe queryString is still not set in various Url*Method's constructors. It does 
get set in setUrl. The simplest fix is to call setUrl in these constructors."
0,"update tests so that both Query.XPATH and Query:SQL are treated as optional featuresIn JCR 2.0, both Query.XPATH and Query.SQL are optional (or, actually, deprecated).

We either need to modify the tests so that they pass on a repository that doesn't support them (-> NotExecutableException), or remove them altogether."
1,"Proxy-Authorization header received on server side
 
 I'm following example
 http://hc.apache.org/httpcomponents-client-ga/examples.html
 Proxy authentication
 
 but it seems that not only proxy is receiving credentials for proxy.
 In log, which is generated at target.host I can see header
 Proxy-Authorization: Basic ....

--------- HEADER
Host:target.host:443
Connection:Keep-Alive
User-Agent:Apache-HttpClient/4.1 (java 1.5)
Proxy-Authorization:Basic Z
--------- POST


Dusan"
1,"DirectoryTaxonomyReader.refresh misbehaves with ref countsDirectoryTaxonomyReader uses the internal IndexReader in order to track its own reference counting. However, when you call refresh(), it reopens the internal IndexReader, and from that point, all previous reference counting gets lost (since the new IndexReader's refCount is 1).

The solution is to track reference counting in DTR itself. I wrote a simple unit test which exposes the bug (will be attached with the patch shortly)."
1,"QueryParser does not correctly handle escaped characters within quoted stringsThe Lucene query parser incorrectly handles escaped characters inside quoted strings; specifically, a quoted string that ends with an (escaped) backslash followed by any additional quoted string will not be properly tokenized. Consider the following example:

bq. {{(name:""///mike\\\\\\"") or (name:""alphonse"")}}

This is not a contrived example -- it derives from an actual bug we've encountered in our system. Running this query will throw an exception, but removing the second clause resolves the problem. After some digging I've found that the problem is with the way quoted strings are processed by the lexer: you'll notice that Mike's name is followed by three escaped backslashes right before the ending quote; looking at the JavaCC code for the query parser highlights the problem:

{code:title=QueryParser.jj|borderStyle=solid}
<DEFAULT> TOKEN : {
  <AND:       (""AND"" | ""&&"") >
| <OR:        (""OR"" | ""||"") >
| <NOT:       (""NOT"" | ""!"") >
| <PLUS:      ""+"" >
| <MINUS:     ""-"" >
| <LPAREN:    ""("" >
| <RPAREN:    "")"" >
| <COLON:     "":"" >
| <STAR:      ""*"" >
| <CARAT:     ""^"" > : Boost
| <QUOTED:     ""\"""" (~[""\""""] | ""\\\"""")* ""\"""">
...
{code}

Take a look at the way the QUOTED token is constructed -- there is no lexical processing of the escaped characters within the quoted string itself. In the above query the lexer matches everything from the first quote through all the backslashes, _treating the end quote as an escaped character_, thus also matching the starting quote of the second term. This causes a lexer error, because the last quote is then considered the start of a new match.

I've come to understand that the Lucene query handler is supposed to be able to handle unsanitized human input; indeed the lexer above would handle a query like {{""blah\""}} without complaining, but that's a ""best-guess"" approach that results in bugs with legal, automatically generated queries. I've attached a patch that fixes the erroneous behavior but does not maintain leniency with malformed queries; I believe this is the correct approach because the two design goals are fundamentally at odds. I'd appreciate any comments."
0,"Allow importing of ACL with unknown principalsIt should be possible to import ACLs with principals that are not known to the principal provider, yet."
0,adding EmptyDocIdSet/IteratorAdding convenience classes for EmptyDocIdSet and EmptyDocIdSetIterator
0,"Ability to group search results by fieldIt would be awesome to group search results by specified field. Some functionality was provided for Apache Solr but I think it should be done in Core Lucene. There could be some useful information like total hits about collapsed data like total count and so on.

Thanks,
Artyom"
0,"LoginModuleConfig should allow to specify principalProvider-name in addition to the classGilles Metz reported this issue regarding login module configuration with Day's CRX based on Jackrabbit, which in
previous versions allowed to specify multiple prinicpal providers of the same class but with different configurations.
With JR 2.0 and 2.1 this is not supported as the pp class name is used as key in the registry and does not allow
to specify a separate key/name.





"
0,"GCJ makefile hardcodes compiler commandssrc/gcj/Makefile hardcodes the command names for gcj, gcjh, and g++. This makes it difficult to 
compile with a particular version of GCJ if multiple are installed with suffixes (eg, gcj-4.0)

Steps to reproduce:
1. Configure, compile, and install GCC/GCJ with something like --program-suffix=-4.0
2. cd ~/src/lucene && ant gcj

Expected results:
Somehow be able to specify my compiler.

Actual results:
Can't find 'gcj' executable, or worse runs wrong version. :)

Suggested fix: as is common with variable names like CC to force a C compiler, allow the builder to 
override the compiler commands used by setting optional environment variables GCJ etc.
Patch to be attached.

Additional info:
Building Lucene from SVN 2005-04-19."
0,"JCR2SPI: Remove validation check for same-named Node and PropertyJSR 170 disallowed a parent node to have a property and a child node with the same name.

This limitation has been removed with JSR 283 and the RI (jackrabbit-core) already removed the check.
I would suggest to change Jcr2Spi accordingly and leave this validation to the underlying SPI impl.

If I'm not mistaken this JSR 170 requirement is asserted in a single place (ItemStateValidator)."
1,"StandardTokenizer splitting all of Korean words into separate charactersStandardTokenizer splits all those Korean words inth separate character tokens. For example, ""?????"" is one Korean word that means ""Hello"", but StandardAnalyzer separates it into five tokens of ""?"", ""?"", ""?"", ""?"", ""?""."
0,"Refactor segmentInfos from IndexReader into its subclassesReferences to segmentInfos in IndexReader cause different kinds of problems
for subclasses of IndexReader, like e. g. MultiReader.

Only subclasses of IndexReader that own the index directory, namely 
SegmentReader and MultiSegmentReader, should have a SegmentInfos object
and be able to access it.

Further information:
http://www.gossamer-threads.com/lists/lucene/java-dev/51808
http://www.gossamer-threads.com/lists/lucene/java-user/52460

A part of the refactoring work was already done in LUCENE-781"
0,Remove Author tags from codeRemove all author tags from the code.
0,"IndexingAggregateTest#testNtFileAggregate fails occasionallyIt may happen that the text extraction from a plain/text resource times out due to the tough extractor time out set on the indexing-test workspace.

The test should check if the indexing queue is empty before it executes a query."
1,"FileNotFoundException thrown by Directory.copy()java.io.FileNotFoundException: segments_bu
        at org.apache.lucene.store.RAMDirectory.openInput(RAMDirectory.java:234)
        at org.apache.lucene.store.Directory.copy(Directory.java:190)

"
0,"Make Lucene - Java 1.9.1 Available in Maven2 repository in iBibilio.orgPlease upload 1.9.1 release to iBiblio so that Maven users can easily use the latest release.  Currently 1.4.3 is the most recently available version: http://www.ibiblio.org/maven2/lucene/lucene/

Please read the following FAQ for more information: http://maven.apache.org/project-faq.html"
0,"Repository does not start if text filter dependencies are missingWhen the search index is configured with a text filter class that requries another jar file and that jar file is missing the repository will not start and log the following misleading error:

Caused by: javax.jcr.RepositoryException
    at org.apache.jackrabbit.core.SearchManager.initializeQueryHandler(SearchManager.java:536)
    at org.apache.jackrabbit.core.SearchManager.<init>(SearchManager.java:278)
    at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getSearchManager(RepositoryImpl.java:1430)
    at org.apache.jackrabbit.core.RepositoryImpl.initWorkspace(RepositoryImpl.java:538)
    at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:245)
    at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:482)
    at org.jbpm.jcr.impl.JackrabbitJcrService.start(JackrabbitJcrService.java:119)
    ... 63 more
Caused by: java.lang.IllegalArgumentException
    at org.apache.commons.collections.BeanMap.put(BeanMap.java:374)
    at org.apache.jackrabbit.core.config.BeanConfig.newInstance(BeanConfig.java:97)
    at org.apache.jackrabbit.core.SearchManager.initializeQueryHandler(SearchManager.java:530)
    ... 69 more "
1,Session logout doesn't release locks acquired using addLockTokenSession.addLockToken doesn't register locks with the session so when logout is called they are not released. Locks acquired this way maintain a reference to the Session after logout and new sessions attempting to acquire the locks will fail.
0,"Try harder to prevent SIGSEGV on cloned MMapIndexInputsWe are unmapping mmapped byte buffers which is disallowed by the JDK, because it has the risk of SIGSEGV when you access the mapped byte buffer after unmapping.

We currently prevent this for the main IndexInput by setting its buffer to null, so we NPE if somebody tries to access the underlying buffer. I recently fixed also the stupid curBuf (LUCENE-3200) by setting to null.

The big problem are cloned IndexInputs which are generally not closed. Those still contain references to the unmapped ByteBuffer, which lead to SIGSEGV easily. The patch from Mike in LUCENE-3439 prevents most of this in Lucene 3.5, but its still not 100% safe (as it uses non-volatiles).

This patch will fix the remaining issues by also setting the buffers of clones to null when the original is closed. The trick is to record weak references of all clones created and close them together with the original. This uses a ConcurrentHashMap<WeakReference<MMapIndexInput>,?> as store with the logic borrowed from WeakHashMap to cleanup the GCed references (using ReferenceQueue).

If we respin 3.5, we should maybe also get this in."
0,"FST should allow controlling how hard builder tries to share suffixesToday we have a boolean option to the FST builder telling it whether
it should share suffixes.

If you turn this off, building is much faster, uses much less RAM, and
the resulting FST is a prefix trie.  But, the FST is larger than it
needs to be.  When it's on, the builder maintains a node hash holding
every node seen so far in the FST -- this uses up RAM and slows things
down.

On a dataset that Elmer (see java-user thread ""Autocompletion on large
index"" on Jul 6 2011) provided (thank you!), which is 1.32 M titles
avg 67.3 chars per title, building with suffix sharing on took 22.5
seconds, required 1.25 GB heap, and produced 91.6 MB FST.  With suffix
sharing off, it was 8.2 seconds, 450 MB heap and 129 MB FST.

I think we should allow this boolean to be shade-of-gray instead:
usually, how well suffixes can share is a function of how far they are
from the end of the string, so, by adding a tunable N to only share
when suffix length < N, we can let caller make reasonable tradeoffs. 
"
0,"RemoveVersionTest.testReferentialIntegrityException assumes availability of ref properties and same name sibilingsThis test case:

- assumes availability of Reference properties (should throw NotExecutable when not available), and

- takes advantage if same name siblings (the child node identified by the nodename2 config variable has already been created by the test setup code)"
0,"Jackrabbit Utilites upgrade to Jackrabbit 2.1.0I'm including a patch for the jcrutil project in the sandbox, for the S3 DataStore to work with 2.1.0, as well as the VFS.  Also using Tika for MIME type resolution.  Please look this over and feel free to improve, this is something I played with but didn't stress test."
1,"NameSet does not implement equals(Object) and hashCode() methodsThe merge context uses the NameSet.equals(NameSet) method to compare two sets; however, the NameSet class does not override the default Object.equals(Object) method, and does not inherit from AbstractSet<E>.  Therefore, the merge check fails, even though the mixin sets are the same.  Object instance equivalence is being performed as opposed to set equivalence.  Behavior is observed when more than one thread is checking the ISM at a given time.  Demonstration code available upon request.

From NodeStateMerger, line 83:
                // mixin types
                if (!state.getMixinTypeNames().equals(overlayedState.getMixinTypeNames())) {
                    // the mixins have been modified but by just looking at the diff we
                    // can't determine where the change happened since the diffs of either
                    // removing a mixin from the overlayed or adding a mixin to the
                    // transient state would look identical...
                    return false;
                }

Proposed solution:
- Implement NameSet.equals(...) method:
	public boolean equals(Object obj) {
		if (obj != null && obj instanceof NameSet) {
			NameSet oo = (NameSet) obj;
			return oo.names.equals(this.names);
		}
		return false;
	}"
1,"InstantiatedIndexReader does not handle #termDocs(null) correct (AllTermDocs)This patch contains core changes so someone else needs to commit it.

Due to the incompatible #termDocs(null) behaviour at least MatchAllDocsQuery, FieldCacheRangeFilter and ValueSourceQuery fails using II since 2.9.

AllTermDocs now has a superclass, AbstractAllTermDocs that also InstantiatedAllTermDocs extend.

Also:

 * II-tests made less plausable to pass on future incompatible changes to TermDocs and TermEnum
 * IITermDocs#skipTo and #next mimics the behaviour of document posisioning from SegmentTermDocs#dito when returning false
 * II now uses BitVector rather than sets for deleted documents
"
1,"Large Lucene index can hit false OOM due to Sun JRE issueThis is not a Lucene issue, but I want to open this so future google
diggers can more easily find it.

There's this nasty bug in Sun's JRE:

  http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6478546

The gist seems to be, if you try to read a large (eg 200 MB) number of
bytes during a single RandomAccessFile.read call, you can incorrectly
hit OOM.  Lucene does this, with norms, since we read in one byte per
doc per field with norms, as a contiguous array of length maxDoc().

The workaround was a custom patch to do large file reads as several
smaller reads.

Background here:

  http://www.nabble.com/problems-with-large-Lucene-index-td22347854.html
"
0,"Remove JakarteRegExCapabilities shim to access package protected fieldTo access the prefix in Jakarta RegExes we use a shim class in the same package as jakarta. I will remove this and replace by reflection like Robert does in his ICUTokenizer rule compiler.

Shim classes have the problem wth signed artifacts, as you cannot insert a new class into a foreign package if you sign regex classes.

This shim-removal also allows users to use later jakarta regex versions, if they are in classpath and cannot be removed (even if they have bugs). Performance is no problem, as the prefix is only get once per TermEnum."
0,"ConjunctionScorer tune-upI just recently ran a load test on the latest code from lucene , which is using a new BooleanScore and noticed the ConjunctionScorer was crunching through objects , especially while sorting as part of the skipTo call. It turns a linked list into an array, sorts the array, then converts the array back to a linked list for further processing by the scoring engines below.

'm not sure if anyone else is experiencing this as I have a very large index (> 4 million items) and I am issuing some heavily nested queries

Anyway, I decide to change the link list into an array and use a first and last marker to ""simulate"" a linked list.

This scaled much better during my load test as the java gargbage collector was less - umm - virulent "
0,"[PATCH] Field.toString could be more helpfulorg.apache.lucene.document.Field.toString defaults to using Object.toString
for some sensible fields. e.g. !isStored && isIndexed && !isTokenized
fields. This makes debugging slightly more difficult than is really needed.

Please find pasted below possible alternative:

 /** Prints a Field for human consumption. */
  public final String toString() {
  	StringBuffer result = new StringBuffer();
  	if (isStored) {
  		if (isIndexed) {
  			if (isTokenized) {
  				result.append(""Text"");
  			} else {
  				result.append(""Keyword"");
  			}
  		} else {
			// XXX warn on tokenized not indexed?
  			result.append(""Unindexed"");
  		}
  	} else {
  		if (isIndexed) {
  			if (isTokenized) {
  				result.append(""Unstored"");
  			} else {
  				result.append(""UnstoredUntokenized"");
  			}
  		} else {
			result.append(""Nonsense_UnstoredUnindexed"");
  		}
  	}
  	
  	result.append('<');
  	result.append(name);
  	result.append(':');
  	if (readerValue != null) {
  		result.append(readerValue.toString());
  	} else {
  		result.append(stringValue);
  	}
  	result.append('>');
  	return result.toString();
  }


NB Im working against CVS HEAD"
1,"DataStore: changing the modified date fails if the file is open for reading (Windows only)If the file is open for reading, Windows doesn't allow to change the last modified time using File.setLastModified():

org.apache.jackrabbit.core.data.DataStoreException: Failed to update record
modified date: 2ac72495fd1e270777821b8a872903c79c84a8d9
        at org.apache.jackrabbit.core.data.FileDataStore.addRecord(FileDataStore.java:250)
        at org.apache.jackrabbit.core.value.BLOBInDataStore.getInstance(BLOBInDataStore.java:119)
        at org.apache.jackrabbit.core.value.InternalValue.getBLOBFileValue(InternalValue.java:619)
        at org.apache.jackrabbit.core.value.InternalValue.create(InternalValue.java:369)
        at org.apache.jackrabbit.core.value.InternalValueFactory.create(InternalValueFactory.java:94)
        at org.apache.jackrabbit.core.value.ValueFactoryImpl.createBinary(ValueFactoryImpl.java:74)

Test case and possible workaround:

import java.io.File;
import java.io.FileInputStream;
import java.io.RandomAccessFile;
public class Test {
    public static void main(String... args) throws Exception {
        String name = ""test.txt"";
        File test = new File(name);
        RandomAccessFile r = new RandomAccessFile(name, ""rw"");
        r.write(0);
        r.close();
        long mod = test.lastModified();
        Thread.sleep(3000);
        FileInputStream in = new FileInputStream(name);
        if (!test.setLastModified(test.lastModified()+1)) {
        	if (!test.canWrite()) {
        		System.out.println(""Can't write to "" + name);
        	} else {
            	System.out.println(""canWrite ok"");
            	r = new RandomAccessFile(name, ""rw"");
            	int old = r.read();
            	r.seek(0);
            	r.write(old);
            	r.close();
        	}
        } else {
        	System.out.println(""setLastModified ok"");
        }
        System.out.println(""Modified old: "" + mod);
        System.out.println(""Modified now: "" + test.lastModified());
        in.close();
        System.out.println(""input closed"");
        if (!test.setLastModified(test.lastModified()+1)) {
        	if (!test.canWrite()) {
        		System.out.println(""Can't write to "" + name);
        	} else {
            	System.out.println(""canWrite ok"");
        	}
        } else {
        	System.out.println(""setLastModified ok"");
        }
        new File(name).delete();
    }
}
"
1,"Adding nodes from concurrently running sessions cause exceptionsExceptions are thrown when trying to add child nodes to one parent node from different sessions running concurrently. One of the following exceptions is always thrown:

 * Exception in thread ""Thread-8"" java.lang.RuntimeException: javax.jcr.nodetype.ConstraintViolationException: /A/7 needs to be saved as well.
 * Exception in thread ""Thread-8"" java.lang.RuntimeException: javax.jcr.RepositoryException: /A: unable to update item.: Unable to
resolve path for item: 016b885a-64aa-45b9-a990-05cbabb4586f/{http://www.jcp.org/jcr/1.0}primaryType: Unable to resolve path for item: 016b885a-64aa-45b9-a990-05cbabb4586f/{http://www.jcp.org/jcr/1.0}primaryType

According to JCR-584 ""Improve handling of concurrent node modifications"" the following scenario ""session 1 adds or removes child node 'x', session 2 adds or removes child node 'y'"" should run gracefully, but the following test constantly fails:

public void testSync() throws Exception
{
       Node rootNode = getSession ().getRootNode ();
       rootNode.addNode (""A"");
       rootNode.save();

       final Session session1 = getRepository().login (new SimpleCredentials (""userName"", ""password"".toCharArray()));
       final Session session2 = getRepository().login (new SimpleCredentials (""userName"", ""password"".toCharArray()));

       Thread thread1 = new Thread (new Runnable()
       {
               public void run()
               {
                       try
                       {
                               addNodes (""A"", session1, 0);
                       }
                       catch (RepositoryException ex)
                       {
                               throw new RuntimeException (ex);
                       }
               }
       });

       Thread thread2 = new Thread (new Runnable()
       {
               public void run()
               {
                       try
                       {
                               addNodes (""A"", session2, 1001);
                       }
                       catch (RepositoryException ex)
                       {
                               throw new RuntimeException (ex);
                       }
               }
       });

       thread1.start();
       thread2.start();

       thread1.join();
       thread2.join();
}

private void addNodes (String parentName, Session session, int startIndex)
       throws RepositoryException
{
       Node parentNode = session.getRootNode().getNode (parentName);
       for (int i = startIndex; i < startIndex + 100; i++)
       {
               String name = Integer.toString (i);
               parentNode.addNode (name);
               parentNode.save();
       }
}

BTW: exceptions were also thrown when I tried to add nodes from one thread and remove some of them from another one. Each thread used it's own session, each node had unique name.

"
1,Exception not caugh in DefaultResponseParserThe method hasProtocolVersion in o.a.h.message.BaseLineParser (httpcore-alpha6) throws an IndexOutOfBoundsException which is not caught by the parseHead method in the o.a.h.impl.conn.DefaultResponseParser.
0,"Make BaseTokenStreamTestCase a bit more evilThrow an exception from the Reader while tokenizing, stop after not consuming all tokens, sometimes spoon-feed chars from the reader..."
1,"Redirect and Kerberos authentication in conflictWe are using the HttpClient to connect to a Website that uses Kerberos-Authentication.

Beware this trigger word: Kerberos! I think this is *not* the problem, but please read on.

Here is the sequence of events:

Client: GET /
Server: Unauthorized.
Client: GET / and includes authentication.
Server: 302 to /something on the same host (this shows that in principle authentication works)
Client: GET /something,  does not include authentication
Server: Unauthorized

Client quits with 401-Unauthorized.

I would have expected one of the following instead:

1) Client immediately sends authorization information with the redirected GET /something
2) Client re-requests the /something with authorization after 401-Unauthorized.

We could get around the problem by setting the ConnectionReuseStrategy to a constant false.

It would be great if someone could tell me if HttpClient works as expected or whether there is a bug or misconfiguration lurking.

Thanks,
Harald.
"
1,"Document with no term vectors mixed with ones that have term vectors cause EOFException during mergeAnother spinoff from here:

  http://www.gossamer-threads.com/lists/lucene/java-dev/53306

Thank you to Andi Vajda for capturing the issue in a compact test!

This is the same logical error from LUCENE-1008, but in this case the
bug is in TermVectorsWriter: we are failing to write the ""0"" field
count to the tvd file when the document has no vectors.  I have a unit
test showing the issue & simple fix.
"
0,"EntryCollectorTest failure on certain Java versionsThe testCache test case in the EntryCollectorTest class uses array comparison for a set of permissions, which causes test failures on certain Java versions where the ordering of permissions is different than expected."
0,changes-to-html: fixes and improvementsThe [Lucene Hudson Changes.html|http://hudson.zones.apache.org/hudson/job/Lucene-trunk/lastSuccessfulBuild/artifact/lucene/build/docs/changes/Changes.html] looks bad because changes2html.pl doesn't properly handle some new usages in CHANGES.txt.
0,"Javadoc: does not mention Expires; clarify validateThe Expires attribute processing is not mentioned in any of the Javadoc, as far
as I can tell. 

Also, the public method parseAttribute() actually handles the attributes, but
does not contain the details. It would be useful if there was at least a
backlink to the parse() documentation.

It's not clear from the Javadoc whether parse() automatically calls validate()
or not. It doesn't."
1,"JNDI data sources with BundleDbPersistenceManager: UnsupportedOperationExceptionWhen using the org.apache.tomcat.dbcp.dbcp.BasicDataSourceFactory, the BundleDbPersistenceManager can not open a database connection via JNDI because the method DataSource.getConnection(user, password) is not supported. Instead, DataSource.getConnection() must be used for this to work.

ConnectionFactory.getConnection should be changed to call this method if user name and password are empty."
1,"BundlePersistenceManager.externalBLOBs can not be configuredIf you try to configure the property externalBLOBs through the workspace.xml it does not work.
The BundlePersistenceManager has not Method setExternalBLOBs(boolean externalBLOBs) so it can not be configured
because its not bean conform. See the DatabasePersistenceManager which has such a Method
"
1,"no-cache directives with field names are transmitted downstream""Field names MUST NOT be included with the no-cache directive in a request.""

http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.4

Currently, the cache implementation allows a request containing something like:
    Cache-Control: no-cache=""Content-Location""
to be passed downstream towards the origin.

This is another one of those tricky situations where our client has passed us a non-compliant request.
"
0,"Multiple PropertyDefs with same name not possiblewhen adding property defs with the same name but different settings, for example one singlevalued, one multivalued, only the last is respected when creating a property.

problem is the 'namedItemDefs' HashMap in the EffectiveNodeType which rather must contain a list of defs rather than the def itself.
"
0,"Remove PropDefId and NodeDefIdthe PropDefIds and NodeDefIds are used to quickly lookup a childnode- or property definition in the nodetype registry (or effective nodetype).
this is heavily used during reading, when calling Property.getDefinition() usually when checking the isMultiple() flag. and of course while writing when getting the definition for the property or childnode. 

however, this poses problems when a nodetype is changed that is still used in the content. if a property definition is changed due to an altered nodetype, subsequent accesses to that property result in a ""invalid propdefid"" warning in the log - but the id is recomputed. this is especially a problem when upgrade jackrabbit from 1.x to 2.0, where some of the builtin nodetypes are defined differently.

i think that it should be feasible to remove the propdefids and nodedefids and compute the definition on demand. i think this can be implemented without performance loss, when some sort of 'signatures' of the items are computed to quickly find the definitions in the effective node type. furthermore, the most common usecase for using the property definition is probably the isMultiple() check - which is now on the Property interface itself - which does not need a definition lookup at all.

and last but not least, it saves 8 bytes per item in the persistence layer."
0,"Build file for Highlighter contrib works when run in isolation, but not when core dist is runBuild.xml for Highlighter does not work when compilation is triggered by clean core dist call.

Patch has changes to fix this by updating build.xml to follow xml-query-parser build.xml"
0,cipher 
1,"Merging of compressed string Fields may hit NPEThis bug was introduced with LUCENE-1219 (only present on 2.4).

The bug happens when merging compressed string fields, but only if bulk-merging code does not apply because the FieldInfos for the segment being merged are not congruent.  This test shows the bug:

{code}
  public void testMergeCompressedFields() throws IOException {
    File indexDir = new File(System.getProperty(""tempDir""), ""mergecompressedfields"");
    Directory dir = FSDirectory.getDirectory(indexDir);
    try {
      for(int i=0;i<5;i++) {
        // Must make a new writer & doc each time, w/
        // different fields, so bulk merge of stored fields
        // cannot run:
        IndexWriter w = new IndexWriter(dir, new WhitespaceAnalyzer(), i==0, IndexWriter.MaxFieldLength.UNLIMITED);
        w.setMergeFactor(5);
        w.setMergeScheduler(new SerialMergeScheduler());
        Document doc = new Document();
        doc.add(new Field(""test1"", ""this is some data that will be compressed this this this"", Field.Store.COMPRESS, Field.Index.NO));
        doc.add(new Field(""test2"", new byte[20], Field.Store.COMPRESS));
        doc.add(new Field(""field"" + i, ""random field"", Field.Store.NO, Field.Index.TOKENIZED));
        w.addDocument(doc);
        w.close();
      }

      byte[] cmp = new byte[20];

      IndexReader r = IndexReader.open(dir);
      for(int i=0;i<5;i++) {
        Document doc = r.document(i);
        assertEquals(""this is some data that will be compressed this this this"", doc.getField(""test1"").stringValue());
        byte[] b = doc.getField(""test2"").binaryValue();
        assertTrue(Arrays.equals(b, cmp));
      }
    } finally {
      dir.close();
      _TestUtil.rmDir(indexDir);
    }
  }
{code}

It's because in FieldsReader, when we load a field ""for merge"" we create a FieldForMerge instance which subsequently does not return the right values for getBinary{Value,Length,Offset}."
0,"ProxyHost/HttpHost: Checks for null when javadoc document null okThe constructor javadocs for ProxyHost and HttpHost all state that null is an allowed value - but there's an check in the HttpHost constructor for this which throws IllegalArgumentException.

(Actually allowing null as documented would also allow for a spring wiring remaining the same when using a proxy or not - steering the values from a propertyfile.)"
0,"Make SessionProvider pluggable in JCRWebdavServerServletAlthough there's a SessionProvider interface in o.a.j.server, the SessionProviderImpl implementation class is hard-coded into JCRWebdavServerServlet."
1,"VersionIteratorImpl problem?I meet with problem in VersionIterator:
Classic nextVersion()/hasNext() loop  for VersionIterator become endless. 

I think problem with peek/pop misprint:

    public Version nextVersion() {
.......
        InternalVersion ret = (InternalVersion) successors.<b>peek</b>();
......
    }

I change to
InternalVersion ret = (InternalVersion) successors.<b>pop</b>();


"
1,"Uncommitted changes or connection leak with Container Managed TransactionsApparently the connector doesn't support CMT (container managed transactions). if the jcr session is closed inside a CMT the AS (application server) throws an exception on commit. And if the jcr session is leaved open, the AS commits the TX successfully but it causes a connection leak by leaving the session open."
0,"TCK: AbstractImportXmlTest incorrectly assumes mix:referenceable can be added to created nodeisMixRefRespected() assumes that if the NodeTypeManager contains mix:referenceable, addMixin can be called to add mix:referenceable to a created node.  This assumption is incorrect for at least two reasons.  First, the created node might already be mix:referenceable, either because its primary node type is a subtype of mix:referenceable or because the implementation added mix:referenceable as a mixin type in creating the node.  Second, a repository may restrict the nodes to which mix:referenceable can be added.  In the extreme case, the repository may not allow mix:referenceable to be added to any node using addMixin, in which case the only referenceable nodes would be those which are mix:referenceable by virtue of primary type or the implementation's adding mix:referenceable as a mixin type at node creation.

Proposal: test canAddMixin before calling addMixin.
"
1,"TestBooleanMinShouldMatch test failureant test -Dtestcase=TestBooleanMinShouldMatch -Dtestmethod=testRandomQueries -Dtests.seed=505d62a62e9f90d0:-60daa428161b404b:-406411290a98f416

I think its an absolute/relative epsilon issue"
0,"Provide more options for OCM CRUD API Writers to enhance the functionalityI am working on an Extension to Object Content Manager and from that angle require a few methods and variable from the base classes to be exposed with protected access.  I have modifier only the getters and these should not cause any issues to the current functionality.  Request a review and addition to the trunk.  

1. added a clone implementation to FilterImpl
2.  Exposed : 
public CollectionConverter getCollectionConverter(Session session, CollectionDescriptor collectionDescriptor) from ObjectConverter"
0,"FirstPassGroupingCollector should use pollLast()Currently FirstPassGroupingCollector uses last and remove method (TreeSet) for replacing a more relevant grouping during grouping.
This can be replaced by pollLast since Lucene trunk is now Java 6. TermFirstPassGroupingCollectorJava6 in Solr can be removed as well."
1,"Node.restore() may throw InvalidItemStateExceptionIt seems that ItemManager cache is not maintained correctly. I'm getting InvalidItemStateException: 'propertyId' has been modified externally tryin restore/checkout versionable nodes in single thread.

ItemState should be evicted from ItemStateManager cache when modified, it seems that status of ItemState is changed to MODIFIED, but itemState remains in the cache."
0,"build.xml's tar task should use longfile=""gnu""The default (used now) is the same, but we get all those nasty false warnings filling the screen."
0,"should allow receiving secure cookies from non-secure chanelCurrently, httpclient will throw an exception if a secure cookie is received 
from a non-secure chanel. Although RFC doesn't specify explicitly on if the 
client should allow receiving secure cookie from non-secure channel, the 
default setting in browser seems to allow it.

Try the following link in IE:

http://www.snapfish.com

The default cookie policy in httpclient should be the same."
0,Dont use nt:frozenNode to create nodesSome test cases may end up trying to create a new node of type nt:frozenNode which fails.
0,"Implement StandardTokenizer with the UAX#29 StandardIt would be really nice for StandardTokenizer to adhere straight to the standard as much as we can with jflex. Then its name would actually make sense.

Such a transition would involve renaming the old StandardTokenizer to EuropeanTokenizer, as its javadoc claims:

bq. This should be a good tokenizer for most European-language documents

The new StandardTokenizer could then say

bq. This should be a good tokenizer for most languages.

All the english/euro-centric stuff like the acronym/company/apostrophe stuff can stay with that EuropeanTokenizer, and it could be used by the european analyzers.
"
0,"WikipediaTokenizer needs a way of not tokenizing certain parts of the textIt would be nice if the WikipediaTokenizer had a way of, via a flag, leaving categories, links, etc. as single tokens (or at least some parts of them)

Thus, if we came across [[Category:My Big Idea]] there would be a way of outputting, as a single token ""My Big Idea"".  

Optionally, it would be good to output both ""My Big Idea"" and the individual tokens as well.

I am not sure of how to do this in JFlex, so any insight would be appreciated."
0,"Small improvements to ArrayUtil.getNextSizeSpinoff from java-dev thread ""Dynamic array reallocation algorithms"" started on Jan 12, 2010.

Here's what I did:

  * Keep the +3 for small sizes

  * Added 2nd arg = number of bytes per element.

  * Round up to 4 or 8 byte boundary (if it's 32 or 64 bit JRE respectively)

  * Still grow by 1/8th

  * If 0 is passed in, return 0 back

I also had to remove some asserts in tests that were checking the actual values returned by this method -- I don't think we should test that (it's an impl. detail)."
1,"Creating a node of type nt:hierarchyNode (or derived) on a JCR 1.0 compliant repository failsWhen creating a node of type nt:hierarchyNode (or derived) on a JCR 1.0 compliant repository, the auto-created property named jcr:created does not get a default value and an exception:

javax.jcr.RepositoryException: createFromDefinition not implemented for: {http://www.jcp.org/jcr/1.0}created

The code in SessionItemStateManager#computeSystemGeneratedPropertyValues handles jcr:created only when found in node type mix:created. In JCR 1.0, however, this property was declared in nt:hierarchyNode. Adding this extra case would allow interoperation with such a repository.



"
1,"Inconsistent state when removing mix:lockable from a locked Nodewhen the lock holder removes mix:lockable from a locked node, the lock related properties get removed.
however, the lock still is live and present on the node.

i would have expected that either

- removing mix:lockable was not allowed or
- the lock was automatically released

test code:

    public void testRemoveMixLockableFromLockedNode() throws RepositoryException {

        Node n = testRootNode.addNode(nodeName1);
        n.addMixin(mixLockable);
        testRootNode.save();

        Lock l = n.lock(true, true);
        n.removeMixin(mixLockable);
        n.save();

        assertFalse(n.isNodeType(mixLockable));                                <===== ok
        assertFalse(l.isLive());                                                                    <===== lock is still live
        assertFalse(n.isLocked());                                                            <=====  node is still locked
        List tokens = Arrays.asList(superuser.getLockTokens());
        assertFalse(tokens.contains(l.getLockToken()));                    <=====  session contains the token

        assertFalse(n.hasProperty(jcrLockOwner));                             <=====  ok. prop got removed.
        assertFalse(n.hasProperty(jcrlockIsDeep));                             <=====  ok. prop got removed.
        n.unlock();                                                                                         <===== LockException (node not lockable)
    }"
0,"paging collectorhttp://issues.apache.org/jira/browse/LUCENE-2127?focusedCommentId=12796898&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12796898

Somebody assign this to Aaron McCurry and we'll see if we can get enough votes on this issue to convince him to upload his patch.  :)"
0,"Updated Snowball packageUpdated Snowball contrib package

 * New org.tartarus.snowball java package with patched SnowballProgram to be abstract to avoid using reflection.
 * Introducing Hungarian, Turkish and Romanian stemmers
 * Introducing constructor SnowballFilter(SnowballProgram)

It is possible there have been some changes made to the some of there stemmer algorithms between this patch and the current SVN trunk of Lucene, an index might thus not be compatible with new stemmers!

The API is backwards compatibile and the test pass."
0,"contrib/bdb-persistence: update berkeleydb versionberkeleydb dependency should be updated to 2.0.83, already available at ibiblio. At this moment project.xml lists 1.7.1, which is very old.
There are no code changes required, and the PM works correctly with berkeleydb 2.0.83

		<dependency>
			<groupId>berkeleydb</groupId>
			<artifactId>je</artifactId>
			<version>2.0.83</version>
			<type>jar</type>
		</dependency>"
0,"Make FSIndexInput and FSIndexOutput inner classes of FSDirectoryI would like make FSIndexInput and FSIndexOutput protected, static, inner classes of FSDirectory. Currently these classes are located in the same source file as FSDirectory, which means that classes outside the store package can not extend them.

I don't see any performance impacts or other side effects of this trivial patch. All unit tests pass."
1,"JCR2SPI: updating events swallowed (CacheBehavior.OBSERVATION)with CacheBehavior.OBSERVATION the hierarchy held within jcr2spi is updated based on events.

if Session-A persistently adds a mix:referenceable to a Node that is already loaded in Session-B, the latter will not be informed about this change.

Reason: upon processing the SPI Event (-> HierarchyEventListener#onEvent) the parent is retrieved by the Event ItemId, which in the former case contains a uniqueID part, which is not known yet to the listening Session-B.
Consequently the NodeEntry affected by the event is not updated.

Possible fix: If looking up the parent entry of the event doesn't succeed, a 2nd lookup using the Event path should be performed."
1,"Session.impersonate non-functionalCurrently SessionImpl.impersonate simply calls Repository.login with the given credentials and the workspace name of the session. If the credentials are incomplete in that the password is missing, the method throws a ""LoginException: Failed to authenticate userID"", which is actually a misleading text, as the reason is not failure to authenticate userID but at best that the session has not enough access rights to impersonate as userID.

For my application, it is crucial, that Session.impersonate is implemented in the sense that this method allows creation of a session with password-less credentials. I accept this method to fail, but it should fail with a correct message."
1,"Avoid premature publication of XAItemStateManagerThe XAItemStateManager constructor calls the super constructor (LocalItemStateManager)  which registers the instance as a listener with the SharedItemStateManager. The construction of the instance has not yet been finished, but it is accessible from the SharedItemStateManager. This can result in strange exceptions like the following:

java.lang.NullPointerException
        at org.apache.jackrabbit.core.state.XAItemStateManager.stateModified(XAItemStateManager.java:580)
        at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyStateModified(StateChangeDispatcher.java:111)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.stateModified(SharedItemStateManager.java:400)
        at org.apache.jackrabbit.core.virtual.AbstractVISProvider.stateModified(AbstractVISProvider.java:445)
        at org.apache.jackrabbit.core.state.ItemState.notifyStateUpdated(ItemState.java:244) 

The NPE is caused by the commitLogs field being null (it has not yet been initialized to its final value)."
1,"Possible HttpClient codepage issue (ascii/ebcdic) on WebSphere z/OSI am working with Cactus 1.4.1 on WebSphere NT and also WebSphere z/OS
(mainframe). The problem seems to be with HTTPClient. I have tried with the
nuild on the 7th December.

I am trying to get basic cactus servlet tests working on WebSphere z/OS.
Everything works fine through WebSphere NT, however, when the same application
is deployed to WebSphere z/OS then we get the following error:

<?xml version=""1.0"" encoding=""UTF-8"" ?><?xml-stylesheet type=""text/xsl""
href=""junit-noframes.xsl""?><testsuites><testsuite name=""TestCactusServlet""
tests=""1"" failures=""0"" errors=""1"" time=""10.184""><testcase name=""testNeal""
time=""10.182""><error message=""Error in parsing the status  line from the
response: unable to find line starting with &quot;HTTP/&quot;""
type=""org.apache.commons.httpclient.HttpRecoverableException"">org.apache.commons.httpclient.HttpRecoverableException:
Error in parsing the status  line from the response: unable to find line
starting with &quot;HTTP/&quot;
	at
org.apache.commons.httpclient.HttpMethodBase.readStatusLine(HttpMethodBase.java:1791)
	at
org.apache.commons.httpclient.HttpMethodBase.readResponse(HttpMethodBase.java:1559)
	at
org.apache.commons.httpclient.HttpMethodBase.processRequest(HttpMethodBase.java:2219)
	... etc ...

I have verified that the Application Server on WebSPhere z/OS is working fine. I
set the logging on the cactus to DEBUG and noticed that the data that the
HTTPClient is retrieving from the connection is scrambled in some way. For example:

16:06:20,213 [WebSphere t=009d7920] DEBUG ent.HttpClientConnectionHelper  -
>getCookieString = [null] 
16:06:20,317 [WebSphere t=009d7920] DEBUG httpclient.wire                 - >> ""@a??? etc...

On WebSphere NT the data at this point looks OK.

What springs to mind is maybe ascii/ebcdic conversion problem. z/OS uses unicode
 for java, as it should. However, the HTTPClient creates it own socket
connection to the app server and therefore it is connecting to non java code. In
such a situation codepage conversion is necessary.

Could anybody adsvise on how to get this to work?

Regards,

Neal Johnston-Ward"
0,"When sorting by field, IndexSearcher should not compute scores by defaultIn 2.9 we've added the ability to turn off scoring (maxScore &
trackScores, separately) when sorting by field.

I expect most apps don't use the scores when sorting by field, and
there's a sizable performance gain when scoring is off, so I think for
2.9 we should not score by default, and add show in CHANGES how to
enable scoring if you rely on it.

If there are no objections, I'll commit that change in a day or two
(it's trivial).
"
0,"IndexModifier has incomplete JavadocsA lot of public and protected members of org.apache.lucene.index.IndexModifier 
don't have javadocs."
1,"empty host header with ip addressfile: HttpMethodBase.java method: addHostRequestHeader

HttpClient writes an empty Host header if the host is referred using IP address.
HTTP 1.1 RFC is not too clear what should be used in this case. However, other
HTTP 1.1 implementations (e.g. Java 1.4.0) uses IP address instead of dns name
in the header.

Furthermore, some HTTP server implementations (e.g. Jetty) will return ""400 bad
request"" if it encounters an empty Host header. That may be a bug in Jetty, but
it might be a good idea to use IP address in Host header to increase compability."
1,"addIndexesNoOptimize should not enforce maxMergeDocs/maxMergeSize limitIf you pass an index that has a segment > maxMergeDocs or maxMergeSize
to addIndexesNoOptimize, it throws an IllegalArgumentException.

But this check isn't reasonable because segment merging can easily
produce segments over these sizes since those limits apply to each
segment being merged, not to the final size of the segment produced.

So if you set maxMergeDocs to X, build up and index, then try to add
that index to another index that also has maxMergeDocs X, you can
easily hit the exception.

I think it's being too pedantic; I plan to just remove the checks for
sizes."
1,"Duplicate hits and missing hits in sorted searchIf using a searcher that subclasses from IndexSearcher I get different result sets (besides the ordering of course). The problem only occurrs if the searcher is wrapped by (Parallel)MultiSearcher and the index is not too small. The number of hits returned by un unsorted and a sorted search are identical but the hits are referencing different documents. A closer look at the result sets revealed that the sorted search returns duplicate hits.

I created test cases for Lucene 1.4.3 as well as for the head release. The problem showed up for both, the number of duplicates beeing bigger for the head realease. The test cases are written for package org.apache.lucene.search. There are messages describing the problem written to the console. In order to see all those hints the asserts are commented out. So dont't be confused if junit reports no errors. (Sorry, beeing a novice user of the bug tracker I don't see any means to attach the test cases on this screen. Let's see.)"
0,"Add deleteByQuery to IndexWriterThis has been discussed several times recently:

  http://markmail.org/message/awlt4lmk3533epbe
  http://www.gossamer-threads.com/lists/lucene/java-user/57384#57384

If we add deleteByQuery to IndexWriter then this is a big step towards
allowing IndexReader to be readonly.

I took the approach suggested in that first thread: I buffer delete
queries just like we now buffer delete terms, holding the max docID
that the delete should apply to.

Then, I also decoupled flushing deletes (mapping term or query -->
actual docIDs that need deleting) from flushing added documents, and
now I flush deletes only when a merge is started, or on commit() or
close().  SegmentMerger now exports the docID map it used when
merging, and I use that to renumber the max docIDs of all pending
deletes.

Finally, I turned off tracking of memory usage of pending deletes
since they now live beyond each flush.  Deletes are now only
explicitly flushed if you set maxBufferedDeleteTerms to something
other than DISABLE_AUTO_FLUSH.  Otherwise they are flushed at the
start of every merge."
0,Privilege content representation should be of property type NAMEthe content representation of jcr privileges should reflect that fact that privilege names changed from simple string to JCR name.
1,"WorkspaceInfo.dispose() does not deregister SharedItemStateManager from virtual item state providersAutomatic disposal of idle workspaces frees unused workspaces but corresponding SharedItemStateManager (and releated PersistenceManager) is still kept in memory referenced by virtual item state providers,  this can lead to memory leaks."
0,"Make it posible not to include TF information in indexTerm Frequency is typically not needed  for all fields, some CPU (reading one VInt less and one X>>>1...) and IO can be spared by making pure boolen fields possible in Lucene. This topic has already been discussed and accepted as a part of Flexible Indexing... This issue tries to push things a bit faster forward as I have some concrete customer demands.

benefits can be expected for fields that are typical candidates for Filters, enumerations, user rights, IDs or very short ""texts"", phone  numbers, zip codes, names...

Status: just passed standard test (compatibility), commited for early review, I have not tried new feature, missing some asserts and one two unit tests

Complexity: simpler than expected

can be used via omitTf() (who used omitNorms() will know where to find it :)  "
1,"Crashes when it gets a redirectI get the following crash when VFS (not my code) calls HttpClient. This code 
worked with some older version of HttpClient (is my belief) but doesn't appear 
to work with CVS HEAD, hence this posting.

Note: I'm sorry, but I don't know which Method it was calling, but hopefully a 
redirect is a redirect and the bug stands irrespective of that.

This is major to me (and Ruper) 'cos it is the first thing it does before 
attempting to read the contents of that location.

regards,

Adam

java.lang.NullPointerException
	at 
org.apache.commons.httpclient.HttpMethodDirector.processRedirectResponse
(HttpMethodDirector.java:454)
	at org.apache.commons.httpclient.HttpMethodDirector.isRetryNeeded
(HttpMethodDirector.java:639)
	at org.apache.commons.httpclient.HttpMethodDirector.executeMethod
(HttpMethodDirector.java:145)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:378)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:268"
0,JSR 283: EventJournalImplement the event journal as specified in JSR 283.
1,"Deadlock with MultiThreadedHttpConnectionManagerI'm getting a dealock with the MultiThreadedHttpConnectionManager. Usually, it
works fine, but when a web page is redirected, it blocks. 

Ludovic.

[ERROR] Redirect to http://sourceforge.net/
Full thread dump Java HotSpot(TM) Client VM (1.4.2_03-b02 mixed mode):

""MultiThreadedHttpConnectionManager cleanup"" daemon prio=5 tid=0x02d566f0
nid=0xe14 in Object.wait() [2e9f000..2e9fd8c]
        at java.lang.Object.wait(Native Method)
        - waiting on <0x10513be8> (a java.lang.ref.ReferenceQueue$Lock)
        at java.lang.ref.ReferenceQueue.remove(Unknown Source)
        - locked <0x10513be8> (a java.lang.ref.ReferenceQueue$Lock)
        at java.lang.ref.ReferenceQueue.remove(Unknown Source)
        at
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$ReferenceQueueThread.run(MultiThreadedHttpConnectionManager.java:805)

""Signal Dispatcher"" daemon prio=10 tid=0x0003da00 nid=0xd44 waiting on condition
[0..0]

""Finalizer"" daemon prio=9 tid=0x009bca30 nid=0xce8 in Object.wait()
[2b5f000..2b5fd8c]
        at java.lang.Object.wait(Native Method)
        - waiting on <0x10504b80> (a java.lang.ref.ReferenceQueue$Lock)
        at java.lang.ref.ReferenceQueue.remove(Unknown Source)
        - locked <0x10504b80> (a java.lang.ref.ReferenceQueue$Lock)
        at java.lang.ref.ReferenceQueue.remove(Unknown Source)
        at java.lang.ref.Finalizer$FinalizerThread.run(Unknown Source)

""Reference Handler"" daemon prio=10 tid=0x009bb600 nid=0xfa4 in Object.wait()
[2b1f000..2b1fd8c]
        at java.lang.Object.wait(Native Method)
        - waiting on <0x10504be8> (a java.lang.ref.Reference$Lock)
        at java.lang.Object.wait(Unknown Source)
        at java.lang.ref.Reference$ReferenceHandler.run(Unknown Source)
        - locked <0x10504be8> (a java.lang.ref.Reference$Lock)

""main"" prio=5 tid=0x00035e28 nid=0xf68 in Object.wait() [7f000..7fc3c]
        at java.lang.Object.wait(Native Method)
        - waiting on <0x105170e8> (a
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$ConnectionPool)
        at
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager.doGetConnection(MultiThreadedHttpConnectionManager.java:388)
        - locked <0x105170e8> (a
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$ConnectionPool)
        at
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager.getConnection(MultiThreadedHttpConnectionManager.java:296)
        at
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:645)
        at
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:529)
        at net.sourceforge.cvsgrab.WebBrowser.executeMethod(WebBrowser.java:201)
        at net.sourceforge.cvsgrab.WebBrowser.getResponse(WebBrowser.java:257)
        at net.sourceforge.cvsgrab.WebBrowser.getDocument(WebBrowser.java:295)
        at
net.sourceforge.cvsgrab.CvsWebInterface.loadDocument(CvsWebInterface.java:111)
        at
net.sourceforge.cvsgrab.CvsWebInterface.getDocumentForDetect(CvsWebInterface.java:216)
        at
net.sourceforge.cvsgrab.CvsWebInterface.findInterface(CvsWebInterface.java:86)
        at net.sourceforge.cvsgrab.CVSGrab.detectWebInterface(CVSGrab.java:688)
        at net.sourceforge.cvsgrab.CVSGrab.grabCVSRepository(CVSGrab.java:616)
        at net.sourceforge.cvsgrab.CVSGrab.run(CVSGrab.java:317)
        at net.sourceforge.cvsgrab.CVSGrab.main(CVSGrab.java:206)

""VM Thread"" prio=5 tid=0x009f76d0 nid=0x550 runnable

""VM Periodic Task Thread"" prio=10 tid=0x009f8208 nid=0x560 waiting on condition
""Suspend Checker Thread"" prio=10 tid=0x009bed88 nid=0xe84 runnable"
0,"Pass ClientConnectionManager to DefaultHttpClient constructorCopied from my mailing list post, Oleg suggested I post it to JIRA for 4.1 fix.

I'm trying to find the least verbose way of configuring a DefaultHttpClient with a ThreadSafeClientConnManager.

The example code given for this goes through a manual process of configuring HttpParams and SchemeRegistry objects, which is more or less copied from the DefaultHttpClient.createHttpParams() and createClientConnectionManager() methods.

It's a bit of a chicken and egg situation - DefaultHttpClient can create its own HttpParams and SchemeRegistry, which are themselves fine, but only once its been constructed, and the constructor requires the ThreadSafeClientConnManager, but that in turn requires the HttpParams and SchemeRegistry objects.  The only way out is to manually construct the HttpParams and SchemeRegistry, which is a waste.

It seems to me that DefaultHttpClient's constructor should take a ClientConnectionManagerFactory instead of a ClientConnectionManager. That way, the createClientConnectionManager() method already has the factory reference, and doesn't have to grub around in the HttpParams object to find it.

The code would then become:

new DefaultHttpClient(new ThreadSafeClientConnManagerFactory(), null);

where ThreadSafeClientConnManagerFactory.newInstance() just constructs ThreadSafeClientConnManager.  There's no manual construction of HttpParams and SchemeRegistry, you just leave it up to DefaultHttpClient.
"
1,"Session.getUserID returns first principal in the set obtained from Subject.getPrincipals()this may lead to a wrong value for the UserID (e.g. the name of a Group principal).

jsr 170 defines the getUserID() to return "" the user ID associated with this Session."" and implies (javadoc) that the method has a relation to the login.

This issues has already been partially addressed while working on jsr 283 access control (trunk)."
0,"Some enhancements to jackrabbit commonsI would like to suggest a couple of  enhancements to the commons module. 

The patch was created against rev. 417443 and the tests did not reveal any 
problems.

Summary of suggestion modifications:

QName
-------------------------------------------------------------------------------------------------------------------------
- reduce QName to its core functionality and put conversion from and to JCR name to
  a separate class 'NameFormat'
- in order not to break existing code, all methods that deal with the conversion in QName
  are marked deprecated.
- add constant for the name of the root node.

Path
-------------------------------------------------------------------------------------------------------------------------
- reduce Path to its core functionality and put conversion from and to JCR path to
  a separate class 'PathFormat'
- in order not to break existing code, all methods that deal with the conversion in Path
  are marked deprecated.
- introduce new constants for UNDEFINED_INDEX (0) and DEFAULT_INDEX (1), that
   are currently hardcoded throughout  the jackrabbit project.
- new method Path.getElement(int) [PathElement]
- make PathElement constants public (used by PathFormat)

Path.PathBuilder
-------------------------------------------------------------------------------------------------------------------------
- additional constructor  PathBuilder(Path)

Path.PathElement
-------------------------------------------------------------------------------------------------------------------------
- add PathElement.getNormalizedIndex() that always asserts a 1-based index.
- change subclasses to be private (no usage within the jackrabbit, except inside Path).

PathMap
-------------------------------------------------------------------------------------------------------------------------
- move o.a.j.core.PathMap  to o.a.j.util.PathMap in order to make it available in the
  commons module.

NamespaceResolver
-------------------------------------------------------------------------------------------------------------------------
- add methods for resolution of paths:
   > getQPath(String jcrPath) [Path]
   > getJCRPath(Path qPath) [String]

NamespaceListener
-------------------------------------------------------------------------------------------------------------------------
- add method 'namespaceRemove(String)'

ValueHelper
-------------------------------------------------------------------------------------------------------------------------
currently  JCR value objects are 'manually' created in the ValueHelper despite the
fact, that JSR170 defines a ValueFactory interface. Consequently the ValueHelper
present in the commons module can only be used by implementations that use
the same value implementations.

- add new helper methods that take a ValueFactory as argument.
- in order not to break existing code the original methods are marked deprecated and
  may be removed at a later time.

consequently:
- modify signature of  InternalValue.create that include a value conversion to take a
  ValueFactory param and adjust all usages inside the core package.

ValueFactoryImpl
-------------------------------------------------------------------------------------------------------------------------
- createValue(String, int ): used to call the conversion on ValueHelper. with the 
   changes suggested to ValueHelper, the code must be changed in order to
   created instances of the Value implementations within the factory.
- together with the modification to ValueHelper, stefan suggested to replace the public 
  constructor with a static 'getInstance' method. All usages within jackrabbit.core, were
   modified accordingly.

Text
-------------------------------------------------------------------------------------------------------------------------
- add getName(String, boolean) where the boolean flag indicates whether  a trailing slash 
   should be ignored.
- add getRelativeParent(String, int, boolean) where the boolean flag indicates whether  a 
  trailing slash should be ignored."
0,"Terms dict should block-encode termsWith PrefixCodedTermsReader/Writer we now encode each term standalone,
ie its bytes, metadata, details for postings (frq/prox file pointers),
etc.

But, this is costly when something wants to visit many terms but pull
metadata for only few (eg respelling, certain MTQs).  This is
particularly costly for sep codec because it has more metadata to
store, per term.

So instead I think we should block-encode all terms between indexed
term, so that the metadata is stored ""column stride"" instead.  This
makes it faster to enum just terms.
"
0,"jcr-server should honor a webdav request's Content-Type and Content-Language headerswhen processing a PUT or a POST, the DavResource should have access to the Content-Type and Content-Language headers presented in the webdav request, if any.

when the client explicitly communicates these headers, their values should take priority over server calculations (such as that done in SetContentTypeCommand), or at least be input into server calculations

furthermore, the dav getcontentlanguage is not at all supported by at least the simple webdav server.
"
0,"DateParser refactoring; Stateful cookie specsPresently DateParser is tightly coupled with the DefaultHttpParams class. I find
this sub-optimal from the design standpoint. Moreover, I believe that date
patterns should be specifiable at the method, host, and client levels, not only
global one. Currently this is not the case, and out of sync with the rest of the
preference framework, which can result in quite a bit of confusion.

When refactoring the DateParser class I also realized that the cookie specs were
shared by all the HttpMethod instances and as such had to be stateless. Even
though it is presently the case, technically there's nothing that prevents the
user from implementing a stateful cookie spec, plugging it into HttpClient, and
by doing so potentially causing quite unpleasant concurrency issues. Therefore,
I believe pluggable cookie specs MAY NOT be shared. There should be a cookie
spec instance created per method invocation 

Oleg"
0,"Remove build.xml from jackrabbit-coreAfter JCR-1203 the build.xml within jackrabbit-core contains only a single Ant task, that could just as well be moved into the pom.xml file to be run inline with the maven antrun plugin."
0,"Reduce Fieldable, AbstractField and Field complexityIn order to move field type like functionality into its own class, we really need to try to tackle the hierarchy of Fieldable, AbstractField and Field.  Currently AbstractField depends on Field, and does not provide much more functionality that storing fields, most of which are being moved over to FieldType.  Therefore it seems ideal to try to deprecate AbstractField (and possible Fieldable), moving much of the functionality into Field and FieldType."
0,"Get rid of NonMatchingScorer from BooleanScorer2Over in LUCENE-1614 Mike has made a comment about removing NonMatchinScorer from BS2, and return null in BooleanWeight.scorer(). I've checked and this can be easily done, so I'm going to post a patch shortly. For reference: https://issues.apache.org/jira/browse/LUCENE-1614?focusedCommentId=12715064&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12715064.

I've marked the issue as 2.9 just because it's small, and kind of related to all the search enhancements done for 2.9."
0,Add a link to the release archiveIt would be nice if the [Releases page|http://lucene.apache.org/java/docs/releases.html] contained a link to the release archive at http://archive.apache.org/dist/lucene/java/.
1,DbDataStore keeps ResultSets openThe DbDataStore does not always close the ResultSet which can lead to memory leaks and/or large memory usage. It seems that  this already has been fixed in trunk and 1.5.
0,"FSImport.java link on wiki is deadThe link for FSImport.java 

http://svn.apache.org/repos/asf/jackrabbit/trunk/contrib/examples/src/java/org/apache/jackrabbit/examples/FSImport.java

from wiki page


http://wiki.apache.org/jackrabbit/ExamplesPage

is dead, could it be updated please?
"
1,"AlreadyClosedException on initial index creationHappens when the indexing queue is checked while creating an initial index. This is probably a regression caused by JCR-2035.

Caused by: org.apache.lucene.store.AlreadyClosedException: this Directory is closed
        at org.apache.lucene.store.Directory.ensureOpen(Directory.java:220)
        at org.apache.lucene.store.FSDirectory.getFile(FSDirectory.java:533)
        at org.apache.jackrabbit.core.query.lucene.directory.FSDirectoryManager$FSDir.list(FSDirectoryManager.java:149)
        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:533)
        at org.apache.lucene.index.DirectoryIndexReader.open(DirectoryIndexReader.java:115)
        at org.apache.lucene.index.IndexReader.open(IndexReader.java:316)
        at org.apache.lucene.index.IndexReader.open(IndexReader.java:263)
        at org.apache.jackrabbit.core.query.lucene.AbstractIndex.getIndexReader(AbstractIndex.java:245)
        at org.apache.jackrabbit.core.query.lucene.AbstractIndex.removeDocument(AbstractIndex.java:225)
        at org.apache.jackrabbit.core.query.lucene.PersistentIndex.removeDocument(PersistentIndex.java:90)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex$DeleteNode.execute(MultiIndex.java:1952)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.executeAndLog(MultiIndex.java:1085)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.checkIndexingQueue(MultiIndex.java:1308)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createIndex(MultiIndex.java:1177)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createIndex(MultiIndex.java:1191)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createIndex(MultiIndex.java:1191)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createIndex(MultiIndex.java:1191)
[...]

I assume there is an index merge happening at the same time that closes index segments."
0,"Unit tests for HttpConnHttpConn needs more test coverage.
Starting at 0%.
"
1,"Exception in deleteDocument, undeleteAll or setNorm in IndexReader can fail to release write lock on closeI hit this while working on LUCENE-140

We have 3 cases in the IndexReader methods above where we have this pattern:

  if (directoryOwner) acquireWriteLock();
  doSomething();
  hasChanges = true;

The problem is if you hit an exception in doSomething(), and hasChanges was not already true, then hasChanges will not have been set to true yet the write lock is held.  If you then try to close the reader without making any other changes, then the write lock is not released because in IndexReader.close() (well, in commit()) we only release write lock if hasChanges is true.

I think the simple fix is to swap the order of hasChanges = true and doSomething().  I already fixed one case of this under LUCENE-140 commit yesterday; I will fix the other two under this issue."
0,"Compressed fields should be ""externalized"" (from Fields into Document)Right now, as of 2.0 release, Lucene supports compressed stored fields.  However, after discussion on java-dev, the suggestion arose, from Robert Engels, that it would be better if this logic were moved into the Document level.  This way the indexing level just stores opaque binary fields, and then Document handles compress/uncompressing as needed.

This approach would have prevented issues like LUCENE-629 because merging of segments would never need to decompress.

See this thread for the recent discussion:

    http://www.gossamer-threads.com/lists/lucene/java-dev/38836

When we do this we should also work on related issue LUCENE-648."
0,"Single-pass grouping collector based on doc blocksLUCENE-3112 enables adding/updating a contiguous block of documents to
the index, guaranteed (yet, experimental!) to retain adjacent docID
assignment through the full life of the index as long the app doesn't
delete individual docs from the block.

When an app does this, it can enable neat features like LUCENE-2454
(nested documents), post-group facet counting (LUCENE-3097).

It also makes single-pass grouping possible, when you group by
the ""identifier"" field shared by the doc block, since we know we will
see a given group only once with all of its docs within one block.

This should be faster than the fully general two-pass collectors we
already have.

I'm working on a patch but not quite there yet...
"
1,"AbstractLoginModule must not call abort() in commit()AbstractLoginModule.commit() currently may call abort() when it detects that the login did not succeed. abort() will reset any state in the login module, including state shared between multiple login modules like Principals in the Subject. When there actually are multiple module, this will delete shared state that was set by other login modules. Moreover, the method commit() is only called when the overall authentication succeeded. Thus, it seems strange to call abort() from within commit().
"
0,"porting of ProxyClient from 3.1 to 4.x APIWith 3.1 version of HttpClient it was possible to establish a tunneled connection to a generic non http server through an authenticated proxy, but since 3.1 version does not support NTLMv2 and Kerberos authentication, that are supported in 4.x version, it is very useful to port the features provided by ProxyClient to 4.x API."
0,"Connection not closed after ""Connection: close"" requestIn HTTP specification at http://www.w3.org/Protocols/rfc2616/rfc2616-
sec8.html , under chapter ""Negotiation"", it is stated :
""If either the client or the server sends the close token in the Connection 
header, that request becomes the last one for the connection.""

HttpClient (v2.0.2 and v3.0 alpha2) is currently closing connection only if 
server has sent ""Connection: close"" header, and not when request contains it."
0,"Speed up SegementDocsEnum by making it more friendly for JIT optimizationsSince we moved the bulk reading into the codec ie. make all  bulk reading codec private in LUCENE-3584 we have seen some performance [regression|http://people.apache.org/~mikemccand/lucenebench/Term.html] on different CPUs. I tried to optimize the implementation to make it more eligible for runtime optimizations, tried to make loops JIT friendly by moving out branches where I can, minimize member access in all loops, use final members where possible and specialize the two common cases With & Without LiveDocs.

I will attache a patch and my benchmark results in a minute."
0,"Error from maven when generating the task listThis message scrolls by when runing the site:generate goal.

tasklist:generate:
Non-fatal error while parsing file:
/home/jsdever/cvs-commit/jakarta-commons/httpclient/src/java/org/apache/commons/httpclient/HeaderElement.java
Non-fatal error while parsing file:
/home/jsdever/cvs-commit/jakarta-commons/httpclient/src/java/org/apache/commons/httpclient/URI.java"
1,"Item.remove fails if a child-item is not visible to the editing sessionthe following test setup fails:

- a given session is allowed to remove a node
- the node has a policy child node which is not visible to the editing session (missing ac-read permission)
  OR the node has another invisible child item which could - based on the permissions above - be removed by that session.

calling Node.remove however fails with accessdeniedexception because the internal remove
mechanism accesses all child items to mark them removed. however, the access is executed
using the regular itemmgr calls that are used to retrieve the items using the JCR API which
results in accessdenied exception as those child items are not visible to the session.
since the items can be removed i would argue that this is a bug in the internal remove process.
"
0,"[PATCH] Remove Stutter in ItemValidatorItemValidator duplicates code for no reason. Remove the duplication

        if (permissions > Permission.NONE) {
            Path path = item.getPrimaryPath();
            if (!accessMgr.isGranted(item.getPrimaryPath(), permissions)) {
                return false;
            }

to

        if (permissions > Permission.NONE) {
            Path path = item.getPrimaryPath();
            if (!accessMgr.isGranted(path, permissions)) {
                return false;
            }

"
1,"Response Folded Headers throws HttpExceptionAs of 4/4/02 CVS repository the HttpMethodBase class
doesn't handle folded headers in the 
readResponseHeaders method

HTTP/1.1 and HTTP/1.0 descriptions of folded headers (see 
section 2.2 Basic Rules)
http://www.ietf.org/rfc/rfc2616.txt
http://www-
old.ics.uci.edu/pub/ietf/http/rfc1945.html#Basic-Rules

I've prepared a patch and was 
emailed to jakarta-commons@jakarta.apache.org"
1,"InstantiatedIndexReader.norms called from MultiReader bugSmall bug in InstantiatedIndexReader.norms(String field, byte[] bytes, int offset) where the offset is not applied properly in the System.arraycopy"
0,"System Reqs page should be release specificThe System Requirements page, currently under the Main->Resources section of the website should be part of a given version's documentation, since it will be changing for a given release.  

I will ""deprecate"" the existing one, but leave it in place(with a message) to cover the existing releases that don't have this, but will also add it to the release docs for future releases."
1,"setFetchSize() fails in getAllNodeIds()I get the following exception from the PersistenceManagerIteratorTest on Windows:

org.apache.jackrabbit.core.state.ItemStateException: getAllNodeIds failed.
        at org.apache.jackrabbit.core.persistence.pool.BundleDbPersistenceManager.getAllNodeIds(BundleDbPersistenceManager.java:1043)
        at org.apache.jackrabbit.core.data.PersistenceManagerIteratorTest.testGetAllNodeIds(PersistenceManagerIteratorTest.java:106)
Caused by: java.sql.SQLException: Invalid parameter value '10'000' for Statement.setFetchSize(int rows).
        at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.EmbedConnection.newSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.ConnectionChild.newSQLException(Unknown Source)
        at org.apache.derby.impl.jdbc.EmbedStatement.setFetchSize(Unknown Source)
        at org.apache.commons.dbcp.DelegatingStatement.setFetchSize(DelegatingStatement.java:279)
        at org.apache.commons.dbcp.DelegatingStatement.setFetchSize(DelegatingStatement.java:279)
        at org.apache.jackrabbit.core.util.db.ConnectionHelper.reallyExec(ConnectionHelper.java:372)
        at org.apache.jackrabbit.core.util.db.ConnectionHelper$3.call(ConnectionHelper.java:353)
        at org.apache.jackrabbit.core.util.db.ConnectionHelper$3.call(ConnectionHelper.java:349)
        at org.apache.jackrabbit.core.util.db.ConnectionHelper$RetryManager.doTry(ConnectionHelper.java:472)
        at org.apache.jackrabbit.core.util.db.ConnectionHelper.exec(ConnectionHelper.java:349)
        at org.apache.jackrabbit.core.persistence.pool.BundleDbPersistenceManager.getAllNodeIds(BundleDbPersistenceManager.java:1020)

It's caused by the following code in ConnectionHelper when 0 < maxRows < 10000:

            stmt.setMaxRows(maxRows);
            stmt.setFetchSize(10000);

A simple fix would be:


            stmt.setMaxRows(maxRows);
            stmt.setFetchSize(Math.min(10000, maxRows));
"
0,"RepositoryImpl.activeSessions should use Session instead of SessionImplTurn Map<SessionImpl, SessionImpl> activeSessions into Map<Session, Session> activeSessions as there is not clear need for the use of SessionImpl."
0,"FieldCache rewrite method for MultiTermQueriesFor some MultiTermQueries, like RangeQuery we have a FieldCacheRangeFilter etc (in this case its particularly optimized).

But in the general case, since LUCENE-2784 we can now have a rewrite method to rewrite any MultiTermQuery 
using the FieldCache, because MultiTermQuery's getEnum no longer takes IndexReader but Terms, and all the 
FilteredTermsEnums are now just real TermsEnum decorators.

In cases like low frequency queries this is actually slower (I think this has been shown for numeric ranges before too),
but for the really high-frequency cases like especially ugly wildcards, regexes, fuzzies, etc, this can be several times faster 
using the FieldCache instead, since all the terms are in RAM and automaton can blast through them quicker.
"
0,"New Analysis  ContributionsWith the advent of the new TeeTokenFilter and SinkTokenizer, there now exists some interesting new things that can be done in the analysis phase of indexing.  See LUCENE-1058.

This patch provides some new implementations of SinkTokenizer that may be useful."
0,"jUnit test-cases: success of some DocumentViewImportTest tests depends on Xerxes version being usedif e.g. xerxes v. 2.4.0 is used instead of v. 2.6.2 as specified in project.xml certain tests in DocumentViewImportTest fail.

e.g. 

Testcase: testWorkspaceImportXml(org.apache.jackrabbit.test.api.DocumentViewImportTest):	FAILED
Xml text is not correctly stored. expected:<......> but was:<...
       ...>
junit.framework.ComparisonFailure: Xml text is not correctly stored. expected:<......> but was:<...
       ...>
	at org.apache.jackrabbit.test.api.DocumentViewImportTest.checkXmlTextNode(DocumentViewImportTest.java:240)
	at org.apache.jackrabbit.test.api.DocumentViewImportTest.checkImportSimpleXMLTree(DocumentViewImportTest.java:174)
	at org.apache.jackrabbit.test.api.DocumentViewImportTest.performTests(DocumentViewImportTest.java:143)
	at org.apache.jackrabbit.test.api.DocumentViewImportTest.doTestImportXML(DocumentViewImportTest.java:115)
	at org.apache.jackrabbit.test.api.DocumentViewImportTest.testWorkspaceImportXml(DocumentViewImportTest.java:73)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:401)
"
1,"Concurrency issues in SegmentInfo.files() could lead to ConcurrentModificationExceptionThe multi-threaded call of the files() in SegmentInfo could lead to the ConcurrentModificationException if one thread is not finished additions to the ArrayList (files) yet while the other thread already obtained it as cached (see below). This is a rare exception, but it would be nice to fix. I see the code is no longer problematic in the trunk (and others ported from flex_1458), looks it was fixed while implementing post 3.x features. The fix to 3.x and 2.9.x branches could be the same - create the files set first and populate it, and then assign to the member variable at the end of the method. This will resolve the issue. I could prepare the patch for 2.9.4 and 3.x, if needed.

--

INFO: [19] webapp= path=/replication params={command=fetchindex&wt=javabin} status=0 QTime=1
Jul 30, 2010 9:13:05 AM org.apache.solr.core.SolrCore execute
INFO: [19] webapp= path=/replication params={command=details&wt=javabin} status=0 QTime=24
Jul 30, 2010 9:13:05 AM org.apache.solr.handler.ReplicationHandler doFetch
SEVERE: SnapPull failed
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at java.util.AbstractCollection.addAll(AbstractCollection.java:305)
        at org.apache.lucene.index.SegmentInfos.files(SegmentInfos.java:826)
        at org.apache.lucene.index.DirectoryReader$ReaderCommit.<init>(DirectoryReader.java:916)
        at org.apache.lucene.index.DirectoryReader.getIndexCommit(DirectoryReader.java:856)
        at org.apache.solr.search.SolrIndexReader.getIndexCommit(SolrIndexReader.java:454)
        at org.apache.solr.handler.SnapPuller.fetchLatestIndex(SnapPuller.java:261)
        at org.apache.solr.handler.ReplicationHandler.doFetch(ReplicationHandler.java:264)
        at org.apache.solr.handler.ReplicationHandler$1.run(ReplicationHandler.java:146)
"
0,EffectiveNodeType#getNamedNodeDefs returns array QItemDefinition instead of QNodeDefinition... thus requires unnecessary casting...
1,"BLOBFileValue.read(byte[] b, long pos) ignores return value of InputStream.skipInputStream.skip(long n) returns a long, which may be different from the parameter n (possibly lower).
Currently in BLOBFileValue.read(byte[] b, long pos) the return value is ignored."
0,"nightly build failedjavadoc tasked failed due to new project structure in contrib/gdata-server
added correct package structure to java/trunk/build.xml

javadoc creation successful.

Patch added as attachment.

regards simon"
1,"Several bugs in last SVN commitJust a few bugs in the last SVN commit, but since I work with it, i thought useful to mention them :
1) org.apache.jackrabbit.ocm.reflection.ReflectionUtils should handle Set --> just add it in defaultImplementation
2) in org.apache.jackrabbit.ocm.manager.collectionconverter.ManageableObjectsUtil.getManageableObjects, correct defaultImplementation test :
        		if (defaultImplementation == null)
        		{
        			throw new JcrMappingException(""No default implementation for the interface "" + manageableObjectsClass);

Thank you and keep up the good work!

Sincerely yours,

Stephane"
0,"make it easier to access default stopwords for language analyzersDM Smith made the following comment: (sometimes it is hard to dig out the stop set from the analyzers)

Looking around, some of these analyzers have very different ways of storing the default list.
One idea is to consider generalizing something like what Simon did with LUCENE-1965, LUCENE-1962,
and having all stopwords lists stored as .txt files in resources folder.

{code}
  /**
   * Returns an unmodifiable instance of the default stop-words set.
   * @return an unmodifiable instance of the default stop-words set.
   */
  public static Set<String> getDefaultStopSet()
{code}
"
0,"DirListingExportHandler: Should not implement PropertyHandlerissue found by Roland Porath:

if the DirListingExportHandler is used with some other collection nodetype that nt:folder (that may allow other properties) the list of dav properties obtained upon PROPFIND (being delegated to PropertyHandler) results in an imcomplete list.

since the only benefit of the DirListingExportHandler is to display something nice(?) upon a GET to a folder, i'd suggest to remove the implementation of PropertyHandler from the DirListingExportHandler.

angela"
0,"Enhance QueryUtils and CheckHIts to wrap everything they check in MultiReader/MultiSearchermethods in CheckHits & QueryUtils are in a good position to take any Searcher they are given and not only test it, but also test MultiReader & MultiSearcher constructs built around them"
0,"Consolidate CND related classes from SPI and Corecurrently SPI Commons and Core, both have a CND Reader/Writer. they should be consolidated; i.e. core should use the SPI one."
1,"NTCollectionConverterImpl throws a null pointer exception on updateWhen calling update on a node which has no child nodes stored (but which can have child nodes) the code can generate a null pointer exception. In the case where one goes to remove JCR nodes which are not present in the current objects collection of child objects the code is calling getCollectionNodes().iterator(). However, since is not checking for the case where getCollectionNodes() returns null if there are no child nodes present a null pointer exception will be generated. "
0,"UserImporter should trigger execution AuthorizableActions in case of user/group creationin accordance to the new implementation specific extensions made to user mangement in JCR-3118 the user-importer
should be adjusted as well."
1,"[PATCH] RussianAnalyzer's tokenizer skips numbers from input text,RussianAnalyzer's tokenizer skips numbers from input text, so that resulting token stream miss numbers. Problem can be solved by adding numbers to RussianCharsets.UnicodeRussian. See test case below  for details.

{code:title=TestRussianAnalyzer.java|borderStyle=solid}

public class TestRussianAnalyzer extends TestCase {

  Reader reader = new StringReader(""text 1000"");

  // test FAILS
  public void testStemmer() {
    testAnalyzer(new RussianAnalyzer());
  }

  // test PASSES
  public void testFixedRussianAnalyzer() {
    testAnalyzer(new RussianAnalyzer(getRussianCharSet()));
  }

  private void testAnalyzer(RussianAnalyzer analyzer) {
    try {
      TokenStream stream = analyzer.tokenStream(""text"", reader);
      assertEquals(""text"", stream.next().termText());
      assertNotNull(stream.next());
    } catch (IOException e) {
      fail(e.getMessage());
    }
  }

  private char[] getRussianCharSet() {
    int length = RussianCharsets.UnicodeRussian.length;
    final char[] russianChars = new char[length + 10];

    System
        .arraycopy(RussianCharsets.UnicodeRussian, 0, russianChars, 0, length);
    russianChars[length++] = '0';
    russianChars[length++] = '1';
    russianChars[length++] = '2';
    russianChars[length++] = '3';
    russianChars[length++] = '4';
    russianChars[length++] = '5';
    russianChars[length++] = '6';
    russianChars[length++] = '7';
    russianChars[length++] = '8';
    russianChars[length] = '9';
    return russianChars;
  }
}

{code} "
1,"Netscape proxy problem wtih POSTDescription:

When using httpClient to POST to a http url through a Netscape proxy server, 
the httpClient failed due to read error when reading status line.  The log seem 
to indicate that the proxy is talking HTTP/1.0 and does not expect the POST 
data to come.  I am using a modified version of the ClientApp from examples.  I 
will attach both the test program and log files.

Workaround:

If use PostMethod.setUseExpect (true), it will work.  But in many cases, it 
would be slower.

Related issues:

In doing the test, I also found out that the httpClient PostMehtod does not 
work when the request body is NOT set (not calling setRequestBody).  It also 
does not work with empty body (setRequestBody ("""")).  The attached 
clientApp.properties file has flags to test each case and I will attach the 
logs as well.  Excuse my ignorance, I do not know for sure what the HTTP spec. 
says about the body in the POST method.  But at least if the caller/app is 
wrong in not setting the body, some exception should be thrown.  It could also 
be my server's problem, please let me know if that is the case (I am using 
weblogic server 6.1)."
0,"Add optional state attribute to managed client connectionsProvide an optional state attribute to managed client connections. The connection state can represent a user identify in case of connection based authentication schemes such as NTLM or SSL, thus allowing for connection re-use on a per user identity basis."
0,"Merge CharTermAttribute and deprecations to stableThis should be merged to trunk until flex lands, so the analyzers can be ported to new api."
0,"mvn eclipse:eclipse inconsistentmvn eclipse:eclipse result is inconsistent, due to deprecated avacc-maven-plugin usage.
should use piped ""jjtree-javacc"" goal.
core :
          <execution>
            <id>fulltext-jjtree</id>
            <configuration>
              <sourceDirectory>${basedir}/src/main/javacc/fulltext</sourceDirectory>
              <outputDirectory>${project.build.directory}/generated-src/main/java</outputDirectory>
              <timestampDirectory>${project.build.directory}/generated-src/javacc-timestamp</timestampDirectory>
              <packageName>org.apache.jackrabbit.core.query.lucene.fulltext</packageName>
            </configuration>
            <goals>
              <goal>jjtree-javacc</goal>
            </goals>
          </execution>

spi-commons:
					<execution>
						<id>sql-jjtree-javacc</id>
						<configuration>
							<sourceDirectory>${basedir}/src/main/javacc/sql</sourceDirectory>
							<outputDirectory>${project.build.directory}/generated-src/main/java</outputDirectory>
							<timestampDirectory>${project.build.directory}/generated-src/javacc-timestamp</timestampDirectory>
							<packageName>org.apache.jackrabbit.spi.commons.query.sql</packageName>
						</configuration>
						<goals>
							<goal>jjtree-javacc</goal>
						</goals>
					</execution>
					<execution>
						<id>xpath-jjtree-javacc</id>
						<configuration>
							<sourceDirectory>${basedir}/src/main/javacc/xpath</sourceDirectory>
							<outputDirectory>${project.build.directory}/generated-src/main/java</outputDirectory>
							<timestampDirectory>${project.build.directory}/generated-src/javacc-timestamp</timestampDirectory>
							<packageName>org.apache.jackrabbit.spi.commons.query.xpath</packageName>
						</configuration>
						<goals>
							<goal>jjtree-javacc</goal>
						</goals>
					</execution>"
1,"Updates to multiple workspaces (e.g. in a transaction) locked in cluster journalRunning a transaction that updates multiple workspaces (e.g. a versioning operation) will be locked, as they all try to acquire a non-reentrant lock in the cluster's journal. Short-term fix is to make the lock re-entrant. In the long run, a transaction context sensitive lock may be more appropriate.

How to reproduce: enable clustering in the test environment and let the test o.a.j.core.XATest.testSetVersionLabel() run. This will result in a deadlock when committing the operation.

This was initially reported by Rafa Kwiecie as a problem when using springmodules and clustering but turned out to be general problem with transactions and clustering. Thanks for reporting it!"
0,"Benchmarks Enhancements (precision/recall, TREC, Wikipedia)Would be great if the benchmark contrib had a way of providing precision/recall benchmark information ala TREC.  I don't know what the copyright issues are for the TREC queries/data (I think the queries are available, but not sure about the data), so not sure if the is even feasible, but I could imagine we could at least incorporate support for it for those who have access to the data.  It has been a long time since I have participated in TREC, so perhaps someone more familiar w/ the latest can fill in the blanks here.

Another option is to ask for volunteers to create queries and make judgments for the Reuters data, but that is a bit more complex and probably not necessary.  Even so, an Apache licensed set of benchmarks may be useful for the community as a whole.  Hmmm.... 

Wikipedia might be another option instead of Reuters to setup as a download for benchmarking, as it is quite large and I believe the licensing terms are quite amenable.  Having a larger collection would be good for stressing Lucene more and would give many users a demonstration of how Lucene handles large collections.

At any rate, this kind of information could be useful for people looking at different indexing schemes, formats, payloads and different query strategies.

"
0,"Fixed README.txt on textfilters project- Fixed a little mistake: changed org.apache.jackrabbit.core.query..RTFTextFilter to org.apache.jackrabbit.core.query.RTFTextFilter

- Added the OpenOfficeTextFilter to the sample configuration line."
0,"davex remoting has  a performance bottleneck due limit of 2 http connectionsThe spi2dav service implementation use of HttpClient did not support configuration of the maximum amount of http connections to the server.  The default value, in the HttpClient code, is two. This was a performance bottleneck.  This work makes the number of connections configurable via a parameter to the map passed to the repository factory.  

It also fixes a concurrency issue which was exposed by the increased concurrency effected by this work.  This fix is a replacement of a HashMap cache of client connections with a ConcurrentHashMap, thanks to the java 1.5 available in Jackrabbit 2.x

USAGE: 
Set the number of connections (Spi2davRepositoryServiceFactory.PARAM_MAX_HTTP_CONNECTIONS) when creating a factory via the dav or davex rep factories.  Default is 20.

NOTE: 
See also the server side fixes: JCR-3027  The patch on that ticket allows configuration of the concurrency level on the server, which should be tuned in conjunction with the client side connection levels.  
 "
0,"add shutdown() or logoutAll() method to TransientRepositoryIt would be usefull to be able to explicitly ask a TransientRepository to shut down, instead of relying on all sessions to be closed by session.logout()."
1,"RangeQuery & RangeFilter used with collation seek to lowerTerm using compareTo()The constructor for RangeTermEnum initializes a TermEnum starting with lowerTermText, but when a collator is defined, all terms in the given field need to be checked, since collation can introduce non-Unicode orderings.  Instead, the RangeTermEnum constructor should test for a non-null collator, and if there is one, point the TermEnum at the first term in the given field.

LUCENE-1424 introduced this bug."
0,spi2davex NodeInfoImpl should use HashSet instead of ArrayList for childInfosThe subsequent contains call is prohibitively expensive since it returns in an equals call for all existing child infos. 
1,"spi2dav: avoid reusing the same document in repositoryserviceimpl... instead each call should create it's own document (credits due to jukka :)
that seems to avoid that odd npe in DomUtil."
1,TestPagedBytes failureant test -Dtestcase=TestPagedBytes -Dtestmethod=testDataInputOutput -Dtests.seed=268db1f3329b70d:3125365bc9c56c90:116e02aa4a70ec2f -Dtests.multiplier=5
1,NOT_ANALYZED fields can double-count offsetsIf the same field name has 2 NOT_ANALYZED field instances then the offsets are double-counted.
1,Unable to add/lock and unlock/remove Node with shared Session in 2 TransactionsIf you try to unlock and remove a node the NodeState can be run out of sync between the two operations.
0,"Class DisjunctionSumScorer does not need to be public.See title, patch follows."
0,"Public API inconsistencyorg.apache.lucene.index.SegmentInfos is public, and contains public methods (which is good for expert-level index manipulation tools such as Luke). However, SegmentInfo class has package visibility. This leads to a strange result that it's possible to read SegmentInfos, but it's not possible to access its details (SegmentInfos.info(int)) from a user application.

The solution is to make SegmentInfo class public."
1,"Instantiated/IndexWriter discrepanies * RAMDirectory seems to do a reset on tokenStreams the first time, this permits to initialise some objects before starting streaming, InstantiatedIndex does not.
 * I can Serialize a RAMDirectory but I cannot on a InstantiatedIndex because of : java.io.NotSerializableException: org.apache.lucene.index.TermVectorOffsetInfo

http://www.nabble.com/InstatiatedIndex-questions-to20576722.html

"
1,"NRT reader/writer over RAMDirectory memory leakwith NRT reader/writer, emptying an index using:
writer.deleteAll()
writer.commit()
doesn't release all allocated memory.

for example the following code will generate a memory leak:

/**
	 * Reveals a memory leak in NRT reader/writer<br>
	 * 
	 * The following main() does 10K cycles of:
	 * <ul>
	 * <li>Add 10K empty documents to index writer</li>
	 * <li>commit()</li>
	 * <li>open NRT reader over the writer, and immediately close it</li>
	 * <li>delete all documents from the writer</li>
	 * <li>commit changes to the writer</li>
	 * </ul>
	 * 
	 * Running with -Xmx256M results in an OOME after ~2600 cycles
	 */
	public static void main(String[] args) throws Exception {
		RAMDirectory d = new RAMDirectory();
		IndexWriter w = new IndexWriter(d, new IndexWriterConfig(Version.LUCENE_33, new KeywordAnalyzer()));
		Document doc = new Document();
		
		for(int i = 0; i < 10000; i++) {
			for(int j = 0; j < 10000; ++j) {
				w.addDocument(doc);
			}
			w.commit();
			IndexReader.open(w, true).close();

			w.deleteAll();
			w.commit();
		}
		
		w.close();
		d.close();
	}	"
1,"Version history recovery fails in case a version does not have a jcr:frozenNodeWith JCR-2551 in place, a version recovery mode has been introduced. Problem now is that in case a version is encountered that misses a mandatory jcr:frozenNode, an InternalError is thrown by o.a.j.c.version.InternalVersionHistoryImpl#createVersionInstance. Since o.a.j.c.RepositoryChecker#checkVersionHistory only catches Exception, it fails to catch it properly which leads to a complete repository shutdown.

Throwing for example a RuntimeException instead fixes the problem."
1,"Invalid journal records during XATransactionsDuring the prepare phase of a XATransaction, XAItemStateManager.prepare calls ShareItemStageManager.beginUpdate that, in case of a ClusterNode, calls ClusterNode.updatePrepared that does a ChangeLogRecord.write().

This last method is located in ClusterRecord and systematically write the begin and the end of the journal record.

As a consequence, useless corrupted records are written in the journal everytime a transaction ends without jackrabbit update! This causes polution of the journal, as other cluster nodes try to sync with the corrupted updates and fail doing so as ClusterRecordDeserializer can't deserialize it (the record identifier is empty).

ChangeLogRecord (and even other ClusterRecord implementations too) should only write if there's effective updates.

I propose the following solution:
*) add the following method in Changelog so clients can know if there's effective updates:
    public boolean hasUpdates() {
    	return !(addedStates.isEmpty() && modifiedStates.isEmpty() && deletedStates.isEmpty() && modifiedRefs.isEmpty());
    }

*) change ClusterRecord with:
    public final void write() throws JournalException {
    	
    	if (hasUpdates()) {
	        record.writeString(workspace);
	        doWrite();
	        record.writeChar(END_MARKER);
    	}
    }
    
    protected abstract boolean hasUpdates();

*) implement hasUpdates for every ClusterRecord implementation:
 ----> ChangeLogRecord:
    protected boolean hasUpdates() {
    	return changes.hasUpdates() || !events.isEmpty();
    }
 ----> LockRecord and NamespaceRecord:
    protected boolean hasUpdates() {
    	return true;
    }

 ----> NodeTypeRecord:
    protected boolean hasUpdates() {
    	return !collection.isEmpty();
    }

Best regards,

Stephane Landelle"
0,Upgrade to Tika 0.10Apache Tika 0.10 was released some while ago. It contains lots of improvements and fixes for full text extraction.
1,TermsFilter.getDocIdSet(context) NPE on missing fieldIf the context does not contain the field for a term when calling TermsFilter.getDocIdSet(AtomicReaderContext context) then a NullPointerException is thrown due to not checking for null Terms before getting iterator.
1,"Surround Query doesn't properly handle equals/hashcodeIn looking at using the surround queries with Solr, I am hitting issues caused by collisions due to equals/hashcode not being implemented on the anonymous inner classes that are created by things like DistanceQuery (branch 3.x, near line 76)"
1,"CorruptIndexException on indexing after a failure occurs after segments file creation but before any bytes are writtenFSDirectory.createOutput(..) uses a RandomAccessFile to do its work.  On my system the default FSDirectory.open(..) creates an NIOFSDirectory.  If createOutput is called on a segments_* file and a crash occurs between RandomAccessFile creation (file system shows a segments_* file exists but has zero bytes) but before any bytes are written to the file, subsequent IndexWriters cannot proceed.  The difficulty is that it does not know how to clear the empty segments_* file.  None of the file deletions will happen on such a segment file because the opening bytes cannot not be read to determine format and version.

An initial proposed patch file is attached below.

"
1,"error handling duplicate connection headersHttpMethodBase.shouldCloseConnection() does not correctly handle the case when
more than one connection header exists.  Reported by Ross Rankin."
1,"FieldBoostMapAttribute in contrib/qp is broken.While looking for more SuppressWarnings in lucene, i came across two of them in contrib/queryparser.

even worse, i found these revolved around using maps with CharSequence as key.

From the javadocs for CharSequence:

This interface does not refine the general contracts of the equals and hashCode methods. The result of comparing two objects that implement CharSequence is therefore, in general, undefined. Each object may be implemented by a different class, and there is no guarantee that each class will be capable of testing its instances for equality with those of the other. {color:red} It is therefore inappropriate to use arbitrary CharSequence instances as elements in a set or as keys in a map. {color}

"
1,"Jackrabbit thread contention issue due to fat lockHello,

We are running jackrabbit 1.4.5 using a persistent file data store within a weblogic container and encountering a variety of thread locking issues. To get around the problem, we are forced synchronize thread access to the JCR repository or reduce thread worker count to 1 which has a heavy performance impact on our application. I'm not exactly sure what the problem is and was wondering someone is looking into this issue and if there is a workaround/fix planned?

<Oct 30, 2008 10:45:32 AM PDT> <Error> <WebLogicServer> <BEA-000337> <[STUCK] ExecuteThread: '5' for queue: 'weblogic.kernel.Default (self-tuning)' has been busy for ""1,863"" seconds working on the request ""weblogic.work.j2ee.J2EEWorkManager$WorkWithListener@2117cc9"", which is more than the configured time (StuckThreadMaxTime) of ""1,800"" seconds. Stack trace:
Thread-94 ""[STUCK] ExecuteThread: '5' for queue: 'weblogic.kernel.Default (self-tuning)'"" <alive, in native, suspended, waiting, priority=1, DAEMON> {
    -- Waiting for notification on: EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock@152c384[fat lock]
    java.lang.Object.wait(Object.java:???)
    java.lang.Object.wait(Object.java:474)
    EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock.acquire(Unknown Source)
    org.apache.jackrabbit.core.journal.AbstractJournal.lockAndSync(AbstractJournal.java:235)
    org.apache.jackrabbit.core.journal.DefaultRecordProducer.append(DefaultRecordProducer.java:49)

}

>
<Oct 30, 2008 10:45:32 AM PDT> <Error> <WebLogicServer> <BEA-000337> <[STUCK] ExecuteThread: '2' for queue: 'weblogic.kernel.Default (self-tuning)' has been busy for ""1,916"" seconds working on the request ""weblogic.work.j2ee.J2EEWorkManager$WorkWithListener@227b6d4"", which is more than the configured time (StuckThreadMaxTime) of ""1,800"" seconds. Stack trace:
Thread-25 ""[STUCK] ExecuteThread: '2' for queue: 'weblogic.kernel.Default (self-tuning)'"" <alive, in native, suspended, waiting, priority=1, DAEMON> {
    -- Waiting for notification on: EDU.oswego.cs.dl.util.concurrent.LinkedNode@42d58e0[fat lock]
    java.lang.Object.wait(Object.java:???)
    java.lang.Object.wait(Object.java:474)
    EDU.oswego.cs.dl.util.concurrent.SynchronousChannel.put(Unknown Source)
    EDU.oswego.cs.dl.util.concurrent.PooledExecutor$WaitWhenBlocked.blockedAction(Unknown Source)
    EDU.oswego.cs.dl.util.concurrent.PooledExecutor.execute(Unknown Source)
...
    org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:334)
    org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:307)
    org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:317)
    org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1072)
    ^-- Holding lock: org.apache.jackrabbit.core.query.lucene.VolatileIndex@3eb0f41[thin lock]
    org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:895)
    org.apache.jackrabbit.jca.JCASessionHandle.save(JCASessionHandle.java:178)
    com.qpass.inventory.ingestion.IngestionServiceImpl$1.doInJCR(IngestionServiceImpl.java:124)
    com.qpass.inventory.model.JCRTemplate.execute(JCRTemplate.java:17)
    com.qpass.inventory.ingestion.IngestionServiceImpl.ingestProduct(IngestionServiceImpl.java:93)
    com.qpass.inventory.ingestion.bulk.AbstractBulkIngester.ingestProduct(AbstractBulkIngester.java:42)
    com.qpass.inventory.ingestion.bulk.ZipFileBulkIngester.processFile(ZipFileBulkIngester.java:35)
    com.qpass.inventory.ingestion.bulk.IngestionWorker.processFile(IngestionWorker.java:26)
    com.qpass.inventory.ingestion.bulk.IngestionWorker$1.run(IngestionWorker.java:64)
    org.springframework.scheduling.commonj.DelegatingWork.run(DelegatingWork.java:61)
    weblogic.work.j2ee.J2EEWorkManager$WorkWithListener.run(J2EEWorkManager.java:245)
    weblogic.work.ExecuteThread.execute(ExecuteThread.java:206)
    weblogic.work.ExecuteThread.run(ExecuteThread.java:173)
}

>
<Oct 30, 2008 10:45:32 AM PDT> <Error> <WebLogicServer> <BEA-000337> <[STUCK] ExecuteThread: '1' for queue: 'weblogic.kernel.Default (self-tuning)' has been busy for ""1,891"" seconds working on the request ""weblogic.work.j2ee.J2EEWorkManager$WorkWithListener@2117c83"", which is more than the configured time (StuckThreadMaxTime) of ""1,800"" seconds. Stack trace:
Thread-24 ""[STUCK] ExecuteThread: '1' for queue: 'weblogic.kernel.Default (self-tuning)'"" <alive, in native, suspended, waiting, priority=1, DAEMON> {
    -- Waiting for notification on: EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock@152c384[fat lock]
    java.lang.Object.wait(Object.java:???)
    java.lang.Object.wait(Object.java:474)
    EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock.acquire(Unknown Source)
    org.apache.jackrabbit.core.journal.AbstractJournal.lockAndSync(AbstractJournal.java:235)
    org.apache.jackrabbit.core.journal.DefaultRecordProducer.append(DefaultRecordProducer.java:49)
    org.apache.jackrabbit.core.cluster.ClusterNode$WorkspaceUpdateChannel.updateCreated(ClusterNode.java:556)
...


<Oct 30, 2008 11:21:30 AM PDT> <Warning> <netuix> <BEA-423420> <Redirect is executed in begin or refresh action. Redirect url is /console/console.portal?_nfpb=true&_pageLabel=HomePage1.>
<Oct 30, 2008 11:44:32 AM PDT> <Error> <WebLogicServer> <BEA-000337> <[STUCK] ExecuteThread: '4' for queue: 'weblogic.kernel.Default (self-tuning)' has been busy for ""1,803"" seconds working on the request ""Http Request: /inventory/rpc/searchService"", which is more than the configured time (StuckThreadMaxTime) of ""1,800"" seconds. Stack trace:
Thread-51 ""[STUCK] ExecuteThread: '4' for queue: 'weblogic.kernel.Default (self-tuning)'"" <alive, in native, suspended, waiting, priority=1, DAEMON> {
    -- Waiting for notification on: java.lang.Object@1569e04[fat lock]
    java.lang.Object.wait(Object.java:???)
    java.lang.Object.wait(Object.java:474)
    org.apache.jackrabbit.core.query.lucene.MultiIndex.getIndexReader(MultiIndex.java:694)
    org.apache.jackrabbit.core.query.lucene.SearchIndex.getIndexReader(SearchIndex.java:825)
    org.apache.jackrabbit.core.query.lucene.SearchIndex.executeQuery(SearchIndex.java:682)
    org.apache.jackrabbit.core.query.lucene.QueryResultImpl.executeQuery(QueryResultImpl.java:242)
    org.apache.jackrabbit.core.query.lucene.QueryResultImpl.getResults(QueryResultImpl.java:271)
    org.apache.jackrabbit.core.query.lucene.QueryResultImpl.<init>(QueryResultImpl.java:177)
    org.apache.jackrabbit.core.query.lucene.QueryImpl.execute(QueryImpl.java:105)
    org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:174)
    com.qpass.inventory.service.QueryProfiler.execute(QueryProfiler.java:20)
    com.qpass.inventory.service.SearchServiceImpl$1.doInJCR(SearchServiceImpl.java:59)
    com.qpass.inventory.model.JCRTemplate.execute(JCRTemplate.java:17)
    com.qpass.inventory.service.SearchServiceImpl.doSearch(SearchServiceImpl.java:54)
    com.qpass.inventory.ui.impl.SearchUIServiceImpl.search(SearchUIServiceImpl.java:48)
    sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:???)
    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:27)
    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    java.lang.reflect.Method.invoke(Method.java:570)
    org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:309)
    org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:182)
    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:148)
    org.acegisecurity.intercept.method.aopalliance.MethodSecurityInterceptor.invoke(MethodSecurityInterceptor.java:62)
    org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:148)
    org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:151)
    $Proxy74.search(Unknown Source)
    sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:???)
    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:27)
    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    java.lang.reflect.Method.invoke(Method.java:570)
    org.gwtwidgets.server.spring.GWTRPCServiceExporter.invokeMethodOnService(GWTRPCServiceExporter.java:157)
    org.gwtwidgets.server.spring.GWTRPCServiceExporter.processCall(GWTRPCServiceExporter.java:295)
    com.google.gwt.user.server.rpc.RemoteServiceServlet.doPost(RemoteServiceServlet.java:173)
    org.gwtwidgets.server.spring.GWTRPCServiceExporter.handleRequest(GWTRPCServiceExporter.java:361)
    com.qpass.base.ui.security.GWTServiceExporter.handleRequest(GWTServiceExporter.java:45)
    org.springframework.web.servlet.mvc.HttpRequestHandlerAdapter.handle(HttpRequestHandlerAdapter.java:49)
    org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:831)
    org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:781)
    org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:567)
    org.springframework.web.servlet.FrameworkServlet.doPost(FrameworkServlet.java:511)
    javax.servlet.http.HttpServlet.service(HttpServlet.java:736)
    javax.servlet.http.HttpServlet.service(HttpServlet.java:851)
    weblogic.servlet.internal.StubSecurityHelper$ServletServiceAction.run(StubSecurityHelper.java:224)
    weblogic.servlet.internal.StubSecurityHelper.invokeServlet(StubSecurityHelper.java:108)
    weblogic.servlet.internal.ServletStubImpl.execute(ServletStubImpl.java:198)
    weblogic.servlet.internal.TailFilter.doFilter(TailFilter.java:26)
    weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:41)
    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:259)
    org.acegisecurity.intercept.web.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:93)
    org.acegisecurity.intercept.web.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:71)
    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:259)
    com.qpass.usersecurity.auth.UpdatePermissionsOnContextChangeFilter.doFilter(UpdatePermissionsOnContextChangeFilter.java:44)
    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:259)
    org.acegisecurity.ui.AbstractProcessingFilter.doFilter(AbstractProcessingFilter.java:191)
    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:259)
    org.acegisecurity.context.HttpSessionContextIntegrationFilter.doFilter(HttpSessionContextIntegrationFilter.java:195)
    org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:259)
    org.acegisecurity.util.FilterChainProxy.doFilter(FilterChainProxy.java:122)
    org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:236)
    org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:154)
    weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:41)
    com.qpass.base.applicationcontext.RequestContextFilter.doFilter(RequestContextFilter.java:103)
    org.springframework.web.filter.DelegatingFilterProxy.invokeDelegate(DelegatingFilterProxy.java:236)
    org.springframework.web.filter.DelegatingFilterProxy.doFilter(DelegatingFilterProxy.java:154)
    weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:41)
    org.springframework.web.filter.CharacterEncodingFilter.doFilterInternal(CharacterEncodingFilter.java:90)
    org.springframework.web.filter.OncePerRequestFilter.doFilter(OncePerRequestFilter.java:61)
    weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:41)
    weblogic.servlet.internal.RequestEventsFilter.doFilter(RequestEventsFilter.java:24)
    weblogic.servlet.internal.FilterChainImpl.doFilter(FilterChainImpl.java:41)
    weblogic.servlet.internal.WebAppServletContext$ServletInvocationAction.run(WebAppServletContext.java:3214)
    weblogic.security.acl.internal.AuthenticatedSubject.doAs(AuthenticatedSubject.java:308)
    weblogic.security.service.SecurityManager.runAs(SecurityManager.java:117)
    weblogic.servlet.internal.WebAppServletContext.securedExecute(WebAppServletContext.java:1946)
    weblogic.servlet.internal.WebAppServletContext.execute(WebAppServletContext.java:1868)
    weblogic.servlet.internal.ServletRequestImpl.run(ServletRequestImpl.java:1331)
    weblogic.work.ExecuteThread.execute(ExecuteThread.java:206)
    weblogic.work.ExecuteThread.run(ExecuteThread.java:173)
}




<Oct 2, 2008 2:09:36 PM PDT> <Error> <WebLogicServer> <BEA-000337> <[STUCK] ExecuteThread: '1' for queue: 'weblogic.kernel.Default (self-tuning)' has been busy for ""696"" seconds working on the request ""weblogic.work.j2ee.J2EEWorkManager$WorkWithListener@863e564"", which is more than the configured time (StuckThreadMaxTime) of ""600"" seconds. Stack trace:
Thread-21 ""[STUCK] ExecuteThread: '1' for queue: 'weblogic.kernel.Default (self-tuning)'"" <alive, in native, suspended, priority=1, DAEMON> {
    java.io.FileOutputStream.writeBytes(FileOutputStream.java:???)
    java.io.FileOutputStream.write(FileOutputStream.java:260)
    java.io.BufferedOutputStream.write(BufferedOutputStream.java:100)
    ^-- Holding lock: java.io.BufferedOutputStream@39d70a5[thin lock]
    org.apache.jackrabbit.core.persistence.util.FileSystemBLOBStore.put(FileSystemBLOBStore.java:88)
    org.apache.jackrabbit.core.persistence.bundle.util.BundleBinding.writeState(BundleBinding.java:561)
    org.apache.jackrabbit.core.persistence.bundle.util.BundleBinding.writeBundle(BundleBinding.java:245)
    org.apache.jackrabbit.core.persistence.bundle.Oracle9PersistenceManager.storeBundle(Oracle9PersistenceManager.java:114)
    ^-- Holding lock: org.apache.jackrabbit.core.persistence.bundle.Oracle9PersistenceManager@140f7b9[thin lock]
    org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.putBundle(AbstractBundlePersistenceManager.java:703)
    org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.store(AbstractBundlePersistenceManager.java:526)
    ^-- Holding lock: org.apache.jackrabbit.core.persistence.bundle.Oracle9PersistenceManager@140f7b9[thin lock]
    org.apache.jackrabbit.core.persistence.bundle.BundleDbPersistenceManager.store(BundleDbPersistenceManager.java:517)
    ^-- Holding lock: org.apache.jackrabbit.core.persistence.bundle.Oracle9PersistenceManager@140f7b9[thin lock]
    org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:699)
    org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:873)
    org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:334)
    org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:334)
    org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:307)
    org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:317)
    org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1072)
    ^-- Holding lock: org.apache.jackrabbit.core.XASessionImpl@1f2653b[thin lock]
    org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:895)
    org.apache.jackrabbit.jca.JCASessionHandle.save(JCASessionHandle.java:178)
    com.qpass.inventory.ingestion.IngestionServiceImpl$1.doInJCR(IngestionServiceImpl.java:140)
    com.qpass.inventory.model.JCRTemplate.execute(JCRTemplate.java:17)
    com.qpass.inventory.ingestion.IngestionServiceImpl.ingestProduct(IngestionServiceImpl.java:112)
    ^-- Holding lock: java.lang.Object@849ca9e[fat lock]
    com.qpass.inventory.ingestion.bulk.AbstractBulkIngester.ingestProduct(AbstractBulkIngester.java:42)
    com.qpass.inventory.ingestion.bulk.ZipFileBulkIngester.processFile(ZipFileBulkIngester.java:35)
    com.qpass.inventory.ingestion.bulk.IngestionWorker.processFile(IngestionWorker.java:26)
    com.qpass.inventory.ingestion.bulk.IngestionWorker$1.run(IngestionWorker.java:64)
    org.springframework.scheduling.commonj.DelegatingWork.run(DelegatingWork.java:61)
    weblogic.work.j2ee.J2EEWorkManager$WorkWithListener.run(J2EEWorkManager.java:245)
    weblogic.work.ExecuteThread.execute(ExecuteThread.java:206)
    weblogic.work.ExecuteThread.run(ExecuteThread.java:173)
"
1,"Simple Webdav: dir with same name as repository has incorrect behaviorUsing a default repository named ""default"" for example, creating a directory named ""default"" in that repository will have issues.  The directory will not behave correctly. 

Viewing the dir in Jetty http://localhost:8080/jackrabbit/repository/default/default/ will show the contents of the ""default"" directory.  However, clicking on any of the contents will go to an incorrect URL.  E.g. if a directory named ""test"" was created, then the URL for test will be ""http://localhost:8080/jackrabbit/repository/default/test/""  instead of ""http://localhost:8080/jackrabbit/repository/default/default/test/"".

Notice that there is only one ""default"" in the path provided by jackrabbit.  

This causes the contents of such directories to be inaccessible"
0,Allow random seed to be set in DeleteByPercentTaskNeed this to make index identical on multiple runs.  
1,"DatabaseFileSystem: mysql.ddl works for mysql5 but not mysql 4.1.20Perhaps a new column ( primary key ) could get added to the table called uid, which is actually an md5checksum of FSENTRY_PATH and FSENTRY_NAME."
1,"SingleClientConnectionManager Needs to Recreate UniquePoolEntryDue to the change yesterday of adding some state into DefaultClientConnection (remembering when shutdown was called & aborting the next opening), SingeClientConnectionManager now breaks when subsequent requests are performed if the first one encountered an exception or was aborted.  

Attaching a patch with the fix + a testcase (that previously failed)."
1,"PerFieldCodecWrapper causes crashes if not all per field codes have been usedIf a PerFieldCodecWrapper is used an SegmentMerger tries to merge two segments where one segment only has a subset of the field PerFieldCodecWrapper defines SegmentMerger tries to open non-existing files since Codec#files(Directory, SegmentInfo, Set<String>) blindly copies the expected files into the given set. This also hits exceptions in CheckIndex and addIndexes(). 
The reason for this is that PerFieldCodecWrapper simply iterates over the codecs it knows and adds all files without checking if they are present in the given Directory. We need to have some mechnanism that check if the ""required"" files for a codec are present and only add the files to the set if that field is really there.

"
1,FastVectorHighlighter adds a multi value separator (space) to the end of the highlighted textThe FVH adds an additional ' ' (the multi value separator) to the end of the highlighted text.
1,"NodeState and NodeStateListener deadlock

Java stack information for the threads listed above:
===================================================
""jmssecondaryApplnJobExecutor-8"":
	at org.apache.jackrabbit.core.state.NodeState.getChildNodeEntry(NodeState.java:300)
	- waiting to lock <0x9e6c6d08> (a org.apache.jackrabbit.core.state.NodeState)
	at org.apache.jackrabbit.core.CachingHierarchyManager.nodeModified(CachingHierarchyManager.java:316)
	- locked <0xa09882a8> (a java.lang.Object)
	at org.apache.jackrabbit.core.CachingHierarchyManager.stateModified(CachingHierarchyManager.java:293)
	at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyStateModified(StateChangeDispatcher.java:111)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.stateModified(SessionItemStateManager.java:889)
	at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyStateModified(StateChangeDispatcher.java:111)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.stateModified(LocalItemStateManager.java:452)
	at org.apache.jackrabbit.core.state.XAItemStateManager.stateModified(XAItemStateManager.java:602)
	at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyStateModified(StateChangeDispatcher.java:111)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.stateModified(SharedItemStateManager.java:400)
	at org.apache.jackrabbit.core.state.ItemState.notifyStateUpdated(ItemState.java:244)
	at org.apache.jackrabbit.core.state.ChangeLog.persisted(ChangeLog.java:297)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:749)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:1115)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:351)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:354)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:325)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1111)
	- locked <0x9b1b0be0> (a org.apache.jackrabbit.core.XASessionImpl)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:915)
	at org.apache.jackrabbit.jca.JCASessionHandle.save(JCASessionHandle.java:180)
        ...
	at sun.reflect.GeneratedMethodAccessor1067.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:36)
	at sun.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:243)
	at javax.management.modelmbean.RequiredModelMBean.invokeMethod(RequiredModelMBean.java:1074)
	at javax.management.modelmbean.RequiredModelMBean.invoke(RequiredModelMBean.java:955)
	at org.springframework.jmx.export.SpringModelMBean.invoke(SpringModelMBean.java:88)
	at org.jboss.mx.server.RawDynamicInvoker.invoke(RawDynamicInvoker.java:164)
	at org.jboss.mx.modelmbean.RequiredModelMBeanInvoker.invoke(RequiredModelMBeanInvoker.java:127)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)
	at org.jboss.system.server.jmx.LazyMBeanServer.invoke(LazyMBeanServer.java:291)
	at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:288)
	at $Proxy692.doDiscoveryNow(Unknown Source)
        ...
	at org.springframework.jms.listener.AbstractMessageListenerContainer.doInvokeListener(AbstractMessageListenerContainer.java:531)
	at org.springframework.jms.listener.AbstractMessageListenerContainer.invokeListener(AbstractMessageListenerContainer.java:466)
	at org.springframework.jms.listener.AbstractMessageListenerContainer.doExecuteListener(AbstractMessageListenerContainer.java:435)
	at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.doReceiveAndExecute(AbstractPollingMessageListenerContainer.java:322)
	at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.receiveAndExecute(AbstractPollingMessageListenerContainer.java:260)
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.invokeListener(DefaultMessageListenerContainer.java:944)
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.run(DefaultMessageListenerContainer.java:868)
	at java.lang.Thread.run(Thread.java:619)
""jmssecondaryApplnJobExecutor-7"":
	at org.apache.jackrabbit.core.CachingHierarchyManager.nodeAdded(CachingHierarchyManager.java:362)
	- waiting to lock <0xa09882a8> (a java.lang.Object)
	at org.apache.jackrabbit.core.state.StateChangeDispatcher.notifyNodeAdded(StateChangeDispatcher.java:159)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.nodeAdded(SessionItemStateManager.java:947)
	at org.apache.jackrabbit.core.state.NodeState.notifyNodeAdded(NodeState.java:882)
	at org.apache.jackrabbit.core.state.NodeState.addChildNodeEntry(NodeState.java:351)
	- locked <0x9e6c6d08> (a org.apache.jackrabbit.core.state.NodeState)
	at org.apache.jackrabbit.core.NodeImpl.createChildNode(NodeImpl.java:541)
	- locked <0xa00619a8> (a org.apache.jackrabbit.core.NodeImpl)
	at org.apache.jackrabbit.core.NodeImpl.internalAddChildNode(NodeImpl.java:802)
	at org.apache.jackrabbit.core.NodeImpl.internalAddNode(NodeImpl.java:735)
	at org.apache.jackrabbit.core.NodeImpl.addNodeWithUuid(NodeImpl.java:2200)
	- locked <0xa00619f8> (a org.apache.jackrabbit.core.NodeImpl)
	at org.apache.jackrabbit.core.NodeImpl.addNode(NodeImpl.java:2133)
	- locked <0xa00619f8> (a org.apache.jackrabbit.core.NodeImpl)
        ...
	at sun.reflect.GeneratedMethodAccessor1067.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:36)
	at sun.reflect.GeneratedMethodAccessor110.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:243)
	at javax.management.modelmbean.RequiredModelMBean.invokeMethod(RequiredModelMBean.java:1074)
	at javax.management.modelmbean.RequiredModelMBean.invoke(RequiredModelMBean.java:955)
	at org.springframework.jmx.export.SpringModelMBean.invoke(SpringModelMBean.java:88)
	at org.jboss.mx.server.RawDynamicInvoker.invoke(RawDynamicInvoker.java:164)
	at org.jboss.mx.modelmbean.RequiredModelMBeanInvoker.invoke(RequiredModelMBeanInvoker.java:127)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:659)
	at org.jboss.system.server.jmx.LazyMBeanServer.invoke(LazyMBeanServer.java:291)
	at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:288)
	at $Proxy692.doDiscoveryNow(Unknown Source)
        ...
	at org.springframework.jms.listener.AbstractMessageListenerContainer.doInvokeListener(AbstractMessageListenerContainer.java:531)
	at org.springframework.jms.listener.AbstractMessageListenerContainer.invokeListener(AbstractMessageListenerContainer.java:466)
	at org.springframework.jms.listener.AbstractMessageListenerContainer.doExecuteListener(AbstractMessageListenerContainer.java:435)
	at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.doReceiveAndExecute(AbstractPollingMessageListenerContainer.java:322)
	at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.receiveAndExecute(AbstractPollingMessageListenerContainer.java:260)
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.invokeListener(DefaultMessageListenerContainer.java:944)
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.run(DefaultMessageListenerContainer.java:868)
	at java.lang.Thread.run(Thread.java:619)

Found 1 deadlock.

"
0,"AbstractJCRTest fails on level 1 repositoriesIf a test case indicates that it's not read-only, org.apache.jackrabbit.test.AbstractJCRTest tries to cleanup the test root in the setUp method. This will cause the test case to fail, because a Level 1 repository will throw an UnsupportedOperationException here.

Proposal: before trying the cleanup, check for L2 functionality and throw a NotExecutableException otherwise:

            if (! isSupported(Repository.LEVEL_2_SUPPORTED)) {
              cleanUp();
              String msg = ""Test case requires level 2 functionality"";
              throw new NotExecutableException(msg);
            }
"
0,"javacc on Win32 (cygwin) creates wrong line endings - fix them with 'ant replace'""ant javacc"" in Windows/Cygwin generates files with wrong line endings (\r  or \r\n instead of *Nix's \n). 
I managed to get rid of those using    perl -p -e 's/(\r\n|\n|\r)/\n/g'
Some useful info on line ending issues is in http://en.wikipedia.org/wiki/Newline

After wasting some time to get rid of those, I modified javacc-QueryParser build.xml task to take care of that.
So now QueryParser files created with ""ant javacc"" are fixed (if required) to have \n as line ends.

Should probably do that also for the other javacc targets: javacc-HTMLParser and javacc-StandardAnalyzer(?)
"
0,Optimize first execution queries for DescendantSelfAxisWeight/ChildAxisQueryThe first execution of a query involving DescendantSelfAxisWeight/ChildAxisQuery is slow. Consecutive queries are faster because the hierarchy is cached
1,"Under Heavy load in a Cluster HTTP Threads Block and stall requestsUnder Heavy load created by mounting both nodes in the cluster in OSX Finder and then uploading large numebers of files to each node at the same time ( a few 1000), eventually one of the nodes stops responding and the Finder mount timesout and disconnects.

Once that happens that node becomes unusable.
More mount attempts will prompt for a password indicating HTTP is still running, but will timeout once the connection is authenticated.
Access by the Web Browser will prompt for a password, conenct and provide a once only listing of any collection in the workspace. If you try to refresh that collection, the HTTP request hangs forever."
1,"URI.getHost() generates IllegalArgumentExceptionHi guys,

I don't know if I'm doing something wrong or not but the following code:

   URI uri = new URI(""mailto:eay@cryptsoft.com"", true);
   System.out.println(uri.getHost());

generates the following exception:

java.lang.IllegalArgumentException: Component array of chars may not be null
	at org.apache.commons.httpclient.URI.decode(URI.java:1722)
	at org.apache.commons.httpclient.URI.getHost(URI.java:2780)

Could you help?

Also, I'm sorry I put the report under version ""3.0 Final"" but I couldn't find 
an entry for ""3.0-RC1"" (which I'm using at the moment).

Thanks a lot!

Bisser"
0,"small improvements to DWPTThreadPoolWhile working on another issue I cleaned up DWTPThreadPool a little, fixed some naming issues and fixed some todos... patch is coming soon..."
0,Put JavaDoc resources in src/main/javadocThe Maven javadoc plugin suggests that Javadoc resources like package.html files and doc-files subdirectories should be placed in the src/main/javadoc folder (see http://maven.apache.org/plugins/maven-javadoc-plugin/faq.html). I'll move the javadoc files unless anyone argues otherwise.
0,"Avoid Maven 3 warningsJackrabbit trunk builds fine with the Maven 3 alpha releases, but there are some warnings about deprecated ${pom....} variables and unspecified reporting plugin versions that we might want to fix."
1,"potential infinite loop around InternalVersionImpl.getSuccessorsThere's an infinite loop waiting to happen when the underlying persisted version storage is defect:

{noformat}
at
org.apache.jackrabbit.core.version.InternalVersionImpl.getSuccessors(InternalVersionImpl.java:148)

at
org.apache.jackrabbit.core.version.InternalVersionHistoryImpl.init(InternalVersionHistoryImpl.java:165)

at
org.apache.jackrabbit.core.version.InternalVersionHistoryImpl.reload(InternalVersionHistoryImpl.java:180)

at
org.apache.jackrabbit.core.version.InternalVersionHistoryImpl.getVersion(InternalVersionHistoryImpl.java:299) 
{noformat}

(line numbers from 2.2)

What happens here is that when a version can not be instantiated, reload() is called, which in turn calls init(), which, as part of piece of code labeled ""fix legacy"" will call getSuccessors(), which in turn wants to instantiate versions.

"
0,"Queries with too many asterisks causing 100% CPU usageIf a search query has many adjacent asterisks (e.g. fo**************obar), I can get my webapp caught in a loop that does not seem to end in a reasonable amount of time and may in fact be infinite. For just a few asterisks the query eventually does return some results, but as I add more it takes a longer and longer amount of time. After about six or seven asterisks the query never seems to finish. Even if I abort the search, the thread handling the troublesome query continues running in the background and pinning a CPU.

I found the problem in src/java/org/apache/lucene/search/WildcardTermEnum.java on Lucene 3.0.1 and it looks like 3.0.2 ought to be affected as well. I'm not sure about trunk, though. I have a patch that fixes the problem for me in 3.0.1."
1,"MultiTermsEnum over-shares between different Docs/AndPositionsEnumRobert found this in working on LUCENE-2352.

MultiTermsEnum incorrectly shared sub-enums on two different invocation of .docs/AndPositionsEnum."
0,FastVectorHighlighter: enable FragListBuilder and FragmentsBuilder to be set per-field override
0,"Default retry count three even if documentation says it's fiveThe exception handling documentation (http://hc.apache.org/httpclient-3.x/exception-handling.html) says ""HttpClient will automatically retry up to 5 times those methods..."", but in DefaultHttpMethodRetryHandler  e.g. in trunk (http://svn.apache.org/viewvc/httpcomponents/oac.hc3x/trunk/src/java/org/apache/commons/httpclient/DefaultHttpMethodRetryHandler.java?revision=608014&view=markup) you can see that the retry count is three:

    public DefaultHttpMethodRetryHandler(int retryCount, boolean requestSentRetryEnabled) {
        super();
        this.retryCount = retryCount;
        this.requestSentRetryEnabled = requestSentRetryEnabled;
    }
    
    /**
     * Creates a new DefaultHttpMethodRetryHandler that retries up to 3 times
     * but does not retry methods that have successfully sent their requests.
     */
    public DefaultHttpMethodRetryHandler() {
        this(3, false);
    }"
0,"Unit tests do not fail if a ConcurrentMergeScheduler thread hits an exceptionNow that CMS is the default, it's important to fail any unit test that
hits an exception in a CMS thread.  But they do not fail now.  The
preferred solution (thanks to Erik Hatcher) is to fix all Lucene unit
tests to subclass from a new LuceneTestCase (in o.a.l.util) base that
asserts that there were no such exceptions during the test.
"
0,"Improve random testingWe have quite a few random tests, but there's no way to ""crank"" them.

The idea here is to add a multiplier which can be increased by a sysprop. For example, we could set this to something higher than 1 for hudson."
0,"Incorrect/incomplete product name in META-INF/NOTICE fileThe NOTICE file in the HttpClient jars is incorrect.

It states:

=====

HttpClient
Copyright 1999-2009 Apache Software Foundation
<snip/>
======

The leading blank line should be deleted, and ""HttpClient"" should be ""Apache HttpComponents Client - HttpClient""  (or similar) as is the case for the source archive.

Similarly for HttpMime"
0,"Make tests using java.util.Random reproducible on failureThis is a patch for LuceneTestCase to support logging of the Random seed used in randomized tests. The patch also includes an example implementation in TestTrieRangeQuery.

It overrides the protected method runTest() and inserts a try-catch around the super.runTest() call. Two new methods newRandom() and newRandom(long) are available for the test case. As each test case is run in an own TestCase object instance (so 5 test methods in a class instantiate 5 instances each method working in separate), the random seed is saved on newRandom() and when the test fails with any Throwable, a message with the seed (if not null) is printed out. If newRandom was never called no message will be printed.

This patch has only one problem: If a single test method calls newRandom() more than once, only the last seed is saved and printed out. But each test method in a Testcase should call newRandom() exactly once for usage during the execution of this test method. And it is not thread save (no sync, no volatile), but for tests it's unimportant.

I forgot to mention: If a test fails, the message using the seed is printed to stdout. The developer can then change the test temporarily:

{code}LuceneTestCase.newRandom() -> LuceneTestCase.newRandom(long){code}

using the seed from the failed test printout.

*Reference:*
{quote}
: By allowing Random to randomly seed itself, we effectively test a much
: much larger space, ie every time we all run the test, it's different.  We can
: potentially cast a much larger net than a fixed seed.

i guess i'm just in favor of less randomness and more iterations.

: Fixing the bug is the ""easy"" part; discovering a bug is present is where
: we need all the help we can get ;)

yes, but knowing a bug is there w/o having any idea what it is or how to 
trigger it can be very frustrating.

it would be enough for tests to pick a random number, log it, and then use 
it as the seed ... that way if you get a failure you at least know what 
seed was used and you can then hardcode it temporarily to reproduce/debug

-Hoss
{quote}"
0,"Item states cached in UpdatableItemStateManager not discarded on logoutThe SessionItemStateManager disposes only the TransientItemStateManager but not the UpdatableItemStateManager. The latter doesn't release the resources and the cached items keep listening the overlayed state events until the ReferenceMap do its magic. According to my tests in certain circumstances it slows down jackrabbit very much. 

In the charts you can see the time it takes to jackrabbit to save each child node. The first test uses a single session and the second creates a new session for each added node.

The patch I attach makes both tests take the same time. Opinions?

05-08-16-one session.GIF
/ login
  / loop
    / add node
    / save
/ logout


05-08-16-one session per save.GIF
/ loop
  / login
  / add node
  / save
  / logout

"
0,Remove deprecated classes in jackrabbit-webdav and the corresponding impls in jcr-server
1,"Lock.getNode() does not return lock holderspec: "" N.getLock().getNode() (where N is a locked node) ... If N is in the subtree of the lock holder, H, then this call will return H.""

now N is returned."
1,"MultiPhraseQuery sums its own idf instead of Similarity.MultiPhraseQuery is a generalized version of PhraseQuery, and computes IDF the same way by default (by summing across the terms).

The problem is it doesn't let the Similarity do this: PhraseQuery calls Similarity.idfExplain(Collection<Term> terms, IndexSearcher searcher),
but MultiPhraseQuery just sums itself, calling Similarity.idf(int, int) for each term.

"
0,"JCAManagedConnectionFactory should chain cause exceptionIn JCAManagedConnectionFactory, methods openSession and createRepository both throw ResourceException without setting the cause exception.  This can result in the actual error being swallowed silently, and only stepping through the running code at this point will reveal the actual error (eg: Persistent Store configuration error will appear as a pool exception).

Jukka Zitting on 12-Oct-2010 said:

This constructor is not available in J2EE version 1.3, so for now
we've been using the ResourceException.setLinkedException() method for
this (see JCR-761). To address your need we could either upgrade the
platform requirement to J2EE 1.4 or start using the J2SE method
Exception.initCause() instead of setLinkedException(). Can you file an
improvement issue in Jira about this?"
0,"Choose a specific Directory implementation running the CheckIndex mainIt should be possible to choose a specific Directory implementation to use during the CheckIndex process when we run it from its main.
What about an additional main parameter?
In fact, I'm experiencing some problems with MMapDirectory working with a big segment, and after some failed attempts playing with maxChunkSize, I decided to switch to another FSDirectory implementation but I needed to do that on my own main.
Should we also consider to use a FileSwitchDirectory?
I'm willing to contribute, could you please let me know your thoughts about it?"
1,"ConcurrentModificationException in IndexMergerThe IndexMerger.start() method can cause the following ConcurrentModificationException to be thrown since it doesn't protect against concurrent access to the busyMergers list. The workers started by the start() method will remove themselves from the busyMergers list, which makes it possible for a quick worker to concurrently modify the list before the start() method is finished iterating through it.

java.util.ConcurrentModificationException
	at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
	at java.util.AbstractList$Itr.next(AbstractList.java:343)
	at org.apache.jackrabbit.core.query.lucene.IndexMerger.start(IndexMerger.java:122)
	at org.apache.jackrabbit.core.query.lucene.MultiIndex.<init>(MultiIndex.java:325)
	at org.apache.jackrabbit.core.query.lucene.SearchIndex.doInit(SearchIndex.java:507)
	at org.apache.jackrabbit.core.query.AbstractQueryHandler.init(AbstractQueryHandler.java:78)
	at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$1.getQueryHandler(RepositoryConfigurationParser.java:630)
	at org.apache.jackrabbit.core.config.WorkspaceConfig.getQueryHandler(WorkspaceConfig.java:215)
	at org.apache.jackrabbit.core.config.WorkspaceConfig.getQueryHandler(WorkspaceConfig.java:215)
	at org.apache.jackrabbit.core.SearchManager.<init>(SearchManager.java:173)
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getSearchManager(RepositoryImpl.java:1868)
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.doPostInitialize(RepositoryImpl.java:2077)
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.initialize(RepositoryImpl.java:1996)
	at org.apache.jackrabbit.core.RepositoryImpl.initStartupWorkspaces(RepositoryImpl.java:535)
"
0,Add a filtered RangeIteratorIt would be useful to have a FilteredRangeIterator utility class that can be used to apply arbitrary filters on other RangeIterators.
0,"Distinct field value count per groupSupport a second pass collector that counts unique field values of a field per group.
This is just one example of group statistics that one might want."
0,"Change log level in UserManagerImpl#getAuthorizable(NodeImpl) and UserImporter#handlePropInfoThis is current implementation:

Authorizable getAuthorizable(NodeImpl n) throws RepositoryException {
        Authorizable authorz = null;
        if (n != null) {
            String path = n.getPath();
            if (n.isNodeType(NT_REP_USER) && Text.isDescendant(usersPath, path)) {
                authorz = createUser(n);
            } else if (n.isNodeType(NT_REP_GROUP) && Text.isDescendant(groupsPath, path)) {
                authorz = createGroup(n);
            } else {
                /* else some other node type or outside of the valid user/group
                   hierarchy  -> return null. */
                log.debug(""Unexpected user nodetype "" + n.getPrimaryNodeType().getName());
            }
        } /* else no matching node -> return null */
        return authorz;
    }


It seems that 'else' branch can be improved, at least by increasing log level. But I think, that best way is to throw exception.
Current message can also be misleading, in case when user type is correct but check Text.isDescendant fails.

Above method is called from within UserImporter#handlePropInfo

...
Authorizable a = userManager.getAuthorizable(parent);
if (a == null) {
     log.debug(""Cannot handle protected PropInfo "" + protectedPropInfo + "". Node "" + parent + "" doesn't represent a valid Authorizable."");
     return false;
} 
....

Here again log level is debug. Because at this point we have return statement, property 'principalName' is not set, and if we try to save session following exception will be thrown:

javax.jcr.nodetype.ConstraintViolationException: /home/public/users/b/bb2: mandatory property {internal}password does not exist
     at org.apache.jackrabbit.core.ItemSaveOperation.validateTransientItems(ItemSaveOperation.java:537)
     at org.apache.jackrabbit.core.ItemSaveOperation.perform(ItemSaveOperation.java:216)
     at org.apache.jackrabbit.core.session.SessionState.perform(SessionState.java:200)
     at org.apache.jackrabbit.core.ItemImpl.perform(ItemImpl.java:91)
     at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:329)
    ...
 

So if the log level is not set to 'debug' it is not obvious why mentioned property is missing. Use case and root cause is that 'path' (/home/public/users/b/bb2)  is not descendant of 'usersPath' (/home/users).

Regards,
Miroslav"
1,"null domains break Cookie.javathe domain is assumed to be non-null in a few places in Cookie.java, see matches
method, for example"
0,"[PATCH] setIndexInterval() in IndexWriterFollowing a discussion with Doug (see
http://article.gmane.org/gmane.comp.jakarta.lucene.devel/5804) here is a patch
that add a setIndexInterval() in IndexWriter.

This patch adds also a getDirectory method to IndexWriter and modifies 
SegmentMerger, IndexWriter and TermInfosWriter.

This patch passes all tests.

Any comments/criticisms welcome.

Julien"
0,"Norm codec strategy in SimilarityThe static span and resolution of the 8 bit norms codec might not fit with all applications. 

My use case requires that 100f-250f is discretized in 60 bags instead of the default.. 10?
"
0,"Move SmartChineseAnalyzer & resources to own contrib projectSmartChineseAnalyzer depends on  a large dictionary that causes the analyzer jar to grow up to 3MB. The dictionary is quite big compared to all the other resouces / class files contained in that jar. 
Having a separate analyzer-cn contrib project enables footprint-sensitive users (e.g. using lucene on a mobile phone) to include analyzer.jar without getting into trouble with disk space.

Moving SmartChineseAnalyzer to a separate project could also include a small refactoring as Robert mentioned in [LUCENE-1722|https://issues.apache.org/jira/browse/LUCENE-1722] several classes should be package protected, members and classes could be final, commented syserr and logging code should be removed etc.

I set this issue target to 2.9 - if we can not make it until then feel free to move it to 3.0
"
0,"Completeness/Freshness of NamespaceRegistry and NodeTypeRegistryWe need to define the requirements on completeness and freshness of RepositoryService.getRegisteredNamespaces().

Right now the optimistic assumption seems to be that an SPI provider is able to report all namespaces that can occur in a repository beforehand. Even if it can do that (and I know of potential targets for SPI that simply can't), this seems to be quite a waste of time if these namespace prefixes aren't actually used later on.

Furthermore, in SPI namespace prefixes aren't really relevant, except to enable the transient layer to return ""meaningful"" prefixes instead of automatically generated ones.

Therefore my propoal would be to:

1) Clarify that the Map returned from getRegisteredNamespaces() isn't required to be complete,

2) Enhance JCR2SPI to auto-generate prefixes when it encounters namespaces not in the registry.

I expect this to also affect RepositoryService.(un)registerNamespace(...), but let's discuss the underlying issue first...

"
1,"PROPPATCH on collection gets 403 ForbiddenDefaultHandler.canImport(PropertyImportContext, boolean) prevents setting properties (PROPPATCH) on collections through WebDAV ... returns 403 Forbidden. It checks to see whether the contextItem is not a collection, or has a jcr:content node. This test fails for a collection and should probably allow collections or nodes that have a jcr:content subnode. Here is a patch for the change

Index: jackrabbit-jcr-server/src/main/java/org/apache/jackrabbit/server/io/DefaultHandler.java
===================================================================
--- jackrabbit-jcr-server/src/main/java/org/apache/jackrabbit/server/io/DefaultHandler.java	(revision 567695)
+++ jackrabbit-jcr-server/src/main/java/org/apache/jackrabbit/server/io/DefaultHandler.java	(working copy)
@@ -570,7 +570,7 @@
         }
         Item contextItem = context.getImportRoot();
         try {
-            return contextItem != null && contextItem.isNode() && (!isCollection || ((Node)contextItem).hasNode(JcrConstants.JCR_CONTENT));
+            return contextItem != null && contextItem.isNode() && (isCollection || ((Node)contextItem).hasNode(JcrConstants.JCR_CONTENT));
         } catch (RepositoryException e) {
             log.error(""Unexpected error: "" + e.getMessage());
             return false;
"
1,"JCR-RMI problem with large binary valuesAs reported on the mailing list, a JCR-RMI connection will hang when given a binary value that's larger than the default 64kB buffer size."
0,"CachingHttpClient returns a 503 response when the backend HttpClient produces an IOExceptionThe CachingHttpClient returns an HTTP 503 response when the backend HttpClient throws an IOException.

It happens for instance when the backend is DefaultHttpClient (AbstractHttpClient), issuing a request to a server not listening on the target port.
Well, it sounds tricky, but it makes the HttpClient not having a consistant behaviour in an implementation using both caching and regular clients.

If a 503 should really be returned in that case, I suggest the AbstractHttpClient to return it and the CachingHttpClient to just propagate any exception thrown by the backend.
"
0,"Add InputStream buffering.Currently HttpClient does not buffer the InputStream received from the socket. 
Perhaps doing so would improve performance.

Reported by Tony Bigbee."
0,"HyphenationCompoundWordTokenFilter fails to load DTD in Crimson parser (JDK 1.4)HyphenationCompoundWordTokenFilter loads the DTD in its XML parser from memory by supplying EntityResolver. In Java 1.4 (affects Lucene 2.9, but also later versions if not Apache Xerces is used as XML parser) this does not work, because Cromson does not even ask the entity resolver, if no base URI is known. As the hyphenation file is loaded from Reader/InputStream no base URI is known. Crimson needs at least a non-null systemId to proceed.

This patch (Lucene 2.9 only)  fakes this by supplying a fake systemId to the InputSource."
1,"FuzzyQuery produces a ""java.lang.NegativeArraySizeException"" in PriorityQueue.initialize if I use Integer.MAX_VALUE as BooleanQuery.MaxClauseCountPriorityQueue creates an ""java.lang.NegativeArraySizeException"" when initialized with Integer.MAX_VALUE, because Integer overflows. I think this could be a general problem with PriorityQueue. The Error occured when I set BooleanQuery.MaxClauseCount to Integer.MAX_VALUE and user a FuzzyQuery for searching."
0,"generate-maven-artifacts target should include all non-Mavenized Lucene & Solr dependenciesCurrently, in addition to deploying artifacts for all of the Lucene and Solr modules to a repository (by default local), the {{generate-maven-artifacts}} target also deploys artifacts for the following non-Mavenized Solr dependencies (lucene_solr_3_1 version given here):

# {{solr/lib/commons-csv-1.0-SNAPSHOT-r966014.jar}} as org.apache.solr:solr-commons-csv:3.1
# {{solr/lib/apache-solr-noggit-r944541.jar}} as org.apache.solr:solr-noggit:3.1
\\ \\
The following {{.jar}}'s should be added to the above list (lucene_solr_3_1 version given here):
\\ \\
# {{lucene/contrib/icu/lib/icu4j-4_6.jar}}
# {{lucene/contrib/benchmark/lib/xercesImpl-2.9.1-patched-XERCESJ}}{{-1257.jar}}
# {{solr/contrib/clustering/lib/carrot2-core-3.4.2.jar}}**
# {{solr/contrib/uima/lib/uima-an-alchemy.jar}}
# {{solr/contrib/uima/lib/uima-an-calais.jar}}
# {{solr/contrib/uima/lib/uima-an-tagger.jar}}
# {{solr/contrib/uima/lib/uima-an-wst.jar}}
# {{solr/contrib/uima/lib/uima-core.jar}}
\\ \\
I think it makes sense to follow the same model as the current non-Mavenized dependencies:
\\ \\
* {{groupId}} = {{org.apache.solr/.lucene}}
* {{artifactId}} = {{solr-/lucene-}}<original-name>,
* {{version}} = <lucene-solr-release-version>.

**The carrot2-core jar doesn't need to be included in trunk's release artifacts, since there already is a Mavenized Java6-compiled jar.  branch_3x and lucene_solr_3_1 will need this Solr-specific Java5-compiled maven artifact, though."
0,"Lucene's nightly Hudson builds don't have svn version in MANIFEST.MFSolr had the same issue but apparently made a configuration change to the Hudson configuration to get it working:

    https://issues.apache.org/jira/browse/SOLR-684

Also I opened this INFRA issue:

    https://issues.apache.org/jira/browse/INFRA-1721

which says the svnversion exe is located in this path:

    /opt/subversion-current/bin

In that INRA issue, /etc/init.d/tomcat was also fixed in theory so that svnversion would be on the path the next time Hudson is restarted.  Still, in case that doesn't work, or it changes in the future, it seems a good idea to make the same change that Solr made to Lucene's Hudson configuration.

Hoss can you detail what you needed to do for Solr?  Or maybe just do it also for Lucene ;)  Thanks!"
1,"UUID check in BundleFsPersistenceManager.getListRecursive() leads to endless loopThe UUID comparison in getListRecursive() is wrong and leads to an endless loop when the test PersistenceManagerIteratorTest.getAllNodeIds() is run on a workspace using BundleFsPersistenceManager.

I'm not sure this always happens, but for sure in a workspace with no content (just root and jcr:system nodes).

There's also an problem with the test case. In batch mode the after NodeId is set to the last id returned by the previous get all nodes fetch. This means batch retrieval is never actually tested, because there is no NodeId after the last one."
1,"Query for all node fails after restartThe query handler initially indexes the node type definitions exposed at /jcr:system/jcr:nodetypes. However after a restart or a node type registration the UUIDs of those nodes will change because they consist of VirtualNodeStates. The index will still use the UUIDs of the initial indexing and will return a query result that refers to UUIDs that do not exist in the workspace anymore.

As an short term solution we should disable indexing of VirtualNodeStates.

Please note, this does not only apply to xpath queries but also sql queries of course."
1,"[jcr-rmi] workspace.copy doesn't workswapped parameters in org/apache/jackrabbit/rmi/client/ClientWorkspace/ClientWorkspace. 
"
0,"application-defined routesAllow applications to specifiy a route as request parameter (or in the context).
This functionality is a replacement for RoutedRequest, which is removed by HTTPCLIENT-715."
0,IndexReader.FieldOption has incomplete JavadocsIndexReader.FieldOption has no javadocs at all.
0,"GData Server - Milestone 3 Patch, Bugfixes, DocumentationFor Milestone 3 added Features:

- Update Delete Concurrency
- Version control
- Second storage impl. based on Db4o. (Distributed Storage)
- moved all configuration in one single config file.
- removed dependencies in testcases.
- added schema validation for and all  xml files in the project (Configuration etc.)
- added JavaDoc
- much better Performance after reusing some resources
- added recovering component to lucene based storage to recover entries after a server crash or OOM Error (very simple)

- solved test case fail on hyperthread / multi core machines (@ hossman: give it a go)

@Yonik && Doug could you get that stuff in the svn please

regards simon

"
1,"incorrect snippet returned with SpanScorerThis problem was reported by my customer. They are using Solr 1.3 and uni-gram, but it can be reproduced with Lucene 2.9 and WhitespaceAnalyzer.

{panel:title=Query}
(f1:""a b c d"" OR f2:""a b c d"") AND (f1:""b c g"" OR f2:""b c g"")
{panel}

The snippet we expected is:
{panel}
x y z <B>a</B> <B>b</B> <B>c</B> <B>d</B> e f g <B>b</B> <B>c</B> <B>g</B>
{panel}

but we got:
{panel}
x y z <B>a</B> b c <B>d</B> e f g <B>b</B> <B>c</B> <B>g</B>
{panel}

Program to reproduce the problem:
{code}
public class TestHighlighter {

  static final String CONTENT = ""x y z a b c d e f g b c g"";
  static final String PH1 = ""\""a b c d\"""";
  static final String PH2 = ""\""b c g\"""";
  static final String F1 = ""f1"";
  static final String F2 = ""f2"";
  static final String F1C = F1 + "":"";
  static final String F2C = F2 + "":"";
  static final String QUERY_STRING =
    ""("" + F1C + PH1 + "" OR "" + F2C + PH1 + "") AND (""
    + F1C + PH2 + "" OR "" + F2C + PH2 + "")"";
  static Analyzer analyzer = new WhitespaceAnalyzer();
  
  public static void main(String[] args) throws Exception {
    QueryParser qp = new QueryParser( F1, analyzer );
    Query query = qp.parse( QUERY_STRING );
    CachingTokenFilter stream = new CachingTokenFilter( analyzer.tokenStream( F1, new StringReader( CONTENT ) ) );
    Scorer scorer = new SpanScorer( query, F1, stream, false );
    Highlighter h = new Highlighter( scorer );
    System.out.println( ""query : "" + QUERY_STRING );
    System.out.println( h.getBestFragment( analyzer, F1,  CONTENT ) );
  }
}
{code}
"
0,"Remove unused LuceneQueryBuilder.createQuery() methodThe following method is not used anymore in Jackrabbit and can be removed:

    public static Query createQuery(QueryRootNode root,
                                    SessionImpl session,
                                    ItemStateManager sharedItemMgr,
                                    NamespaceMappings nsMappings,
                                    Analyzer analyzer,
                                    PropertyTypeRegistry propReg)
            throws RepositoryException;"
0,"Convert PrecedenceQueryParser to new TokenStream APIAdriano Crestani provided a patch, that updates the PQP to use the new TokenStream API...all tests still pass. 
I hope this helps to keep the PQP 
"
0,"Chunked transfer encoding not isolated from application.Chunked transfer encoding is not being supported transparently by the
HttpMethodBase object, causing chunk data to be embedded in response body data
and forcing the application to handle the HTTP/1.1 implementation of chunked
transfer encoding.

The included patch now properly parses chunk data as per RFC 2068 and provides
body content consistently, regardless of whether chunked transfer encoding was
used by the server or not. This relieves the application from the requirement of
implementing RFC 2068.

Patch sent to mailing list as per guidelines to address this deficiency."
1,"Merge error during add to index (IndexOutOfBoundsException)I've been batch-building indexes, and I've build a couple hundred indexes with 
a total of around 150 million records.  This only happened once, so it's 
probably impossible to reproduce, but anyway... I was building an index with 
around 9.6 million records, and towards the end I got this:

java.lang.IndexOutOfBoundsException: Index: 54, Size: 24
        at java.util.ArrayList.RangeCheck(ArrayList.java:547)
        at java.util.ArrayList.get(ArrayList.java:322)
        at org.apache.lucene.index.FieldInfos.fieldInfo(FieldInfos.java:155)
        at org.apache.lucene.index.FieldInfos.fieldName(FieldInfos.java:151)
        at org.apache.lucene.index.SegmentTermEnum.readTerm(SegmentTermEnum.java
:149)
        at org.apache.lucene.index.SegmentTermEnum.next
(SegmentTermEnum.java:115)
        at org.apache.lucene.index.SegmentMergeInfo.next
(SegmentMergeInfo.java:52)
        at org.apache.lucene.index.SegmentMerger.mergeTermInfos
(SegmentMerger.java:294)
        at org.apache.lucene.index.SegmentMerger.mergeTerms
(SegmentMerger.java:254)
        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:93)
        at org.apache.lucene.index.IndexWriter.mergeSegments
(IndexWriter.java:487)
        at org.apache.lucene.index.IndexWriter.maybeMergeSegments
(IndexWriter.java:458)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:310)
        at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:294)"
0,"rename jcr-browser contrib projectThere's a project called jcr-browser at sourceforge, it's a desktop browser mantained by sandro boehme. I'll rename the contrib project to jcr-navigator unless someone proposes a better name :). "
1,"URLEncodedUtils fails to parse form-url-encoded entities that specify a charsetIf a form-url-encoded HTTP entity specifies a charset in its Content-Type header, then URLEncodedUtils.parse(HttpEntity) fails to parse it.

An entity with content type ""application/x-www-form-urlencoded; charset=UTF-8"" should be detected as form-url-encoded and parsed as such, honoring the specified character set. Currently the code requires an exact, case-insensitive match with ""application/x-www-form-urlencoded"" for an entity to be detected as form-url-encoded.

It appears that the author of URLEncodedUtils.parse(HttpEntity) tried to take character sets into account, but expected to find them in the Content-Encoding header instead of as a parameter in the Content-Length header. The HTTP 1.1 spec makes it clear that the Content-Encoding header is for specifying transformations like gzip compression or the identity transformation -- not for specifying the entity's character set.

Here are some helpful links.
http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.4
http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.5
http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.11

This is related to: https://issues.apache.org/jira/browse/HTTPCLIENT-884"
1,"Workspace.getImportHandler() doesn't handle namespace declarations in document view when they are reported as attributesXMIDocumentViewImportTest is copy of DocumentViewImportTest EXCEPT that createSimpleDocument is overridden.

New simple document is typical of XMI serializations from Eclipse Modeling Framework (EMF).

Four out of eight tests fail due to bad uri    Trace below:

javax.jcr.NamespaceException: www.apache.org/jackrabbit/test/namespaceImportTest7: is not a registered namespace uri.
	at org.apache.jackrabbit.core.NamespaceRegistryImpl.getPrefix(NamespaceRegistryImpl.java:378)
	at org.apache.jackrabbit.core.LocalNamespaceMappings.getPrefix(LocalNamespaceMappings.java:193)
	at org.apache.jackrabbit.core.SessionImpl.getNamespacePrefix(SessionImpl.java:1307)
	at org.apache.jackrabbit.test.api.XMIDocumentViewImportTest.checkImportSimpleXMLTree(XMIDocumentViewImportTest.java:176)
	at org.apache.jackrabbit.test.api.XMIDocumentViewImportTest.performTests(XMIDocumentViewImportTest.java:154)
	at org.apache.jackrabbit.test.api.XMIDocumentViewImportTest.doTestImportXML(XMIDocumentViewImportTest.java:119)
	at org.apache.jackrabbit.test.api.XMIDocumentViewImportTest.testWorkspaceImportXml(XMIDocumentViewImportTest.java:70)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:393)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)

"
0,"add new snowball languagesSnowball added new languages. This patch adds support for them.

http://snowball.tartarus.org/algorithms/armenian/stemmer.html
http://snowball.tartarus.org/algorithms/catalan/stemmer.html
http://snowball.tartarus.org/algorithms/basque/stemmer.html
"
0,"StopFilter should have option to incr positionIncrement after stop wordI've seen this come up on the mailing list a few times in the last month, so i'm filing a known bug/improvement arround it...

StopFilter should have an option that if set, records how many stop words are ""skipped"" in a row, and then sets that value as the positionIncrement on the ""next"" token that StopFilter does return."
0,"3.0 not compile-time compatible with 2.0 library usageTo my surprise Oleg says this was the intent, yet the Jakarta-Slide webdavclient
libraries do not compile out of the box.  Patch for that issue to follow."
0,JSR 283: NodeType Management
0,"TCK: testSaveMovedRefNodeNodeUUIDTest.testSaveMovedRefNode
SessionUUIDTest.testSaveMovedRefNode

makes the assumption, that moving a referenceable node with session A is visible as 'move' operation within session B as well."
0,"[patch] Fix overly specific casting in coreseveral places in core, casts are made to overly concrete classes when, interfaces are only needed. Doing so ties the algorithms to specific implementations, unnecessarily. patch fixes these."
0,"CharArraySet.clear()I needed CharArraySet.clear() for something I was working on in Solr in a tokenstream.

instead I ended up using CharArrayMap<Boolean> because it supported .clear()

it would be better to use a set though, currently it will throw UOE for .clear() because AbstractSet will call iterator.remove() which throws UOE.

In Solr, the very similar CharArrayMap.clear() looks like this:
{code}
  @Override
  public void clear() {
    count = 0;
    Arrays.fill(keys,null);
    Arrays.fill(values,null);
  }
{code}

I think we can do a similar thing as long as we throw UOE for the UnmodifiableCharArraySet

will submit a patch later tonight (unless someone is bored and has nothing better to do)"
0,EventFilterImpl should implement toStringThis would simplify logging and debugging.
1,"addIndexes(IndexReader) incorrectly applies existing deletesIf you perform these operations:
# deleteDocuments(Term) for all the new documents
# addIndexes(IndexReader)
# commit

Then addIndexes applies the previous deletes on the incoming indexes as well, which is incorrect. If you call addIndexes(Directory) instead, the deletes are applied beforehand, as they should. The solution, as Mike indicated here: http://osdir.com/ml/general/2011-03/msg20876.html is to add *flush(false,true)* to addIndexes(IndexReader).

I will create a patch with a matching unit test shortly."
0,"Add support for type whitelist in TypeTokenFilterA usual use case for TypeTokenFilter is allowing only a set of token types. That is, listing allowed types, instead of filtered ones. I'm attaching a patch to add a useWhitelist option for that."
0,"Please add maven-notice-plugin to gump-trunkHi,

the httpclient build currently fails in Gump because it cannot find the maven-notice-plugin.  We could grab it from http://svn.apache.org/repos/asf/httpcomponents/maven-notice-plugin/trunk/ but since you have a directory with externals just for Gump it would be a lot easier if you added an external for it as well.

Thanks

Stefan"
1,"NodeType.canSetProperty() does not include type conversionfor example, NodeType.canSetProperty(String propertyName, Value value) must return true if the property requires a StringValue and value is a DateValue (but does not)"
1,In case of ConnectTimeoutException : HttpRequestRetryHandler is not used.
0,"Improve automaton's MinimizeOperations.minimizeHopcroft() to not create so many objectsMinimizeOperations.minimizeHopcroft() creates a lot of objects because of strange arrays and useless ArrayLists with fixed length. E.g. it created List<List<List<>>>. This patch minimizes this and makes the whole method much more GC friendler by using simple arrays or avoiding empty LinkedLists at all (inside reverse array). 

minimize() is called very very often, especially in tests (MockAnalyzer).

A test for the method is prepared by Robert, we found a bug somewhere else in automaton, so this is pending until his issue and fix arrives."
0,"Flexibility to turn on/off any flush triggersSee discussion at http://www.gossamer-threads.com/lists/lucene/java-dev/53186

Provide the flexibility to turn on/off any flush triggers - ramBufferSize, maxBufferedDocs and maxBufferedDeleteTerms. One of ramBufferSize and maxBufferedDocs must be enabled."
0,"Consolidate CustomScoreQuery, ValueSourceQuery and BoostedQuery Lucene's CustomScoreQuery and Solr's BoostedQuery do essentially the same thing: they boost the scores of Documents by the value from a ValueSource.  BoostedQuery does this in a direct fashion, by accepting a ValueSource. CustomScoreQuery on the other hand, accepts a series of ValueSourceQuerys.  ValueSourceQuery seems to do exactly the same thing as FunctionQuery.

With Lucene's ValueSource being deprecated / removed, we need to resolve these dependencies and simplify the code.

Therefore I recommend we do the following things:

- Move CustomScoreQuery (and CustomScoreProvider) to the new Queries module and change it over to use FunctionQuerys instead of ValueSourceQuerys.  
- Deprecate Solr's BoostedQuery in favour of the new CustomScoreQuery.  CSQ provides a lot of support for customizing the scoring process.
- Move and consolidate all tests of CSQ and BoostedQuery, to the Queries module and have them test CSQ instead."
0,"Rewrite TrieRange to use MultiTermQueryIssue for discussion here: http://www.lucidimagination.com/search/document/46a548a79ae9c809/move_trierange_to_core_module_and_integration_issues

This patch is a rewrite of TrieRange using MultiTermQuery like all other core queries. This should make TrieRange identical in functionality to core range queries."
1,Disallow unregistering of node types still (possibly) in use
0,"Separate javadocs for core and contribsA while ago we had a discussion on java-dev about separating the javadocs
for the contrib modules instead of having only one big javadoc containing 
the core and contrib classes.

This patch:
* Adds new targets to build.xml: 
  ** ""javadocs-all"" Generates Javadocs for the core, demo, and contrib 
    classes
  ** ""javadocs-core"" Generates Javadocs for the core classes
  ** ""javadocs-demo"" Generates Javadocs for the demo classes
  ** ""javadocs-contrib"" Using contrib-crawl it generates the Javadocs for 
    all contrib modules, except ""similarity"" (currently empty) and gdata.
* Adds submenues to the Javadocs link on the Lucene site with links to
  the different javadocs
* Includes the javadocs in the maven artifacts

Remarks:
- I removed the ant target ""javadocs-internal"", because I didn't want to
  add corresponding targets for all new javadocs target. Instead I 
  defined a new property ""javadoc.access"", so now  
  ""ant -Djavadoc.access=package"" can be used in combination with any of
  the javadocs targets. Is this ok?
- I didn't include gdata (yet) because it uses build files that don't 
  extend Lucenes standard build files.
  
Here's a preview:
http://people.apache.org/~buschmi/site-preview/index.html

Please let me know what you think about these changes!"
0,"TextFilterService uses Sun specific classesThe TextFilterService uses the Sun specific and actually undocumented class sun.misc.Service class to lookup TextFilter implementations. This approach will not work on all JVM implementations.

The Service should rather use javax.imageio.spi.ServiceRegistry, which is part of the standard J2SE API."
0,"Consider making HostConfiguration immutableHostConfiguration class should be immutable. This should also allow methods of this class to be non-synchronized.

Oleg"
0,"PayloadsThis patch adds the possibility to store arbitrary metadata (payloads) together with each position of a term in its posting lists. A while ago this was discussed on the dev mailing list, where I proposed an initial design. This patch has a much improved design with modifications, that make this new feature easier to use and more efficient.

A payload is an array of bytes that can be stored inline in the ProxFile (.prx). Therefore this patch provides low-level APIs to simply store and retrieve byte arrays in the posting lists in an efficient way. 

API and Usage
------------------------------   
The new class index.Payload is basically just a wrapper around a byte[] array together with int variables for offset and length. So a user does not have to create a byte array for every payload, but can rather allocate one array for all payloads of a document and provide offset and length information. This reduces object allocations on the application side.

In order to store payloads in the posting lists one has to provide a TokenStream or TokenFilter that produces Tokens with payloads. I added the following two methods to the Token class:
  /** Sets this Token's payload. */
  public void setPayload(Payload payload);
  
  /** Returns this Token's payload. */
  public Payload getPayload();

In order to retrieve the data from the index the interface TermPositions now offers two new methods:
  /** Returns the payload length of the current term position.
   *  This is invalid until {@link #nextPosition()} is called for
   *  the first time.
   * 
   * @return length of the current payload in number of bytes
   */
  int getPayloadLength();
  
  /** Returns the payload data of the current term position.
   * This is invalid until {@link #nextPosition()} is called for
   * the first time.
   * This method must not be called more than once after each call
   * of {@link #nextPosition()}. However, payloads are loaded lazily,
   * so if the payload data for the current position is not needed,
   * this method may not be called at all for performance reasons.
   * 
   * @param data the array into which the data of this payload is to be
   *             stored, if it is big enough; otherwise, a new byte[] array
   *             is allocated for this purpose. 
   * @param offset the offset in the array into which the data of this payload
   *               is to be stored.
   * @return a byte[] array containing the data of this payload
   * @throws IOException
   */
  byte[] getPayload(byte[] data, int offset) throws IOException;

Furthermore, this patch indroduces the new method IndexOutput.writeBytes(byte[] b, int offset, int length). So far there was only a writeBytes()-method without an offset argument. 

Implementation details
------------------------------
- One field bit in FieldInfos is used to indicate if payloads are enabled for a field. The user does not have to enable payloads for a field, this is done automatically:
   * The DocumentWriter enables payloads for a field, if one ore more Tokens carry payloads.
   * The SegmentMerger enables payloads for a field during a merge, if payloads are enabled for that field in one or more segments.
- Backwards compatible: If payloads are not used, then the formats of the ProxFile and FreqFile don't change
- Payloads are stored inline in the posting list of a term in the ProxFile. A payload of a term occurrence is stored right after its PositionDelta.
- Same-length compression: If payloads are enabled for a field, then the PositionDelta is shifted one bit. The lowest bit is used to indicate whether the length of the following payload is stored explicitly. If not, i. e. the bit is false, then the payload has the same length as the payload of the previous term occurrence.
- In order to support skipping on the ProxFile the length of the payload at every skip point has to be known. Therefore the payload length is also stored in the skip list located in the FreqFile. Here the same-length compression is also used: The lowest bit of DocSkip is used to indicate if the payload length is stored for a SkipDatum or if the length is the same as in the last SkipDatum.
- Payloads are loaded lazily. When a user calls TermPositions.nextPosition() then only the position and the payload length is loaded from the ProxFile. If the user calls getPayload() then the payload is actually loaded. If getPayload() is not called before nextPosition() is called again, then the payload data is just skipped.
  
Changes of file formats
------------------------------
- FieldInfos (.fnm)
The format of the .fnm file does not change. The only change is the use of the sixth lowest-order bit (0x20) of the FieldBits. If this bit is set, then payloads are enabled for the corresponding field. 

- ProxFile (.prx)
ProxFile (.prx) -->  <TermPositions>^TermCount
TermPositions   --> <Positions>^DocFreq
Positions       --> <PositionDelta, Payload?>^Freq
Payload         --> <PayloadLength?, PayloadData>
PositionDelta   --> VInt
PayloadLength   --> VInt 
PayloadData     --> byte^PayloadLength

For payloads disabled (unchanged):
PositionDelta is the difference between the position of the current occurrence in the document and the previous occurrence (or zero, if this is the first   occurrence in this document).
  
For Payloads enabled:
PositionDelta/2 is the difference between the position of the current occurrence in the document and the previous occurrence. If PositionDelta is odd, then PayloadLength is stored. If PositionDelta is even, then the length of the current payload equals the length of the previous payload and thus PayloadLength is omitted.

- FreqFile (.frq)

SkipDatum     --> DocSkip, PayloadLength?, FreqSkip, ProxSkip
PayloadLength --> VInt

For payloads disabled (unchanged):
DocSkip records the document number before every SkipInterval th document in TermFreqs. Document numbers are represented as differences from the previous value in the sequence.

For payloads enabled:
DocSkip/2 records the document number before every SkipInterval th  document in TermFreqs. If DocSkip is odd, then PayloadLength follows. If DocSkip is even, then the length of the payload at the current skip point equals the length of the payload at the last skip point and thus PayloadLength is omitted.


This encoding is space efficient for different use cases:
   * If only some fields of an index have payloads, then there's no space overhead for the fields with payloads disabled.
   * If the payloads of consecutive term positions have the same length, then the length only has to be stored once for every term. This should be a common case, because users probably use the same format for all payloads.
   * If only a few terms of a field have payloads, then we don't waste much space because we benefit again from the same-length-compression since we only have to store the length zero for the empty payloads once per term.

All unit tests pass."
1,"Lock.obtain(timeout) behaves incorrectly for large timeoutsBecause timeout is a long, but internal values derived from timeout
are ints, its possible to overflow those internal values into negative
numbers and cause incorrect behavior.

Spinoff from this thread:

  http://www.gossamer-threads.com/lists/lucene/java-user/54376

"
0,"revise max-per-host configurationMax-per-host settings for ThreadSafeClientConnManagers are currently stored in HttpParams, where the parameter value is a map from HttpRoute (formerly HostConfiguration) to Integer. This has several drawbacks:

1) maintaining a map as a value in HttpParams doesn't match my understanding of how params should be used
2) the maximums based on HttpRoute are really specific to the TSCCM implementation and not a generic parameterization

some of the options are:

a) revise to define a more generic parameterization approach
b) revise into an implementation specific parameterization approach
c) define an implementation (TSCCM) specific configuration interface and a default implementation keeping the map as run-time data

cheers,
  Roland
"
0,"Post grouping facetingThis issues focuses on implementing post grouping faceting.
* How to handle multivalued fields. What field value to show with the facet.
* Where the facet counts should be based on
** Facet counts can be based on the normal documents. Ungrouped counts. 
** Facet counts can be based on the groups. Grouped counts.
** Facet counts can be based on the combination of group value and facet value. Matrix counts.   

And properly more implementation options.

The first two methods are implemented in the SOLR-236 patch. For the first option it calculates a DocSet based on the individual documents from the query result. For the second option it calculates a DocSet for all the most relevant documents of a group. Once the DocSet is computed the FacetComponent and StatsComponent use one the DocSet to create facets and statistics.  

This last one is a bit more complex. I think it is best explained with an example. Lets say we search on travel offers:
|||hotel||departure_airport||duration||
|Hotel a|AMS|5
|Hotel a|DUS|10
|Hotel b|AMS|5
|Hotel b|AMS|10

If we group by hotel and have a facet for airport. Most end users expect (according to my experience off course) the following airport facet:
AMS: 2
DUS: 1

The above result can't be achieved by the first two methods. You either get counts AMS:3 and DUS:1 or 1 for both airports."
0,"[GSoC] Implementing State of the Art Ranking for LuceneLucene employs the Vector Space Model (VSM) to rank documents, which compares
unfavorably to state of the art algorithms, such as BM25. Moreover, the architecture is
tailored specically to VSM, which makes the addition of new ranking functions a non-
trivial task.

This project aims to bring state of the art ranking methods to Lucene and to implement a
query architecture with pluggable ranking functions.

The wiki page for the project can be found at http://wiki.apache.org/lucene-java/SummerOfCode2011ProjectRanking."
0,"advertise support for RFC4918 (WebDAV) compliance class 3With the recent changes for PROPFIND/allprop/include (JCR-1769) and the parsing of Destination/If headers (JCR-1770), we can advertise RFC 4918 L3 support (http://greenbytes.de/tech/webdav/rfc4918.html#rfc.section.18.3).

Note we still have test failures for tagged If headers, but this has nothing to do with compliance class 3.

"
1,"Parametrizing H1 and H2The DFR normalizations {{H1}} and {{H2}} are parameter-free. This is in line with the [original article|http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.101.742], but not with the [thesis|http://theses.gla.ac.uk/1570/], where H2 accepts a {{c}} parameter, nor with [information-based models|http://dl.acm.org/citation.cfm?id=1835490], where H1 also accepts a {{c}} parameter."
0,"Data Store: enable and fix testsCurrently the unit test TestTwoGetStreams fails in the trunk (it worked in older versions). This should be fixed. 

Also, the data store is disabled by default, so this test doesn't run by default. The data store should be enabled for testing."
1,"webdav: nullpointer exception while getting the tikka detector seems to be introduced by https://issues.apache.org/jira/browse/JCR-2334

05.11.2009 14:28:27 *MARK * servletengine: Servlet threw exception: 
java.lang.NullPointerException
	at org.apache.jackrabbit.server.io.DefaultHandler.detect(DefaultHandler.java:668)
	at org.apache.jackrabbit.server.io.XmlHandler.canExport(XmlHandler.java:152)
	at org.apache.jackrabbit.server.io.DefaultHandler.canExport(DefaultHandler.java:557)
	at org.apache.jackrabbit.server.io.PropertyManagerImpl.exportProperties(PropertyManagerImpl.java:58)
	at org.apache.jackrabbit.webdav.simple.DavResourceImpl.initProperties(DavResourceImpl.java:320)
	at org.apache.jackrabbit.webdav.simple.DeltaVResourceImpl.initProperties(DeltaVResourceImpl.java:248)
	at org.apache.jackrabbit.webdav.simple.VersionControlledResourceImpl.initProperties(VersionControlledResourceImpl.java:320)
	at org.apache.jackrabbit.webdav.simple.DavResourceImpl.getProperties(DavResourceImpl.java:300)
	at org.apache.jackrabbit.webdav.MultiStatusResponse.<init>(MultiStatusResponse.java:181)
	at org.apache.jackrabbit.webdav.MultiStatus.addResourceProperties(MultiStatus.java:62)
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doPropFind(AbstractWebdavServlet.java:447)
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(AbstractWebdavServlet.java:235)
	at com.day.crx.j2ee.CRXDavServlet.service(CRXDavServlet.java:76)
	at com.day.crx.j2ee.ResourceServlet.service(ResourceServlet.java:97)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
	at com.day.j2ee.servletengine.ServletRuntimeEnvironment.service(ServletRuntimeEnvironment.java:228)
	at com.day.j2ee.servletengine.RequestDispatcherImpl.doFilter(RequestDispatcherImpl.java:315)
	at com.day.j2ee.servletengine.RequestDispatcherImpl.service(RequestDispatcherImpl.java:334)
	at com.day.j2ee.servletengine.RequestDispatcherImpl.service(RequestDispatcherImpl.java:378)
	at com.day.j2ee.servletengine.ServletHandlerImpl.execute(ServletHandlerImpl.java:313)
	at com.day.j2ee.servletengine.DefaultThreadPool$DequeueThread.run(DefaultThreadPool.java:134)
	at java.lang.Thread.run(Thread.java:613)"
1,"BasicCookieStore.getCookies() returns non-threadsafe collectionBasicCookieStore.getCookies() is a simple method.  It's synchronized, and it returns an unmodifiable wrapper around the underlying cookie list.  If the caller were to then iterate over it as another thread were to manipulate the cookie list via BasicCookieStore, this would create a thread un-safe situation because both threads aren't doing their reading/writing with the same lock (the reader doesn't even have a lock).

I suggest fixing this by using CopyOnWriteArrayList, or by making a defensive copy in getCookies()

This issue might apply to some of the other basic implementations of some of the interfaces but I haven't checked."
0,"JSR 283 namespace handlingJSR 283 makes namespace handling more flexible and user-friendly (less exceptions, no unexpected mapping changes during a session, etc.). The changes are mostly compliant with JSR 170, so we can implement them already for Jackrabbit 1.x.

TODO: JSR 283 namespace handling + related TCK tests"
0,"Optimize queries that check for the existence of a property//*[@mytext] is transformed into the org.apache.jackrabbit.core.query.lucene.MatchAllQuery, that through the MatchAllWeight uses the MatchAllScorer.  The calculateDocFilter() in MatchAllScorer  does not scale and becomes slow for growing number of nodes. 

Solution: lucene documents will get a new Field:

public static final String PROPERTIES_SET = ""_:PROPERTIES_SET"".intern();

that holds the available properties of this document. 

NOTE: Lucene indices build without this performance improvement should still work and fall back to the original implementation"
1,"Generated cluster node id should be persistedIf no cluster node id is specified in the configuration, a cluster node id is automatically generated. This id is never persisted, so after another startup a new, probably different cluster node id is used. Instead, an automatically generated cluster id should be persisted inside the repository home."
1,"Redirect 302 to the same URL causes max redirects exceptionI noticed that if the server returns a 302 without a URL in the link, the 
HttpClient follows the empty URL up to the maximum times (100 by default).  
Instead it should check and if the URL is an empty string it shouldn't try to 
follow the redirect.

12:18:17,430 [U:          ] [main                ] ERROR 
HttpMethodBase               - Narrowly avoided an infinite loop in execute
12:18:17,430 [U:          ] [main                ] DEBUG 
URLMonitor                   - Method.execute attempt 1 failed 
http://www.stagecoach.co.uk: 
org.apache.commons.httpclient.HttpRecoverableException: Maximum redirects (100) 
exceeded
12:18:17,430 [U:          ] [main                ] DEBUG 
URLMonitor                   - HttpRecoverableException 
(http://www.stagecoach.co.uk) : 
org.apache.commons.httpclient.HttpRecoverableException: Maximum redirects (100) 
exceeded
	at org.apache.commons.httpclient.HttpMethodBase.execute
(HttpMethodBase.java:1065)
	at com.verideon.veriguard.domain.URLMonitor.monitor(URLMonitor.java:189)
	at com.verideon.veriguard.domain.URLMonitor.monitor(URLMonitor.java:101)
	at com.verideon.veriguard.domain.TestURLMonitor.getPage
(TestURLMonitor.java:58)
	at com.verideon.veriguard.domain.TestURLMonitor.monitorURL
(TestURLMonitor.java:47)
	at com.verideon.veriguard.domain.TestURLMonitor.testMonitorURLStageCoach
(TestURLMonitor.java:138)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke
(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke
(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:324)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests
(RemoteTestRunner.java:392)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run
(RemoteTestRunner.java:276)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main
(RemoteTestRunner.java:167)

Result with telnet:

GET /
HTTP/1.1 302 Object moved
Server: Microsoft-IIS/5.0
Date: Tue, 01 Jul 2003 10:05:58 GMT
X-Powered-By: ASP.NET
Location: http://www.stagecoach.co.uk
Connection: Keep-Alive
Content-Length: 121
Content-Type: text/html
Set-Cookie: ASPSESSIONIDCQCSRAAB=IFJJLEADOPDDNNGHLPFBIIIE; path=/
Cache-control: private

<head><title>Object moved</title></head>
<body><h1>Object Moved</h1>This object may be found <a HREF="""">here</a>.</body>
Connection closed by foreign host."
1,"Virtual host setting does not apply when parsing and matching cookiesVirtual host setting does not apply when parsing and matching cookies.

Problem has been reported on the httpclient-dev list by Dan Levine"
0,Replace customized QueryParser.jjtWe should rather use the Lucene default and implement a  Jackrabbit  version that extends from it. This eases maintenance when moving from one Lucene version to another.
1,"Rollback doesn't preserve integrity of original indexAfter several ""updateDocuments"" calls a rollback call does not return the index to the prior state.
This seems to occur if the number of updates exceeds the RAM buffer size i.e. when some flushing of updates occurs.

Test fails in Lucene 2.4, 2.9, 3.0.1 and 3.0.2

JUnit to follow.
"
0,"ms-sql tablespace support for FileSystem and PersistenceManagerTrunk and released version 1.5.0 does not have complete support for ms-sqlserver tablespaces.  This patch was originally submitted via JCR-1295 but was not applied to the 1.4 trunk.

"
1,"SO_TIMEOUT not set early enough for SOCKS proxies in PlainSocketFactoryI've created my own delegating SchemeSocketFactory implementation which supports setting SOCKS proxies on socket creation. In the connectSocket implementation, I previously just delegated to PlainSocketFactory.

The problem there was, that the SO_TIMEOUT was not set on the socket before the connection was established through the SOCKS proxy. This lead to a stop on the native read0 method because the socket is endlessly waiting for a read to occur from the proxy, so it can continue with the the connect to the actual socket destination through the proxy. I made sure I set the SO_TIMEOUT parameter in HttpParams, but it did not get honored by PlainSocketFactory.

To fix this and make HttpClient honor SO_TIMEOUT for SOCKS proxies, the following line has to be added:
  sock.setSoTimeout(HttpConnectionParams.getSoTimeout(params));
in PlainSocketFactory.connectSocket(...).

Heres the complete fixed method:

PlainSocketFactory:            
    public Socket connectSocket(
            final Socket socket,
            final InetSocketAddress remoteAddress,
            final InetSocketAddress localAddress,
            final HttpParams params) throws IOException, ConnectTimeoutException {
        if (remoteAddress == null) {
            throw new IllegalArgumentException(""Remote address may not be null"");
        }
        if (params == null) {
            throw new IllegalArgumentException(""HTTP parameters may not be null"");
        }
        Socket sock = socket;
        if (sock == null) {
            sock = createSocket();
        }
        if (localAddress != null) {
            sock.setReuseAddress(HttpConnectionParams.getSoReuseaddr(params));
            sock.bind(localAddress);
        }
        
        //FIX for SOCKS proxies which get stalled if they don't answer
        sock.setSoTimeout(HttpConnectionParams.getSoTimeout(params));
        
        int timeout = HttpConnectionParams.getConnectionTimeout(params);
        try {
            sock.connect(remoteAddress, timeout);
        } catch (SocketTimeoutException ex) {
            throw new ConnectTimeoutException(""Connect to "" + remoteAddress.getHostName() + ""/""
                    + remoteAddress.getAddress() + "" timed out"");
        }
        return sock;
    }

Currently I've implemented this in my delegating SchemeSocketFactory, because PlainSocketFactory misses this setting.

Dunno if there are other implementations of SocketFactory in HttpClient, which might need this fix. Anyway I hope this helps other people who get  headaches about halting threads because they use SOCKS proxies. :)"
0,Faster packaging of the standalone jarCurrently the standalone jar is created by first unpacking all the dependencies to target/classes and then packaging the resulting directory tree into the resulting jar file. This takes quite a while as all the writing and reading of uncompressed class files requires lots of disk IO. We could avoid these extra copies by using the bundle or assembly plugin to build the standalone jar.
1,"NPE in RepositoryServiceImpl.getPropertyInfo()under unknown conditions, i get a NPE in get property info, such as the 'getValue()' of the getstring dav property is null:

            } else if (props.contains(JCR_GET_STRING)) {
                // single valued non-binary property
                String str = props.get(JCR_GET_STRING).getValue().toString();
                QValue qValue = ValueFormat.getQValue(str, propertyType, getNamePathResolver(sessionInfo), getQValueFactory(sessionInfo));
                return new PropertyInfoImpl(propertyId, p, propertyType, qValue);
            } else {

the other properties in the propset are:
 - getstring: null
 - type: String
 - length: 0

the property in question is the last property of a node and it's an empty string. the error only occurs on certain usage patterns, but consistently. maybe depending on the fetch-depth or internal cache.

extending the check to:
            } else if (props.contains(JCR_GET_STRING) && props.get(JCR_GET_STRING).getValue() != null) {

solves the problem.
 
"
1,"Deleting binary property does not remove 'blob file' in filesystemwhen deleting a binary property or its containing node, the 'blob-file' sometime does not get removed.

the reason for this, is an open FileInputStream, that gets referenced in the property value."
0,"automaton spellcheckerThe current spellchecker makes an n-gram index of your terms, and queries this for spellchecking.
The terms that come back from the n-gram query are then re-ranked by an algorithm such as Levenshtein.

Alternatively, we could just do a levenshtein query directly against the index, then we wouldn't need
a separate index to rebuild.
"
0,"drop java 5 ""support""its been discussed here and there, but I think we need to drop java 5 ""support"", for these reasons:
* its totally untested by any continual build process. Testing java5 only when there is a release candidate ready is not enough. If we are to claim ""support"" then we need a hudson actually running the tests with java 5.
* its now unmaintained, so bugs have to either be hacked around, tests disabled, warnings placed, but some things simply cannot be fixed... we cannot actually ""support"" something that is no longer maintained: we do find JRE bugs (http://wiki.apache.org/lucene-java/SunJavaBugs) and its important that bugs actually get fixed: cannot do everything with hacks.
* because of its limitations, we do things like allow 20% slower grouping speed. I find it hard to believe we are sacrificing performance for this.

So, in summary: because we don't test it at all, because its buggy and unmaintained, and because we are sacrificing performance, I think we need to cutover the build system for the next release to require java 6.
"
1,Port fix for HTTPCLIENT-633 to 4.0The fix for MultiThreadedHttpConnectionManager from HTTPCLIENT-633 should be ported to ThreadSafeClientConnManager in 4.0.
0,"Allow to configure DB persistence managers through JDNICurrently, DB persistence managers have hardcoded urls. Even more, they will use a single connection with the drawbacks that this have regarding concurrency, performance and transactionality. 

It would be fairly better to allow to configure DB persistence managers through JDNI references to DataSource. So giving responsability to application server. Concurrency, performance and transactionability will be highly boosted with this approach. 

This could be a sample configuration :

<PersistenceManager class=""org.apache.jackrabbit.core.state.db.SimpleDbPersistenceManager"">
       <param name=""dataSource"" value=""jdbc/JackrabbitDS""/>

        
       .... think also about a way to pass params to data source, it should be simple ....

       <param name=""schema"" value=""mysql""/>
       <param name=""schemaObjectPrefix"" value=""${
wsp.name}_""/>
       <param name=""externalBLOBs"" value=""false""/>
   </PersistenceManager>"
0,JE Directory ImplementationI've created a port of DbDirectory to JE
0,"The servlet-api dependency scope should be ""provided"" in jackrabbit-jcr-serverUsing the default ""compile"" scope for servlet-api in jackrabbit-jcr-server causes warnings about the scope not being ""provided"" when building jackrabbit-webapp."
0,"CachingSpanFilter synchronizing on a none final protected objectCachingSpanFilter and CachingWrapperFilter expose their internal cache via a protected member which is lazily instantiated in the getDocSetId method. The current code yields the chance to double instantiate the cache and internally synchronizes on a protected none final member. My first guess is that this member was exposed for testing purposes so it should rather be changed to package private. 

This patch breaks backwards compat while I guess the cleanup is kind of worth breaking it."
0,"Contrib: another highlighter approachMark Harwoods highlighter package is a great contribution to Lucene, I've used it a lot! However, when you have *large* documents (fields), highlighting can be quite time consuming if you increase the number of bytes to analyze with setMaxDocBytesToAnalyze(int). The default value of 50k is often too low for indexed PDFs etcetera, which results in empty highlight strings.

This is an alternative approach using term position vectors only to build fragment info objects. Then a StringReader can read the relevant fragments and skip() between them. This is a lot faster. Also, this method uses the *entire* field for finding the best fragments so you're always guaranteed to get a highlight snippet.

Because this method only works with fields which have term positions stored one can check if this method works for a particular field using following code (taken from TokenSources.java):

        TermFreqVector tfv = (TermFreqVector) reader.getTermFreqVector(docId, field);
        if (tfv != null && tfv instanceof TermPositionVector)
        {
          // use FulltextHighlighter
        }
        else
        {
          // use standard Highlighter
        }

Someone else might find this useful so I'm posting the code here."
0,"IndexWriter does not properly account for the RAM consumed by pending deletesIndexWriter, with autoCommit false, is able to carry buffered deletes for quite some time before materializing them to docIDs (thus freeing up RAM used).

It's only on triggering a merge (or, commit/close) that the deletes are materialized and the RAM is freed.

I expect this in practice is a smallish amount of RAM, but we should still fix it.

I don't have a patch yet so if someone wants to grab this, feel free!!"
0,"Impl toString() in MergePolicy and its extensionsThese can be important to see for debugging.

We lost them in the cutover to IWC.

Just opening this issue to remind us to get them back, before releasing..."
0,"Make not yet final core/contrib TokenStream/Filter implementations finalLucene's analysis package is designed in a way, that you can plug different *implementations* of analysis in chains of TokenStreams and TokenFilters. An analyzer is build of several TokenStreams/Filters that do the tokenization of text. If you want to modify the behaviour of tokenization, you implement a new subclass of TokenStream/-Filter/Tokenizer.

Most classes in the core are correctly implemented like that. They are itsself final or their implementation methods are final (CharTokenizer).

A lot of problems with backwards-compatibility of LUCENE-1693 are some classes in Lucene's core/contrib not yet final:
- KeywordTokenizer should be declared final or its implementation methods should be final
- StandardTokenizer should be declared final or its implementation methods should be final
- ISOLatin1Filter is deprecated, so it will be removed in 3.0, nothing to do.

CharTokenizer is the abstract base class of several other classes. The design is correct: Child classes cannot override the implementation, they can only change the behaviour of this final implementation.

Contrib should be checked, that all implementation classes are at least final or they are designed in the same way like CharTokenizer."
0,Make it possible to get multiple nodes in one call via davexI'm working on this currently
1,"WebDAV LocatorFactoryImpl$Locator.getHref() constructs root resource URLs incorrectlycadaver was reporting an error when i tried to open / in my repository's default workspace at <https://localhost:8443/webdav/).

in tracking down the problem, i saw something strange - the multistatus response's href had an extra ""/"" tacked onto the end:

  <D:multistatus xmlns:D=""DAV:"">
    <D:response>
      <D:href>https://localhost:8443/webdav//</D:href>

WebdavServlet (rather, my subclass of it) is mapped as the default servlet of a webapp mounted at /webdav. i've configured the WebdavServlet with a resource path prefix of """" (incidentally, i'm not sure what that's meant to be used for - i see that when the value is not empty, it's appended to the response's href, but i don't know in what circumstance that would be useful).

when i requested a child node such as <https://localhost:8443/webdav/bcm>, the response's href was formed as expected:

  <D:multistatus xmlns:D=""DAV:"">
    <D:response>
      <D:href>https://localhost:8443/webdav/bcm/</D:href>

i found that LocatorFactoryImpl$Locator.getHref() was adding the extra ""/"" since the requested resource was a collection. i patched the method to not add the character when itemPath == ""/"", and cadaver stopped complaining. all is well.

i also patched WebdavServlet to default to an empty resource path prefix if one is not specified as a servlet init parameter.
"
0,"Tiered flushing of DWPTs by RAM with low/high water marksNow that we have DocumentsWriterPerThreads we need to track total consumed RAM across all DWPTs.

A flushing strategy idea that was discussed in LUCENE-2324 was to use a tiered approach:  
- Flush the first DWPT at a low water mark (e.g. at 90% of allowed RAM)
- Flush all DWPTs at a high water mark (e.g. at 110%)
- Use linear steps in between high and low watermark:  E.g. when 5 DWPTs are used, flush at 90%, 95%, 100%, 105% and 110%.

Should we allow the user to configure the low and high water mark values explicitly using total values (e.g. low water mark at 120MB, high water mark at 140MB)?  Or shall we keep for simplicity the single setRAMBufferSizeMB() config method and use something like 90% and 110% for the water marks?"
0,"rep:excerpt() should also work on propertiesCurrently the rep:excerpt() function can only be used to create an excerpt with highlight information from a node, the function should also support highlighting of string properties."
1,"small float underflow detection bugUnderflow detection in small floats has a bug, and can incorrectly result in a byte value of 0 for a non-zero float."
0,Use repository service wide namespace cacheThe jcr2spi layer requests namespaces for each new session that is created. It should rather cache them and make them available to other sessions.
1,"ResponseCachingPolicy uses integers for sizesResponseCachingPolicy currently uses integers for interpreting the size of Content-Length, as well internally.

This causes issues in attempting to use the module for caching entities that are over 2GB in size, the module does not fail gracefully, but throws a NumberFormatException

I have a patch that fixes this, by promoting the int -> long, which should allow for larger entities to be cached, it also updates the public facing API where possible, I don't think that the promotion should break compatibility massively

The changes can also be seen here:
https://github.com/GregBowyer/httpclient/commit/1197d3f94bd2eedcec32646cd6146748ca2e6fa1"
1,"NullPointerException from SegmentInfos.FindSegmentsFile.run() if FSDirectory.list() returns NULL Found this bug while running unit tests to verify an upgrade of our system from 1.4.3 to 2.1.0.  This bug did *not* occur during 1.4.3, it is new to 2.x (I'm pretty sure it's 2.1-only)

If the index directory gets deleted out from under Lucene after the FSDirectory has been created, then attempts to open an IndexWriter or IndexReader will result in an NPE.  Lucene should be throwing an IOException in this case.

Repro:
    1) Create an FSDirectory pointing somewhere in the filesystem (e.g. /foo/index/1)
    2) rm -rf the parent dir (rm -rf /foo/index)
    3) Try to open an IndexReader

Result: NullPointerException on line ""for(int i=0;i<files.length;i++) { "" -- 'files' is NULL.
 
Expect: IOException


....  

This is happening because of a missing NULL check in SegmentInfos$FindSegmentsFile.run():

        if (0 == method) {
          if (directory != null) {
            files = directory.list();
          } else {
            files = fileDirectory.list();
          }

          gen = getCurrentSegmentGeneration(files);

          if (gen == -1) {
            String s = """";
            for(int i=0;i<files.length;i++) { 
              s += "" "" + files[i];
            }
            throw new FileNotFoundException(""no segments* file found: files:"" + s);
          }
        }


The FSDirectory constructor will make sure the index dir exists, but if it is for some reason deleted out from underneath Lucene after the FSDirectory is instantiated, then java.io.File.list() will return NULL.  Probably better to fix FSDirectory.list() to just check for null and return a 0-length array:

(in org/apache/lucene/store/FSDirectory.java)
314c314,317
<         return directory.list(IndexFileNameFilter.getFilter());
---
>     String[] toRet = directory.list(IndexFileNameFilter.getFilter());
>     if (toRet == null)
>         return new String[]{};
>     return toRet;
"
0,Interface TermFreqVector has incomplete JavadocsWe should improve the Javadocs of org.apache.lucene.index.TermFreqVector
1,"ItemState constructor throws IllegalArgumentExceptionWhen running ConcurrentReadWriteTest it may happen that a reading session gets an IllegalArgumentException:

Exception in thread ""Thread-7"" java.lang.IllegalArgumentException: illegal status: 0
	at org.apache.jackrabbit.core.state.ItemState.<init>(ItemState.java:138)
	at org.apache.jackrabbit.core.state.PropertyState.<init>(PropertyState.java:79)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.getPropertyState(LocalItemStateManager.java:121)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.getItemState(LocalItemStateManager.java:152)
	at org.apache.jackrabbit.core.state.XAItemStateManager.getItemState(XAItemStateManager.java:226)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:175)
	at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:495)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:326)
	at org.apache.jackrabbit.core.LazyItemIterator.prefetchNext(LazyItemIterator.java:90)
	at org.apache.jackrabbit.core.LazyItemIterator.<init>(LazyItemIterator.java:75)
	at org.apache.jackrabbit.core.ItemManager.getChildProperties(ItemManager.java:485)
	at org.apache.jackrabbit.core.NodeImpl.getProperties(NodeImpl.java:2481)
	at org.apache.jackrabbit.core.ConcurrentReadWriteTest$1$1.execute(ConcurrentReadWriteTest.java:61)
	at org.apache.jackrabbit.core.AbstractConcurrencyTest$Executor.run(AbstractConcurrencyTest.java:107)
	at java.lang.Thread.run(Thread.java:595)

Status 0 is STATUS_UNDEFINED. I think the following happens: when the reading session retrieves the ItemState from the SharedItemStateManager it is still valid but a short time later the writing session removes the item and changes the status to STATUS_UNDEFINED. Then the reading session tries to create an overlayed ItemState for the LocalItemStateManager using the changed status.

Adding the STATUS_UNDEFINED to the list of 'valid' status in the ItemState constructor seems to solve the issue, but I'm not sure if that's the right way to do it.

Opinions?"
0,"Unclosed sessions in test casesSome tests may throw exceptions in the setUp() method and leave the session open that was opened in the super class setUp() method. For jackrabbit-core, this is not really a problem, because the memory footprint of a session is quite small, but in jcr2spi the memory footprint is considerable higher, which may lead to out of memory errors when running the tests."
0,"Can't use proxy server with httpsThere doesn't seem to be a way to configure HttpClient to use both HTTPS and a
proxy server at the same time.  It's not clear if this was just an oversight or
if there was a deliberate decision to not support this combination for some reason.

Assuming that it was an oversight, the fix seems to just require one more
variation of startSession() in HttpClient.java which would be the following:

   public void startSession(String host, int port,
                            String proxyhost, int proxyport, boolean https) {
       connection = new HttpConnection(proxyhost,proxyport,host,port,https);
   }"
0,"Database Data StoreWe want to have a database backed data store implementation.
An implementation using files is already available as part of JCR-926.
"
1,"SQL2 query: QOMFormatter create incorrect NOT conditionsThen the following query is parsed:
SELECT test.* FROM test WHERE (NOT test.name = 'Hello') AND test.id = 3
then the SQL statement generated, it becomes:
SELECT test.* FROM test WHERE NOT test.name = 'Hello' AND test.id = 3
which is parsed differently and becomes:
SELECT test.* FROM test WHERE NOT (test.name = 'Hello' AND test.id = 3)"
0,Return null for optional configuration elementsTwo recently introduced configuration elements are optional but the configuration parser still returns an instance when the elements are missing in the configuration. The parser should return null when an element is not there in order to distinguish it from the case where an empty element is present. 
0,"Deprecate non-pooled bundle DB persistence managersIn JCR-1456 and Jackrabbit 2.0 we introduced database connection pooling, but decided to keep the existing database bundle persistence managers intact to avoid potential regressions. We haven't seen such problems even though pooled bundle persistence has been the default since the 2.0 release, so I think it would be safe to deprecate all the non-pooled bundle DB PMs.

And in order to remove duplicate code (that has already complicated some changes within o.a.j.persistence), I'd also take the extra step of  making the o.a.j.p.bundle.* classes extend respective the o.a.j.p.pool.* classes. This would automatically allow also old non-pooled configurations to benefit from connection pooling."
0,"Similarity.lengthNorm and positionIncrement=0Calculation of lengthNorm factor should in some cases take into account the number of tokens with positionIncrement=0. This should be made optional, to support two different scenarios:

* when analyzers insert artificially constructed tokens into TokenStream (e.g. ASCII-fied versions of accented terms, stemmed terms), and it's unlikely that users submit queries containing both versions of tokens: in this case lengthNorm calculation should ignore the tokens with positionIncrement=0.

* when analyzers insert synonyms, and it's likely that users may submit queries that contain multiple synonymous terms: in this case the lengthNorm should be calculated as it is now, i.e. it should take into account all terms no matter what is their positionIncrement.

The default should be backward-compatible, i.e. it should count all tokens.

(See also the discussion here: http://markmail.org/message/vfvmzrzhr6pya22h )"
0,Add Rewriteable Support to SortField.toStringI missed adding support for the new Rewriteable SortField type to toString().
0,"Change access levels in SearchIndex and NodeIndexer for better inheranceI want to change NodeIndexer#addBinaryValue logic in JR 1.5.6, therefore i needed to:
* create a custom class extending NodeIndexer for changing binary field indexation
* create a custom class extending SearchIndex for using this custom NodeIndexer

I was obliged to:
* override SearchIndex#createTextExtractor in order to store created TextExtractor because of private attribute
* put both classes into package org.apache.jackrabbit.core.query.lucene because NodeIndexer#createDoc(...) is protected

In trunk TextExtractor has now a getter but there are still some private attributes.
And NodeIndexer#createDoc(...) is still protected and there are some private methods."
1,"redefinition of xml-namespace mapping should not be allowedthe following throws an exception, but should work:

// remap xml namespace -> works
Session.setNamespacePrefix(""foobar"", ""http://www.w3.org/XML/1998/namespace"");

// revert mapping -> throws exception
Session.setNamespacePrefix(""xml"", ""http://www.w3.org/XML/1998/namespace"");"
0,"NamespaceRegistryTest makes assumptions about legal namesorg.apache.jackrabbit.test.api.NamespaceRegistryTest.testRegisterNamespace() makes the assumption that once a namespace is registered, it can be used in a node name. In practice, many repositories have their own restrictions on node naming, in particular may not support namespace prefixes in node names at all.

Proposal: change the test case so that it's independant of the repository's ability to create new nodes in that namespace.
"
1,"FileDataStore: garbage collection can delete files that are still neededIt looks like the FileDataStore garbage collection (both regular scan and persistence manager scan) can delete files that are still needed.

Currently it looks like the reason is the last access time resolution of the operating system. This is 2 seconds for FAT and Mac OS X, NTFS 100 ns, and 1 second for other file systems. That means file that are scanned at the very beginning are sometimes deleted, because they have a later last modified time then when the scan was started."
0,"Drop the Dumpable interfaceI belive the o.a.j.core.util.Dumpable interface was originally used for diagnostic purposes, but AFAIUI we don't use it anywhere anymore. I'd like to drop the interface and refactor the dump() methods in various Jackrabbit classes to more detailed toString() methods that would be more useful to debuggers and other general-purpose diagnostic tools."
1,"Spell Checker suggestSimilar throws NPE when IndexReader is not null and field is nullThe SpellChecker.suggestSimilar(String word, int numSug, IndexReader ir,   String field, boolean morePopular) throws a NullPointerException when the IndexReader is not null, but the Field is.  The Javadocs say that it is fine to have the field be null, but doesn't comment on the fact that the IndexReader also needs to be null in that case.

"
0,"Add convenient constructor to PerFieldAnalyzerWrapper for Dependency InjectionIt would be good if PerFieldAnalyzerWrapper had a constructor which took in an analyzer map, rather than having to repeatedly call addAnalyzer -- this would make it much easier/cleaner to use this class in e.g. Spring XML configurations.

Relatively trivial change, patch to be attached."
1,"MultiThreadedHttpConnectionManager daemon Thread never GC'dOne of my colleagues was invoking HttpClient by way of a loop something like this:

for (int i = 0; i < 300; i++) {
    GetMethod method = new
GetMethod(""http://cvs.apache.org/viewcvs/jakarta-commons/httpclient/"");
    try {
        HttpClient httpClient = new HttpClient(new
MultiThreadedHttpConnectionManager());
        httpClient.executeMethod(method);
        byte[] bytes = method.getResponseBody();
    } finally {
        // always release the connection after we're done
        method.releaseConnection();
    }
}

He's in the process of revising his code so that he doesn't do this loop, which
other developers might point out as a non-optimal use, but along the way, he
discovered that the daemon thread that the MultiThreadedHttpConnectionManager
makes does not get garbage collected.  Of course, the connection manager itself
is also never gc'd.  While I think we can avoid this problem in our code, in the
more general case, clients may not actually be able to control the number of
MultiThreadedConnectionManagers they create, which could eventually cause
problems.  This makes me think the problem is deserving of a patch.

We found this problem with 2.0rc2, although presumably it also exists with the
CVS HEAD.

Patch to follow."
0,Collection parameter of CompactNodeTypeDefWriter#write should be covariantThe Collection<QNodeTypeDefinition> parameter of the CompactNodeTypeDefWriter#write methods should have type Collection<? extends QNodeTypeDefinition>. 
0,"Clover setup currently has some problems(tracking as a bug before it get lost in email...
  http://www.nabble.com/Clover-reports-missing-from-hudson--to15510616.html#a15510616
)

The clover setup for Lucene currently has some problems, 3 i think...

1) instrumentation fails on contrib/db/ because it contains java packages the ASF Clover lscence doesn't allow instrumentation of.  i have a patch for this.

2) running instrumented contrib tests for other contribs produce strange errors...

{{monospaced}}
    [junit] Testsuite: org.apache.lucene.analysis.el.GreekAnalyzerTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.126 sec
    [junit]
    [junit] ------------- Standard Error -----------------
    [junit] [CLOVER] FATAL ERROR: Clover could not be initialised. Are you sure you have Clover
in the runtime classpath? (class
java.lang.NoClassDefFoundError:com_cenqua_clover/CloverVersionInfo)
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAnalyzer(org.apache.lucene.analysis.el.GreekAnalyzerTest):    Caused
an ERROR
    [junit] com_cenqua_clover/g
    [junit] java.lang.NoClassDefFoundError: com_cenqua_clover/g
    [junit]     at org.apache.lucene.analysis.el.GreekAnalyzer.<init>(GreekAnalyzer.java:157)
    [junit]     at
org.apache.lucene.analysis.el.GreekAnalyzerTest.testAnalyzer(GreekAnalyzerTest.java:60)
    [junit]
    [junit]
    [junit] Test org.apache.lucene.analysis.el.GreekAnalyzerTest FAILED
{{monospaced}}

...i'm not sure what's going on here.  the error seems to happen both when
trying to run clover on just a single contrib, or when doing the full
build ... i suspect there is an issue with the way the batchtests fork
off, but I can't see why it would only happen to contribs (the regular
tests fork as well)

3) according to Grant...

{{quote}}
...There is also a bit of a change on Hudson during the migration to the new servers that needs to be ironed  out. 
{{quote}}
"
0,"Make cache limits configurableThe cache settings of the CacheManager class (JCR-619) can be adjusted programmatically (JCR-725), but it would be nice if there was also a way to set them with system properties or ideally as a part of the repository configuration."
0,"Contrib/Jcr-Server: Improve package structure+ org
  + apache
    + jackrabbit
       + webdav
         + <dav-specific packages as currently present>
         + spi_jcr (formerly spi)
         + spi_simple (formerly dav-package below server/simple        
         + client (webdav-client lib)

       + server
         + jcr (jcr-server-classes formerly below server)
         + simple
   
       + client
         + jcr (client-side jcr impl. without dav-dependency)
    
         
"
0,"WorkspaceUpdateChannel.updateCommitted logs too muchOn each cluster record update, an info message is logged.

I think this is too much and logging should be reduced to the DEBUG level."
0,Remove RepositoryService exists()RepositoryService exists() is not used anywhere. I suggest to remove it. 
0,"MockCharFilter offset correction is wrongThis is a fake charfilter used in basetokenstreamtestcase.

it occasionally doubles some characters, and corrects offsets.

its used to find bugs where analysis components would fail otherwise with charfilters,
but its correctOffset is actually wrong (harmless to any tests today, but still wrong).
"
1,"shareable nodes: wrong path returned, causes remove() to delete wrong nodeIt seems that for shareable nodes it can happen that getPath() returns the wrong path (one of another node in the shared set):

/**
* Verify that shared nodes return correct paths.
*/
public void testPath() throws Exception {
   Node a1 = testRootNode.addNode(""a1"");
   Node a2 = a1.addNode(""a2"");
   Node b1 = a1.addNode(""b1"");
   b1.addMixin(""mix:shareable"");
   testRootNode.save();

   //now we have a shareable node N with path a1/b1

   Session session = testRootNode.getSession();
   Workspace workspace = session.getWorkspace();
   String path = a2.getPath() + ""/b2"";
   workspace.clone(workspace.getName(), b1.getPath(), path, false);

   //now we have another shareable node N' in the same shared set as N with path a1/a2/b2

   //using the path a1/a2/b2, we should get the node N' here
   Item item = session.getItem(path);
   String p = item.getPath();
   assertFalse(""unexpectedly got the path from another node from the same shared set "", p.equals(b1.getPath()));
} 

Note that when this happens, a subsequent remove() deletes the wrong node.

(Thanks Manfred for spotting this one)."
0,"IndexWriter.getReader() allocates file handlesI am not sure if this is a ""bug"" or really just me not reading the Javadocs right...

The IR returned by IW.getReader() leaks file handles if you do not close() it, leading to starvation of the available file handles/process. If it was clear from the docs that this was a *new* reader and not some reference owned by the writer then this would probably be ok. But as I read the docs the reader is internally managed by the IW, which at first shot lead me to believe that I shouldn't close it.

So perhaps the docs should be amended to clearly state that this is a caller-owns reader that *must* be closed? Attaching a simple app that illustrates the problem."
0,"Uploade Lucene 2.1 to ibiblioPlease uploaded Lucene (specifically lucene-core) 2.1.0 to ibiblio. I see 2.0.0 but not 2.1.0.

Thanks!"
0,EnwikiConentSource does not work with parallel tasks
1,"Possible NPE in HttpHostHttpHost line 167 says:
        if (this.port != this.protocol.getDefaultPort()) {

However, a few lines above, protocol is checked for null.

Line 167 should probably read:

        if (this.protocol != null && this.port != this.protocol.getDefaultPort()) {
"
1,"Nodes having OPV=Ignore are removed on restoreJCR1.0 Specification mentions:

8.2.11.5 IGNORE
  Child Node
    On checkin of N, no state information about C will be stored in VN.
    On restore of VN, the child node C of the current N will remain and not be removed.
  Property
    On checkin of N, no state information about P will be stored in VN.
    On restore of VN, the property P of the current N will remain and not be removed.

but the current implementation removed the ignore child."
1,Lucene can incorrectly set the position of tokens that start a field with positonInc 0.More info in LUCENE-1465
0,"Move XML QueryParser to queryparser moduleThe XML QueryParser will be ported across to queryparser module.

As part of this work, we'll move the QP's demo into the demo module."
0,"SimpleFieldsHelper emits a lot warningsThe SimpleFieldsHelper.retrieveSimpleField method is used to load JCR properties into simple Java object fields according to the mapping descriptor. If the node does not have the named property, a WARN message is emited.

If the missing property is defined as optional in the node type definition, it is quite normal, that it may be missing. Therefore emitting a WARN message does not seem appropriate. It would be better, to do the following (in order):

   If the missing property is declared to be required in the descriptor, throw an exception
   else if the descriptor has a default value for the missing property, use that value
   else if the property is defined with a default value in the node type definition, use that value
   else emit a DEBUG message and leave the field undefined

Not sure, whether it makes absolute sense to define a property as mandatory in the descriptor but not in the node type definition. Are there any opinions on that ?"
0,"Deprecate BLOBStore (use the DataStore instead)I believe the blob store should be deprecated in favor of the data store (in the source code, and in the documentation). The blob store should still be supported in version 2.x of course."
0,Lower log level for index updates from queueThe log level is currently at info and should be lowered to debug.
0,"Update dependency versions for commons-collections, slf4j and derbySome of the dependencies used by the 2.0-beta1 could be upgraded:
commons-collections from 3.1 to 3.2.1
slf4j from 1.5.3 to 1.5.8
derby from 10.2.1.6 to 10.5.3.0

Not sure about derby but the other two seems to be just drop in replacements for their older verisons."
0,"deprecate term and getTerm in MultiTermQueryThis means moving getTerm and term up to sub classes as appropriate and reimplementing equals, hashcode as appropriate in sub classes."
0,"benchmark pkg: specify trec_eval submission output from the command linethe QueryDriver for the trec benchmark currently requires 4 command line arguments.
the third argument is ignored (i typically populate this with ""bogus"")
Instead, allow the third argument to specify the submission.txt file for trec_eval.

while I am here, add a usage() documenting what the arguments to this driver program do."
0,"Random Failure TestSizeBoundedOptimize#testFirstSegmentTooLargeI am seeing this on trunk  

{noformat}

[junit] Testsuite: org.apache.lucene.index.TestSizeBoundedOptimize
    [junit] Testcase: testFirstSegmentTooLarge(org.apache.lucene.index.TestSizeBoundedOptimize):	FAILED
    [junit] expected:<2> but was:<1>
    [junit] junit.framework.AssertionFailedError: expected:<2> but was:<1>
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:882)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:848)
    [junit] 	at org.apache.lucene.index.TestSizeBoundedOptimize.testFirstSegmentTooLarge(TestSizeBoundedOptimize.java:160)
    [junit] 
    [junit] 
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.658 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestSizeBoundedOptimize -Dtestmethod=testFirstSegmentTooLarge -Dtests.seed=7354441978302993522:-457602792543755447 -Dtests.multiplier=3
    [junit] NOTE: test params are: codec=Standard, locale=sv_SE, timezone=Mexico/BajaNorte
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestSizeBoundedOptimize]
    [junit] ------------- ---------------- ---------------
    [junit] Test org.apache.lucene.index.TestSizeBoundedOptimize FAILED
{noformat}

when running with this seed
ant test -Dtestcase=TestSizeBoundedOptimize -Dtestmethod=testFirstSegmentTooLarge -Dtests.seed=7354441978302993522:-457602792543755447 -Dtests.multiplier=3"
0,ORM PersistenceManagers don't compileORM PMs are out of synch with the latest changes of the api. 
1,"TestParallelTermEnum fails with Sep codecreproduceable in the 'preflexfixes' branch (since we test all codecs there) with: ant test-core -Dtestcase=TestParallelTermEnum -Dtests.codec=Sep

But I think there are probably more tests like this that have only been run with Standard and we might find more like this.
I don't think this should block LUCENE-2554.

{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestParallelTermEnum
    [junit] Testcase: test1(org.apache.lucene.index.TestParallelTermEnum):      Caused an ERROR
    [junit] read past EOF
    [junit] java.io.IOException: read past EOF
    [junit]     at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:154)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)
    [junit]     at org.apache.lucene.store.DataInput.readVInt(DataInput.java:86)
    [junit]     at org.apache.lucene.index.codecs.sep.SingleIntIndexInput$Reader.next(SingleIntIndexInput.java:64)
    [junit]     at org.apache.lucene.index.codecs.sep.SepPostingsReaderImpl$SepDocsEnum.nextDoc(SepPostingsReaderImpl.java:316)
    [junit]     at org.apache.lucene.index.TestParallelTermEnum.test1(TestParallelTermEnum.java:188)
    [junit]     at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:316)
    [junit]
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.009 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: random codec of testcase 'test1' was: Sep
    [junit] ------------- ---------------- ---------------
{noformat}
"
0,"When thread is interrupted we should throw a clear exceptionThis is the 3.0 followon from LUCENE-1573.  We should throw a dedicated exception, not just RuntimeException.

Recent discussion from java-dev ""Thread.interrupt()"" subject: http://www.lucidimagination.com/search/document/8423f9f0b085034e/thread_interrupt"
1,"session.setNamespacePrefix() creates ambiguous mappings1.) assume the following initial global mappings in the NamespaceRegistry 
(prefixes in lowercase, URIs in uppercase):

a  <-> A
b  <-> B
c  <-> C

2.) locally remap  the namespaces in a session using the following code:

            session.setNamespacePrefix(""x"", ""B"");
            session.setNamespacePrefix(""b"", ""C"");
            session.setNamespacePrefix(""c"", ""B"");

this results in the following session-local mappings:

a  <-> A
c  <-> B
b  <-> C

3.) now the following stmt:

            session.setNamespacePrefix(""b"", ""A"");

produces this ambiguous mapping:

b  <-> A
c  <?> B
c  <?> C

"
1,"Cannot clone BasicClientCookie2 without specified portsThe clone method returns a null pointer exception when called on a BasicClientCookie2 that does not use any ports properties.
In other words, it is impossible to clone a BasicClientCookie2 instance without ports specification.

In the clone() method, they are two main instructions :
 - calling clone() method on super
 - calling clone() method on the ports integer array (which is null)

It may be a good idea to check whether the array is null or not

"
0,ant test won't run in 'out of the box' installationone possible solution would be to remove 'lib' from the junit.classpath
1,"Saving concurrent sessions executing random operations causes a corrupt JCRRun the attached unit test. Several concurrent sessions add, move, and remove nodes. Then the index is removed and the repository is again started. The repository is in an inconsistent state and the index cannot be rebuild. Also a lot of exceptions occur. See (see Output before patch.txt). Note that the unit test also suffers from the deadlock of issue http://issues.apache.org/jira/browse/JCR-2525 about half the time."
1,"ConnPoolByRoute driving RouteSpecificPool to IllegalStateHi all,

I encountered an issue on ConnPoolByRoute / RouteSpecificPool on HTTPClient 4.0.1, akin to HTTPCLIENT-747 (it also leads to a java.lang.IllegalStateException: No entry created for this pool. HttpRoute[{}XXX] ), but it is not a concurrency issue (no race condition, just a logic error if I understood it correctly).

From my understanding, the error lies in ConnPoolByRoute#getEntryBlocking
Quoting from the code (line 309-314) :
RouteSpecificPool rospl = getRoutePool(route, true);
... 
} else if (hasCapacity && !freeConnections.isEmpty()) {

deleteLeastUsedEntry();
entry = createEntry(rospl, operator);

} else { ...

The short version of the issue is : under certain circumstances, #deleteLeastUsedEntry can remove rospl from the map of known RootSpecificPool. But as this code still holds on to the rospl instance, it will modify its state in a way the pool will never recover from later, not having any other way to access this instance when the connection gets released.

A Step by Step guide to what's going wrong.
0) You have to be in a condition that leads to the execution of said code extract (i.e. no free entry on the current route - but the route already is registered to the global pool -, current Route has capacity, max connections reached for the global pool, but there are free connections to destroy).
2) We arrive in deleteLeastUsedEntry(). We get the last entry from a queue. It can be that this entry is bound to the same (hashCode() wise) Route that the one we are getting a connection to (i.e. rospl instance held in the #getEntryBlocking context)
3) this entry can be the last of its pool, thus at this point, rospl.isUnused() == true
4) As a consequence, deleteEntry() will remove rospl from the routeToPool map
5) Back in the getEntryBlocking method, we do entry = createEntry(rospl, operator), which will do createdEntry() on the ""locally-scoped"" rospl instance that has just been removed from routeToPool 
6) When the connexion from this new entry is released at some point in the future, the rospl instance that got the createdEntry() does not exist anymore, and it is a new one that gets the freeEntry() call
7) App breaks : this newly created RouteSpecificPool throws IllegalStateException.

Step 0, though, is a rare condition that I only reached during stress tests, and on a SSL client-auth server. This is so because this is the only condition that I know of in HTTPClient, where there is a keep-alive connection in the RouteSpecificPool that can not be reused (when the State is set to the X500 principals of the client cert in the pool, but not in the request).

Possible fix (from what I understand) :
The rospl instance variable in the context of getEntryBlocking() should be protected against the consequences of #deleteLeastUsedEntry().
Not being confortable with all issues at hand, nor with the code base, the simplest thing I can think of would be to preemptively reset the rospl variable after deleteLeastUsedEntry(), thus writing the previous code extract as :

} else if (hasCapacity && !freeConnections.isEmpty()) {

deleteLeastUsedEntry();
// delete may have made deprecated the RouteSpecificPool instance
rospl = getRoutePool(route, true);
entry = createEntry(rospl, operator);

} else { ...


I have a test case that I will attach to this issue ASAP.
It is a simple example that triggers the above conditions with 3 HttpGet calls, in a serial fashion. As stated previsouly, these calls need nothing particular, except that one of these calls must go to a HTTPS server with client-side certificate authentication (I guess NTLM would be OK, anything that will place a non null state along with the route in BasicEntryPool).

I hope code is self-explainatory. I get 100% failure in my setup. Just configure your 2 URLS, configure classpath, set your keystores system properties, and launch.

Workaround :
Best workaround I found is : do not get to step 0.
The most robust way I found to do that (i.e. a way that does not involve things like setting max pool size to a gigantic number that can never be reached, ...) is to actively set the ClientContext.USER_TOKEN attribute in an exec context while submitting the request to the client.
Step 0 triggers when there is an idle connection that waits, and when this idle connection can not be reused, which can only happen if the request's ""USER_TOKEN"" does not match the BasicPoolEntry#getState(). As, in the SSL case, the state is the SSL Cert's X500PrincipalName, and I know it in advance, it's easy to set up front.

By the way, this taught me that I never could benefit from connection reuse strategies in this SSL case, as connections would always get into the pool with a USER_TOKEN that my requests never had. Don't know if it's mentionned somewhere in the documentation, but this is a noteworthy fact to me.

Please feel free to comment / correct any mistakes."
0,"Typo in API_CHANGES_3_0.txtDigestSheme should be DigestScheme :)  Also, why is there not an httpclient
component in the list?"
0,"support array/offset/ length setters for Field with binary datacurrently Field/Fieldable interface supports only compact, zero based byte arrays. This forces end users to create and copy content of new objects before passing them to Lucene as such fields are often of variable size. Depending on use case, this can bring far from negligible  performance  improvement. 

this approach extends Fieldable interface with 3 new methods   
getOffset(); gettLenght(); and getBinaryValue() (this only returns reference to the array)

   "
0,"org.apache.jackrabbit.server.io.IOUtil getTempFile misses dot in tmp suffixAt line 168:
File tmpFile = File.createTempFile(""__importcontext"", ""tmp"");
Suffix tmp has no use because the dot is missing.

Should be:
File tmpFile = File.createTempFile(""__importcontext"", "".tmp"");

"
0,Contrib-Spatial should use DocSet API rather then deprecated BitSet APIContrib-Spatial should be rewritten to use the new DocIdSet Filter API with OpenBitSets instead of j.u.BitSets. FilteredDocIdSet can be used to replace (I)SerialChainFilter.
0,"Performance of AC Evaluation1. Performance in access control evaluation
=====================================================================

 - main focus on
   > read performance
   > resource-based access control in .a.j.c/s/authorization/acl/*

 - comparison admin vs. anonymous with full permissions
 - comparision between shortcut and ACL-evaluation.
 - comparison JR 1.4 vs JR 2.0 [actually i will compare Day's CRX as it already provided
   some custom AC stuff with JR 1.4]


2. Potential Problems
=====================================================================

   I would expect the most significant problems to be found in

a) ACLProvider#retrieveResultEntries: calculating effective ACEs
     for each session separately.

b) AclPermission:
     Each instance registering an event listener in order to
     keep the result cache up to date

c) AclPermission:
     Resolution form Path to Item or to nearest existing Item "
1,"document with no term vector fields after documents with term vector fields corrupts the indexIf a document with no term-vector-enabled fields is added after
document(s) that did have term vectors, as part of a single set of
buffered docs, then the term-vector documents file is corrupted
because we fail to write a ""0"" vInt.

Thanks to Grant for spotting this!

Spinoff from this thread:

    http://www.gossamer-threads.com/lists/lucene/java-dev/53306
"
0,"More clarification, improvements and correct behaviour of backwards testsBackwards tests are used since 2.9 to assert, that the new Lucene version supports drop-in-replacement over the previous version. For this all tests from the previous version are compiled against the old version but then run against the new JAR file.

At the beginning the test suite was checking out another branch and doing this, but this was replaced in 3.1 by directly embedding the previous source tree and the previous tests into the backwards/ subdirectory of the SVN source. The whole idea has several problems:

- Tests not only check *public* APIs, they also check internals and sometimes even fields or package-private methods. This is allowed to change in later versions, so we must be able to change the tests, to support this behaviour. This can be done by modifying the backwards tests to pass, but still use the public API unchanged. Sometimes we simply comment out tests, that test internals and not public APIs. For those tests, I would like to propose a Java Annotation for trunk tests like @LuceneInternalTest - so we can tell the tests runner for backwards (when this test is moved as backwards layer, e.g in 4.1, that it runs all tests *but* not this marked one. This can be done easily with Junit3/4 in LuceneTestCase(J4). This is not part of this issue, but a good idea.
- Sometimes we break backwards compatibility. Currently we do our best to change the tests to reflect this, but this is unneeded and stupi, as it brings two problems. The backwards tests should be compiled against the old version of Lucene. If we change this old Version in the backwards folder, its suddenly becomes nonsense. At least the JAR artifacts of the previous version should stay *unchanged* in all cases! If we break backwards, the correct way to do this, is to simply disable coresponding tests! There is no need to make them work again, as we broke backwards, wy test plugin? The trunk tests already check the functionality, backwards tests only check API. If we fix the break in backwards, we do the contra of what they are for.

So I propose the following and have implemented in a patch for 3.x branch:

- Only include the *tests* and nothing else into the backwards branch, no source files of previous Lucene Core.
- Add the latest released JAR artifact of lucene-core.jar into backwards/lib, optimally with checksum (md5/sh1). This enforces that it is not changed and exactly do what they are for: To compile the previous tests against. This is the only reason for this JAR file.
- If we break backwards, simply *disable* the tests by commenting out, ideally with a short note and the JIRA issue that shows the break.
- If we change inner behaviour of classes, that are not public, dont fix, disable tests. Its simple: backwards tests are only for API compatibility testsing of public APIs. If a test uses internals it should not be run. For that we should use a new annotation in trunk (see above).

This has several good things:

- we can package backwards tests in src ZIP. Its not a full distrib, only the core tests and the JAR file. This enables people that doenloaded the src ZIP files to also run backwrads tests
- Your SVN checkout is not so big and backwards tests run faster!

There are some problems, with one example in the attached patch:

- If we have mock classes in the tests (e.g. MockRAMDirectory) that extend Lucene classes and have access to their internal APIs, a change in these APIs will make them fail to work unchanged. The above example (MockRAMDir) is used in lots of tests and uses a internal RAMDir field that changed type in 3.1. But we cannot disable all tests using this dir (no tests will survive). As we cannot change the previous versions JAR to reflect this, you have to use some trick in this interbal test class. In this case I removed static linking of this field and replaced by reflection. This enables compilation against old JAR, but supports running in new version. This is really a special case, but works good here.

Any comments?"
0,"Provide possibility to import protected items using Session import functionalitySessionImporter and WorkspaceImporter currently skip all protected items encountered during import except for some special cases
(see JCR-2172 and WorkspaceImporter#postProcessNode).
The specification only mandates that protected content is treated in a consistent manner, but allows the implementation to either import or ignore it.

Find attached a patch containing some initials steps to allow to extend the default import behavior:
Instead of skipping protected items (and in case of nodes the complete tree below it), they should be passed to a separate handler,
that may or may not be able to deal with them and needs to assert, that they are in a valid format.

The patch includes:

- Abstract classes for that protected item import
- Default implementations that never import protected nodes (same behavior as we have today)
- An example implementation for the AC-content (just to see if it works for simple cases) + some trivial tests.
- Changes to SessionImporter to demonstrate how import of protected items would be enabled.

The patch doesn't include yet:

- Changes to WorkspaceImporter (would +- be according to SessionImporter)
- Changes to WorkspaceImpl/SessionImpl as well as configuration that would allow to modify the default behavior.
- Examples for import of protected properties.
- Examples for workspace import.

The patch has the following limitations or TODOs:

- Proper handling of protected references properties or non-protected ref properties with the tree defined by a protected node.
- Test / Careful review if the various ImportUUIDBehaviors are/can properly be covered, specially in case of ""replace-existing"".

The patch in addition addresses:

- An inconsistency in the SessionImporter:
  > Attempt to import protected content below an existing protected node => skipped
  > Attempt to import protected content that doesn't yet exist => first node is imported, ConstraintViolationException for child-nodes.
  > This behavior is also reflected in the Node-stack... where in the first case 'null' is pushed, in the second case the first protected node.
     (see also JCR-2172 for details).
"
0,"Remove deprecated charset support from Greek and Russian analyzersThis removes the deprecated support for custom charsets.

One thing I found is that once these charsets are removed, RussianLowerCaseFilter is the same as LowerCaseFilter.
So I marked it deprecated to be removed in 3.1
"
0,"Highlighter should support all MultiTermQuery subclasses without castsIn order to support MultiTermQuery subclasses the Highlighter component applies instanceof checks for concrete classes from the lucene core. This prevents classes like RegexQuery in contrib from being supported. Introducing dependencies on other contribs is not feasible just for being supported by the highlighter.

While the instanceof checks and subsequent casts might hopefully go somehow away  in the future but for supporting more multterm queries I have a alternative approach using a fake IndexReader that uses a RewriteMethod to force the MTQ to pass the field name to the given reader without doing any real work. It is easier to explain once you see the patch - I will upload shortly.
"
1,"Deadlock between SingleClientConnManager.releaseConnection() and SingleClientConnManager.shutdown()It's possible to create a deadlock within SingleClientConnectionManager.

When JMeter interrupts a test, it calls HttpUriRequest.abort(), and as part of thread end processing it calls SingleClientConnManager.shutdown().

See deadlock details below.

I don't yet know why the shutdown is called before the abort finishes; that is probably a bug.

However, there may be a issue with the locking strategy within SCCM, hence this report.

""Thread-18"":
        at org.apache.http.impl.conn.SingleClientConnManager.releaseConnection(SingleClientConnManager.java:258)
        - waiting to lock <0x19e00118> (a org.apache.http.impl.conn.SingleClientConnManager)
        at org.apache.http.impl.conn.AbstractClientConnAdapter.abortConnection(AbstractClientConnAdapter.java:323)
        - locked <0x19e00148> (a org.apache.http.impl.conn.SingleClientConnManager$ConnAdapter)
        at org.apache.http.client.methods.HttpRequestBase.abort(HttpRequestBase.java:161)
        at org.apache.jmeter.protocol.http.sampler.HTTPHC4Impl.interrupt(HTTPHC4Impl.java:1090)
        at org.apache.jmeter.protocol.http.sampler.HTTPSamplerProxy.interrupt(HTTPSamplerProxy.java:77)
        at org.apache.jmeter.threads.JMeterThread.interrupt(JMeterThread.java:580)
        at org.apache.jmeter.engine.StandardJMeterEngine.tellThreadsToStop(StandardJMeterEngine.java:552)
        at org.apache.jmeter.engine.StandardJMeterEngine.access$2(StandardJMeterEngine.java:547)
        at org.apache.jmeter.engine.StandardJMeterEngine$StopTest.run(StandardJMeterEngine.java:284)
        at java.lang.Thread.run(Thread.java:662)
""Thread Group 1-1"":
        at org.apache.http.impl.conn.AbstractPooledConnAdapter.detach(AbstractPooledConnAdapter.java:106)
        - waiting to lock <0x19e00148> (a org.apache.http.impl.conn.SingleClientConnManager$ConnAdapter)
        at org.apache.http.impl.conn.SingleClientConnManager.shutdown(SingleClientConnManager.java:342)
        - locked <0x19e00118> (a org.apache.http.impl.conn.SingleClientConnManager)
        at org.apache.jmeter.protocol.http.sampler.HTTPHC4Impl.closeThreadLocalConnections(HTTPHC4Impl.java:1076)
        at org.apache.jmeter.protocol.http.sampler.HTTPHC4Impl.threadFinished(HTTPHC4Impl.java:1065)
        at org.apache.jmeter.protocol.http.sampler.HTTPSamplerProxy.threadFinished(HTTPSamplerProxy.java:71)
        at org.apache.jmeter.threads.JMeterThread$ThreadListenerTraverser.addNode(JMeterThread.java:553)
        at org.apache.jorphan.collections.HashTree.traverseInto(HashTree.java:986)
        at org.apache.jorphan.collections.HashTree.traverse(HashTree.java:969)
        at org.apache.jmeter.threads.JMeterThread.threadFinished(JMeterThread.java:528)
        at org.apache.jmeter.threads.JMeterThread.run(JMeterThread.java:308)
        at java.lang.Thread.run(Thread.java:662)
"
0,"Remove unnecessary String concatenation in IndexWriterI've noticed a couple of places in IndexWriter where a boolean string is created by bool + """", or integer by int + """". There are some places (in setDiagonstics) where a string is concatenated with an empty String ...
The patch uses Boolean.toString and Integer.toString, as well as remove the unnecessary str + """"."
1,"o.a.j.core.integration.PrepareTestRepository fails on 2nd and every subsequent invocationconsole output: 

-------------------------------------------------------------------------------
Test set: org.apache.jackrabbit.core.integration.PrepareTestRepository
-------------------------------------------------------------------------------
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 3.428 sec <<< FAILURE!
testPrepareTestRepository(org.apache.jackrabbit.core.integration.PrepareTestRepository)  Time elapsed: 3.397 sec  <<< ERROR!
javax.jcr.RepositoryException: Invalid node type definition: {http://www.apache.org/jackrabbit/test}versionable already exists: {http://www.apache.org/jackrabbit/test}versionable already exists
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:308)
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:488)
	at org.apache.jackrabbit.core.integration.PrepareTestRepository.testPrepareTestRepository(PrepareTestRepository.java:49)
"
0,"fail build if contrib tests fail to compilespinoff of LUCENE-885, from Steven's comments...

Looking at the current build (r545324) it looks like the some contrib failures are getting swallowed. Things like lucli are throwing errors along the lines of

 [subant] /home/barronpark/smparkes/work/lucene/trunk/common-build.xml:366: srcdir ""/home/barronpark/smparkes/work/lucene/trunk/contrib/lucli/src/test"" does not exist!

but these don't make it back up to the top level status.

It looks like the current state will bubble up junit failures, but maybe not build failures?

...

It's ""test-compile-contrib"" (if you will) that fails and rather being contrib-crawled, that's only done as the target of ""test"" in each contrib directory, at which point, it's running in the protected contrib-crawl.

Easy enough to lift this loop into another target, e.g., build-contrib-test. And that will start surfacing errors, which I can work through.
"
1,"NullPointerExc. when indexing empty field with term vectorsMark Harwood mentioned this on the user's list. Running the attached code 
you'll get this exception: 
 
Exception in thread ""main"" java.lang.NullPointerException 
	at 
org.apache.lucene.index.TermVectorsReader.clone(TermVectorsReader.java:303) 
	at 
org.apache.lucene.index.SegmentReader.getTermVectorsReader(SegmentReader.java:473) 
	at 
org.apache.lucene.index.SegmentReader.getTermFreqVectors(SegmentReader.java:507) 
	at 
org.apache.lucene.index.SegmentMerger.mergeVectors(SegmentMerger.java:204) 
	at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:94) 
	at 
org.apache.lucene.index.IndexWriter.mergeSegments(IndexWriter.java:618) 
	at 
org.apache.lucene.index.IndexWriter.flushRamSegments(IndexWriter.java:571) 
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:339) 
	at TVBug.main(TVBug.java:16)"
1,"Don't leak deleted open file handles with pooled readersIf you have CFS enabled today, and pooling is enabled (either directly
or because you've pulled an NRT reader), IndexWriter will hold open
SegmentReaders against the non-CFS format of each merged segment.

So even if you close all NRT readers you've pulled from the writer,
you'll still see file handles open against files that have been
deleted.

This count will not grow unbounded, since it's limited by the number
of segments in the index, but it's still a serious problem since the
app had turned off CFS in the first place presumably to avoid risk of
too-many-open-files.  It's also bad because it ties up disk space
since these files would otherwise be deleted.
"
0,"Performance improvement for SegmentMerger.mergeNorms()This patch makes two improvements to SegmentMerger.mergeNorms():

1) When the SegmentMerger merges the norm values it allocates a new byte array to buffer the values for every field of every segment. The size of such an array equals the size of the corresponding segment, so if large segments are being merged, those arrays become very large, too.
We can easily reduce the number of array allocations by reusing a byte array to buffer the norm values that only grows, if a segment is larger than the previous one.

2) Before a norm value is written it is checked if the corresponding document is deleted. If not, the norm is written using IndexOutput.writeByte(byte[]). This patch introduces an optimized case for segments that do not have any deleted docs. In this case the frequent call of IndexReader.isDeleted(int) can be avoided and the more efficient method IndexOutput.writeBytes(byte[], int) can be used.


This patch only changes the method SegmentMerger.mergeNorms(). All unit tests pass."
0,"Mark contrib/wikipedia as experimentalI am going to add javadocs to trunk and 2_3 branch that mark the WikipediaTokenizer as experimental.  I think it is fine to release, but I want people to know that the grammar may change in the next release (although I will try to keep it the same)"
1,"IndexMerger throws null pointer exception without stacktraceI get the following errors in my log file randomly.  It seems to happen most often when creating the lucene indices, but has happened at other times as well:

[IndexMerger] ERROR - Error while merging indexes: java.lang.NullPointerException

The code at org.apache.jackrabbit.core.query.lucene.IndexMerger line 344 appears to be the point where the error is logged, but no other information is provided because the throwable isn't sent to the log (only the toString() version of the exception).  I haven't been able to tell if any indexes are corrupt when this happens.

I suggest that the logger be changed to determine where the null pointer is coming from first, then resolve the actual issue that is occurring.
"
0,"Massive Code Duplication in Contrib Analyzers - unifly the analyzer ctorsDue to the variouse tokenStream APIs we had in lucene analyzer subclasses need to implement at least one of the methodes returning a tokenStream. When you look at the code it appears to be almost identical if both are implemented in the same analyzer.  Each analyzer defnes the same inner class (SavedStreams) which is unnecessary.
In contrib almost every analyzer uses stopwords and each of them creates his own way of loading them or defines a large number of ctors to load stopwords from a file, set, arrays etc.. those ctors should be removed / deprecated and eventually removed.


"
0,"Allow usage of HyphenationCompoundWordTokenFilter without dictionaryWe should allow to use the HyphenationCompoundWordTokenFilter without a dictionary. This produces a lot of ""nonword"" tokens but might be useful sometimes."
1,"MoreLikeThis reuses a reader after it has already closed itMoreLikeThis has a fatal bug whereby it tries to reuse a reader for multiple fields:

{code}
    Map<String,Int> words = new HashMap<String,Int>();
    for (int i = 0; i < fieldNames.length; i++) {
        String fieldName = fieldNames[i];
        addTermFrequencies(r, words, fieldName);
    }
{code}

However, addTermFrequencies() is creating a TokenStream for this reader:

{code}
    TokenStream ts = analyzer.reusableTokenStream(fieldName, r);
    int tokenCount=0;
    // for every token
    CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);
    ts.reset();
    while (ts.incrementToken()) {
        /* body omitted */
    }
    ts.end();
    ts.close();
{code}

When it closes this analyser, it closes the underlying reader.  Then the second time around the loop, you get:

{noformat}
Caused by: java.io.IOException: Stream closed
	at sun.nio.cs.StreamDecoder.ensureOpen(StreamDecoder.java:27)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:128)
	at java.io.InputStreamReader.read(InputStreamReader.java:167)
	at com.acme.util.CompositeReader.read(CompositeReader.java:101)
	at org.apache.lucene.analysis.standard.StandardTokenizerImpl.zzRefill(StandardTokenizerImpl.java:803)
	at org.apache.lucene.analysis.standard.StandardTokenizerImpl.getNextToken(StandardTokenizerImpl.java:1010)
	at org.apache.lucene.analysis.standard.StandardTokenizer.incrementToken(StandardTokenizer.java:178)
	at org.apache.lucene.analysis.standard.StandardFilter.incrementTokenClassic(StandardFilter.java:61)
	at org.apache.lucene.analysis.standard.StandardFilter.incrementToken(StandardFilter.java:57)
	at com.acme.storage.index.analyser.NormaliseFilter.incrementToken(NormaliseFilter.java:51)
	at org.apache.lucene.analysis.LowerCaseFilter.incrementToken(LowerCaseFilter.java:60)
	at org.apache.lucene.search.similar.MoreLikeThis.addTermFrequencies(MoreLikeThis.java:931)
	at org.apache.lucene.search.similar.MoreLikeThis.retrieveTerms(MoreLikeThis.java:1003)
	at org.apache.lucene.search.similar.MoreLikeThis.retrieveInterestingTerms(MoreLikeThis.java:1036)
{noformat}

My first thought was that it seems like a ""ReaderFactory"" of sorts should be passed in so that a new Reader can be created for the second field (maybe the factory could be passed the field name, so that if someone wanted to pass a different reader to each, they could.)

Interestingly, the methods taking File and URL exhibit the same issue.  I'm not sure what to do about those (and we're not using them.)  The method taking File could open the file twice, but the method taking a URL probably shouldn't fetch the same URL twice.
"
0,"OracleDatabaseJournal should assume Oracle defaultsOracleFileSystem and OraclePersistenceManager assumes defaults for certain properties, such as driver and databaseType.  These defaults should be extended to OracleDatabaseJournal for consistency and ease of use."
0,"Add matchVersion to StandardAnalyzerI think we should add a matchVersion arg to StandardAnalyzer.  This
allows us to fix bugs (for new users) while keeping precise back
compat (for users who upgrade).

We've discussed this on java-dev, but I'd like to now make it concrete
(patch attached).  I think it actually works very well, and is a
simple tool to help us carry out our back-compat policy.

I coded up an example with StandardAnalyzer:

  * The ctor now takes a required arg (Version matchVersion).  You
    pass Version.LUCENE_CURRENT to always get lates & greatest, or eg
    Version.LUCENE_24 to match 2.4's bugs/settings/behavior.

  * StandardAalyzer conditionalizes the ""replace invalid acronym"" and
    ""enable position increment in StopFilter"" based on matchVersion.

  * It also prevents creating zillions of ctors, over time, as we need
    to change settings in the class.  EG StandardAnalyzer now has 2
    settings that are version dependent, and there's at least another
    2 issues open on fixing some more of its bugs.

The migration is also very clean: we'd only add this to classes on an
""as needed"" basis.  On the first release that adds the arg, the
default remains back compatible with the prior release.  Then, going
forward, we are free to fix issues on that class and conditionalize by
matchVersion.

The javadoc at the top of StandardAnalyzer clearly calls out what
version specific behavior is done:

{code}
 * <p>You must specify the required {@link Version}
 * compatibility when creating StandardAnalyzer:
 * <ul>
 *   <li> As of 2.9, StopFilter preserves position
 *        increments by default
 *   <li> As of 2.9, Tokens incorrectly idenfied as acronyms
 *        are corrected (see <a href=""https://issues.apache.org/jira/browse/LUCENE-1068"">LUCENE-1608</a>
 * </ul>
 *
{code}
"
0,JSR 283: Built-In Node Typessync definitions of built-in node types with spec
0,Move privilege reader/writer to spi-commons and use qualified namesthe current privilege reader in jcr-commons uses a PrivilegeDefinition that is based on pure string rather than qualified names. suggest to move that to the spi-commons and use org.apache.jackrabbit.spi.Name for the privilege names.
0,"Support lower-/upper-case functionsThe query languages should support lower- and upper-case functions when matching property values to string literals.

Example 1: find all nodes with a string property foo with a lower-cased value that equals 'bar':

In XPath that's:

//*[fn:lower-case(@foo) = 'bar']

An in SQL:
SELECT * FROM nt:base WHERE LOWER(foo) = 'bar'

Example 2: find all nodes with a string property foo with an upper-cased value that matches '%JCR%'

XPath: //*[jcr:like(fn:upper-case(@foo), '%JCR%')]

SQL: SELECT * FROM nt:base WHERE UPPPER(foo) LIKE '%JCR%'"
1,"HttpClient does not properly handle 'application/x-www-form-urlencoded' encodingAs always I'd like to pass on my thanks, I'm finding HttpClient really useful.

The problem occurs because I use Struts map based ActionForm and these generate 
request parameters of the form:

<input type=""text"" name=""searchSelection(c)"">

When this is submitted using the PostMethod class the generateRequestBody() is 
called and in turn this calls the URI.encode() method with a BitSet of the 
acceptable characters. In this case the '(' and ')' characters are marked as 
acceptable.

The problem is that this does not work correctly when I submit it to my remote 
server. If however I issue the request directly (from a webpage rather than 
using HttpClient) it works and when I examine the request input stream I can see 
that the parameter has been re-written so that 'select(c)' is displayed as 
'select%28c%29'.

This may be my error because of encoding problems or the fact I am not setting 
the content type etc. correctly. Or it could be a bug. I'm afraid my HTTP 
knowledge is not good enough.

Chris Mein"
0,"Introduce cache for frequently used index lookupsSome queries heavily use hierarchy relations to resolve location steps. E.g. ChildAxisQuery or DescendantSelfAxisQuery. Currently those hierarchy relations are looked up from the native lucene index which is not very efficient. The index should maintain a cache of frequently used hierarchy lookups. 
That is, calls like IndexReader.termDocs() on terms with field: UUID or PARENT"
0,"Add System.getProperty(""tempDir"") as final static to LuceneTestCase(J4)Almost every test calls System.getProperty(""tempDir"") and some of them check the return value for null. In other cases the test simply fails from within eclipse.

We should add this to LuceneTestCase(J4) as a static final constant. For enabling tests run in eclipse, we can add a fallback to ""."", if the Sysprop is not defined."
1,"TestDocValuesIndexing.testAddIndexes failures on docvalues branchdoc values branch r1124825, reproducible 
{code}
    [junit] Testsuite: org.apache.lucene.index.values.TestDocValuesIndexing
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.716 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestDocValuesIndexing -Dtestmethod=testAddIndexes -Dtests.seed=5939035003978436534:-6429764582682717131
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=MockRandom, BYTES_VAR_DEREF=MockRandom, INTS=Pulsing(freqCutoff=13)}, locale=da_DK, timezone=Asia/Macao
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDocValuesIndexing]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=88582432,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAddIndexes(org.apache.lucene.index.values.TestDocValuesIndexing):     Caused an ERROR
    [junit] null
    [junit] java.nio.channels.ClosedChannelException
    [junit]     at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:88)
    [junit]     at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:603)
    [junit]     at org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:161)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:222)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)
    [junit]     at org.apache.lucene.store.DataInput.readInt(DataInput.java:73)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readInt(BufferedIndexInput.java:162)
    [junit]     at org.apache.lucene.store.DataInput.readLong(DataInput.java:115)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readLong(BufferedIndexInput.java:175)
    [junit]     at org.apache.lucene.store.MockIndexInputWrapper.readLong(MockIndexInputWrapper.java:136)
    [junit]     at org.apache.lucene.index.values.PackedIntsImpl$IntsEnumImpl.<init>(PackedIntsImpl.java:263)
    [junit]     at org.apache.lucene.index.values.PackedIntsImpl$IntsEnumImpl.<init>(PackedIntsImpl.java:249)
    [junit]     at org.apache.lucene.index.values.PackedIntsImpl$IntsReader.getEnum(PackedIntsImpl.java:239)
    [junit]     at org.apache.lucene.index.values.DocValues.getEnum(DocValues.java:54)
    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.getValuesEnum(TestDocValuesIndexing.java:484)
    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.testAddIndexes(TestDocValuesIndexing.java:202)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1304)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1233)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.values.TestDocValuesIndexing FAILED
{code}

and

{code}

    [junit] Testsuite: org.apache.lucene.index.values.TestDocValuesIndexing
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.94 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestDocValuesIndexing -Dtestmethod=testAddIndexes -Dtests.seed=-3677966427932339626:-4746638811786223564
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=Standard, BYTES_FIXED_DEREF=MockSep, FLOAT_64=SimpleText}, locale=ca, timezone=Asia/Novosibirsk
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDocValuesIndexing]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=88596152,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAddIndexes(org.apache.lucene.index.values.TestDocValuesIndexing):     Caused an ERROR
    [junit] Bad file descriptor
    [junit] java.io.IOException: Bad file descriptor
    [junit]     at java.io.RandomAccessFile.seek(Native Method)
    [junit]     at org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput.readInternal(SimpleFSDirectory.java:101)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:222)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)
    [junit]     at org.apache.lucene.store.MockIndexInputWrapper.readByte(MockIndexInputWrapper.java:105)
    [junit]     at org.apache.lucene.index.values.Floats$FloatsReader.load(Floats.java:281)
    [junit]     at org.apache.lucene.index.values.SourceCache$DirectSourceCache.load(SourceCache.java:101)
    [junit]     at org.apache.lucene.index.values.DocValues.getSource(DocValues.java:101)
    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.getSource(TestDocValuesIndexing.java:472)
    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.getValuesEnum(TestDocValuesIndexing.java:482)
    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.testAddIndexes(TestDocValuesIndexing.java:203)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1304)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1233)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.values.TestDocValuesIndexing FAILED
{code}"
1,"HTMLTextExtractor modifying UTF-8 encoded StringTrying to extract an HTML that is UTF-8 encoded is modifying the UTF-8 special char (like , , ,  etc).

This cause a wrong search, because lucene use this extractor to index content.

See attachments for an example of the problem."
0,"Removing-nodes with unexpected nodetypetobias adds a logic with JCR-973 that the DefaultHandler checks the nodetype of the jcr:content node
and if it does not match it will be deleted and created with the new one.
i think its dangerous to automacially change a node type.
if the node was added programatically and then saved through webdav it could happen that the nodetype will be changed
from nt:resource to the nt:unstructured.
if some logic depends on the nodetype the failure search will be hard ;-)"
0,"junit dependency in pom.xml with default compile scopeThe dependency set as defined in:
http://www.ibiblio.org/maven2/commons-httpclient/commons-httpclient/3.0.1/commons-httpclient-3.0.1.pom
includes junit, but not with a <scope>test</test>. I suppose the junit dependency should have a test scope. Could someone fix this? Because of this my application is packaged including junit 3.8.1, which adds 118KiB for nothing.
"
0,"bad project.xmlproject.xml in the ""api"" module actually contains an error (nested comment) and can't be read by maven."
0,"Node.getReferences() does not properly reflect saved but not yet committed changesNode.getReferences() currently only returns committed references. The specification however says:

<quote>
Some level 2 implementations may only return properties that have been
saved (in a transactional setting this includes both those properties
that have been saved but not yet committed, as well as properties that
have been committed). Other level 2 implementations may additionally
return properties that have been added within the current Session but are
not yet saved.
</quote>

Jackrabbit does not support the latter, but at least has to support the first."
0,"JCR2SPI: improve ItemDefinitionProviderImpl.getMatchingPropdef to better handle multiple residualsWhen a new property is set with unknown type (missing PropertyType parameter), ItemDefinitionProviderImpl.getMatchingPropdef() is used to find an applicable property definition.

There may be cases where multiple residual property defs may match, for instance, when the repository allows only a certain set of property types on that node type.

In this case, when the set of allowable types includes STRING, that propdef should be returned. After all, the client did not specify the type, so STRING is most likely the best match.
"
0,Improved READMEs for building sequence of trunk and jcr-server
1,"Crash when querying an index using multiple term positions.file: MultipleTermPositions.java, line: 201, function: skipTo(int).

This refers to the source that can currently be downloaded from the lucene site,
Lucene v. 1.4.3.

The function peek() returns null (because top() also retruned null). There is no
check for this, as far as I can understand. The function doc() is called on a
null-object, which results in a NullPointerException.

I switched the specified line to this one:

while(_termPositionsQueue.peek() != null && target >
_termPositionsQueue.peek().doc())

This got rid of the crash for me."
1,"JCR2SPI NodeEntryImpl throws NPE during reorderNodesTwo folder nodes are created below root. From the root node, the 2nd folder is ordered before the first node. The request is batched up correctly, but upon save, NodeEntryImpl throws a NullPointerException in the first line of the completeTransientChanges method, because revertInfo.oldParent is null.

Test code:

		final String FOLDER1 = ""folder1"", FOLDER2 = ""folder2"";
		
		// Create folder 1 on server in root
		Session serverSession = login(repository, creds);
		Node serverRootNode = serverSession.getRootNode();
		Node serverFolder1 = serverRootNode.addNode(FOLDER1, ""nt:folder"");
		
		// Create folder 2 on server in root
		Node serverFolder2 = serverRootNode.addNode(FOLDER2, ""nt:folder"");
		serverSession.save();
		
		// Validate order (TODO)
		
		// Perform reorder via client
		Session clientSession = login(clientRepository, creds);
		Node clientRootNode = clientSession.getRootNode();
		clientRootNode.orderBefore(FOLDER2, FOLDER1);
		clientSession.save(); <== Throws NPE

Call Stack:

    [junit] java.lang.NullPointerException
    [junit]     at org.apache.jackrabbit.jcr2spi.hierarchy.NodeEntryImpl.completeTransientChanges(NodeEntryImpl.java:1354)
    [junit]     at org.apache.jackrabbit.jcr2spi.hierarchy.NodeEntryImpl.access$1100(NodeEntryImpl.java:60)
    [junit]     at org.apache.jackrabbit.jcr2spi.hierarchy.NodeEntryImpl$RevertInfo.statusChanged(NodeEntryImpl.java:1465)
    [junit]     at org.apache.jackrabbit.jcr2spi.state.ItemState.setStatus(ItemState.java:257)
    [junit]     at org.apache.jackrabbit.jcr2spi.state.NodeState.adjustNodeState(NodeState.java:554)
    [junit]     at org.apache.jackrabbit.jcr2spi.state.NodeState.persisted(NodeState.java:276)
    [junit]     at org.apache.jackrabbit.jcr2spi.state.ChangeLog.persisted(ChangeLog.java:135)
    [junit]     at org.apache.jackrabbit.jcr2spi.WorkspaceManager.execute(WorkspaceManager.java:479)
    [junit]     at org.apache.jackrabbit.jcr2spi.state.SessionItemStateManager.save(SessionItemStateManager.java:149)
    [junit]     at org.apache.jackrabbit.jcr2spi.ItemImpl.save(ItemImpl.java:239)
    [junit]     at org.apache.jackrabbit.jcr2spi.SessionImpl.save(SessionImpl.java:317)
    [junit]     at TestWsNodeReorder.testReorderNodes(TestWsNodeReorder.java:72)


I'm using an SPI I implemented, in conjunction with the jcr2spi and spi2jcr bridges, coupled with a back-end Jackrabbit in-memory filesystem. So there's always the possibility that node or property SPI calls inject errors and cause this downstream problem."
0,"Allow to control how payloads are mergedLucene handles backwards-compatibility of its data structures by
converting them from the old into the new formats during segment
merging. 

Payloads are simply byte arrays in which users can store arbitrary
data. Applications that use payloads might want to convert the format
of their payloads in a similar fashion. Otherwise it's not easily
possible to ever change the encoding of a payload without reindexing.

So I propose to introduce a PayloadMerger class that the SegmentMerger
invokes to merge the payloads from multiple segments. Users can then
implement their own PayloadMerger to convert payloads from an old into
a new format.

In the future we need this kind of flexibility also for column-stride
fields (LUCENE-1231) and flexible indexing codecs.

In addition to that it would be nice if users could store version
information in the segments file. E.g. they could store ""in segment _2
the term a:b uses payloads of format x.y"".
"
0,"Connection is not released back to the pool if a runtime exception is thrown in HttpMethod#releaseConnection methodthe default config of leaving the HttpClientParams.CONNECTION_MANAGER_TIMEOUT as zero means 
that the first time the connection manager fails to immediately get a connection you application hangs. 
(at least using MultiThreadedHttpConnectionManager.)

this is because the zero gets passed onto a call to Object.wait(long timeout) and, from the docs, ""If 
timeout is zero, however, then real time is not taken into consideration and the thread simply waits 
until notified."". 

since nothing ever ""notify()""s the thread everything just stops...

the default behaviour of the client more should be more predictable. you don't expect it to hang your 
entire app if it can't get a connection, you expect it to timeout then throw an exception or give some 
other kind of feedback.

it would make sense to give a default of, say, arbitrarily, 10 seconds or so. this would save every single 
user of the classes having to dig around in the code/documentation and explictly set this param. they 
might decide that the default value isn't right and hence change it, but that's tweaking behaviour, not 
correcting it. i certainly thought it was a bug in the code (yours or mine), not my config and have been 
fretting around it for a while.

best,
garry"
0,"Send all variants' ETags on ""variant miss""From section 13.6 of RFC 2616:

If an entity tag was assigned to a cached representation, the forwarded request SHOULD be conditional and include the entity tags in an If-None-Match header field from all its cache entries for the resource. This conveys to the server the set of entities currently held by the cache, so that if any one of these entities matches the requested entity, the server can use the ETag header field in its 304 (Not Modified) response to tell the cache which entry is appropriate. If the entity-tag of the new response matches that of an existing entry, the new response SHOULD be used to update the header fields of the existing entry, and the result MUST be returned to the client.

Presently, we simply forward the request to the request without the conditionals.  This improvement would consist of adding the conditionals to the request, and properly handling the response.  An example of such would be the following:

 - request resource with ""Accept-Encoding: gzip"", response has ""Etag: etag1"", ""Vary: Accept-Encoding""
 - request resource with ""Accept-Encoding: deflate"", request is forwarded with ""If-None-Match: etag1"" added, response is 200, with ""ETag: etag2""
 - request resource with ""Accept-Encoding: gzip, deflate"", request is forwarded with ""If-None-Match: etag1, etag2"" added, response is 304, with ""ETag: etag1"" indicating we should use the first response for this request"
1,"httpclient charset encooding loosing problemfile: org\apache\commons\httpclient\HttpConstants.java 


line: near 261


---------------------------------


public static String getContentString(final byte[] data, String charset) {


        return getContentString(data, 0, data.length);


    }


---------------------------------


must be


---


        return getContentString(data, 0, data.length, charset);


---"
1,"Changes of JCR-313 introduced db-transaction problemthe fix of JCR-313 changed the autocommit from 'true' to 'false', resulting the DatabaseFileSystems not to write back correctly anymore."
0,"JMX Stats for the SessionI've named them Core stats. This will include:
 - number of sessions currently opened
 - session read / write operations per second

The stats refresh once a minute.
This is disabled by default, so it will not affect performance."
0,AbstractWebdavServlet: add protected method sendUnauthorized
0,"Make CMS smarter about thread prioritiesSpinoff from LUCENE-2161...

The hard throttling CMS does (blocking the incoming thread that wants
to launch a new merge) can be devastating when it strikes during NRT
reopen.

It can easily happen if a huge merge is off and running, but then a
tiny merge is needed to clean up recently created segments due to
frequent reopens.

I think a small change to CMS, whereby it assigns a higher thread
priority to tiny merges than big merges, should allow us to increase
the max merge thread count again, and greatly reduce the chance that
NRT's reopen would hit this.
"
0,"benchmark/stats package is obsolete and unused - remove itThis seems like a leftover from the original benchmark implementation and can thus be removed.
"
0,Avoid using MultiTermDocsSimilar to MatchAllQuery also RangeQuery and WildcardQuery will result in use of MultiTermDocs. Those queries should also use the MultiScorer. See also issue JCR-791.
0,"Support underscore in domain name, or provide better exception
When calling on HttpClient.execute with a url that contain underscore ('_'), you get NullPointerException.
Tracing it down show that java.net.Uri complains that it is illegal name. Which is true according to the RFC.
But it seems that most browser allow it, and some companies support it.

I think HttpClient should either support underscores, or atleast provide a better exception.

"
0,"FieldComparator.TermOrdValComparator compares by value unnecessarilyDigging on LUCENE-2504, I noticed that TermOrdValComparator's compareBottom method falls back on compare-by-value when it needn't.

Specifically, if we know the current bottom ord ""matches"" the current segment, we can skip the value comparison when the ords are the same (ie, return 0) because the ords are exactly comparable.

This is hurting string sort perf especially for optimized indices (and also unoptimized indices), and especially for highly redundant (not many unique values) fields.  This affects all releases >= 2.9.x, but trunk is likely more severely affected since looking up a value is more costly."
1,FineGrainedISMLocking problemsThe FineGrainedISMLocking strategy suffers from the same deadlock issue as was reported in JCR-2753 against DefaultISMLocking. Additionally the FineGrainedISMLocking class will also fail to function properly with XA transactions since it uses the current thread instead of the current transaction id to track re-entrancy.
1,"AnalyzingQueryParser can't work with leading wildcards.The getWildcardQuery mehtod in AnalyzingQueryParser.java need the following changes to accept leading wildcards:

	protected Query getWildcardQuery(String field, String termStr) throws ParseException
	{
		String useTermStr = termStr;
		String leadingWildcard = null;
		if (""*"".equals(field))
		{
			if (""*"".equals(useTermStr))
				return new MatchAllDocsQuery();
		}
		boolean hasLeadingWildcard = (useTermStr.startsWith(""*"") || useTermStr.startsWith(""?"")) ? true : false;

		if (!getAllowLeadingWildcard() && hasLeadingWildcard)
			throw new ParseException(""'*' or '?' not allowed as first character in WildcardQuery"");

		if (getLowercaseExpandedTerms())
		{
			useTermStr = useTermStr.toLowerCase();
		}

		if (hasLeadingWildcard)
		{
			leadingWildcard = useTermStr.substring(0, 1);
			useTermStr = useTermStr.substring(1);
		}

		List tlist = new ArrayList();
		List wlist = new ArrayList();
		/*
		 * somewhat a hack: find/store wildcard chars in order to put them back
		 * after analyzing
		 */
		boolean isWithinToken = (!useTermStr.startsWith(""?"") && !useTermStr.startsWith(""*""));
		isWithinToken = true;
		StringBuffer tmpBuffer = new StringBuffer();
		char[] chars = useTermStr.toCharArray();
		for (int i = 0; i < useTermStr.length(); i++)
		{
			if (chars[i] == '?' || chars[i] == '*')
			{
				if (isWithinToken)
				{
					tlist.add(tmpBuffer.toString());
					tmpBuffer.setLength(0);
				}
				isWithinToken = false;
			}
			else
			{
				if (!isWithinToken)
				{
					wlist.add(tmpBuffer.toString());
					tmpBuffer.setLength(0);
				}
				isWithinToken = true;
			}
			tmpBuffer.append(chars[i]);
		}
		if (isWithinToken)
		{
			tlist.add(tmpBuffer.toString());
		}
		else
		{
			wlist.add(tmpBuffer.toString());
		}

		// get Analyzer from superclass and tokenize the term
		TokenStream source = getAnalyzer().tokenStream(field, new StringReader(useTermStr));
		org.apache.lucene.analysis.Token t;

		int countTokens = 0;
		while (true)
		{
			try
			{
				t = source.next();
			}
			catch (IOException e)
			{
				t = null;
			}
			if (t == null)
			{
				break;
			}
			if (!"""".equals(t.termText()))
			{
				try
				{
					tlist.set(countTokens++, t.termText());
				}
				catch (IndexOutOfBoundsException ioobe)
				{
					countTokens = -1;
				}
			}
		}
		try
		{
			source.close();
		}
		catch (IOException e)
		{
			// ignore
		}

		if (countTokens != tlist.size())
		{
			/*
			 * this means that the analyzer used either added or consumed
			 * (common for a stemmer) tokens, and we can't build a WildcardQuery
			 */
			throw new ParseException(""Cannot build WildcardQuery with analyzer "" + getAnalyzer().getClass()
					+ "" - tokens added or lost"");
		}

		if (tlist.size() == 0)
		{
			return null;
		}
		else if (tlist.size() == 1)
		{
			if (wlist.size() == 1)
			{
				/*
				 * if wlist contains one wildcard, it must be at the end,
				 * because: 1) wildcards at 1st position of a term by
				 * QueryParser where truncated 2) if wildcard was *not* in end,
				 * there would be *two* or more tokens
				 */
				StringBuffer sb = new StringBuffer();
				if (hasLeadingWildcard)
				{
					// adding leadingWildcard
					sb.append(leadingWildcard);
				}
				sb.append((String) tlist.get(0));
				sb.append(wlist.get(0).toString());
				return super.getWildcardQuery(field, sb.toString());
			}
			else if (wlist.size() == 0 && hasLeadingWildcard)
			{
				/*
				 * if wlist contains no wildcard, it must be at 1st position
				 */
				StringBuffer sb = new StringBuffer();
				if (hasLeadingWildcard)
				{
					// adding leadingWildcard
					sb.append(leadingWildcard);
				}
				sb.append((String) tlist.get(0));
				sb.append(wlist.get(0).toString());
				return super.getWildcardQuery(field, sb.toString());
			}
			else
			{
				/*
				 * we should never get here! if so, this method was called with
				 * a termStr containing no wildcard ...
				 */
				throw new IllegalArgumentException(""getWildcardQuery called without wildcard"");
			}
		}
		else
		{
			/*
			 * the term was tokenized, let's rebuild to one token with wildcards
			 * put back in postion
			 */
			StringBuffer sb = new StringBuffer();
			if (hasLeadingWildcard)
			{
				// adding leadingWildcard
				sb.append(leadingWildcard);
			}
			for (int i = 0; i < tlist.size(); i++)
			{
				sb.append((String) tlist.get(i));
				if (wlist != null && wlist.size() > i)
				{
					sb.append((String) wlist.get(i));
				}
			}
			return super.getWildcardQuery(field, sb.toString());
		}
	}
"
0,"Contrib JCR-Server: enable PROPPATCH for simple-davresourceimplement as suggested:

- jcr-properties are exposed as webdav properties
- PROPPATCH is forwarded to javax.jcr.Property.setValue() and Item.remove()"
0,"Add new bit set impl for caching filtersI think OpenBitSet is trying to satisfy too many audiences, and it's
confusing/error-proned as a result.  It has int/long variants of many
methods.  Some methods require in-bound access, others don't; of those
others, some methods auto-grow the bits, some don't.  OpenBitSet
doesn't always know its numBits.

I'd like to factor out a more ""focused"" bit set impl whose primary
target usage is a cached Lucene Filter, ie a bit set indexed by docID
(int, not long) whose size is known and fixed up front (backed by
final long[]) and is always accessed in-bounds.
"
0,"Decide if we should remove lines numbers from latest ChangesAs Lucene dev has grown, a new issue has arisen - many times, new changes invalidate old changes. A proper changes file should just list the changes from the last version, not document the dev life of the issues. Keeping changes in proper order now requires a lot of renumbering sometimes. The numbers have no real meaning and could be added to more rich versions (such as the html version) automatically if desired.

I think an * makes a good replacement myself. The issues already have ids that are stable, rather than the current, decorational numbers which are subject to change over a dev cycle.

I think we should replace the numbers with an asterix for the 2.9 section and going forward (ie 4. becomes *).

If we don't get consensus very quickly, this issue won't block."
1,"[PATCH] When locks are disabled, IndexWriter.close() throws NullPointerExceptionIf locks are disabled (via setting the System property 'disableLuceneLocks' to
true), IndexWriter throws a NullPointerException on closing. The reason is that
the attempt to call writeLock.release() fails because writeLock is null.
To correct this, just check for this case before releasing. A (trivial) patch is
attached."
0,"Reset zzBuffer in StandardTokenizerImpl* when lexer is reset.When indexing large documents, the lexer buffer may stay large forever. This sub-issue resets the lexer buffer back to the default on reset(Reader).

This is done on the enclosing issue."
0,"Rename lucene/solr dev jar files to -SNAPSHOT.jarCurrently the lucene dev jar files end with '-dev.jar' this is all fine, but it makes people using maven jump through a few hoops to get the -SNAPSHOT naming convention required by maven.  If we want to publish snapshot builds with hudson, we would need to either write some crazy scripts or run the build twice.

I suggest we switch to -SNAPSHOT.jar.  Hopefully for the 3.x branch and for the /trunk (4.x) branch"
0,JCR mapping: Upgrade to Maven 2Upgrade the JCR Mapping components to Maven 2 before a release.
0,"Visibility of Scorer.score(Collector, int, int) is wrongThe method for scoring subsets in Scorer has wrong visibility, its marked protected, but protected methods should not be called from other classes. Protected methods are intended for methods that should be overridden by subclasses and are called by (often) final methods of the same class. They should never be called from foreign classes.

This method is called from another class out-of-scope: BooleanScorer(2) - so it must be public, but it's protected. This does not lead to a compiler error because BS(2) is in same package, but may lead to problems if subclasses from other packages override it. When implementing LUCENE-2838 I hit a trap, as I thought tis method should only be called from the class or Scorer itsself, but in fact its called from outside, leading to bugs, because I had not overridden it. As ConstantScorer did not use it I have overridden it with throw UOE and suddenly BooleanQuery was broken, which made it clear that it's called from outside (which is not the intention of protected methods).

We cannot fix this in 3.x, as it would break backwards for classes that overwrite this method, but we can fix visibility in trunk.
"
1,"In XA transaction session.addLockToken() does not have effectFollowing sequence does not work as expected:
1. first tx (and first session)
  create node
  make it lockable
2. second tx (and second session)
  lock this node and save lock token
3. third tx (and third session)
  add saved lock token to session
  modify this locked node -> fails as if lock token was not added to session3

The same sequence works as expected without transactions.
I had to separate transactions 1 and 2 because JCR-1633 prevents node from being locked in same tx in which it was created."
0,"NodeEntryImpl.getWorkspaceId() very inefficient NodeEntryImpl.getWorkspaceId() calculates its path on each call by calling itself recursively. Further each call to getWorkspaceId() results in various calls to the path and item factories which might be somewhat expensive by themselves. 

In my test scenario I have a RepositoryService.getItemInfos() call returning ~1000 items. Processing these items results in about 2700000 (!) calls to getWorkspaceId(). Profiler data shows, that 98% of the time to process the 1000 items is spent in getWorkspaceId()  and related calls. "
1,"CacheEntryUpdater does not properly update cache entry resourceCacheEntryUpdater#updateCacheEntry() copies the old cache entry's resource, though I believe it should only do so if the response is a 304.  Otherwise it should take the response from the server to update the entry.  This method gets called when validating a cache entry and the server returns a 200 or 304."
0,"Improve performance of MatchAllScorerThe BitSets created in MatchAllScorer should be cached per IndexReader. This enhancement should also take care that the supplied IndexReader may in fact be a CombinedIndexReader or a CachingMultiReader with multiple contained IndexReaders. To achieve a good cache efficiency the BitSets must be cached per contained IndexReader and combined later.

See also thread on dev list: http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/10976"
0,"Incomplete lucene-core-1.9.1 in Maven2 repositoryI'm new to Lucene and am setting up a project using v1.9.1 to use Maven2 instead of ANT.
The project would not build with Maven2 due to lacking lucene classes.
I tracked the problem down to that the lucene-core-1.9.1 jar file that Maven2 downloaded from the repository was smaller (2.3KB) than the one I got from the local ANT repository (408KB).
Can you please update the v1.9.1 file on the Maven2 [1], [2] repositories so other developers don't get frustrated by the incomplete jar?


[1] http://repo1.maven.org/maven2/org/apache/lucene/lucene-core/1.9.1/
[2] http://mirrors.ibiblio.org/pub/mirrors/maven2/org/apache/lucene/lucene-core/1.9.1/

This issue is a copy of a mail sent to the java-dev@lucene.apache.org list april 4. 2007."
1,"StandardTokenizer loses Korean charactersWhile using StandardAnalyzer, exp. StandardTokenizer with Korean text stream, StandardTokenizer ignores the Korean characters. This is because the definition of CJK token in StandardTokenizer.jj JavaCC file doesn't have enough range covering Korean syllables described in Unicode character map.
This patch adds one line of 0xAC00~0xD7AF, the Korean syllables range to the StandardTokenizer.jj code."
0,"Add support for Digest authentication to the Authenticator classHere's some code initially whipped up by Geza for Apache Axis, now adapted to
HTTPClient that adds support for Digest authentication to the Authenticator
class. I have tested this code against tomcat 4.0.4 with a sample code that
calls an Apache Axis Web Service. One caveat according to Geza, the code ""Right
now does not support qop-int""."
0,"Core: Misleading method naming with Workspace and Session (move283, copy283, clone283)there seems to be leftovers from previous drafts of the jsr 283 within jackrabbit core, namely:

- WorkspaceImpl#clone283
- WorkspaceImpl#copy283
- WorkspaceImpl#move283
- SessionImpl#move283

giving the impression that with jsr 283 those method would return 'String'. this however is not the case.
therefore i suggest to remove those methods again."
0,"FieldSortedHitQueue.lessThan() should not be finalThe final seems to provide little benefit and it takes away the ability to
specialize this method (which I need to do, forcing a customization of Lucene to
remove the final)."
0,create jcr-browser contrib projectIf noone oposes I'll start a contrib project called jcr-browser. A live demo of the initial code is available at http://edgarpoce.dyndns.org:8080/jcr-browser/
0,"TestIndexWriter failes for SimpleTextCodecI just ran into this failure since SimpleText obviously takes a lot of disk space though.

{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter
    [junit] Testcase: testCommitOnCloseDiskUsage(org.apache.lucene.index.TestIndexWriter):	FAILED
    [junit] writer used too much space while adding documents: mid=608162 start=5293 end=634214
    [junit] junit.framework.AssertionFailedError: writer used too much space while adding documents: mid=608162 start=5293 end=634214
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit] 	at org.apache.lucene.index.TestIndexWriter.testCommitOnCloseDiskUsage(TestIndexWriter.java:1047)
    [junit] 
    [junit] 
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 3.281 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testCommitOnCloseDiskUsage -Dtests.seed=-7526585723238322940:-1609544650150801239
    [junit] NOTE: test params are: codec=SimpleText, locale=th_TH, timezone=UCT
    [junit] ------------- ---------------- ---------------
    [junit] Test org.apache.lucene.index.TestIndexWriter FAILED
{noformat}

I did not look into SimpleText but I guess we need either change the threshold for this test or exclude SimpleText from it.

any ideas?"
1,"Oracle JNDI DataSource supportWhen org.apache.jackrabbit.core.persistence.bundle.util.ConnectionFactory tries to get a connection from a JNDI Datasource without login and pasword, if no user/password are specified, they re retrieved as empty strings, not null, so it tries to do a ds.getConnection(user, password), which fails. Please complete the test line 66 as :
if ((user == null || user.length() > 0) && (password == null || password.length() > 0)) {

Sincerely,

Stphane Landelle"
0,serialVersionUID for AuthorizableExistsException
1,"[PATCH] MultiSearcher problems with Similarity.docFreq()When MultiSearcher invokes its subsearchers, it is the subsearchers' docFreq()
that is accessed by Similarity.docFreq().  This causes idf's to be computed
local to each index rather than globally, which causes ranking across multiple
indices to not be equivalent to ranking across the entire global collection.

The attached files (if I can figure out how to attach them) provide a potential
partial solution for this.  They properly fix a simple test case, RankingTest,
that was provided by Daniel Naber.

The changes are:
  1.  Searcher:  Add topmostSearcher() field with getter and setter to record
the outermost Searcher.  Default to this.
  2.  MultiSearcher:  Pass down the topmostSearcher when creating the subsearchers.
  3.  IndexSearcher:  Call Query.weight() everywhere with the topmostSearcher
instead of this.
  4.  Query:  Provide a default implementation of Query.combine() so that
MultiSearcher works with all queries.

Problems or possible problems I see:
  1.  This does not address the same issue with RemoteSearchable. 
RemoteSearchable is not a Searcher, nor can it be due to lack of multiple
inheritance in Java, but Query.weight() requires a Searcher.  Perhaps
Query.weight() should be changed to take a Searchable, but this requires
changing many places and I suspect would break apps.
  2.  There may be other places that topmostSearcher should be used instead of this.
  3.  The default implementation for Query.combine() is a guess on my part - it
works for TermQuery.  It's fragile in that the default implementation will hide
bugs caused by queries that inadvertently omit a more precise Query.combine()
method.
  4.  The prior comment on Query.combine() indicates that whoever wrote it was
fully aware of this problem and so probably had another usage in mind, so the
whole issue may just be Daniel's usage in the test case.  It's not apparent to
me, so I probably don't understand something."
1,"ParallelMultiSearcher should shut down thread pool on closeParallelMultiSearcher does not shut down its internal thread pool on close. As a result, programs that create multiple instances of this class over their lifetime end up ""leaking"" threads."
1,"IndexReader.getCurrentVersion() and isCurrent should use commit lock.There is a race condition if one machine is checking the current version of an index while another wants to update the segments file in IndexWriter.close().

java.io.IOException: Cannot delete segments
	at org.apache.lucene.store.FSDirectory.renameFile(FSDirectory.java:213)
	at org.apache.lucene.index.SegmentInfos.write(SegmentInfos.java:90)
	at org.apache.lucene.index.IndexWriter$3.doBody(IndexWriter.java:503)
	at org.apache.lucene.store.Lock$With.run(Lock.java:109)
	at org.apache.lucene.index.IndexWriter.mergeSegments(IndexWriter.java:501)
	at org.apache.lucene.index.IndexWriter.flushRamSegments(IndexWriter.java:440)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:242)

On the windows platform reading the contents of a file disallows deleting the file.

I use Lucene to maintain an index of +-700.000 documents, one server adds documents, while other servers handle the searches.
The search servers poll the index version regularly to check if they have to reopen their IndexSearcher.
Once in a while (about once every two days on average), IndexWriter.close() fails because it cannot delete the previous segments file, even though it hold the commit lock.
The reason is probably that search servers are reading the segments file to check the version without using the commit lock.
"
1,"Multiple namespace definitions in CND prevent definition of node type without child nodesThe BNF in http://jackrabbit.apache.org/api-1/org/apache/jackrabbit/core/nodetype/compact/CompactNodeTypeDefReader.html
defines:

[...]
cnd ::= {ns_mapping | node_type_def}
[...]

so multiple namespace definitions should not affect the node type definitions.

However, the following CND definition will fail:

<namespace= 'http://www.mynamespace.co.uk/namespace'>
<nt = 'http://www.jcp.org/jcr/nt/1.0'>
[namespace:document] > nt:file
   - namespace:name (string) mandatory

<namespace= 'http://www.mynamespace.co.uk/namespace'>
<nt = 'http://www.jcp.org/jcr/nt/1.0'>
[namespace:document2] > nt:file
   - namespace:name (string) mandatory


Remove the second set of namespace definitions, and all's well:

<namespace= 'http://www.mynamespace.co.uk/namespace'>
<nt = 'http://www.jcp.org/jcr/nt/1.0'>
[namespace:document] > nt:file
   - namespace:name (string) mandatory

[namespace:document2] > nt:file
   - namespace:name (string) mandatory"
0,"Port 80 is needed to run testsI'm trying to upgrade to Cactus 1.4.1 and StrutsTest 1.9.  My tests where 
working about fine a month ago with nightly builds of both.  Now I get the 
following error:

    [junit] Testcase: testCreate took 0.2 sec
    [junit]     Caused an ERROR
    [junit] port out of range:-1
    [junit] java.lang.IllegalArgumentException: port out of range:-1
    [junit]     at java.net.InetSocketAddress.<init>
(InetSocketAddress.java:103)
    [junit]     at java.net.Socket.<init>(Socket.java:119)
    [junit]     at org.apache.commons.httpclient.HttpConnection.open
(HttpConnection.java:260)
    [junit]     at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:255)
    [junit]     at 
org.apache.cactus.client.HttpClientConnectionHelper.dispatch24_connect
(HttpClientConnectionHelper.jav
a;org/apache/cactus/util/log/LogAspect.aj(1k):164)
    [junit]     at 
org.apache.cactus.client.HttpClientConnectionHelper.around24_connect
(HttpClientConnectionHelper.java;
org/apache/cactus/util/log/LogAspect.aj(1k):1236)
    [junit]     at org.apache.cactus.client.HttpClientConnectionHelper.connect
(HttpClientConnectionHelper.java;org/apach
e/cactus/util/log/LogAspect.aj(1k):106)
    [junit]     at org.apache.cactus.client.AbstractHttpClient.callRunTest
(AbstractHttpClient.java;org/apache/cactus/uti
l/log/LogAspect.aj(1k):186)
    [junit]     at org.apache.cactus.client.AbstractHttpClient.dispatch2_doTest
(AbstractHttpClient.java;org/apache/cactu
s/util/log/LogAspect.aj(1k):109)
    [junit]     at org.apache.cactus.client.AbstractHttpClient.around2_doTest
(AbstractHttpClient.java;org/apache/cactus/
util/log/LogAspect.aj(1k):1236)
    [junit]     at org.apache.cactus.client.AbstractHttpClient.doTest
(AbstractHttpClient.java;org/apache/cactus/util/log
/LogAspect.aj(1k):104)
    [junit]     at org.apache.cactus.AbstractWebTestCase.runGenericTest
(AbstractWebTestCase.java:260)
    [junit]     at org.apache.cactus.ServletTestCase.runTest
(ServletTestCase.java:133)
    [junit]     at org.apache.cactus.AbstractTestCase.runBare
(AbstractTestCase.java:195)

I found that I had to add port 80 (:80) get it to work with the new stuff:

cactus.properties from: cactus.contextURL = http://localhost/myApp

To: cactus.contextURL = http://localhost:80/myApp

build.xml from:

    <target name=""tomcat.navigationAction"" depends=""deploy"" if=""tomcat.home"">
        <!-- We suppose our webapp is named ""onpoint"" -->
        <runservertests testURL=""http://localhost/${webapp.name}"" 
            startTarget=""start.tomcat"" 
            stopTarget=""stop.tomcat"" 
            testTarget=""test.navigationAction""/>
    </target>

To:

    <target name=""tomcat.navigationAction"" depends=""deploy"" if=""tomcat.home"">
        <!-- We suppose our webapp is named ""onpoint"" -->
        <runservertests testURL=""http://localhost:80/
${webapp.name}/ServletRedirector?Cactus_Service=RUN_TEST"" 
            startTarget=""start.tomcat"" 
            stopTarget=""stop.tomcat"" 
            testTarget=""test.navigationAction""/>
    </target>"
0,"remove 1.5 only unit test code that snuck inI just tried to run unit tests w/ Java 1.4.2, but hit this:

{code}
common.compile-test:
    [mkdir] Created dir: /lucene/src/diagnostics.1654/build/classes/test
    [javac] Compiling 191 source files to /lucene/src/diagnostics.1654/build/classes/test
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:26: package java.util.concurrent.atomic does not exist
    [javac] import java.util.concurrent.atomic.AtomicInteger;
    [javac]                                    ^
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:262: cannot resolve symbol
    [javac] symbol  : class AtomicInteger 
    [javac] location: class org.apache.lucene.index.TestIndexWriterReader.DeleteThreads
    [javac]     AtomicInteger delCount = new AtomicInteger();
    [javac]     ^
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:332: cannot resolve symbol
    [javac] symbol  : class AtomicInteger 
    [javac] location: class org.apache.lucene.index.TestIndexWriterReader.AddDirectoriesThreads
    [javac]     AtomicInteger count = new AtomicInteger(0);
    [javac]     ^
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:333: cannot resolve symbol
    [javac] symbol  : class AtomicInteger 
    [javac] location: class org.apache.lucene.index.TestIndexWriterReader.AddDirectoriesThreads
    [javac]     AtomicInteger numAddIndexesNoOptimize = new AtomicInteger(0);
    [javac]     ^
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:262: cannot resolve symbol
    [javac] symbol  : class AtomicInteger 
    [javac] location: class org.apache.lucene.index.TestIndexWriterReader.DeleteThreads
    [javac]     AtomicInteger delCount = new AtomicInteger();
    [javac]                                  ^
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:332: cannot resolve symbol
    [javac] symbol  : class AtomicInteger 
    [javac] location: class org.apache.lucene.index.TestIndexWriterReader.AddDirectoriesThreads
    [javac]     AtomicInteger count = new AtomicInteger(0);
    [javac]                               ^
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/index/TestIndexWriterReader.java:333: cannot resolve symbol
    [javac] symbol  : class AtomicInteger 
    [javac] location: class org.apache.lucene.index.TestIndexWriterReader.AddDirectoriesThreads
    [javac]     AtomicInteger numAddIndexesNoOptimize = new AtomicInteger(0);
    [javac]                                                 ^
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/search/TestSort.java:932: cannot resolve symbol
    [javac] symbol  : method getSimpleName ()
    [javac] location: class java.lang.Class
    [javac]           assertEquals(actualTFCClasses[j], tdc.getClass().getSimpleName());
    [javac]                                                         ^
    [javac] /lucene/src/diagnostics.1654/src/test/org/apache/lucene/search/TestTopScoreDocCollector.java:70: cannot resolve symbol
    [javac] symbol  : method getSimpleName ()
    [javac] location: class java.lang.Class
    [javac]         assertEquals(actualTSDCClass[i], tdc.getClass().getSimpleName());
    [javac]                                                      ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -deprecation for details.
    [javac] 9 errors
{code}"
1,"don't call SegmentInfo.sizeInBytes for the merging segmentsSelckin has been running Lucene's tests on the RT branch, and hit this:
{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter
    [junit] Testcase: testDeleteAllSlowly(org.apache.lucene.index.TestIndexWriter):	FAILED
    [junit] Some threads threw uncaught exceptions!
    [junit] junit.framework.AssertionFailedError: Some threads threw uncaught exceptions!
    [junit] 	at org.apache.lucene.util.LuceneTestCase.tearDown(LuceneTestCase.java:535)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1246)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1175)
    [junit] 
    [junit] 
    [junit] Tests run: 67, Failures: 1, Errors: 0, Time elapsed: 38.357 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testDeleteAllSlowly -Dtests.seed=-4291771462012978364:4550117847390778918
    [junit] The following exceptions were thrown by threads:
    [junit] *** Thread: Lucene Merge Thread #1 ***
    [junit] org.apache.lucene.index.MergePolicy$MergeException: java.io.FileNotFoundException: _4_1.del
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:507)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:472)
    [junit] Caused by: java.io.FileNotFoundException: _4_1.del
    [junit] 	at org.apache.lucene.store.FSDirectory.fileLength(FSDirectory.java:290)
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.fileLength(MockDirectoryWrapper.java:549)
    [junit] 	at org.apache.lucene.index.SegmentInfo.sizeInBytes(SegmentInfo.java:287)
    [junit] 	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3280)
    [junit] 	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2956)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:379)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:447)
    [junit] NOTE: test params are: codec=RandomCodecProvider: {=SimpleText, f6=Pulsing(freqCutoff=15), f7=MockFixedIntBlock(blockSize=1606), f8=SimpleText, f9=MockSep, f1=MockVariableIntBlock(baseBlockSize=99), f0=MockFixedIntBlock(blockSize=1606), f3=Pulsing(freqCutoff=15), f2=MockSep, f5=SimpleText, f4=Standard, f=MockFixedIntBlock(blockSize=1606), c=MockSep, termVector=MockRandom, d9=MockFixedIntBlock(blockSize=1606), d8=Pulsing(freqCutoff=15), d5=SimpleText, d4=Standard, d7=MockRandom, d6=MockVariableIntBlock(baseBlockSize=99), d25=MockRandom, d0=MockRandom, c29=MockFixedIntBlock(blockSize=1606), d24=MockVariableIntBlock(baseBlockSize=99), d1=Standard, c28=Standard, d23=SimpleText, d2=MockFixedIntBlock(blockSize=1606), c27=MockRandom, d22=Standard, d3=MockVariableIntBlock(baseBlockSize=99), d21=Pulsing(freqCutoff=15), d20=MockSep, c22=MockFixedIntBlock(blockSize=1606), c21=Pulsing(freqCutoff=15), c20=MockRandom, d29=MockFixedIntBlock(blockSize=1606), c26=Standard, d28=Pulsing(freqCutoff=15), c25=MockRandom, d27=MockRandom, c24=MockSep, d26=MockVariableIntBlock(baseBlockSize=99), c23=SimpleText, e9=MockRandom, e8=MockSep, e7=SimpleText, e6=MockFixedIntBlock(blockSize=1606), e5=Pulsing(freqCutoff=15), c17=MockFixedIntBlock(blockSize=1606), e3=Standard, d12=MockVariableIntBlock(baseBlockSize=99), c16=Pulsing(freqCutoff=15), e4=SimpleText, d11=MockFixedIntBlock(blockSize=1606), c19=MockSep, e1=MockSep, d14=Pulsing(freqCutoff=15), c18=SimpleText, e2=Pulsing(freqCutoff=15), d13=MockSep, e0=MockVariableIntBlock(baseBlockSize=99), d10=Standard, d19=MockVariableIntBlock(baseBlockSize=99), c11=SimpleText, c10=Standard, d16=Pulsing(freqCutoff=15), c13=MockRandom, c12=MockVariableIntBlock(baseBlockSize=99), d15=MockSep, d18=SimpleText, c15=MockFixedIntBlock(blockSize=1606), d17=Standard, c14=Pulsing(freqCutoff=15), b3=MockSep, b2=SimpleText, b5=Standard, b4=MockRandom, b7=MockVariableIntBlock(baseBlockSize=99), b6=MockFixedIntBlock(blockSize=1606), d50=MockFixedIntBlock(blockSize=1606), b9=Pulsing(freqCutoff=15), b8=MockSep, d43=MockSep, d42=SimpleText, d41=MockFixedIntBlock(blockSize=1606), d40=Pulsing(freqCutoff=15), d47=MockVariableIntBlock(baseBlockSize=99), d46=MockFixedIntBlock(blockSize=1606), b0=MockVariableIntBlock(baseBlockSize=99), d45=Standard, b1=MockRandom, d44=MockRandom, d49=MockVariableIntBlock(baseBlockSize=99), d48=MockFixedIntBlock(blockSize=1606), c6=Pulsing(freqCutoff=15), c5=MockSep, c4=MockVariableIntBlock(baseBlockSize=99), c3=MockFixedIntBlock(blockSize=1606), c9=MockVariableIntBlock(baseBlockSize=99), c8=SimpleText, c7=Standard, d30=SimpleText, d32=MockRandom, d31=MockVariableIntBlock(baseBlockSize=99), c1=SimpleText, d34=MockFixedIntBlock(blockSize=1606), c2=MockSep, d33=Pulsing(freqCutoff=15), d36=MockSep, c0=MockFixedIntBlock(blockSize=1606), d35=SimpleText, d38=MockSep, d37=SimpleText, d39=MockRandom, e92=MockFixedIntBlock(blockSize=1606), e93=MockVariableIntBlock(baseBlockSize=99), e90=MockRandom, e91=Standard, e89=MockVariableIntBlock(baseBlockSize=99), e88=SimpleText, e87=Standard, e86=Pulsing(freqCutoff=15), e85=MockSep, e84=MockVariableIntBlock(baseBlockSize=99), e83=MockFixedIntBlock(blockSize=1606), e80=MockFixedIntBlock(blockSize=1606), e81=SimpleText, e82=MockSep, e77=MockVariableIntBlock(baseBlockSize=99), e76=MockFixedIntBlock(blockSize=1606), e79=Pulsing(freqCutoff=15), e78=MockSep, e73=MockSep, e72=SimpleText, e75=Standard, e74=MockRandom, binary=MockFixedIntBlock(blockSize=1606), f98=Pulsing(freqCutoff=15), f97=MockSep, f99=Standard, f94=Standard, f93=MockRandom, f96=MockVariableIntBlock(baseBlockSize=99), f95=MockFixedIntBlock(blockSize=1606), e95=SimpleText, e94=Standard, e97=MockRandom, e96=MockVariableIntBlock(baseBlockSize=99), e99=MockFixedIntBlock(blockSize=1606), e98=Pulsing(freqCutoff=15), id=MockFixedIntBlock(blockSize=1606), f34=MockSep, f33=SimpleText, f32=MockFixedIntBlock(blockSize=1606), f31=Pulsing(freqCutoff=15), f30=MockRandom, f39=MockFixedIntBlock(blockSize=1606), f38=Standard, f37=MockRandom, f36=MockSep, f35=SimpleText, f43=Standard, f42=MockRandom, f45=MockVariableIntBlock(baseBlockSize=99), f44=MockFixedIntBlock(blockSize=1606), f41=MockSep, f40=SimpleText, f47=MockVariableIntBlock(baseBlockSize=99), f46=MockFixedIntBlock(blockSize=1606), f49=Pulsing(freqCutoff=15), f48=MockSep, content=Pulsing(freqCutoff=15), e19=Standard, e18=MockRandom, e17=MockSep, f12=Pulsing(freqCutoff=15), e16=SimpleText, f11=MockSep, f10=MockVariableIntBlock(baseBlockSize=99), e15=MockFixedIntBlock(blockSize=1606), e14=Pulsing(freqCutoff=15), f16=SimpleText, e13=MockFixedIntBlock(blockSize=1606), f15=Standard, e12=Pulsing(freqCutoff=15), e11=MockRandom, f14=Pulsing(freqCutoff=15), e10=MockVariableIntBlock(baseBlockSize=99), f13=MockSep, f19=Pulsing(freqCutoff=15), f18=MockRandom, f17=MockVariableIntBlock(baseBlockSize=99), e29=MockSep, e26=Standard, f21=SimpleText, e25=MockRandom, f20=Standard, e28=MockVariableIntBlock(baseBlockSize=99), f23=MockRandom, e27=MockFixedIntBlock(blockSize=1606), f22=MockVariableIntBlock(baseBlockSize=99), f25=MockRandom, e22=MockSep, f24=MockVariableIntBlock(baseBlockSize=99), e21=SimpleText, f27=MockFixedIntBlock(blockSize=1606), e24=Standard, f26=Pulsing(freqCutoff=15), e23=MockRandom, f29=MockSep, f28=SimpleText, e20=MockFixedIntBlock(blockSize=1606), field=Pulsing(freqCutoff=15), string=MockVariableIntBlock(baseBlockSize=99), e30=Pulsing(freqCutoff=15), e31=MockFixedIntBlock(blockSize=1606), a98=MockFixedIntBlock(blockSize=1606), e34=MockRandom, a99=MockVariableIntBlock(baseBlockSize=99), e35=Standard, f79=Pulsing(freqCutoff=15), e32=SimpleText, e33=MockSep, b97=Pulsing(freqCutoff=15), f77=Pulsing(freqCutoff=15), e38=MockFixedIntBlock(blockSize=1606), b98=MockFixedIntBlock(blockSize=1606), f78=MockFixedIntBlock(blockSize=1606), e39=MockVariableIntBlock(baseBlockSize=99), b99=SimpleText, f75=MockVariableIntBlock(baseBlockSize=99), e36=MockRandom, f76=MockRandom, e37=Standard, f73=Standard, f74=SimpleText, f71=MockSep, f72=Pulsing(freqCutoff=15), f81=Pulsing(freqCutoff=15), f80=MockSep, e40=MockSep, e41=MockRandom, e42=Standard, e43=MockFixedIntBlock(blockSize=1606), e44=MockVariableIntBlock(baseBlockSize=99), e45=MockSep, e46=Pulsing(freqCutoff=15), f86=SimpleText, e47=MockSep, f87=MockSep, e48=Pulsing(freqCutoff=15), f88=MockRandom, e49=Standard, f89=Standard, f82=MockVariableIntBlock(baseBlockSize=99), f83=MockRandom, f84=Pulsing(freqCutoff=15), f85=MockFixedIntBlock(blockSize=1606), f90=SimpleText, f92=MockRandom, f91=MockVariableIntBlock(baseBlockSize=99), str=MockFixedIntBlock(blockSize=1606), a76=MockVariableIntBlock(baseBlockSize=99), e56=MockVariableIntBlock(baseBlockSize=99), f59=MockSep, a77=MockRandom, e57=MockRandom, a78=Pulsing(freqCutoff=15), e54=Standard, f57=MockFixedIntBlock(blockSize=1606), a79=MockFixedIntBlock(blockSize=1606), e55=SimpleText, f58=MockVariableIntBlock(baseBlockSize=99), e52=MockSep, e53=Pulsing(freqCutoff=15), e50=MockFixedIntBlock(blockSize=1606), e51=MockVariableIntBlock(baseBlockSize=99), f51=SimpleText, f52=MockSep, f50=MockFixedIntBlock(blockSize=1606), f55=MockFixedIntBlock(blockSize=1606), f56=MockVariableIntBlock(baseBlockSize=99), f53=MockRandom, e58=MockVariableIntBlock(baseBlockSize=99), f54=Standard, e59=MockRandom, a80=MockVariableIntBlock(baseBlockSize=99), e60=MockVariableIntBlock(baseBlockSize=99), a82=Pulsing(freqCutoff=15), a81=MockSep, a84=SimpleText, a83=Standard, a86=MockRandom, a85=MockVariableIntBlock(baseBlockSize=99), a89=MockRandom, f68=Standard, e65=Pulsing(freqCutoff=15), f69=SimpleText, e66=MockFixedIntBlock(blockSize=1606), a87=SimpleText, e67=SimpleText, a88=MockSep, e68=MockSep, e61=Standard, e62=SimpleText, e63=MockVariableIntBlock(baseBlockSize=99), e64=MockRandom, f60=MockRandom, f61=Standard, f62=MockFixedIntBlock(blockSize=1606), f63=MockVariableIntBlock(baseBlockSize=99), e69=SimpleText, f64=MockSep, f65=Pulsing(freqCutoff=15), f66=Standard, f67=SimpleText, f70=Standard, a93=MockRandom, a92=MockVariableIntBlock(baseBlockSize=99), a91=SimpleText, e71=SimpleText, a90=Standard, e70=Standard, a97=MockSep, a96=SimpleText, a95=MockFixedIntBlock(blockSize=1606), a94=Pulsing(freqCutoff=15), c58=MockRandom, a63=Pulsing(freqCutoff=15), a64=MockFixedIntBlock(blockSize=1606), c59=Standard, c56=SimpleText, d59=MockVariableIntBlock(baseBlockSize=99), a61=MockVariableIntBlock(baseBlockSize=99), c57=MockSep, a62=MockRandom, c54=Pulsing(freqCutoff=15), c55=MockFixedIntBlock(blockSize=1606), a60=SimpleText, c52=MockVariableIntBlock(baseBlockSize=99), c53=MockRandom, d53=MockSep, d54=Pulsing(freqCutoff=15), d51=MockFixedIntBlock(blockSize=1606), d52=MockVariableIntBlock(baseBlockSize=99), d57=MockVariableIntBlock(baseBlockSize=99), b62=MockSep, d58=MockRandom, b63=Pulsing(freqCutoff=15), d55=Standard, b60=MockFixedIntBlock(blockSize=1606), d56=SimpleText, b61=MockVariableIntBlock(baseBlockSize=99), b56=SimpleText, b55=Standard, b54=Pulsing(freqCutoff=15), b53=MockSep, d61=MockVariableIntBlock(baseBlockSize=99), b59=Pulsing(freqCutoff=15), d60=MockFixedIntBlock(blockSize=1606), b58=MockRandom, b57=MockVariableIntBlock(baseBlockSize=99), c62=MockRandom, c61=MockVariableIntBlock(baseBlockSize=99), a59=Standard, c60=SimpleText, a58=MockRandom, a57=MockSep, a56=SimpleText, a55=MockFixedIntBlock(blockSize=1606), a54=Pulsing(freqCutoff=15), a72=SimpleText, c67=MockFixedIntBlock(blockSize=1606), a73=MockSep, c68=MockVariableIntBlock(baseBlockSize=99), a74=MockRandom, c69=MockSep, a75=Standard, c63=SimpleText, c64=MockSep, a70=Pulsing(freqCutoff=15), c65=MockRandom, a71=MockFixedIntBlock(blockSize=1606), c66=Standard, d62=Standard, d63=SimpleText, d64=MockVariableIntBlock(baseBlockSize=99), b70=Pulsing(freqCutoff=15), d65=MockRandom, b71=Standard, d66=Pulsing(freqCutoff=15), b72=SimpleText, d67=MockFixedIntBlock(blockSize=1606), b73=MockVariableIntBlock(baseBlockSize=99), d68=SimpleText, b74=MockRandom, d69=MockSep, b65=MockRandom, b64=MockVariableIntBlock(baseBlockSize=99), b67=MockFixedIntBlock(blockSize=1606), b66=Pulsing(freqCutoff=15), d70=Pulsing(freqCutoff=15), b69=MockSep, b68=SimpleText, d72=SimpleText, d71=Standard, c71=MockFixedIntBlock(blockSize=1606), c70=Pulsing(freqCutoff=15), a69=MockSep, c73=MockSep, c72=SimpleText, a66=Standard, a65=MockRandom, a68=MockVariableIntBlock(baseBlockSize=99), a67=MockFixedIntBlock(blockSize=1606), c32=MockFixedIntBlock(blockSize=1606), c33=MockVariableIntBlock(baseBlockSize=99), c30=MockRandom, c31=Standard, c36=Standard, a41=MockFixedIntBlock(blockSize=1606), c37=SimpleText, a42=MockVariableIntBlock(baseBlockSize=99), a0=MockSep, c34=MockSep, c35=Pulsing(freqCutoff=15), a40=Standard, b84=SimpleText, d79=MockFixedIntBlock(blockSize=1606), b85=MockSep, b82=Pulsing(freqCutoff=15), d77=MockRandom, c38=Standard, b83=MockFixedIntBlock(blockSize=1606), d78=Standard, c39=SimpleText, b80=MockVariableIntBlock(baseBlockSize=99), d75=SimpleText, b81=MockRandom, d76=MockSep, d73=Pulsing(freqCutoff=15), d74=MockFixedIntBlock(blockSize=1606), d83=MockFixedIntBlock(blockSize=1606), a9=MockVariableIntBlock(baseBlockSize=99), d82=Pulsing(freqCutoff=15), d81=MockRandom, d80=MockVariableIntBlock(baseBlockSize=99), b79=MockFixedIntBlock(blockSize=1606), b78=Standard, b77=MockRandom, b76=MockSep, b75=SimpleText, a1=MockFixedIntBlock(blockSize=1606), a35=Pulsing(freqCutoff=15), a2=MockVariableIntBlock(baseBlockSize=99), a34=MockSep, a3=MockSep, a33=MockVariableIntBlock(baseBlockSize=99), a4=Pulsing(freqCutoff=15), a32=MockFixedIntBlock(blockSize=1606), a5=Standard, a39=MockRandom, c40=Standard, a6=SimpleText, a38=MockVariableIntBlock(baseBlockSize=99), a7=MockVariableIntBlock(baseBlockSize=99), a37=SimpleText, a8=MockRandom, a36=Standard, c41=MockSep, c42=Pulsing(freqCutoff=15), c43=Standard, c44=SimpleText, c45=MockVariableIntBlock(baseBlockSize=99), a50=MockSep, c46=MockRandom, a51=Pulsing(freqCutoff=15), c47=Pulsing(freqCutoff=15), a52=Standard, c48=MockFixedIntBlock(blockSize=1606), a53=SimpleText, b93=MockRandom, d88=MockSep, c49=Pulsing(freqCutoff=15), b94=Standard, d89=Pulsing(freqCutoff=15), b95=MockFixedIntBlock(blockSize=1606), b96=MockVariableIntBlock(baseBlockSize=99), d84=MockRandom, b90=MockFixedIntBlock(blockSize=1606), d85=Standard, b91=SimpleText, d86=MockFixedIntBlock(blockSize=1606), b92=MockSep, d87=MockVariableIntBlock(baseBlockSize=99), d92=MockSep, d91=SimpleText, d94=Standard, d93=MockRandom, b87=MockVariableIntBlock(baseBlockSize=99), b86=MockFixedIntBlock(blockSize=1606), d90=MockFixedIntBlock(blockSize=1606), b89=Pulsing(freqCutoff=15), b88=MockSep, a44=SimpleText, a43=Standard, a46=MockRandom, a45=MockVariableIntBlock(baseBlockSize=99), a48=MockFixedIntBlock(blockSize=1606), a47=Pulsing(freqCutoff=15), c51=Pulsing(freqCutoff=15), a49=SimpleText, c50=MockSep, d98=MockVariableIntBlock(baseBlockSize=99), d97=MockFixedIntBlock(blockSize=1606), d96=Standard, d95=MockRandom, d99=MockSep, a20=MockSep, c99=MockRandom, c98=MockVariableIntBlock(baseBlockSize=99), c97=SimpleText, c96=Standard, b19=MockRandom, a16=MockSep, a17=Pulsing(freqCutoff=15), b17=SimpleText, a14=MockFixedIntBlock(blockSize=1606), b18=MockSep, a15=MockVariableIntBlock(baseBlockSize=99), a12=MockRandom, a13=Standard, a10=SimpleText, a11=MockSep, b11=MockVariableIntBlock(baseBlockSize=99), b12=MockRandom, b10=SimpleText, b15=SimpleText, b16=MockSep, a18=MockSep, b13=Pulsing(freqCutoff=15), a19=Pulsing(freqCutoff=15), b14=MockFixedIntBlock(blockSize=1606), b30=MockFixedIntBlock(blockSize=1606), a31=MockVariableIntBlock(baseBlockSize=99), a30=MockFixedIntBlock(blockSize=1606), b28=MockFixedIntBlock(blockSize=1606), a25=Standard, b29=MockVariableIntBlock(baseBlockSize=99), a26=SimpleText, a27=MockVariableIntBlock(baseBlockSize=99), a28=MockRandom, a21=MockFixedIntBlock(blockSize=1606), a22=MockVariableIntBlock(baseBlockSize=99), a23=MockSep, a24=Pulsing(freqCutoff=15), b20=Pulsing(freqCutoff=15), b21=MockFixedIntBlock(blockSize=1606), b22=SimpleText, b23=MockSep, a29=MockVariableIntBlock(baseBlockSize=99), b24=MockRandom, b25=Standard, b26=MockFixedIntBlock(blockSize=1606), b27=MockVariableIntBlock(baseBlockSize=99), b41=Standard, b40=MockRandom, c77=Standard, c76=MockRandom, c75=MockSep, c74=SimpleText, c79=MockVariableIntBlock(baseBlockSize=99), c78=MockFixedIntBlock(blockSize=1606), c80=MockRandom, c83=SimpleText, c84=MockSep, c81=Pulsing(freqCutoff=15), b39=Standard, c82=MockFixedIntBlock(blockSize=1606), b37=Standard, b38=SimpleText, b35=MockSep, b36=Pulsing(freqCutoff=15), b33=MockFixedIntBlock(blockSize=1606), b34=MockVariableIntBlock(baseBlockSize=99), b31=MockRandom, b32=Standard, str2=MockFixedIntBlock(blockSize=1606), b50=MockVariableIntBlock(baseBlockSize=99), b52=Pulsing(freqCutoff=15), str3=SimpleText, b51=MockSep, c86=MockVariableIntBlock(baseBlockSize=99), tvtest=MockSep, c85=MockFixedIntBlock(blockSize=1606), c88=Pulsing(freqCutoff=15), c87=MockSep, c89=Standard, c90=SimpleText, c91=MockSep, c92=MockRandom, c93=Standard, c94=MockFixedIntBlock(blockSize=1606), c95=MockVariableIntBlock(baseBlockSize=99), content1=Pulsing(freqCutoff=15), b46=MockVariableIntBlock(baseBlockSize=99), b47=MockRandom, content3=MockVariableIntBlock(baseBlockSize=99), b48=Pulsing(freqCutoff=15), content4=MockFixedIntBlock(blockSize=1606), b49=MockFixedIntBlock(blockSize=1606), content5=Standard, b42=MockSep, b43=Pulsing(freqCutoff=15), b44=Standard, b45=SimpleText}, locale=tr, timezone=MET
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestMergeSchedulerExternal, TestCharTokenizers, TestCodecs, TestFieldInfos, TestFlushByRamOrCountsPolicy, TestIndexReaderReopen, TestIndexWriter]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=69508608,total=127336448
{noformat}

Simon dug and it looks like this is a trunk issue, caused by LUCENE-1076 (only committed to trunk so far)."
0,"TaxonomyReader/Writer and their Lucene* implementationThe facet module contains two interfaces TaxonomyWriter and TaxonomyReader, with two implementations Lucene*. We've never actually implemented two TaxonomyWriters/Readers, so I'm not sure if these interfaces are useful anymore. Therefore I'd like to propose that we do either of the following:

# Remove the interfaces and remove the Lucene part from the implementation classes (to end up with TW/TR impls). Or,
# Keep the interfaces, but rename the Lucene* impls to Directory*.

Whatever we do, I'd like to make the impls/interfaces impl also TwoPhaseCommit.

Any preferences?"
0,"speed up automaton seeking in nextStringWhile testing, i found there are some queries (e.g. wildcard ?????????) that do quite a lot of backtracking.

nextString doesn't handle this particularly well, when it walks the DFA, if it hits a dead-end and needs to backtrack, it increments the bytes, and starts over completely.

alternatively it could save the path information in an int[], and backtrack() could return a position to restart from, instead of just a boolean.
"
0,Make CacheEntry use an immutable object to represent cache content Make CacheEntry use an immutable object to represent cache content similar to HttpEntity
1,"MatchAllDocsQuery.toString(String field) does not honor the javadoc contractShould be 

public String toString(String field){
  return ""*:*"";
}

QueryParser needs to be able to parse the String form of this query."
0,"Use Apache Tika for text extractionOnce Apache Tika is released with a resolution to TIKA-175 (making Tika available to Java 1.4 projects), we should replace our direct parser library dependencies with Tika parsers. Ideally we'd just use the Tika AutoDetectParser that'll automatically detect the type of a binary and parse it accordingly, solving JCR-728.

I guess we should keep some level of backwards compatibility with existing textFilterClasses=""..."" configurations, perhaps by keeping the existing TextExtractor classes as wrappers around respective Tika parsers."
1,"ManagedConnection#cleanup doesn't refresh the sessionthe ManagedConnection is not cleaned up correctly. I think that the underlying jcr Session should be refreshed by calling
Session#refresh(false) at JCAManagedConnection#cleanup. In the current implementation a new Session see the changes stored in the transient level of a closed session"
0,"make compoundfilewriter publicCompoundFileReader is public, but CompoundFileWriter is not.

I propose we make it public + @lucene.internal instead (just in case someone 
else finds themselves wanting to manipulate cfs files)
"
0,"Contributed utility for determing content type from file type extension/*
 * ====================================================================
 *
 * The Apache Software License, Version 1.1
 *
 * Copyright (c) 2002-2003 The Apache Software Foundation.  All rights
 * reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 *
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 *
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in
 *    the documentation and/or other materials provided with the
 *    distribution.
 *
 * 3. The end-user documentation included with the redistribution, if
 *    any, must include the following acknowlegement:
 *       ""This product includes software developed by the
 *        Apache Software Foundation (http://www.apache.org/).""
 *    Alternately, this acknowlegement may appear in the software itself,
 *    if and wherever such third-party acknowlegements normally appear.
 *
 * 4. The names ""The Jakarta Project"", ""Commons"", and ""Apache Software
 *    Foundation"" must not be used to endorse or promote products derived
 *    from this software without prior written permission. For written
 *    permission, please contact apache@apache.org.
 *
 * 5. Products derived from this software may not be called ""Apache""
 *    nor may ""Apache"" appear in their names without prior written
 *    permission of the Apache Group.
 *
 * THIS SOFTWARE IS PROVIDED ``AS IS'' AND ANY EXPRESSED OR IMPLIED
 * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES
 * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED.  IN NO EVENT SHALL THE APACHE SOFTWARE FOUNDATION OR
 * ITS CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF
 * USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
 * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT
 * OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 * ====================================================================
 *
 * This software consists of voluntary contributions made by many
 * individuals on behalf of the Apache Software Foundation.  For more
 * information on the Apache Software Foundation, please see
 * <http://www.apache.org/>.
 *
 * [Additional notices, if required by prior licensing conditions]
 *
 */

package org.apache.commons.httpclient.contrib.utils;

import java.io.File;
import java.io.IOException;

/**
 * This class provides mappings from file name extensions to content types.
 *
 * @author <a href=""mailto:emdevlin@charter.net"">Eric Devlin</a>
 * 
 * DISCLAIMER: HttpClient developers DO NOT actively support this component.
 * The component is provided as a reference material, which may be inappropriate
 * to be used without additional customization.
 */

public class ContentType {

	/** Mime Type mappings 'liberated' from Tomcat4.1.18/conf/web.xml*/
	public static final String[][] MIME_TYPE_MAPPINGS =	
		{	{ ""abs"",		""audio/x-mpeg"" },
			{ ""ai"",			""application/postscript"" },
			{ ""aif"",		""audio/x-aiff"" },
			{ ""aifc"",		""audio/x-aiff"" },
			{ ""aiff"",		""audio/x-aiff"" },
			{ ""aim"",		""application/x-aim"" },
			{ ""art"",		""image/x-jg"" },
			{ ""asf"",		""video/x-ms-asf"" },
			{ ""asx"",		""video/x-ms-asf"" },
			{ ""au"",			""audio/basic"" },
			{ ""avi"",		""video/x-msvideo"" },
			{ ""avx"",		""video/x-rad-screenplay"" },
			{ ""bcpio"",		""application/x-bcpio"" },
			{ ""bin"",		""application/octet-stream"" },
			{ ""bmp"",		""image/bmp"" },
			{ ""body"",		""text/html"" },
			{ ""cdf"",		""application/x-cdf"" },
			{ ""cer"",		""application/x-x509-ca-cert"" },
			{ ""class"",		""application/java"" },
			{ ""cpio"",		""application/x-cpio"" },
			{ ""csh"",		""application/x-csh"" },
			{ ""css"",		""text/css"" },
			{ ""dib"",		""image/bmp"" },
			{ ""doc"",		""application/msword"" },
			{ ""dtd"",		""text/plain"" },
			{ ""dv"",			""video/x-dv"" },
			{ ""dvi"",		""application/x-dvi"" },
			{ ""eps"",		""application/postscript"" },
			{ ""etx"",		""text/x-setext"" },
			{ ""exe"",		""application/octet-stream"" },
			{ ""gif"",		""image/gif"" },
			{ ""gtar"",		""application/x-gtar"" },
			{ ""gz"",			""application/x-gzip"" },
			{ ""hdf"",		""application/x-hdf"" },
			{ ""hqx"",		""application/mac-binhex40"" },
			{ ""htc"",		""text/x-component"" },
			{ ""htm"",		""text/html"" },
			{ ""html"",		""text/html"" },
			{ ""hqx"",		""application/mac-binhex40"" },
			{ ""ief"",		""image/ief"" },
			{ ""jad"",		""text/vnd.sun.j2me.app-
descriptor"" },
			{ ""jar"",		""application/java-archive"" },
			{ ""java"",		""text/plain"" },
			{ ""jnlp"",		""application/x-java-jnlp-
file"" },
			{ ""jpe"",		""image/jpeg"" },
			{ ""jpeg"",		""image/jpeg"" },
			{ ""jpg"",		""image/jpeg"" },
			{ ""js"",			""text/javascript"" },
			{ ""jsf"",		""text/plain"" },
			{ ""jspf"",		""text/plain"" },
			{ ""kar"",		""audio/x-midi"" },
			{ ""latex"",		""application/x-latex"" },
			{ ""m3u"",		""audio/x-mpegurl"" },
			{ ""mac"",		""image/x-macpaint"" },
			{ ""man"",		""application/x-troff-man"" },
			{ ""me"",			""application/x-troff-me"" },
			{ ""mid"",		""audio/x-midi"" },
			{ ""midi"",		""audio/x-midi"" },
			{ ""mif"",		""application/x-mif"" },
			{ ""mov"",		""video/quicktime"" },
			{ ""movie"",		""video/x-sgi-movie"" },
			{ ""mp1"",		""audio/x-mpeg"" },
			{ ""mp2"",		""audio/x-mpeg"" },
			{ ""mp3"",		""audio/x-mpeg"" },
			{ ""mpa"",		""audio/x-mpeg"" },
			{ ""mpe"",		""video/mpeg"" },
			{ ""mpeg"",		""video/mpeg"" },
			{ ""mpega"",		""audio/x-mpeg"" },
			{ ""mpg"",		""video/mpeg"" },
			{ ""mpv2"",		""video/mpeg2"" },
			{ ""ms"",			""application/x-wais-source"" },
			{ ""nc"",			""application/x-netcdf"" },
			{ ""oda"",		""application/oda"" },
			{ ""pbm"",		""image/x-portable-bitmap"" },
			{ ""pct"",		""image/pict"" },
			{ ""pdf"",		""application/pdf"" },
			{ ""pgm"",		""image/x-portable-graymap"" },
			{ ""pic"",		""image/pict"" },
			{ ""pict"",		""image/pict"" },
			{ ""pls"",		""audio/x-scpls"" },
			{ ""png"",		""image/png"" },
			{ ""pnm"",		""image/x-portable-anymap"" },
			{ ""pnt"",		""image/x-macpaint"" },
			{ ""ppm"",		""image/x-portable-pixmap"" },
			{ ""ps"",			""application/postscript"" },
			{ ""psd"",		""image/x-photoshop"" },
			{ ""qt"",			""video/quicktime"" },
			{ ""qti"",		""image/x-quicktime"" },
			{ ""qtif"",		""image/x-quicktime"" },
			{ ""ras"",		""image/x-cmu-raster"" },
			{ ""rgb"",		""image/x-rgb"" },
			{ ""rm"",			""application/vnd.rn-
realmedia"" },
			{ ""roff"",		""application/x-troff"" },
			{ ""rtf"",		""application/rtf"" },
			{ ""rtx"",		""text/richtext"" },
			{ ""sh"",			""application/x-sh"" },
			{ ""shar"",		""application/x-shar"" },
			{ ""smf"",		""audio/x-midi"" },
			{ ""snd"",		""audio/basic"" },
			{ ""src"",		""application/x-wais-source"" },
			{ ""sv4cpio"",	""application/x-sv4cpio"" },
			{ ""sv4crc"",		""application/x-sv4crc"" },
			{ ""swf"",		""application/x-shockwave-
flash"" },
			{ ""t"",			""application/x-troff"" },
			{ ""tar"",		""application/x-tar"" },
			{ ""tcl"",		""application/x-tcl"" },
			{ ""tex"",		""application/x-tex"" },
			{ ""texi"",		""application/x-texinfo"" },
			{ ""texinfo"",	""application/x-texinfo"" },
			{ ""tif"",		""image/tiff"" },
			{ ""tiff"",		""image/tiff"" },
			{ ""tr"",			""application/x-troff"" },
			{ ""tsv"",		""text/tab-separated-values"" },
			{ ""txt"",		""text/plain"" },
			{ ""ulw"",		""audio/basic"" },
			{ ""ustar"",		""application/x-ustar"" },
			{ ""xbm"",		""image/x-xbitmap"" },
			{ ""xml"",		""text/xml"" },
			{ ""xpm"",		""image/x-xpixmap"" },
			{ ""xsl"",		""text/xml"" },
			{ ""xwd"",		""image/x-xwindowdump"" },
			{ ""wav"",		""audio/x-wav"" },
			{ ""svg"",		""image/svg+xml"" },
			{ ""svgz"",		""image/svg+xml"" },
			{ ""wbmp"",		""image/vnd.wap.wbmp"" },
			{ ""wml"",		""text/vnd.wap.wml"" },
			{ ""wmlc"",		""application/vnd.wap.wmlc"" },
			{ ""wmls"",		""text/vnd.wap.wmlscript"" },
			{ ""wmlscriptc"",	""application/vnd.wap.wmlscriptc"" },
			{ ""wrl"",		""x-world/x-vrml"" },
			{ ""Z"",			""application/x-compress"" },
			{ ""z"",			""application/x-compress"" },
			{ ""zip"",		""application/zip"" } };

    /**
     * Get the content type based on the extension of the file name<br>
     *
     * @param fileName for which the content type is to be determined.
     *
     * @return the content type for the file or null if no mapping was
     * possible.
     */
	public static String get( String fileName  ) {
		String contentType = null;

		if ( fileName != null ) {
			int extensionIndex = fileName.lastIndexOf( '.' );
			if ( extensionIndex != -1 ) {
				if ( extensionIndex + 1 < fileName.length() ) {
					String extension = fileName.substring( 
extensionIndex + 1 );
					for( int i = 0; i < 
MIME_TYPE_MAPPINGS.length; i++ ) {
						if ( extension.equals( 
MIME_TYPE_MAPPINGS[i][0] ) ) {
							contentType = 
MIME_TYPE_MAPPINGS[i][1];
							break;
						}
					}
				}
			}
		}

		return contentType;
	}

    /**
     * Get the content type based on the extension of the file name<br>
     *
     * @param file for which the content type is to be determined.
     *
     * @return the content type for the file or null if no mapping was
     * possible.
     *
     * @throws IOException if the construction of the canonical path for 
	 * the file fails.
     */
	public static String get( File file ) 
		throws IOException
	{
		String contentType = null;

		if ( file != null ) {

			contentType = get( file.getCanonicalPath() );
		}

		return contentType;
	}
}"
0,"Exception root cause is swallowed in various placesWhen re-throwing an exception, the root cause is swallowed in some places in Jackrabbit, mainly when converting to an IOException."
1,"OpenBitSet#hashCode() may return false for identical sets.OpenBitSet uses an internal buffer of long variables to store set bits and an additional 'wlen' index that points 
to the highest used component inside {@link #bits} buffer.

Unlike in JDK, the wlen field is not continuously maintained (on clearing bits, for example). This leads to a situation when wlen may point
far beyond the last set bit. 

The hashCode implementation iterates over all long components of the bits buffer, rotating the hash even for empty components. This is against the contract of hashCode-equals. The following test case illustrates this:

{code}
// initialize two bitsets with different capacity (bits length).
BitSet bs1 = new BitSet(200);
BitSet bs2 = new BitSet(64);
// set the same bit.
bs1.set(3);
bs2.set(3);
        
// equals returns true (passes).
assertEquals(bs1, bs2);
// hashCode returns false (against contract).
assertEquals(bs1.hashCode(), bs2.hashCode());
{code}

Fix and test case attached."
0,"Implement jcr-jackrabbit://... repository URIsThe current file://... URIs used by the Jackrabbit RepositoryFactoryImpl class make it hard to support extra ?parameters and prevent other JCR implementations from using similar repository URI patterns.

Thus I propose that we start supporting a jcr-jackrabbit://... URI pattern in addition to the current file://... pattern."
0,"MultiThreadedConnectionManager Accounting ProblemsgetConnectionsInPool() is certainly a more intutive name. 
At the same, as you already mentioned, certainly there need to be a connection killer method: 
MultiThreadedHttpConnectionManager.destroyIdleConnections(long idleTime) 

Also, I would recommend one method which could spit out connection statistics at any time for the 
given Connection Manager. This will be great method for testing purpose as well. 
MultiThreadedHttpConnectionManager.displayCurrentStatistics(); 
----------------------------------
Curent Connection Statistics
----------------------------------
Total connectinos in Pool = 10
Open connectinos          = 3
Close connections         = 5
Stale connections         = 2 
And, if are even more adventurous we could extend our report to: 
Average wait time for connection = 1356 ms
Maximum wait time for connectino = 1892 ms"
0,"surround test code is incompatible with *Test pattern in test target.Attachments to follow:
renamed BooleanQueryTest to BooleanQueryTst,
renamed ExceptionQueryTest to ExceptionQueryTst,
patch for the remainder of the test code to use the renamed classes."
1,"InitiatedIndex: CCE on casting NumericField to FieldAn unchecked cast to List<Field> throws a ClassCastException when applied to, for example, a NumericField.
Appearently, this has been fixed trunk, but for a 2.9.1 release, this could be helpful.
The patch can be applied against the 2.9.0 tag."
0,FST serialization and deserialization from plain DataInput/DataOutput streams.Currently the automaton can be saved only to a Directory instance (IndexInput/ IndexOutput).
1,"Wire produces invalid log skipping zero bytes in certain casesWireLogInputStream class line 82 check if the byte returned is not -1 meaning end of stream. But the condition is wrong in case if this byte is 0, it should look like

if (l != -1) {
//...
}
"
0,"Demo targets for running the demoNow that the demo build targets are working and build the jar/war, it may be useful for users to also be able to run the demo with something like 'ant run-demo'. This complements existing docs/demo.html."
0,"TwoPhaseCommit interfaceI would like to propose a TwoPhaseCommit interface which declares the methods necessary to implement a 2-phase commit algorithm:
* prepareCommit()
* commit()
* rollback()

The prepare/commit ones have variants that take a (Map<String,String> commitData) following the ones we have in IndexWriter.

In addition, a TwoPhaseCommitTool which implements a 2-phase commit amongst several TPCs.

Having IndexWriter implement that interface will allow running the 2-phase commit algorithm on multiple IWs or IW + any other object that implements the interface.

We should mark the interface @lucene.internal so as to not block ourselves in the future. This is pretty advanced stuff anyway.

Will post a patch soon"
1,"Query with document order fails when result set size > caching hierarchy manager sizeWhen a query returns a lot of nodes in the query result and document order is enabled (which is the default) then the query will fail with error messages in the log:

*ERROR* [main] DocOrderNodeIteratorImpl: Internal error: unable to determine document order of nodes: (DocOrderNodeIteratorImpl.java, line 241)
*ERROR* [main] DocOrderNodeIteratorImpl:    Node1: /stuff/node[2]/node[13]/node9 (DocOrderNodeIteratorImpl.java, line 242)
*ERROR* [main] DocOrderNodeIteratorImpl:    Node2: /stuff/node[2]/node[13]/node5 (DocOrderNodeIteratorImpl.java, line 243)

The critical size seems to be equivalent to the cache size of the caching hierarchy manager. Attached are two test cases. The first one simply creates test nodes and the second one executes a query for those nodes. Using the cache size of 10'000 in the CachingHierarchyManager#DEFAULT_UPPER_LIMIT everything works fine, but when this value is set to 1000 (you need to re-compile the class CachingHierarchyManager) the test fails with the mentioned errors."
1,"add/remove dispatchers from DelegatingObservationDispatcher is not synchronizedthe 'dispatchers' hashset in DelegatingObservationDispatcher is not synchronized and can lead to errors, when a workspace goes offline or is creating during event dispatching."
0,"Prevent logins during repository shutdownRelated to the last comment in JCR-445, should we prevent new sessions from being created during repository shutdown? It is an odd chance to run into a problem like that, but it seems like the issue could be easily solved by making getWorkspaceInfo() synchronized and adding sanityCheck() calls to the createSession() methods."
0,how-to deployment modelshow-to with a detailed description of the steps required for each deployment model. 
0,"remove _X.fnxCurrently we store a global (not per-segment) field number->name mapping in _X.fnx

However, it doesn't actually save us any performance e.g on IndexWriter's init because
since LUCENE-2984 we are to loading the fieldinfos anyway to compute files() for IFD, etc, 
as thats where hasProx/hasVectors is.

Additionally in the past global files like shared doc stores have caused us problems,
(recently we just fixed a bug related to this file in LUCENE-3601).

Finally this is trouble for backwards compatibility as its difficult to handle a global
file with the codecs mechanism."
0,"spi2davex: use srcWorkspaceName to build srcPath for clone and copyspi2davex provides as simple workaround for the missing clone and cross-workspace-copy in spi2dav.
however, the src workspace name isn't used to build the src path."
0,"Generify Security APIThe current security api, namely the Authorizable and Group interface use non-generic collections/iterators.
suggest to change this."
0,"Decouple indexer from Document/Field implsI think we should define minimal iterator interfaces,
IndexableDocument/Field, that indexer requires to index documents.

Indexer would consume only these bare minimum interfaces, not the
concrete Document/Field/FieldType classes from oal.document package.

Then, the Document/Field/FieldType hierarchy is one concrete impl of
these interfaces. Apps are free to make their own impls as well.
Maybe eventually we make another impl that enforces a global schema,
eg factored out of Solr's impl.

I think this frees design pressure on our Document/Field/FieldType
hierarchy, ie, these classes are free to become concrete
fully-featured ""user-space"" classes with all sorts of friendly sugar
APIs for adding/removing fields, getting/setting values, types, etc.,
but they don't need substantial extensibility/hierarchy. Ie, the
extensibility point shifts to IndexableDocument/Field interface.

I think this means we can collapse the three classes we now have for a
Field (Fieldable/AbstracField/Field) down to a single concrete class
(well, except for LUCENE-2308 where we want to break out dedicated
classes for different field types...).
"
0,"Summer of Code GDATA Server  --Project structure and simple version to start with--This is the initial issue for the GDATA - Server project (Google Summer of Code). 
The purpose of the issue is to create the project structure in the svn repository to kick off the project. The source e.g. the project will be located at URL: http://svn.apache.org/repos/asf/lucene/java/trunk/contrib
The attachment includes the diff text file and the jar files included in the lib directory as a tar.gz file.
To get some information about the project see http://wiki.apache.org/general/SimonWillnauer/SummerOfCode2006"
1,SQL2 ISDESCENDANTNODE BooleanQuery#TooManyClauses returnsThe initial fix is not generic enough. It still fails after adding twice the max clauses count.
1,"Field names can be wrong for stored fields / term vectors after mergingThe good news is this bug only exists in trunk... the bad news is it's
been here for some time (created by accident in LUCENE-2881).  But the
good news is it should strike fairly rarely.

SegmentMerger sometimes incorrectly thinks it can bulk-copy TVs/stored
fields when it cannot (because field numbers don't map to the same
names across segments).

I think it happens only with addIndexes, or indexes that have
pre-trunk segments, and then SM falsely thinks it can bulk-merge only
when the last field number has the same field name across segments.
"
0,"Suggestion regarding NodeImpl and PropertyImpl in jackrabbit.coreBoth NodeImpl and PropertyImpl contain in their respective setProperty/setValue (respectively) initial validation checks, that is indentical across the various
variants of each and could possibly in either case be place in a separate method.

in NodeImpl.setProperty (also: addMixin, removeMixin, orderBefore):

- sanityCheck
- is-parent-checked-out
- is-not-protected (missing for setProperty ???)
- is-parent-not-locked

in PropertyImpl.setValue:

- sanityCheck
- is-parent-checked-out
- is-not-protected 
- is-value-compatible-with-multivalue-definition (NOTE: check opposite for setting single
  or multiple values)
- is-parent-not-locked


Second, i'm never sure, in which case ChildNode, ChildProperty is prefered 
over Node/Property (as present in the jcr api)...

regards
angela

"
1,"HttpClient should always override the host of HostConfiguration if an absolute request URI is givenThis bug most likely occurs on all patforms and OS's, but I have only tested it
on WinXP.

The HttpClient.executeMethod(HostConfiguration,HttpMethod,HttpState) will
receive and throw an IllegalArgumentException stating that ""host parameter is
null"" when a  HostConfiguration object is passed in that ONLY has a proxy set
(via HostConfiguration.setProxy(String, int)). Details to reproduce follow--the
bug can be easily reproduced by using the Apache Axis 1.2 CommonsHTTPSender
class (with JVM system props http.proxyHost, http.proxyPort set):

There is a bug in the Apache Commons HTTP Client 3.0rc2 that does not set the
hostname property
in the <code>HostConfiguration</code> object if the following two steps
are performed:<br>
1. You call
<code>HttpClient.executeMethod(HostConfiguration,HttpMethod,HttpState)</code>
with a <code>HostConfiguration</code> object and an <code>HttpMethod</code> object
(created using the HttpMethod(String uri) constructor).This method 
is called in this exact way in the Apache Axis 1.2 client
(CommonsHTTPSender.java lines 132 and 186).<br>
2. That <code>HostConfiguration</code> object only has a proxy set (using
setProxy(String, int)). This method 
is called in this exact way in the Apache Axis 1.2 client
(CommonsHTTPSender.java line 389).<br>

Apache Axis 1.2rc3 CommonsHTTPSender.java did not expose this bug in Commons
HTTP Client 3.0rc2 because
it set the <code>HostConfiguration</code> in a different manner, as follows:<br>
1. Call <code>HttpClient.setHostConfiguration(HostConfiguration)</code> first.
Again,
The <code>HostConfiguration</code> object must only have a proxy set and no host
name.<br>
2. Then call <code>HttpClient.executeMethod(HttpMethod)</code>.<br>

Using the above steps (as in Axis 1.2rc3 CommonsHTTPSender.java, invoke()
method), line 379 in HttpClient.java evaluates to true
because the argument <code>hostConfiguration</code> is null (see line 324 in
HttpClient.java) and the local 
variable <code>defaultHostConfiguration</code> ==
<code>HttpClient.setHostConfiguration(HostConfiguration)</code>
which was set in item #1 above. The hostname then gets set in the
<code>HostConfiguration</code>
object in line 384 of HttpClient.java."
0,"Source release files missing the *.pom.template filesThe source release files should contain the *.pom.template files, otherwise it is not possible to build the maven artifacts using ""ant generate-maven-artifacts"" from official release files."
0,"Allow storing user data when IndexWriter.commit() is calledSpinoff from here:

    http://www.mail-archive.com/java-user@lucene.apache.org/msg22303.html

The idea is to allow optionally passing an opaque String commitUserData to the IndexWriter.commit method.  This String would be stored in the segments_N file, and would be retrievable by an IndexReader.  Applications could then use this to assign meaning to each commit.

It would be nice to get this done for 2.4, but I don't think we should hold the release for it."
1,"Kerberos cross-realm support is brokenThis issue is basically based on the same facts as this issue https://issues.sonatype.org/browse/AHC-71?focusedCommentId=129559#action_129559 
Since the Kerberos code looks the same, I assume that AHC used your code. The same patch can be applied to fix [this http://hc.apache.org/httpcomponents-client-ga/httpclient/xref/org/apache/http/impl/auth/NegotiateScheme.html#200] defective code."
1,"""Socket Closed"" IOException thrown by HttpConnectionHttpClient.java was modified in version 2.0 Final in method executeMethod().
The call to connection.setSoTimeout() used to be in RC3 after the call to
connection.isOpen(), but in the final version the call happens before the call 
to isOpen(). The result of the change is that the setSoTimeout() call could 
throw IOException because of closed socket.

I would fix the problem by adding to HttpConnection.setSoTimeout() (and to 
other similar methods in HttpConnection) an explicit check (a call to isOpen
()) whether the socket is closed as the existence of socket object does not 
guarantee it. I.e the following code:

    public void setSoTimeout(int timeout)
        throws SocketException, IllegalStateException {
        LOG.debug(""HttpConnection.setSoTimeout("" + timeout + "")"");
        soTimeout = timeout;
        if (socket != null) {
            socket.setSoTimeout(timeout);
        }
    }

would be changed to

    public void setSoTimeout(int timeout)
        throws SocketException, IllegalStateException {
        LOG.debug(""HttpConnection.setSoTimeout("" + timeout + "")"");
        soTimeout = timeout;
        if (isOpen()) {
            socket.setSoTimeout(timeout);
        }
    }"
1,"NPE in NearSpansUnordered.isPayloadAvailable() Using RC1 of lucene 2.4 resulted in null pointer exception with some constructed SpanNearQueries

Implementation of isPayloadAvailable() (results in exception)
{code}
 public boolean isPayloadAvailable() {
   SpansCell pointer = min();
   do {
     if(pointer.isPayloadAvailable()) {
       return true;
     }
     pointer = pointer.next;
   } while(pointer.next != null);

   return false;
  }
{code}

""Fixed"" isPayloadAvailable()
{code}
 public boolean isPayloadAvailable() {
   SpansCell pointer = min();
   while (pointer != null) {
     if(pointer.isPayloadAvailable()) {
       return true;
     }
     pointer = pointer.next;
   }

   return false;
  }
{code}

Exception produced:
{code}
  [junit] java.lang.NullPointerException
    [junit]     at org.apache.lucene.search.spans.NearSpansUnordered$SpansCell.access$300(NearSpansUnordered.java:65)
    [junit]     at org.apache.lucene.search.spans.NearSpansUnordered.isPayloadAvailable(NearSpansUnordered.java:235)
    [junit]     at org.apache.lucene.search.spans.NearSpansOrdered.shrinkToAfterShortestMatch(NearSpansOrdered.java:246)
    [junit]     at org.apache.lucene.search.spans.NearSpansOrdered.advanceAfterOrdered(NearSpansOrdered.java:154)
    [junit]     at org.apache.lucene.search.spans.NearSpansOrdered.next(NearSpansOrdered.java:122)
    [junit]     at org.apache.lucene.search.spans.SpanScorer.next(SpanScorer.java:54)
    [junit]     at org.apache.lucene.search.Scorer.score(Scorer.java:57)
    [junit]     at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:137)
    [junit]     at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:113)
    [junit]     at org.apache.lucene.search.Hits.getMoreDocs(Hits.java:113)
    [junit]     at org.apache.lucene.search.Hits.<init>(Hits.java:80)
    [junit]     at org.apache.lucene.search.Searcher.search(Searcher.java:50)
    [junit]     at org.apache.lucene.search.Searcher.search(Searcher.java:40)
    [junit]     at com.attivio.lucene.SpanQueryTest.search(SpanQueryTest.java:79)
    [junit]     at com.attivio.lucene.SpanQueryTest.assertHitCount(SpanQueryTest.java:75)
    [junit]     at com.attivio.lucene.SpanQueryTest.test(SpanQueryTest.java:67)
{code}

will attach unit test that causes exception (and passes with updated isPayloadAvailable())
"
1,"QOM to SQL2 doesn't cast numeric literalsSQL2 statements generated by the QueryObjectModel.getStatement() don't contain CAST(... AS ...) for numeric literals of types DECIMAL, DOUBLE, and LONG. The type information is lost, which can result in incorrect query results (depending on the query engine) if the generated SQL2 statement is executed."
0,"add maxtf to fieldinvertstatethe maximum within-document TF is a very useful scoring value, 
we should expose it so that people can use it in scoring

consider the following sim:
{code}
@Override
public float idf(int docFreq, int numDocs) {
  return 1.0F; /* not used */
}

@Override
public float computeNorm(String field, FieldInvertState state) {
  return state.getBoost() / (float) Math.sqrt(state.getMaxTF());
}
{code}

which is surprisingly effective, but more interesting for practical reasons.

"
0,"Distribution of commons classesjukka started a discussion regarding distribution of commons classes a while ago:

http://www.mail-archive.com/dev@jackrabbit.apache.org/msg06698.html

"
0,"SQL2 parser: improved error message for ambiguous properties in joinsFor queries of the form:

select id from parent inner join child on parent.id=child.parentid

the SQL2 parser currently only returns a generic error message ""This query result contains more than one selector"". 

The error message should point to the problematic token: 

Query: select id(*)from parent inner join child on parent.id=child.parentid; expected: Need to specify the selector name for ""id"" because the query contains more than one selector.
"
0,"uploading large streams through rmiwhen I try to upload a file of 35 Meg, I get an out of memory error.

This is caused because the whole file is read into memory instead of buffering"
0,"Registering multiple node types with the same name in a single file must failRegistering node types from a file (XML or CND) containing multiple definitions with the same name will succeed and only the last definition will be used.
The right behavior is to throw an exception as this kind of file is well-formed but invalid."
1,PdfTextExtractor does not close temp file in case of an errorIf PDF parsing fails in PDFParser.parse() a temp file is not closed and results in an open file handle.
0,Update POI dependency to 3.0.2-FINAL3.0.2-FINAL is the most recent POI release.
0,Improvements to user management (2)follow up issue as JCR-2199 is already closed.
1,ChainedFilter does not work well in the event of filters in ANDNOTFirst ANDNOT operation takes place against a completely false bitset and will always return zero results.  
0,"ContentBody doesn't currently have a setMimeType method.ContentBody and therefore FileBody, StringBody and InputStreamBody do not have a setMimeType method so you can't set the Mime Type, it always defaults. 
Current workaround is to subclass and override getMimeType."
1,"Missing 'node removed' event when removing a versionWhen a version is removed only one 'node removed' event is triggered for the version node. Even though the frozen node under that version also gets removed there is no event for the frozen node.

See failing test cases:
org.apache.jackrabbit.core.observation.VersionEventsTest#testRemoveVersion()
org.apache.jackrabbit.core.observation.VersionEventsTest#testXARemoveVersion()
"
1,"MergeThread throws unchecked exceptions from toString()This causes nearly all thread-dumping routines to fail and in the effect obscure the original problem. I think this
should return a string (always), possibly indicating the underlying writer has been closed or something."
0,"Path should implement SerializableQName already implements Serializable, for ease of use Path should also support Serializable."
0,"Refactoring of FilteredTermsEnum and MultiTermQueryFilteredTermsEnum is confusing as it is initially positioned to the first term. It should instead work like an uninitialized TermsEnum for a field before the first call to next() or seek().
FilteredTermsEnums cannot implement seek() as eg. NRQ or Automaton are not able to support this. Seeking is also not needed for MTQ at all, so seek can just throw UOE.
This issue changes some of the internal behaviour of MTQ and FilteredTermsEnum to allow also seeking in NRQ and Automaton (see comments below)."
0,"Allow controllable printing of the hitsAdds ""print.hits.field"" property to the alg.  If set, then the hits retrieved by Search* tasks are printed, along with the value of the specified field, for each doc."
0,"specialize payload processing from of DocsAndPositionsEnumIn LUCENE-2760 i started working to try to improve the speed of a few spanqueries.
In general the trick there is to avoid processing positions if you dont have to.

But, we can improve queries that read lots of positions further by cleaning up SegmentDocsAndPositionsEnum, 
in nextPosition() this has no less than 3 payloads-related checks.

however, a large majority of users/fields have no payloads at all.
I think we should specialize this case into a separate implementation and speed up the common case.

edit: dyslexia with the jira issue number."
1,"[PATCH] NullPointerException when using nested SpanOrQuery in SpanNotQueryOverview description: 
I'm using the span query classes in Lucene to generate higher scores for 
search results where the search terms are closer together. In certain 
situations I want to exclude terms from the span. When I attempt to exclude 
more than one term I get an error. 
 
The example query I'm using is:  
 
'brighton AND tourism' -pier -contents 
 
I construct the query objects and the toString() version is: 
 
spanNot(spanNear([contents:brighton contents:tourism], 10, false), 
spanOr([contents:pier, contents:road])) 
  
 
Steps to reproduce: 
1. Construct a SpanNearQuery (must have at least one term, but at least two 
makes more sense) 
2. Construct a SpanOrQuery containing two or more terms 
3. Construct a SpanNotQuery to include the first query object and exclude the 
second (SpanOrQuery) 
4. Execute the search 
 
 
Actual Results: 
A null pointer exception is thrown while generating the scores within the 
search. 
 
Stack trace:  
java.lang.NullPointerException   
        at   
org.apache.lucene.search.spans.SpanOrQuery$1.doc(SpanOrQuery.java:174)   
        at   
org.apache.lucene.search.spans.SpanNotQuery$1.next(SpanNotQuery.java:75)   
        at org.apache.lucene.search.spans.SpanScorer.next(SpanScorer.java:50)   
        at org.apache.lucene.search.Scorer.score(Scorer.java:37)   
        at   
org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:92)   
        at org.apache.lucene.search.Hits.getMoreDocs(Hits.java:64)   
        at org.apache.lucene.search.Hits.<init>(Hits.java:43)   
        at org.apache.lucene.search.Searcher.search(Searcher.java:33)   
        at org.apache.lucene.search.Searcher.search(Searcher.java:27)   
        at   
com.runtimecollective.search.LuceneSearch.search(LuceneSearch.java:362)   
 
 
Expected Resuts: 
It executes the search and results where the first search terms (near query) 
are close together but without the second terms (or query) appearing."
1,"isCurrent() and getVersion() on an NRT reader are brokenRight now isCurrent() will always return true for an NRT reader and getVersion() will always return the version of the last commit.  This is because the NRT reader holds the live segmentInfos.

I think isCurrent() should return ""false"" when any further changes have occurred with the writer, else true.   This is actually fairly easy to determine, since the writer tracks how many docs & deletions are buffered in RAM and these counters only increase with each change.

getVersion should return the version as of when the reader was created."
0,"Move to jackrabbit.apache.orgJackrabbit will be moving to

   http://jackrabbit.apache.org/

There are a number of infrastructure tasks that will need to be done by Roy.

There will also be a need to change our documentation and site to point to the new URL
and mailing list addresses, which can be done by anyone.

The existing mailing lists will be moved

    jackrabbit-dev at incubator  -->  dev at jackrabbit.apache.org
    jackrabbit-commits at incubator  -->  commits at jackrabbit.apache.org

and I will add

   users at jackrabbit.apache.org
"
0,"contrib/benchmark:  configurable HTML Parser, external classes to path, exhaustive doc maker""doc making"" enhancements:

1. Allow configurable html parser, with a new html.parser property.
Currently TrecDocMaker is using the Demo html parser. With this new property this can be overridden.

2. allow to add external class path, so the benchmark can be used with modified makers/parsers without having to add code to Lucene.
Run benchmark with e.g. ""ant run-task -Dbenchmark.ext.classpath=/myproj/myclasses""

3. allow to crawl a doc maker until exhausting all its files/docs once, without having to know in advance how many docs it can make. 
This can be useful for instance if the input data is in zip files."
1,"JCR-Server: Workspace.restore not mapped correctly(issue reported by David Kennedy)

Workspace.restore(Version[], boolean) won't work, since versions are not retrieved correctly. The version history that can be access from the request  resource, cannot be used to retrieve the versions needed for a workspace.restore call.

possible short term fix:
From the version-hrefs present in the request body of the UPDATE request, version resources must be built and
the corresponding version item retrieved.

alternative:
find a proper mapping for Workspace.restore(Version[], boolean). having UPDATE on a resource representing a javax.jcr.Node being mapped to a workspace.restore is odd.

"
1,"BoostingTermQuery.explain() bugsThere are a couple of minor bugs in BoostingTermQuery.explain().

1. The computation of average payload score produces NaN if no payloads were found. It should probably be:
float avgPayloadScore = super.score() * (payloadsSeen > 0 ? (payloadScore / payloadsSeen) : 1);

2. If the average payload score is zero, the value of the explanation is 0:
result.setValue(nonPayloadExpl.getValue() * avgPayloadScore);
If the query is part of a BooleanClause, this results in:
""no match on required clause...""
""failure to meet condition(s) of required/prohibited clause(s)""

The average payload score can be zero if the field boost = 0.

I've attached a patch to 'TestBoostingTermQuery.java', however, the test 'testNoPayload' fails in 'SpanScorer.score()' because the doc = -1. It looks like 'setFreqCurrentDoc() should have been called before 'score()'. Maybe someone more knowledgable of spans could investigate this.
"
0,"Move JCRWebdavServerServlet to jcr-server and make it abstractIn line with isse JCR-417, I suggest to partially move the JCRWebdavServerServlet from the jcr-webapp project to the jcr-server project. By partially I mean, that the new (moved) servlet will be abstract and the getRepository() method will be abstract. The jcr-webapp project will still contain a JCRWebdavServerServlet (for backwards compatibility maintaing the same name) which just extends the new servlet and implements the getRepository() method using the RepositoryAccess servlet.

This allows for the reuse of the jcr-server project including the abstract JCRWebdavServerServlet in other environments.
"
0,"There is no way to specify a different auth scheme priority for host and proxyUsing HttpClient 3.0 rc2, you cannot authenticate to a site using Basic
Autentication and to a proxy server using NTLM authentication.

When you indicate a prefference to use NTLM over Basic authentication the
authentication will fail when it tries to authenticate NTML to the Proxy and to
the Site. If you indicate Basic, then NTLM authentication order the Basic
authentication will fail when used for the Proxy (since basic authentication
can't send the domain name it fails).

The email thread from the discussion group is pasted below for refference.

==============================================================
==============================================================

Hi all,
I am trying to authenticate to a server via a proxy which also requires
authentication. It seems that I can get either the proxy authentication to work
OR the site authentication to work, but not both.

Both seem to work independently when I set the credentials (or proxy
credentials) using NTCredentials (e.g. if I connect to the site from a network
not using a proxy I can get it to work, and I can authenticate to the proxy only
to get a 401 authentication failed from the server when using the proxy).

I read in the Authentication tutorial that you can't authenticate using NTLM to
both the proxy and site, so I'm trying various combinations of authentication,
but I can't find any documentation that specifically covers this case and I feel
like I'm just taking stabs in the dark right now.

If anyone can point me in the direction of the light at the end of the tunnel
I'd really appreciate it.

Thanks,
David

----------------

On Wed, Jun 29, 2005 at 09:53:07AM -0700, David Parks wrote:
> Hi all,
> I am trying to authenticate to a server via a proxy which also requires
authentication. It seems that I can get either the proxy authentication to work
OR the site authentication to work, but not both.
> 
> Both seem to work independently when I set the credentials (or proxy
credentials) using NTCredentials (e.g. if I connect to the site from a network
not using a proxy I can get it to work, and I can authenticate to the proxy only
to get a 401 authentication failed from the server when using the proxy).
> 
> I read in the Authentication tutorial that you can't authenticate using NTLM
to both the proxy and site, so I'm trying various combinations of
authentication, but I can't find any documentation that specifically covers this
case and I feel like I'm just taking stabs in the dark right now.

David,

You _really_ can't use NTLM to authenticate with the proxy and the
target host at the same, due to the nature of this authentication
scheme. Really. That was not a joke.

Please consider using one of the following combinations instead:

(1) BASIC proxy + NTLM host if both the clent and the proxy are within a
trusted network segment

(2) NTLM proxy + SSL + BASIC host

Both combinations should provide an adequate (or better in the latter case)
security

Hope this helps

Oleg

> 
> If anyone can point me in the direction of the light at the end of the tunnel
I'd really appreciate it.
> 
> Thanks,
> David
> 
> 

-------------------

Thanks for the reply Oleg. This is what I figured, but I cannot see how to use
different authentication schemes for the Proxy vs. the Site authentication
challenge.

I tried adding the code suggested in the Authentication tutorial:

        List authPrefs = new ArrayList(2);
        authPrefs.add(AuthPolicy.DIGEST);
        authPrefs.add(AuthPolicy.BASIC);
        authPrefs.add(AuthPolicy.NTLM);
         This will exclude the NTLM authentication scheme
        httpclient.getParams().setParameter(AuthPolicy.AUTH_SCHEME_PRIORITY,
authPrefs);

I got a message stating that it was attempting BASIC authentication for the
Proxy and that it failed (probably because the domain doesn't get passed I
guess). So my thought is that I need NTLM for the proxy authentication and Basic
will work for the site authentication.

The question I am then working on is how to direct the HttpClient to select that
order of authentication methods. If I let it take NTLM as the preffered
authentication method then it will try to authenticate both challenges with NTLM.

I sure there is just some little detail I'm missing here somewhere, it's just
hard to find it.

Thanks a lot!
David

------------------

David,

I see the problem. This will require a patch and a new parameter.
Luckily the preference API introduced in HttpClient 3.0 allows up to add
parameters quite easily. Please file a feature request with Bugzilla
ASAP and I'll do my best to hack up a patch before I leave for holidays
(that is Friday, July 1st)

Oleg


--------------

Hi Oleg, thanks, I'll put that request in today.
This helps a lot, at least I know I'm on the right path now.

I am attempting to devise a workaround for this by handling the authentication
manually (setDoAuthentication(false)).

When I see a 401 error I am processing a basic authentication with the site
credentials, when I see a 407 error I want to process an NTLM authentication
with the proxy credentials.

To that end I have the following code that runs after
httpclient.execute(getmethod) executes. The code below works perfectly for the
basic authentication (when the proxy is not in the picture).

In looking up the Handshake of the NTLM authentication I see that I have a
problem with the code below since the handshake includes 2 challenge and
authorization steps before the authentication succeeds. I'm not clear how I
could manually authenticate the NTLM response. I would expect the NTLMScheme
class to contain a Type 1 and Type 3 authenticate() method for processing both
challenge responses. Is there another way of processing the NTLM authentication
after receiving the initial authentication challenge from the server?

        //Check for Proxy or Site authentication
        if(getmethod.getStatusCode() == 401){
            //Authenticate to the site using Basic authentication.
            BasicScheme basicscheme = new BasicScheme();
            String basic_auth_string = basicscheme.authenticate(new
NTCredentials(""cwftp"", ""664A754c"", """", """"), getmethod);
            Header basic_auth_header = new Header(""Authorization"",
basic_auth_string);
            getmethod.addRequestHeader(basic_auth_header);
            try{
                httpclient.executeMethod(getmethod);
            }catch(Exception e){
                logger.log(Level.SEVERE, ""ack!!!!"", e);
            }
            return getmethod;
       }else if(getmethod.getStatusCode() == 407){
            //Authenticate to the site using Basic authentication
            NTLMScheme ntlmscheme = new NTLMScheme();
            String basic_auth_string = ntlmscheme.authenticate(new
NTCredentials(""00mercbac"", ""!@SAMmerc2004"", ""simproxy"", ""CFC""), getmethod);
            Header basic_auth_header = new Header(""Authorization"",
basic_auth_string);
            getmethod.addRequestHeader(basic_auth_header);
            try{
                httpclient.executeMethod(getmethod);
            }catch(Exception e){
                logger.log(Level.SEVERE, ""ack!!!!"", e);
            }
            return getmethod;
       }


Thanks,
David"
0,Upgrade to Lucene 2.0We would like to upgrade to Lucene 1.9.1. There are jar conflicts when integrating with other projects such as Liferay Portal --  which uses v 1.9.1.
0,"Performance improvement: Lazy skipping on proximity fileHello,

I'm proposing a patch here that changes org.apache.lucene.index.SegmentTermPositions to avoid unnecessary skips and reads on the proximity stream. Currently a call of next() or seek(), which causes a movement to a document in the freq file also moves the prox pointer to the posting list of that document.  But this is only necessary if actual positions have to be retrieved for that particular document. 

Consider for example a phrase query with two terms: the freq pointer for term 1 has to move to document x to answer the question if the term occurs in that document. But *only* if term 2 also matches document x, the positions have to be read to figure out if term 1 and term 2 appear next to each other in document x and thus satisfy the query. 

A move to the posting list of a document can be quite expensive. It has to be skipped to the last skip point before that document and then the documents between the skip point and the desired document have to be scanned, which means that the VInts of all positions of those documents have to be read and decoded. 

An improvement is to move the prox pointer lazily to a document only if nextPosition() is called. This will become even more important in the future when the size of the proximity file increases (e. g. by adding payloads to the posting lists).

My patch implements this lazy skipping. All unit tests pass. 


I also attach a new unit test that works as follows:
Using a RamDirectory an index is created and test docs are added. Then the index is optimized to make sure it only has a single segment. This is important, because IndexReader.open() returns an instance of SegmentReader if there is only one segment in the index. The proxStream instance of SegmentReader is package protected, so it is possible to set proxStream to a different object. I am using a class called SeeksCountingStream that extends IndexInput in a way that it is able to count the number of invocations of seek(). 

Then the testcase searches the index using a PhraseQuery ""term1 term2"". It is known how many documents match that query and the testcase can verify that seek() on the proxStream is not called more often than number of search hits.

Example:
Number of docs in the index: 500
Number of docs that match the query ""term1 term2"": 5

Invocations of seek on prox stream (old code): 29
Invocations of seek on prox stream (patched version): 5

- Michael
"
0,"Deprecated AbstractWebdavServlet should be empty and extend new AbstractWebdavServletThe AbstractWebdavServlet has been copied from the jcr-server to the webdav project. The class at the old location has been marked deprecated. I suggest that in addition to marking it deprecated we should have this class extend the new AbstractWebdavServlet class from the webdav project but not contain any fields and methods. This way, users of the old class will always get the newest and best version but can still use the old class.

Will provide a patch for this proposal"
0,"implement PERSIST events for the EventJournalSee <http://www.day.com/specs/jcr/2.0/12_Observation.html#12.6.3%20Event%20Bundling%20in%20Journaled%20Observation>

"
0,"Fix IndexCommit hashCode() and equals() to be consistentIndexCommit's impl of hashCode() and equals() is inconsistent. One uses Dir + version and the other uses Dir + equals. According to hashCode()'s javadoc, if o1.equals(o2), then o1.hashCode() == o2.hashCode(). Simple fix, and I'll add a test case."
0,Speed up repeated TokenStream init by caching isMethodOverridden results
1,"OCM:The UUID of the collection elements changes on update.On ocm.update transaction, the  Current implementation of DefaultCollectionConverterImpl recreates the colleciton-element nodes if there is no id field specificaiton.  This is completely valid for majority of the cases.  But I came across a case where the colleciton element has a uuid field.  In this case also what is happening with the current implementation is that it drops all the elements from the old collection-elements and recreates the new ones.  The major flip side is that now I am left with brand new UUIDs.  I think we should address the uniqueness characteristics specified through UUID also while mapping colleciton elements.

I have a patch and a TestCase to verify the same.  I have implemented it only for the digester.  If people feel the approach is right I will work out an annotation based testcase as well.  I do not think it is going to fail even with annotations.
"
1,"failing Node.lock() might leave inconsistent transient stateWhen I try to node.lock(true, false) a node and the lock fails due to lak of user privilegies, the lock stay in the user transient session. If a perform a node.refresh(false) the node still is locked in the transient session."
0,"Improvements to contrib.benchmark for TREC collectionsThe benchmarking utilities for  TREC test collections (http://trec.nist.gov) are quite limited and do not support some of the variations in format of older TREC collections.  

I have been doing some benchmarking work with Lucene and have had to modify the package to support:
* Older TREC document formats, which the current parser fails on due to missing document headers.
* Variations in query format - newlines after <title> tag causing the query parser to get confused.
* Ability to detect and read in uncompressed text collections
* Storage of document numbers by default without storing full text.

I can submit a patch if there is interest, although I will probably want to write unit tests for the new functionality first.

"
0,"Remove dependency to log4jCurrently two classes in the test cases contain unused references to log4j.
The attached patch removes the unused logger and makes the package independent from lo4j"
0,"Move contribs/modules away from QueryParser dependencySome contribs and modules depend on the core QueryParser just for simplicity in their tests.  We should apply the same process as I did to the core tests, and move them away from using the QueryParser where possible."
1,"Default proxy set at the client level has no effectDefault proxy set at the client level has no effect, as client parameters are not correctly propagated to the HttpRoutePlanner"
0,"[PATCH] LockFactory implementation based on OS native locks (java.nio.*)The current default locking for FSDirectory is SimpleFSLockFactory.
It uses java.io.File.createNewFile for its locking, which has this
spooky warning in Sun's javadocs:

    Note: this method should not be used for file-locking, as the
    resulting protocol cannot be made to work reliably. The FileLock
    facility should be used instead.

So, this patch provides a LockFactory implementation based on FileLock
(using java.nio.*).

All unit tests pass with this patch, on OS X (10.4.8), Linux (Ubuntu
6.06), and Windows XP SP2.

Another benefit of native locks is the OS automatically frees them if
the JVM exits before Lucene can free its locks.  Many people seem to
hit this (old lock files still on disk) now.

I've created this new class:

  org.apache.lucene.store.NativeFSLockFactory

and added a couple test cases to the existing TestLockFactory.

I've left SimpleFSLockFactory as the default locking for FSDirectory
for now.  I think we should get some usage / experience with
NativeFSLockFactory and then later on make it the default locking
implementation?

I also tested changing FSDirectory's default locking to
NativeFSLockFactory and all unit tests still pass (on the above
platforms).

One important note about locking over NFS: some NFS servers and/or
clients do not support it, or, it's a configuration option or mode
that must be explicitly enabled.  When it's misconfigured it's able to
take a long time (35 seconds in my case) before throwing an exception.
To handle this, I acquire & release a random test lock on creating the
NativeFSLockFactory to verify locking is configured properly.

A few other small changes in the patch:

    - Added a ""failure reason"" to Lock.java so that in
      obtain(lockWaitTimeout), if there is a persistent IOException
      in trying to obtain the lock, this can be messaged & included in
      the ""Lock obtain timed out"" that's raised.

    - Corrected javadoc in SimpleFSLockFactory: it previously said the
      wrong system property for overriding lock class via system
      properties

    - Fixed unhandled IOException when opening an IndexWriter for
      create, if the locks dir does not exist (just added
      lockDir.exists() check in clearAllLocks method of
      SimpleFSLockFactory & NativeFSLockFactory.

    - Fixed a few small unrelated issues with TestLockFactory, and
      also fixed tests to accept NativeFSLockFactory as the default
      locking implementation for FSDirectory.

    - Fixed a typo in javadoc in FieldsReader.java

    - Added some more javadoc for the LockFactory.setLockPrefix
"
0,"better diagnostics when version storage is brokenIn InternalVersionManagerBase, the code doesn't do a null-check on the predecessors property, assuming it'll always be present. When this is not the case due to a repository problem, we'll see a NPE.

Proposal: add code that generates a more meaningful error message; making it easier to debug in production."
0,"TCK: ImpersonateTest#testImpersonate should allow LoginExceptionJSR-170 allows Session.impersonate to throw LoginException if the session lacks permission to impersonate another user.  Some repositories may not allow any session to impersonate another user, in which case this test would fail.

Proposal: catch and consume LoginException.

--- ImpersonateTest.java        (revision 422074)
+++ ImpersonateTest.java        (working copy)
@@ -17,11 +17,13 @@
 package org.apache.jackrabbit.test.api;
  
 import org.apache.jackrabbit.test.AbstractJCRTest;
+import org.apache.jackrabbit.test.NotExecutableException;
  
 import javax.jcr.Session;
 import javax.jcr.Credentials;
 import javax.jcr.NodeIterator;
 import javax.jcr.Node;
+import javax.jcr.LoginException;
 import java.security.AccessControlException;
  
 /**
@@ -40,7 +42,14 @@
      */
     public void testImpersonate() throws Exception {
         // impersonate to read-only user
-        Session session = superuser.impersonate(helper.getReadOnlyCredentials());
+        Session session = null;
+
+        try {
+            session = superuser.impersonate(helper.getReadOnlyCredentials());
+        }
+        catch (LoginException e) {
+          throw new NotExecutableException(""impersonate threw LoginException"");
+        }
  
         // get a path to test the permissions on
         String thePath = """";
"
0,Add a filter returning all document without a value in a fieldIn some situations it would be useful to have a Filter that simply returns all document that either have at least one or no value in a certain field. We don't have something like that out of the box and adding it seems straight forward.
1,"Spellchecker's dictionary iterator misbehavesIn LuceneDictionary, the LuceneIterator.hasNext() method has two issues that makes it misbehave:

1) If hasNext is called more than once, items are skipped
2) Much more seriously, when comparing fieldnames it is done with != rather than .equals() with the potential result that nothing is indexed
"
0,"Add VERBOSE to LuceneTestCase and LuceneTestCaseJ4component-build.xml allows to define tests.verbose as a system property when running tests. Both LuceneTestCase and LuceneTestCaseJ4 don't read that property. It will be useful for overriding tests to access one place for this setting (I believe currently some tests do it on their own). Then (as a separate issue) we can move all tests that don't check the parameter to only print if VERBOSE is true.

I will post a patch soon."
0,"Scorer.skipTo(current) remains on current for some scorersBackground in http://www.nabble.com/scorer.skipTo%28%29-contr-tf3880986.html

It appears that several scorers do not strictly follow the spec of Scorer.skipTo(n), and skip to current location remain in current location whereas the spec says: ""beyond current"". 

We should (probably) either relax the spec or fix the implementations."
0,"Remove some synchronization on CachingNamespaceResolverThe methods getQName() and getJCRName() are unnecessarily synchronized and cause monitor contention with concurrent calls to the methods of the NameCache interface (those are also synchronized).

I propose the following change:

Index: CachingNamespaceResolver.java
===================================================================
--- CachingNamespaceResolver.java	(revision 488245)
+++ CachingNamespaceResolver.java	(working copy)
@@ -84,7 +84,7 @@
     /**
      * @deprecated use {@link NameFormat#parse(String, NamespaceResolver)}
      */
-    public synchronized QName getQName(String name)
+    public QName getQName(String name)
             throws IllegalNameException, UnknownPrefixException {
         return NameFormat.parse(name, this);
     }
@@ -92,7 +92,7 @@
     /**
      * @deprecated use {@link NameFormat#format(QName, NamespaceResolver)}
      */
-    public synchronized String getJCRName(QName name)
+    public String getJCRName(QName name)
             throws NoPrefixDeclaredException {
         return NameFormat.format(name, this);
     }
"
1,"XPath relative path support missing for ""is null"" and ""is not null""I believe the change for issue JCR-247 is incomplete, for instance

  //*[@x]

and

  //*[foo/@x]

are parsed into the same query tree."
0,"Provide More of Lucene For MavenPlease provide javadoc & source jars for lucene-core.  Also, please provide the rest of lucene (the jars inside of ""contrib"" in the download bundle) if possible."
1,"ChunkedInputStream broken (2 bugs + fixes, 1 suggestion)Bug 1.

In the     

read(byte[] b, int off, int len)  method

of ChunkedInputStream, the number of bytes to read from the underlying 
InputStream is calculated wrongly. In the code this is done by

len = Math.min(len, chunkSize);

This could (and will) cause the server (also Apache) to indeed serve that 
number of bytes (let's say chunkSize), but it may be that we already had a 
number of bytes on the first read. The result is that the input is now NOT 
positioned on the end of a chunk and the rest of the reader fails because it 
cannot find CRLF or a valid chunksize.

Proposed fix (works, tested)
len = Math.min(len, chunkSize-pos);

Bug 2.

In the calculation of the chunkSize (method getChunkSizeFromInputStream) the 
conversion to int is done by calling             

result = Integer.parseInt(dataString, 16);

This is not robust and causes the occasional crash. The fix is simple and in 
fact implements what is done when the chunkSize is commented (see lines in code 
above)

result = Integer.parseInt(dataString.trim(), 16);

Tested and works.



Suggestion:
Same routine, input state machine. Perhaps just being pedantic..change while 
loop to:

        while (state != 2) {
            int b = in.read();
            if (b == -1) throw new IOException(""chunked stream ended 
unexpectedly"");
            switch (state) {
                case 0:
                    if (b == '\r')
                      state = 1;
                    else
                      baos.write(b);
                    break;
                case 1:
                    if (b == '\n')
                      state = 2;
                    else{
                     // this was not CRLF, so now write '\r' + this char
                      baos.write('\r');
                      baos.write(b);
                      state = 0;
                    }
                    break;
                default: throw new RuntimeException(""assertion failed"");
            }
        }.

In the     

read(byte[] b, int off, int len)  method

of ChunkedInputStream, the number of bytes to read from the underlying 
InputStream is calculated wrongly. In the code this is done by

len = Math.min(len, chunkSize);

This could (and will) cause the server (also Apache) to indeed serve that 
number of bytes (let's say chunkSize), but it may be that we already had a 
number of bytes on the first read. The result is that the input is now NOT 
positioned on the end of a chunk and the rest of the reader fails because it 
cannot find CRLF or a valid chunksize.

Proposed fix (works, tested)
len = Math.min(len, chunkSize-pos);

Bug 1.

In the     

read(byte[] b, int off, int len)  method

of ChunkedInputStream, the number of bytes to read from the underlying 
InputStream is calculated wrongly. In the code this is done by

len = Math.min(len, chunkSize);

This could (and will) cause the server (also Apache) to indeed serve that 
number of bytes (let's say chunkSize), but it may be that we already had a 
number of bytes on the first read. The result is that the input is now NOT 
positioned on the end of a chunk and the rest of the reader fails because it 
cannot find CRLF or a valid chunksize.

Proposed fix (works, tested)
len = Math.min(len, chunkSize-pos);

Bug 2.

In the calculation of the chunkSize (method getChunkSizeFromInputStream) the 
conversion to int is done by calling             

result = Integer.parseInt(dataString, 16);

This is not robust and causes the occasional crash. The fix is simple and in 
fact implements what is done when the chunkSize is commented (see lines in code 
above)

result = Integer.parseInt(dataString.trim(), 16);

Tested and works.



Suggestion:
Same routine, input state machine. Perhaps just being pedantic..change while 
loop to:

        while (state != 2) {
            int b = in.read();
            if (b == -1) throw new IOException(""chunked stream ended 
unexpectedly"");
            switch (state) {
                case 0:
                    if (b == '\r')
                      state = 1;
                    else
                      baos.write(b);
                    break;
                case 1:
                    if (b == '\n')
                      state = 2;
                    else{
                     // this was not CRLF, so now write '\r' + this char
                      baos.write('\r');
                      baos.write(b);
                      state = 0;
                    }
                    break;
                default: throw new RuntimeException(""assertion failed"");
            }
        }.

In the calculation of the chunkSize (method getChunkSizeFromInputStream) the 
conversion to int is done by calling             

result = Integer.parseInt(dataString, 16);

This is not robust and causes the occasional crash. The fix is simple and in 
fact implements what is done when the chunkSize is commented (see lines in code 
above)

result = Integer.parseInt(dataString.trim(), 16);

Tested and works.



Suggestion:
Same routine, input state machine. Perhaps just being pedantic..change while 
loop to:

        while (state != 2) {
            int b = in.read();
            if (b == -1) throw new IOException(""chunked stream ended 
unexpectedly"");
            switch (state) {
                case 0:
                    if (b == '\r')
                      state = 1;
                    else
                      baos.write(b);
                    break;
                case 1:
                    if (b == '\n')
                      state = 2;
                    else{
                     // this was not CRLF, so now write '\r' + this char
                      baos.write('\r');
                      baos.write(b);
                      state = 0;
                    }
                    break;
                default: throw new RuntimeException(""assertion failed"");
            }
        }"
0,refactor consistency checks in BundleDBPersistenceManager into a standalone class that could be re-used for other PMssee subject
0,"Index SplitterIf an index has multiple segments, this tool allows splitting those segments into separate directories.  "
0,"A tokenfilter to decompose compound wordsA tokenfilter to decompose compound words you find in many germanic languages (like German, Swedish, ...) into single tokens.

An example: Donaudampfschiff would be decomposed to Donau, dampf, schiff so that you can find the word even when you only enter ""Schiff"".

I use the hyphenation code from the Apache XML project FOP (http://xmlgraphics.apache.org/fop/) to do the first step of decomposition. Currently I use the FOP jars directly. I only use a handful of classes from the FOP project.

My question now:
Would it be OK to copy this classes over to the Lucene project (renaming the packages of course) or should I stick with the dependency to the FOP jars? The FOP code uses the ASF V2 license as well.

What do you think?"
0,"Wrap IllegalArgumentException from UUID when bad ID passed to Session.getNodeByUUIDHi,

On 6/30/06, David Kennedy <davek@us.ibm.com> wrote:
> When invoking session.getNodeByUUID and passing an invalid ID, an
> IllegalArgumentException is thrown.  Should this be wrapped in an
> ItemNotFoundException or RepositoryException by SessionImpl?

Good point, an ItemNotFoundException would probably be best. Could you
please file a Jira issue for this?

BR,

Jukka Zitting"
0,"TCK vs available property typesThe TCK tests allow configuration of node type / property names to tests specific property types, but they do not take into account that a given repository may not support a specific property type (this is similar to issue JCR-801 about multiple workspace support).

JSR-170 is a bit fuzzy here: it requires all types, but does not require that every type actually exists on a settable node type. In practice, a repository may support reference properties on the builtin nodetypes for version storage, but nowhere else.

Thus, there should be a way to configure the tests so that specific property type tests are left out. Again, there are a few possibilities to do that:

1) reserve a special property name for the case where the test should be skipped (""PROPERTY_TYPE_NOT_SUPPORT""), or

2) add new config flags.

The latter arguably is the cleaner approach, the former avoids introducing new configuration parameters. Thus, I'm leaning to 2). Feedback appreciated.

"
0,"Documentation on SingleClientConnManager(SchemeRegistry schreg) constructor is wrongSeems that the documentation for single-arg constructor SingleClientConnManager(SchemeRegistry schreg) is wrong.

Documentation says that incoming SchemeRegistry parameter can be null:
    schreg - the scheme registry, or null for the default registry

However, the constructor throws an exception in incoming schreg param is null:

    /**
     * Creates a new simple connection manager.
     *
     * @param params    the parameters for this manager
     * @param schreg    the scheme registry, or
     *                  <code>null</code> for the default registry
     */
    public SingleClientConnManager(HttpParams params,
                                   SchemeRegistry schreg) {
        if (schreg == null) {
            throw new IllegalArgumentException
                (""Scheme registry must not be null."");
        }


So this is likely a documentation bug..."
0,Avoid item state reads during Session.logout()Local item states are discarded during Session.logout(). Currently the CachingHierarchyManager is still registered as a item state listener at that time and will cause numerous ItemStateManager.hasItemState() calls. This is unnecessary and just adds overhead to the logout call. In addition it will also contribute to a potential lock contention on the SharedItemStateManager.
0,Namespace handling in AbstractSession should be synchronizedThe AbstractSession base class in o.a.j.commons implicitly assume that the session is never accessed concurrently from more than one thread and thus doesn't synchronize access to the namespace map. This causes problems when the session *is* accessed concurrently. Instead of relying on client code we should enforce thread-safety by explicitly synchronizing potentially unsafe operations on the session instance.
0,"Build should enable unchecked warnings in javacJust have to uncomment this:
{code}
        <!-- for generics in Java 1.5: -->
        <!--<compilerarg line=""-Xlint:unchecked""/>-->
{code}
in common-build.xml.  Test & core are clean, but contrib still has many warnings.  Either we fix contrib with this issue, or, conditionalize this (anyone anty who can do this?) so contrib is off until we can fix it."
0,"make it possible to use searchermanager with distributed statsLUCENE-3555 added explicit stats methods to indexsearcher, but you must
subclass to override this (e.g. populate with distributed stats).

Its also impossible to then do this with SearcherManager.

One idea is make this a factory method (or similar) on IndexSearcher instead,
so you don't need to subclass it to override.

Then you can initialize this in a SearcherWarmer, except there is currently
a lot of hair in what this warming should be. This is a prime example where
Searcher has different meaning from Reader, we should clean this up.

Otherwise, lets make NRT/SearcherManager subclassable in such a way that 
you can return a custom indexsearcher."
1,"Checking of stale connections is brokenHttpConnections that went stale (dropped by server) throw SocketExceptions
instead of silently re-opening themselves, as has been the case with earlier
versions of HttpClient.

I think the problem for this can be found in HttpConnection:

  public boolean closeIfStale() throws IOException {
    if (used && isOpen && isStale()) {
      LOG.debug(""Connection is stale, closing..."");
      close();
      return true;
    }
    return false;
  }

staleness is only checked if used = true, but there is no code in HttpConnection
that sets the used flag. In other words: used is always false and isStale() is
never called."
0,"LayeredSchemeSocketFactory.createLayeredSocket() should have access to HttpParamsWe use a custom implementation of LayeredSchemeSocketFactory that manages a keystore location through HttpParams. That allows us to use different keystores on a per connection basis.

When a proxy is used LayeredSchemeSocketFactory.createLayeredSocket() is invoked which does not have a parameter that passes the HttpParams along. In consequence certificate authentication fails in our implementation. Is there a reason why all other factory methods in the super class have an HttpParams parameter except for LayeredSchemeSocketFactory.createLayeredSocket()?

The downstream bug is here:

369805: certificate authentication with custom keystore fails behind proxy
https://bugs.eclipse.org/bugs/show_bug.cgi?id=369805

Any input would be greatly appreciated."
1,"HttpClient 4.1 ignores request retry handler and stops retrying when a read timeout is followed by a connection refusalI encountered an issue while writing unit tests for the RestBackup(tm) API Client Library, https://github.com/mleonhard/restbackup-java .  HttpClient 4.1 is failing to retry when it encounters a read timeout followed by a connection refusal.  This problem occurs on Windows but not on Linux.  Below is a short program that reproduces the problem.  It performs the expected 5 request attempts on Linux but only 2 on Windows.

My Windows environment is a laptop with Windows 7 Ultimate 64-bit and Oracle Java SE Development Kit Update 21 32-bit.  My Linux environment is Amazon EC2 with Ubuntu 10.04 LTS 32-bit and Oracle Java SE Development Kit Update 21 32-bit.

This is my first bug report to an Apache project.  I'd like to add that I'm a big fan of the Commons libraries and Http Components.

Sincerely,
-Michael

=== RetryBug.java ===

import java.io.IOException;
import java.net.ServerSocket;
import java.util.logging.Logger;

import org.apache.http.client.HttpRequestRetryHandler;
import org.apache.http.client.methods.HttpGet;
import org.apache.http.impl.client.DefaultHttpClient;
import org.apache.http.params.CoreConnectionPNames;
import org.apache.http.protocol.HttpContext;

public class RetryBug {
    private static final Logger _log = Logger.getLogger(RetryBug.class.getName());

    public static void main(String[] args) throws IOException {
        ServerSocket serverSocket = new ServerSocket(0, 1);
        DefaultHttpClient httpClient = new DefaultHttpClient();
        HttpRequestRetryHandler retryHandler = new HttpRequestRetryHandler() {
                public boolean retryRequest(IOException e, int count, HttpContext context) {
                    _log.info(""count="" + count + "" "" + e.toString());
                    return count < 5;
                }
            };
        httpClient.setHttpRequestRetryHandler(retryHandler);
        httpClient.getParams().setIntParameter(CoreConnectionPNames.SO_TIMEOUT, 100);
        try {
            String url = ""http://127.0.0.1:"" + serverSocket.getLocalPort() + ""/"";
            httpClient.execute(new HttpGet(url));
        } finally {
            serverSocket.close();
        }
    }
}


=== Windows 7 ===

C:\RetryBug>md5sum httpcomponents-client-4.1-bin.zip
008ad15560249bcde42cfe34fdb4e858 *httpcomponents-client-4.1-bin.zip

C:\RetryBug>""c:\Program Files (x86)\Java\jdk1.6.0_21\bin\java.exe"" -version
java version ""1.6.0_21""
Java(TM) SE Runtime Environment (build 1.6.0_21-b06)
Java HotSpot(TM) Client VM (build 17.0-b16, mixed mode)

C:\RetryBug>""c:\Program Files (x86)\Java\jdk1.6.0_21\bin\javac.exe"" -cp httpcomponents-client-4.1\lib\commons-codec-1.4.jar;httpcomponents-client-4.1\lib\commons-logging-1.1.1.jar;httpcomponents-client-4.1\lib\httpclient-4.1.jar;httpcomponents-client-4.1\lib\httpcore-4.1.jar RetryBug.java

C:\RetryBug>""c:\Program Files (x86)\Java\jdk1.6.0_21\bin\java.exe"" -cp httpcomponents-client-4.1\lib\commons-codec-1.4.jar;httpcomponents-client-4.1\lib\commons-logging-1.1.1.jar;httpcomponents-client-4.1\lib\httpclient-4.1.jar;httpcomponents-client-4.1\lib\httpcore-4.1.jar;. RetryBug
Mar 9, 2011 9:14:36 PM RetryBug$1 retryRequest
INFO: count=1 java.net.SocketTimeoutException: Read timed out
Mar 9, 2011 9:14:36 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: I/O exception (java.net.SocketTimeoutException) caught when processing request: Read timed out
Mar 9, 2011 9:14:36 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: Retrying request
Exception in thread ""main"" org.apache.http.conn.HttpHostConnectException: Connection to http://127.0.0.1:56361 refused
        at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:158)
        at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:149)
        at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:121)
        at org.apache.http.impl.client.DefaultRequestDirector.tryExecute(DefaultRequestDirector.java:650)
        at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:454)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:820)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:754)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:732)
        at RetryBug.main(RetryBug.java:27)
Caused by: java.net.ConnectException: Connection refused: connect
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:529)
        at org.apache.http.conn.scheme.PlainSocketFactory.connectSocket(PlainSocketFactory.java:120)
        at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:148)
        ... 8 more

C:\RetryBug>


=== Ubuntu 10 ===

$ md5sum httpcomponents-client-4.1-bin.tar.gz
f043c1cc016cb3b720be9fb020bfa755  httpcomponents-client-4.1-bin.tar.gz
$ ~/jdk1.6.0_21/bin/java -version
java version ""1.6.0_21""
Java(TM) SE Runtime Environment (build 1.6.0_21-b06)
Java HotSpot(TM) Client VM (build 17.0-b16, mixed mode, sharing)
$ ~/jdk1.6.0_21/bin/javac -cp httpcomponents-client-4.1/lib/httpclient-cache-4.1.jar:httpcomponents-client-4.1/lib/commons-logging-1.1.1.jar:httpcomponents-client-4.1/lib/httpcore-4.1.jar:httpcomponents-client-4.1/lib/httpclient-4.1.jar:httpcomponents-client-4.1/lib/httpmime-4.1.jar:httpcomponents-client-4.1/lib/commons-codec-1.4.jar RetryBug.java
$ ~/jdk1.6.0_21/bin/java -cp httpcomponents-client-4.1/lib/httpclient-cache-4.1.jar:httpcomponents-client-4.1/lib/commons-logging-1.1.1.jar:httpcomponents-client-4.1/lib/httpcore-4.1.jar:httpcomponents-client-4.1/lib/httpclient-4.1.jar:httpcomponents-client-4.1/lib/httpmime-4.1.jar:httpcomponents-client-4.1/lib/commons-codec-1.4.jar:. RetryBug
Mar 9, 2011 1:09:42 PM RetryBug$1 retryRequest
INFO: count=1 java.net.SocketTimeoutException: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: I/O exception (java.net.SocketTimeoutException) caught when processing request: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: Retrying request
Mar 9, 2011 1:09:42 PM RetryBug$1 retryRequest
INFO: count=2 java.net.SocketTimeoutException: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: I/O exception (java.net.SocketTimeoutException) caught when processing request: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: Retrying request
Mar 9, 2011 1:09:42 PM RetryBug$1 retryRequest
INFO: count=3 java.net.SocketTimeoutException: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: I/O exception (java.net.SocketTimeoutException) caught when processing request: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: Retrying request
Mar 9, 2011 1:09:42 PM RetryBug$1 retryRequest
INFO: count=4 java.net.SocketTimeoutException: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: I/O exception (java.net.SocketTimeoutException) caught when processing request: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: Retrying request
Mar 9, 2011 1:09:51 PM RetryBug$1 retryRequest
INFO: count=5 java.net.SocketTimeoutException: Read timed out
Exception in thread ""main"" java.net.SocketTimeoutException: Read timed out
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:129)
        at org.apache.http.impl.io.AbstractSessionInputBuffer.fillBuffer(AbstractSessionInputBuffer.java:149)
        at org.apache.http.impl.io.SocketInputBuffer.fillBuffer(SocketInputBuffer.java:110)
        at org.apache.http.impl.io.AbstractSessionInputBuffer.readLine(AbstractSessionInputBuffer.java:260)
        at org.apache.http.impl.conn.DefaultResponseParser.parseHead(DefaultResponseParser.java:98)
        at org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:252)
        at org.apache.http.impl.AbstractHttpClientConnection.receiveResponseHeader(AbstractHttpClientConnection.java:281)
        at org.apache.http.impl.conn.DefaultClientConnection.receiveResponseHeader(DefaultClientConnection.java:247)
        at org.apache.http.impl.conn.AbstractClientConnAdapter.receiveResponseHeader(AbstractClientConnAdapter.java:219)
        at org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:298)
        at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:125)
        at org.apache.http.impl.client.DefaultRequestDirector.tryExecute(DefaultRequestDirector.java:622)
        at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:454)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:820)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:754)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:732)
        at RetryBug.main(RetryBug.java:27)
$"
0,"Support for single-workspace repositoriesThere should be a way to configure the test cases in a way such that NodeTest.java can pass although the repository implementation does not support multiple workspaces.

The cleanest approach probably would be to allow javax.jcr.tck.workspacename to stay undefined, and to skip the tests in that case. Alternatives would be a special name indicating lack of support for other workspaces, or an additional config variable.

"
1,WorkspaceInfo.dispose() does not deregister from obs dispatcherJCR-305 introduces an automatic disposing of idle workspaces. this can lead to memory leaks because the observation factory is not deregistered from the delegating one.
1,"JCR2SPI NamespaceRegistryImpl.unregisterNamespace passes prefix to storage when uri is expectedWhen trying to unregister a namespace through SPI, Jackrabbit throws a NamespaceException : <prefix>: is not a registered namespace uri.

javax.jcr.NamespaceRegistry.unregisterNamespace(String prefix) expects the namespace prefix. Though, org.apache.jackrabbit.jcr2spi.NamespaceRegistryImpl.unregisterNamespace(String prefix) calls directly org.apache.jackrabbit.jcr2spi.NamespaceStorage.unregisterNamespace(String uri), which expects the namespace uri.

The namespace registry should first retrieve the uri for the provided prefix."
0,"Javadoc in jackrabbit-jcr-rmi is missing an ending "">"" The javadoc file /jackrabbit-jcr-rmi/src/main/javadoc/apache/rmi/observation/package.html is missing the final "">"" from the ending body tag.
"
0,"Add some ligatures (ff, fi, fl, ft, st) to ISOLatin1AccentFilterISOLatin1AccentFilter removes common diacritics and some ligatures. This patch adds support for additional common ligatures: ff, fi, fl, ft, st."
0,"Implement a backup toolIssue for tracking the progress of the Google Summer of Code project assigned to Nicolas Toper.  The original project requirements are:

""Implement a tool for backing up and restoring content in an Apache Jackrabbit content repository. In addition to the basic content hierarchies, the tool should be able to efficiently manage binary content, node version histories, custom node types, and namespace mappings. Incremental or selective backups would be a nice addition, but not strictly necessary."""
0,"equals and hashCode implementation in org.apache.lucene.search.* packageI would like to talk about the implementation of equals and hashCode method  in org.apache.lucene.search.* package. 

Example One:

org.apache.lucene.search.spans.SpanTermQuery (Super Class)
	<- org.apache.lucene.search.payloads.BoostingTermQuery (Sub Class)

Observation:

* BoostingTermQuery defines equals but inherits hashCode from SpanTermQuery. Definition of equals is a code clone of SpanTermQuery with a change in class name. 

Intention:

I believe the intention of equals redefinition in BoostingTermQuery is not to make the objects of SpanTermQuery and BoostingTermQuery comparable. ie. spanTermQuery.equals(boostingTermQuery) == false && boostingTermQuery.equals(spanTermQuery) == false.


Problem:

With current implementation, the intention might not be respected as a result of symmetric property violation of equals contract i.e.
spanTermQuery.equals(boostingTermQuery) == true (can be) && boostingTermQuery.equals(spanTermQuery) == false. (always)
(Note: Provided their state variables are equal)

Solution:

Change implementation of equals in SpanTermQuery from:

{code:title=SpanTermQuery.java|borderStyle=solid}
  public boolean equals(Object o) {
    if (!(o instanceof SpanTermQuery))
      return false;
    SpanTermQuery other = (SpanTermQuery)o;
    return (this.getBoost() == other.getBoost())
      && this.term.equals(other.term);
  }
{code}

To:
{code:title=SpanTermQuery.java|borderStyle=solid}
  public boolean equals(Object o) {
  	if(o == this) return true;
  	if(o == null || o.getClass() != this.getClass()) return false;
//    if (!(o instanceof SpanTermQuery))
//      return false;
    SpanTermQuery other = (SpanTermQuery)o;
    return (this.getBoost() == other.getBoost())
      && this.term.equals(other.term);
  }
{code}

Advantage:

* BoostingTermQuery.equals and BoostingTermQuery.hashCode is not needed while still preserving the same intention as before.
 
* Any further subclassing that does not add new state variables in the extended classes of SpanTermQuery, does not have to redefine equals and hashCode. 

* Even if a new state variable is added in a subclass, the symmetric property of equals contract will still be respected irrespective of implementation (i.e. instanceof / getClass) of equals and hashCode in the subclasses.


Example Two:


org.apache.lucene.search.CachingWrapperFilter (Super Class)
	<- org.apache.lucene.search.CachingWrapperFilterHelper (Sub Class)

Observation:
Same as Example One.

Problem:
Same as Example one.

Solution:
Change equals in CachingWrapperFilter from:
{code:title=CachingWrapperFilter.java|borderStyle=solid}
  public boolean equals(Object o) {
    if (!(o instanceof CachingWrapperFilter)) return false;
    return this.filter.equals(((CachingWrapperFilter)o).filter);
  }
{code}

To:
{code:title=CachingWrapperFilter.java|borderStyle=solid}
  public boolean equals(Object o) {
//    if (!(o instanceof CachingWrapperFilter)) return false;
    if(o == this) return true;
    if(o == null || o.getClass() != this.getClass()) return false;
    return this.filter.equals(((CachingWrapperFilter)o).filter);
  }
{code}

Advantage:
Same as Example One. Here, CachingWrapperFilterHelper.equals and CachingWrapperFilterHelper.hashCode is not needed.


Example Three:

org.apache.lucene.search.MultiTermQuery (Abstract Parent)
	<- org.apache.lucene.search.FuzzyQuery (Concrete Sub)
	<- org.apache.lucene.search.WildcardQuery (Concrete Sub)

Observation (Not a problem):

* WildcardQuery defines equals but inherits hashCode from MultiTermQuery.
Definition of equals contains just super.equals invocation. 

* FuzzyQuery has few state variables added that are referenced in its equals and hashCode.
Intention:

I believe the intention here is not to make objects of FuzzyQuery and WildcardQuery comparable. ie. fuzzyQuery.equals(wildCardQuery) == false && wildCardQuery.equals(fuzzyQuery) == false.

Proposed Implementation:
How about changing the implementation of equals in MultiTermQuery from:

{code:title=MultiTermQuery.java|borderStyle=solid}
    public boolean equals(Object o) {
      if (this == o) return true;
      if (!(o instanceof MultiTermQuery)) return false;

      final MultiTermQuery multiTermQuery = (MultiTermQuery) o;

      if (!term.equals(multiTermQuery.term)) return false;

      return getBoost() == multiTermQuery.getBoost();
    }
{code}

To:
{code:title=MultiTermQuery.java|borderStyle=solid}
    public boolean equals(Object o) {
      if (this == o) return true;
//      if (!(o instanceof MultiTermQuery)) return false;
      if(o == null || o.getClass() != this.getClass()) return false;

      final MultiTermQuery multiTermQuery = (MultiTermQuery) o;

      if (!term.equals(multiTermQuery.term)) return false;

      return getBoost() == multiTermQuery.getBoost();
    }
{code}

Advantage:

Same as above. Here, WildcardQuery.equals is not needed as it does not define any new state. (FuzzyQuery.equals is still needed because FuzzyQuery defines new state.) 
"
0,"Catch exceptions in Threads created by JUnit tasksOn hudson we had several assertions failed in TestRAMDirectory, that were never caught by the error reportier in JUnit (as the test itsself did not fail). This patch adds a handler for uncaught exceptions to LuceneTestCase(J4) that let the test fail in tearDown()."
0,"warning: unmappable character for encoding UTF8There are a few non-ASCII characters in the Jackrabbit source files that cause warnings at least in my environment. It seems that all the warnings are caused by ""smart quote"" characters.

The exact warnings are: 

/home/hukka/workspace/Jackrabbit/src/java/org/apache/jackrabbit/core/NodeImpl.java:3068: warning: unmappable character for encoding UTF8
            // &#65533;newer&#65533; than N and therefore N should be updated to reflect N'.
               ^
/home/hukka/workspace/Jackrabbit/src/java/org/apache/jackrabbit/core/NodeImpl.java:3068: warning: unmappable character for encoding UTF8
            // &#65533;newer&#65533; than N and therefore N should be updated to reflect N'.
                     ^
/home/hukka/workspace/Jackrabbit/src/java/org/apache/jackrabbit/core/NodeImpl.java:3074: warning: unmappable character for encoding UTF8
            // N' is &#65533;older&#65533; or the &#65533;same age&#65533; as N and therefore N should be left alone.
                     ^
/home/hukka/workspace/Jackrabbit/src/java/org/apache/jackrabbit/core/NodeImpl.java:3074: warning: unmappable character for encoding UTF8
            // N' is &#65533;older&#65533; or the &#65533;same age&#65533; as N and therefore N should be left alone.
                           ^
/home/hukka/workspace/Jackrabbit/src/java/org/apache/jackrabbit/core/NodeImpl.java:3074: warning: unmappable character for encoding UTF8
            // N' is &#65533;older&#65533; or the &#65533;same age&#65533; as N and therefore N should be left alone.
                                    ^
/home/hukka/workspace/Jackrabbit/src/java/org/apache/jackrabbit/core/NodeImpl.java:3074: warning: unmappable character for encoding UTF8
            // N' is &#65533;older&#65533; or the &#65533;same age&#65533; as N and therefore N should be left alone.
                                             ^
/home/hukka/workspace/Jackrabbit/src/java/org/apache/jackrabbit/core/NodeImpl.java:3376: warning: unmappable character for encoding UTF8
        // 2. N&#65533;s jcr:baseVersion property will be changed to point to V.
               ^
/home/hukka/workspace/Jackrabbit/src/java/org/apache/jackrabbit/core/NodeImpl.java:3382: warning: unmappable character for encoding UTF8
        // 3. N&#65533;s jcr:isCheckedOut property is set to false.

/home/hukka/workspace/Jackrabbit/src/test/org/apache/jackrabbit/test/api/version/CheckinTest.java:80: warning: unmappable character for encoding UTF8
        assertEquals(""The versionable checked-out node&#65533;s jcr:predecessors property is copied to the new version on checkin."", Arrays.asList(nPredecessorsValue), Arrays.asList(vPredecessorsValue));
                                                      ^
/home/hukka/workspace/Jackrabbit/src/test/org/apache/jackrabbit/init/NodeTestData.java:95: warning: unmappable character for encoding UTF8
        writer.write(""Hello w&#65533;rld."");
"
0,"Optimize PhraseQueryLooking the scorers for PhraseQuery, I think there are some speedups
we could do:

  * The AND part of the scorer (which advances to the next doc that
    has all the terms), in PhraseScorer.doNext, should do the same
    optimizing as BooleanQuery's ConjunctionScorer, ie sort terms from
    rarest to most frequent.  I don't think it should use a linked
    list/firstToLast() that it does today.

  * We do way too much work now when .score() is not called, because
    we go and find all occurrences of the phrase in the doc, whereas
    we should stop only after finding the first and then go and count
    the rest if .score() is called.

  * For the exact case, I think we can use two int arrays to find the
    matches.  The first array holds the count of how many times a term
    in the phrase ""matched"" a phrase starting at that position.  When
    that count == the number of terms in the phrase, it's a match.
    The 2nd is a ""gen"" array (holds docID when that count was last
    touched), to avoid clearing.  Ie when incrementing the count, if
    the docID != gen, we reset count to 0.  I think this'd be faster
    than the PQ we now use.  Downside of this is if you have immense
    docs (position gets very large) we'd need 2 immense arrays.

It'd be great to do LUCENE-1252 along with this, ie factor
PhraseScorer into two AND'd sub-scorers (LUCENE-1252 is open for
this).  The first one should be ConjunctionScorer, and the 2nd one
checks the positions (ie, either the exact or sloppy scorers).  This
would mean if the PhraseQuery is AND'd w/ other clauses (or, a filter
is applied) we would save CPU by not checking the positions for a doc
unless all other AND'd clauses accepted the doc.
"
1,"NPE when copying nodes with Workspace.copy()I get a NullpointerException when using Workspace.copy():

java.lang.NullPointerException
at org.apache.jackrabbit.core.BatchedItemOperations.copyNodeState(BatchedItemOperations.java:1834)
at org.apache.jackrabbit.core.BatchedItemOperations.copyNodeState(BatchedItemOperations.java:1806)
at org.apache.jackrabbit.core.BatchedItemOperations.copyNodeState(BatchedItemOperations.java:1806)
at org.apache.jackrabbit.core.BatchedItemOperations.copy(BatchedItemOperations.java:423)
at org.apache.jackrabbit.core.WorkspaceImpl.internalCopy(WorkspaceImpl.java:444)
at org.apache.jackrabbit.core.WorkspaceImpl.copy(WorkspaceImpl.java:666)
at xxx.MyClass.myMeth(MyClass.java)

It seems that it happens not all the time. The error occurs since we use Jackrabbit 1.6.0. We do not get the error with previous versions. It seems that we only get the error when trying to copy nodes that were created with Jackrabbit 1.4 and copied with Jackrabbit 1.6."
1,"JCR-2523 break the transaction handling in container managed environmentduring the cleanup (returning to the pool) of an jca managed connection,  an new internal session is created in the object JCAManagedConnection in the method cleanup, this is supposed to fix JCR-2523, The sideeffect is, that the XA-Resource (variable-xaResource) in JCAManagedConnection is not anymore the same XASessionImpl Object like the session Object. Subsequent calls on this connection, lead that the internal session variable is not anymore informed about the current transaction context. (XAItemStateManager, variables tx and txLog are null), because only the xaResource is informed about the new transaction context. Result is that the complete transaction handling does not work anymore.
I attached a sample project which shows this behaviour.
"
0,Typo in message logged upon startup when repository is already in useAs per subject
1,"Missing synchronization in InternalVersionHistoryImplThe InternalVersionHistoryImpl objects can be accessed (and modified) concurrently by multiple sessions, which can in some rare cases result in corruption in the internal cache map data structures. Access to these cache maps should be properly synchronized."
0,"when tests fail, sometimes the testmethod in 'reproduce with' is nullan example is the recent fail: https://builds.apache.org/job/Lucene-3.x/680/

it would be better to not populate -Dtestmethod with anything here..."
1,"Don't commit an empty segments_N when IW is opened with create=trueIf IW is opened with create=true, it forcefully commits an empty
segments_N.  But really it should not: if autoCommit is false, nothing
should be committed until commit or close is explicitly called.

Spinoff from http://www.nabble.com/no-segments*-file-found:-files:-Error-on-opening-index-td23219520.html
"
1,"FSDirectory.openFile(String) causes ClassCastExceptionWhen you call FSDirectory.openFile(String) you get a ClassCastException since FSIndexInput is not an org.apache.lucene.store.InputStream

The workaround is to reimplement using openInput(String). I personally don't need this to be fixed but wanted to document it here in case anyone else runs into this for any reason.

The reason I'm calling this is that I have a requirement on my project to create read only indexes and name the index segments consistently from one build to the next. So, after creating and optimizing the index, I rename the files and rewrite the segments file. It would be nice if I had an API that would allow me to say ""I only want one segment and I want its name to be 'foo'"". For instance IndexWriter.optimize(String segmentName)"
0,PersistenceManager sanity checkLibrary that provides a framework for testing the repository consistency and repairing it if necessary.
0,"single norm file still uses up descriptorsThe new index file format with a single .nrm file for all norms does not decrease file descriptor usage.
The .nrm file is opened once for each field with norms in the index segment."
0,Jcr2Spi: Unneeded call to getPropertyInfo upon creating a new NodeStatecreating a new NodeState may result in additional (but unneeded) calls to getPropertyInfo if a jcr:uuid or jcr:mixinTypes property is present. This can be avoided since the corresponding property values are already present.
0,"Highlighter should try and use maxDocCharsToAnalyze in WeightedSpanTermExtractor when adding a new field to MemoryIndex as well as when using CachingTokenStreamhuge documents can be drastically slower than need be because the entire field is added to the memory index
this cost can be greatly reduced in many cases if we try and respect maxDocCharsToAnalyze

things can be improved even further by respecting this setting with CachingTokenStream

"
1,"IndexReader.clone can leave files openI hit this in working on LUCENE-1516.

When not using compound file format, if you clone an IndexReader, then close the original, then close the clone, the stored fields files (_X.fdt, _X.fdx) remain incorrectly open.

I have a test showing it; fix is trivial.  Will post patch & commit shortly."
1,"trunk TestRollingUpdates.testRollingUpdates seed failuretrunk r1152892
reproducable: always

{code}
junit-sequential:
    [junit] Testsuite: org.apache.lucene.index.TestRollingUpdates
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 1.168 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestRollingUpdates -Dtestmethod=testRollingUpdates -Dtests.seed=-5322802004404580273:-4001225075726350391
    [junit] WARNING: test method: 'testRollingUpdates' left thread running: merge thread: _c(4.0):cv3/2 _h(4.0):cv3 into _k
    [junit] RESOURCE LEAK: test method: 'testRollingUpdates' left 1 thread(s) running
    [junit] NOTE: test params are: codec=RandomCodecProvider: {docid=Standard, body=SimpleText, title=MockSep, titleTokenized=Pulsing(freqCutoff=20), date=MockFixedIntBlock(blockSize=1474)}, locale=lv_LV, timezone=Pacific/Fiji
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestRollingUpdates]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_26 (64-bit)/cpus=8,threads=1,free=128782656,total=158400512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testRollingUpdates(org.apache.lucene.index.TestRollingUpdates):   FAILED
    [junit] expected:<20> but was:<21>
    [junit] junit.framework.AssertionFailedError: expected:<20> but was:<21>
    [junit]     at org.apache.lucene.index.TestRollingUpdates.testRollingUpdates(TestRollingUpdates.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1522)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1427)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.TestRollingUpdates FAILED

{code}"
0,"New JcrUtils utility methodsI'd like to add the following new utility methods to JcrUtils:

    readFile(Node): returns an InputStream for reading file contents
    readFile(Node, OutputStream): writes file contents to the given stream
    getLastModified(Node): returns the jcr:lastModified value
    setLastModified(Node, Calendar): sets the jcr:lastModified value
    getPropertyType(String): like PropertyType.valueFromName(String), but case-insensitive
"
0,"variables should be accessed through gettersSome attention should be placed on classes who shared their variables directly (as opposed to through a getter). This is sometimes OK for subclasses, but rarely good for other classes that use the objects. There's a small number of classes that have non-private variables, especially in the impl.conn & impl.conn.tsccm packages.

See HTTPCLIENT-745 ."
0,"contrib intelligent Analyzer for ChineseI wrote a Analyzer for apache lucene for analyzing sentences in Chinese language. it's called ""imdict-chinese-analyzer"", the project on google code is here: http://code.google.com/p/imdict-chinese-analyzer/

In Chinese, """"(I am Chinese), should be tokenized as """"(I)   """"(am)   """"(Chinese), not """" """" """". So the analyzer must handle each sentence properly, or there will be mis-understandings everywhere in the index constructed by Lucene, and the accuracy of the search engine will be affected seriously!

Although there are two analyzer packages in apache repository which can handle Chinese: ChineseAnalyzer and CJKAnalyzer, they take each character or every two adjoining characters as a single word, this is obviously not true in reality, also this strategy will increase the index size and hurt the performance baddly.

The algorithm of imdict-chinese-analyzer is based on Hidden Markov Model (HMM), so it can tokenize chinese sentence in a really intelligent way. Tokenizaion accuracy of this model is above 90% according to the paper ""HHMM-based Chinese Lexical analyzer ICTCLAL"" while other analyzer's is about 60%.

As imdict-chinese-analyzer is a really fast and intelligent. I want to contribute it to the apache lucene repository."
1,"Overwriting a reference property with different type corrupts rep- create node n1
- create node n2
- n2.setProperty(""prop"", n1)
- save()
- n2.setProperty(""prop"", ""hello, world."")
- save()
- n1.remove()
- save() --> exception

see also ReferencesTest case

btw: removing the property or overwriting with a different reference works."
0,"Typos in MultiThreadedHttpConnectionManagerI've done a review of the MultiThreadedHttpConnectionManager class in 3.0-beta1,
especially focussing on the documentation. In general, it could use a lot of
improvement, IMHO.

This bug report only deals with some typos I found in the class, and some
minimal style improvements that are compatible with the other classes.

I will attach a proposed patch."
0,"Fold AuthSSLProtocolSocketFactory into HttpClient properInclude the functionality of the AuthSSLProtocolSocketFactory class into the
main distribution of HttpClient

http://svn.apache.org/repos/asf/jakarta/commons/proper/httpclient/trunk/src/contrib/org/apache/commons/httpclient/contrib/ssl/"
0,"add svn ignores for eclipse artifactsBe nice to ignore the files eclipse puts into the project root as we do the .idea file for intellij.

The two files are

.project
.classpath

I'm gonna lie and say there's a patch available for this because an svn diff patch with propery changes can't be applied with patch anyway."
0,"Standardize on a common mocking framework (either EasyMock or Mockito)We are currently using EasyMock in the caching module and Mockito in the main module. While Mockito appears to have a somewhat nicer API, the sheer number of test cases based on EasyMock in the caching module makes it much simpler to replace Mockito with EasyMock than the other way around."
0,"FieldsInfo uses deprecated APIThe class FieldsInfo.java uses deprecated API in method ""public void add(Document doc)""
I rused the replacement and created the patch -> see attachment"
1,"After RepositoryImpl instance has been created and shut down, some classes cannot be unloadedI've built a simple web-application, which contains one servlet loaded at start-up. In its init() method an instance of RepositoryImpl() is created, in its destroy() method this instance is stopped (using shutdown()).
From the servlet code, only classes in jackrabbit-core, JCR API and Servlet API are referenced.
jackrabbit-core version is 1.4.5, and jackrabbit-jcr-commons version is 1.4.2. Other jackrabbit libs are all of 1.4 version.

Even if servlet's doGet() method never gets called, when the web-application is redeployed, all its classes still hang in memory, which produces a memory leak.

init() method is 

    public void init() throws ServletException {
        super.init();
        try {
            RepositoryConfig repoConfig = RepositoryConfig.create(getClass().getResourceAsStream(""repository.xml""), ""."");
            repo = RepositoryImpl.create(repoConfig);
        } catch (Exception e) {
            throw new ServletException(e);
        }
    }

while destroy() method is

    public void destroy() {
        repo.shutdown();
        super.destroy();
    }

Even when I applied patches from JCR-1636 and added TransientFileFactory.shutdown() call to destroy() method, nothing has changed.
Tested this in Jetty 6.1.9 and Tomcat 6.0.14."
1,"Missing support for lock timeout and ownerHint in jcr-servertrying to set the lock timeout when creating a lock seems not to work over the davex transport. the timeout is always 2147483.

this was my test code:

import javax.jcr.*;
import javax.jcr.lock.*;

import org.apache.jackrabbit.jcr2spi.RepositoryImpl;
import org.apache.jackrabbit.jcr2spi.config.RepositoryConfig;


String url = ""http://localhost:8080/server/"";
String workspace = ""tests"";

RepositoryConfig config = new RepositoryConfigImplTest(repoUrl);
Repository repo = RepositoryImpl.create(config);

Credentials sc = new SimpleCredentials(""admin"",""admin"".toCharArray());
Session s = repo.login(sc,workspace);

Node t;
if (s.getRootNode().hasNode(""test"")) {
    t = s.getRootNode().getNode(""test"");
} else {
    t = s.getRootNode().addNode(""test"", ""nt:unstructured"");
}
t.addMixin(""mix:lockable"");
s.save();
LockManager m = s.getWorkspace().getLockManager();
Lock l = m.lock(t.getPath(), false, true, 10, ""me"");
System.out.println(l.getSecondsRemaining());

and the output is 2147483


the relevant communication fragment is below, i attach the full trace in case i miss something.

LOCK /server/tests/jcr%3aroot/test HTTP/1.1
Timeout: Second-10
Depth: 0
Link: <urn:uuid0c740bb9-042a-4ef2-b019-1a6c52784c29>; rel=""http://www.day.com/jcr/webdav/1.0/session-id""
Authorization: Basic YWRtaW46YWRtaW4=
User-Agent: Jakarta Commons-HttpClient/3.0
Host: localhost:8080
Content-Length: 254
Content-Type: text/xml; charset=UTF-8

<?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?><D:lockinfo xmlns:D=""DAV:""><D:lockscope><dcr:exclusive-session-scoped xmlns:dcr=""http://www.day.com/jcr/webdav/1.0""/></D:lockscope><D:locktype><D:write/></D:locktype><D:owner>me</D:owner></D:lockinfo>

HTTP/1.1 200 OK
Content-Type: text/xml; charset=utf-8
Content-Length: 450
Lock-Token: <aa724c28-3c24-41e8-a3b4-9fc129adf732>
Server: Jetty(6.1.x)

<?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?><D:prop xmlns:D=""DAV:""><D:lockdiscovery><D:activelock><D:lockscope><dcr:exclusive-session-scoped xmlns:dcr=""http://www.day.com/jcr/webdav/1.0""/></D:lockscope><D:locktype><D:write/></D:locktype><D:depth>0</D:depth><D:timeout>Second-2147483</D:timeout><D:owner>admin</D:owner><D:locktoken><D:href>aa724c28-3c24-41e8-a3b4-9fc129adf732</D:href></D:locktoken></D:activelock></D:lockdiscovery></D:prop>



by the way: if i do not explicitly logout before the program exits, the lock is also not released even though it is session based. should the session not trigger a logout on destruction?"
0,"indexwriter creates unwanted termvector infoI noticed today that when I build a big index in Solr, I get some unwanted termvector info, even though I didn't request any.
This does not happen on 3x - not sure when it started happening on trunk."
0,"TSCCM code cleanupThe ThreadSafeClientConnectionManager, or rather it's ConnPoolByRoute, needs plenty of cleanup.
- use long + TimeUnit for timeout intervals (Java 5 style)
- compute timeout end date once instead of remaining interval
- review which methods should acquire the pool lock,
  and which should expect the caller to have done that
- use factory methods to instantiate some of the helper objects
"
0,"TRStringDistance uses way too much memory (with patch)The implementation of TRStringDistance is based on version 2.1 of org.apache.commons.lang.StringUtils#getLevenshteinDistance(String, String), which uses an un-optimized implementation of the Levenshtein Distance algorithm (it uses way too much memory). Please see Bug 38911 (http://issues.apache.org/bugzilla/show_bug.cgi?id=38911) for more information.

The commons-lang implementation has been heavily optimized as of version 2.2 (3x speed-up). I have reported the new implementation to TRStringDistance."
0,"need members of MultipartRequestEntity to be ""protected"" instead of ""private"" to make it extendable for multipart/relatedAs explained in the mailing-list[1], I'd like to have some of 
MultipartRequestEntity move from ""private"" visibility to ""protected"" visibility,
to be able to extend as MultipartRelatedRequestEntity. Namely, the attribute
""parts"" and the method ""getMultipartBoundary"" would be needed.

Thank you.

[1]
http://mail-archives.apache.org/mod_mbox/jakarta-httpclient-dev/200510.mbox/%3c87irw18ndm.fsf@meuh.mnc.ch%3e"
0,"optimize spanfirstquery, spanpositionrangequerySpanFirstQuery and SpanPositionRangeQuery (SpanFirst is just a special case of this), are currently inefficient.

Take this worst case example: SpanFirstQuery(""the"").
Currently the code reads all the positions for the term ""the"".

But when enumerating spans, once we have passed the allowable range we should move on to the next document (skipTo)
 "
0,"Add new top-level projects to the building documentationThe current building.xml file only mentions jackrabbit and contrib as being top level Jackrabbit projects. With the push towards 1.0, there are now several additional projects at the same level as jackrabbit and contrib. The building.xml page should be updated to mention and link to them. It should also provide a link into the subversion repository that is labeled as something like ""Current Jackrabbit project list""; even though the link location is the same as the link to the repository, the label will help readers know that's where to look for the most up-to-date list of Jackrabbit projects."
0,"improve performance of contrib/TestCompoundWordTokenFiltercontrib/analyzers/compound has some tests that use a hyphenation grammar file.

The tests are currently for german, and they actually are nice, they show how the combination of the hyphenation rules and dictionary work in tandem.
The issue is that the german grammar file is not apache licensed: http://offo.sourceforge.net/hyphenation/licenses.html
So the test must download the entire offo zip file from sourceforge to execute.

I happen to think the test is a great example of how this thing works (with a language where it matters), but we could consider using a different grammar file, for a language that is apache licensed.
This way it could be included in the source with the test and would be more practical.
"
0,"isValid should be invoked after analyze rather than before it so it can validate the output of analyzeThe Synonym map has a protected method String analyze(String word) designed for custom stemming.

However, before analyze is invoked on a word, boolean isValid(String str) is used to validate the word - which causes the program to discard words that maybe useable by the custom analyze method. 

I think that isValid should be invoked after analyze rather than before it so it can validate the output of analyze and allow implemters to decide what is valid for the overridden analyze method. (In fact, if you look at code snippet below, isValid should really go after the empty string check)

This is a two line change in org.apache.lucene.index.memory.SynonymMap

      /*
       * Part B: ignore phrases (with spaces and hyphens) and
       * non-alphabetic words, and let user customize word (e.g. do some
       * stemming)
       */
      if (!isValid(word)) continue; // ignore
      word = analyze(word);
      if (word == null || word.length() == 0) continue; // ignore"
0,Sessions are not logged out in case of exceptionsSome test cases do not logout sessions if an exception occurs.
1,"ConcurrentScheduleManager.addMyself() has wrong intedThis method has the wrong index for the 'size' variable, I think it should b allInstances.size.

{code:java}
private void addMyself() {
    synchronized(allInstances) {
      final int size=0;
      int upto = 0;
      for(int i=0;i<size;i++) {
        final ConcurrentMergeScheduler other = (ConcurrentMergeScheduler) allInstances.get(i);
        if (!(other.closed && 0 == other.mergeThreadCount()))
          // Keep this one for now: it still has threads or
          // may spawn new threads
          allInstances.set(upto++, other);
      }
      allInstances.subList(upto, allInstances.size()).clear();
      allInstances.add(this);
    }
  }
{code}"
0,Support SortedSource in MultiDocValuesMultiDocValues doesn't support Sorted variant ie. SortedSource but throws UnsupportedOperationException. This forces users to work per segment. For consistency we should support sortedsource also if we wrap the DocValues in MDV.
0,"FastVectorHighlighter: add a FragmentBuilder to return entire field contentsIn Highlightrer, there is a Nullfragmenter. There is a requirement its counterpart in FastVectorhighlighter."
1,[PATCH] DbDataStore: Make sure streams are closedStream isn't closed on end of use. this patch fixes it.
1,"FrenchAnalyzer's tokenStream method does not honour the contract of AnalyzerIn {{Analyzer}} :
{code}
/** Creates a TokenStream which tokenizes all the text in the provided
    Reader.  Default implementation forwards to tokenStream(Reader) for 
    compatibility with older version.  Override to allow Analyzer to choose 
    strategy based on document and/or field.  Must be able to handle null
    field name for backward compatibility. */
  public abstract TokenStream tokenStream(String fieldName, Reader reader);
{code}


and in {{FrenchAnalyzer}}

{code}
public final TokenStream tokenStream(String fieldName, Reader reader) {

    if (fieldName == null) throw new IllegalArgumentException(""fieldName must not be null"");
    if (reader == null) throw new IllegalArgumentException(""reader must not be null"");
{code}"
0,"Add a serializing content handlerBoth JCR-1310 and JCR-1343 need XML serialization functionality and we've also previously (JCR-367, JCR-1086) implemented something similar.

It would be good to centralize such code, and so I'd like to use the already referenced code from Cocoon [1] as the basis for a SerializingContentHandler class in jackrabbit-jcr-commons.

[1] https://svn.apache.org/repos/asf/cocoon/trunk/core/cocoon-pipeline/cocoon-pipeline-impl/src/main/java/org/apache/cocoon/serialization/AbstractTextSerializer.java"
0,"improved compound file handlingCurrently CompoundFileReader could use some improvements, i see the following problems
* its CSIndexInput extends bufferedindexinput, which is stupid for directories like mmap.
* it seeks on every readInternal
* its not possible for a directory to override or improve the handling of compound files.

for example: it seems if you were impl'ing this thing from scratch, you would just wrap the II directly (not extend BufferedIndexInput,
and add compound file offset X to seek() calls, and override length(). But of course, then you couldnt throw read past EOF always when you should,
as a user could read into the next file and be left unaware.

however, some directories could handle this better. for example MMapDirectory could return an indexinput that simply mmaps the 'slice' of the CFS file.
its underlying bytebuffer etc naturally does bounds checks already etc, so it wouldnt need to be buffered, not even needing to add any offsets to seek(),
as its position would just work.

So I think we should try to refactor this so that a Directory can customize how compound files are handled, the simplest 
case for the least code change would be to add this to Directory.java:

{code}
  public Directory openCompoundInput(String filename) {
    return new CompoundFileReader(this, filename);
  }
{code}

Because most code depends upon the fact compound files are implemented as a Directory and transparent. at least then a subclass could override...
but the 'recursion' is a little ugly... we could still label it expert+internal+experimental or whatever.
"
0,"DirectoryIndexReader finalize() holding TermInfosReader longer than necessaryDirectoryIndexReader has a finalize method, which causes the JDK to keep a reference to the object until it can be finalized.  SegmentReader and MultiSegmentReader are subclasses that contain references to, potentially, hundreds of megabytes of cached data in a TermInfosReader.

Some options would be removing finalize() from DirectoryIndexReader (it releases a write lock at the moment) or possibly nulling out references in various close() and doClose() methods throughout the class hierarchy so that the finalizable object doesn't references the Term arrays.

Original mailing list message:
http://mail-archives.apache.org/mod_mbox/lucene-java-user/200906.mbox/%3C7A5CB4A7BBCE0C40B81C5145C326C31301A62971@NUMEVP06.na.imtn.com%3E"
0,"introduce HttpRoutePlanner interfaceDefine an interface to determine a route for a given target host.
Create default implementation replacing DefaultHttpClient.determineRoute(...);
Implementations will need access to params and/or request.

The interface fits into HttpConn, but DHC.dR(...) uses client parameters.
Either move parameters to HttpConn, or keep default implementation in HttpClient.

Alternative implementations could evaluate Java system properties related to proxy settings.

"
0,"Wrong trailing index calculation in PatternReplaceCharFilterReimplementation of PatternReplaceCharFilter to pass randomized tests (used to throw exceptions previously). Simplified code, dropped boundary characters, full input buffered for pattern matching."
0,Include to jackrabbit-jcr-rmi and jackrabbit-jcr-servlet in main trunkJackrabbit 2.0 should include the 2.0 version of the RMI component and the related jcr-servlet updates.
0,[PATCH] Javadoc correction for Scorer.java 
0,"Make StopFilter.enablePositionIncrements explicitI think the default for this should be true, ie, do not lose
information when filtering (preserve the positions of the original
tokens).

But, we can't change this without breaking back-compat.

So, as workaround, we should make the parameter explicit so one must
decide up front.
"
0,"repository.xml DTD doesn't allow <DataStore> elementThe repository.xml DTD at http://jackrabbit.apache.org/dtd/repository-1.4.dtd conflicts with the instructions in the wiki page at http://wiki.apache.org/jackrabbit/DataStore

Adding the <DataStore> element as specified in the wiki page violates the DTD.

"
0,"re-sync client with changes in core alpha6 snapshotThere have been API changes in core since it's alpha5 release.
Client needs to be adapted so it's alpha2 (snapshot) builds and runs against the current core API.
"
1,"Bugs in org.apache.lucene.index.TermVectorsReader.clone()A couple of things:

- The implementation can return null which is not allowed.  It should throw a CloneNotSupportedException if that's the case.

- Part of the code reads:

    TermVectorsReader clone = null;
    try {
      clone = (TermVectorsReader) super.clone();
    } catch (CloneNotSupportedException e) {}

    clone.tvx = (IndexInput) tvx.clone();

If a CloneNotSupportedException is caught then ""clone"" will be null and the assignment to clone.tvx will fail with a null pointer exception."
0,"typos on FAQI found out the following typos on the FAQ (http://lucene.sourceforge.net/cgi-
bin/faq/faqmanager.cgi) of lucene:
in 8. Will Lucene work with my Java application ?
- felxible
- applciations"
0,"Contributing a High-performance single-document main memory Apache Lucene fulltext search index.Here is my contribution: a High-performance single-document main memory Apache Lucene fulltext 
search index. I'll try to attach the files, hoping for comments on how to proceed with this..."
1,"Deadlock caused by versioning operations within transactionDeadlock occurs, while running a very simple test, which is just trying
to checkout/checkin node within transaction concurrently from 2 threads.

Find enclosed thread dump, log and simple Java program.
I'm using UserTransaction implementation from jackrabbit test suite.

Regards
Przemo Pakulski
www.cognifide.com


Full thread dump Java HotSpot(TM) Client VM (1.4.2_08-b03 mixed mode):

""Thread-5"" prio=5 tid=0x03054c48 nid=0x180c in Object.wait() [355f000..355fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x1148ef20> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
       at java.lang.Object.wait(Object.java:429)
       at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock.acquire(Unknown Source)
       - locked <0x1148ef20> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.acquireWriteLock(SharedItemStateManager.java:1137)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.access$200(SharedItemStateManager.java:110)
       at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:456)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:651)
       at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:150)
       at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:128)
       - locked <0x11565ac8> (a org.apache.jackrabbit.core.TransactionContext)
       at org.apache.jackrabbit.core.XASessionImpl.prepare(XASessionImpl.java:300)
       at com.oyster.mom.contentserver.jcr.transaction.JackrabbitUserTransaction.commit(JackrabbitUserTransaction.java:102)
       at com.oyster.mom.contentserver.jcr.transaction.JrTestDeadlock.run(JrTestDeadlock.java:97)

""Thread-4"" prio=5 tid=0x0303b348 nid=0x9d0 in Object.wait() [351f000..351fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x1148ef20> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
       at java.lang.Object.wait(Object.java:429)
       at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock.acquire(Unknown Source)
       - locked <0x1148ef20> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.acquireWriteLock(SharedItemStateManager.java:1137)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.access$200(SharedItemStateManager.java:110)
       at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:456)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:651)
       at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:150)
       at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:128)
       - locked <0x1156f558> (a org.apache.jackrabbit.core.TransactionContext)
       at org.apache.jackrabbit.core.XASessionImpl.prepare(XASessionImpl.java:300)
       at com.oyster.mom.contentserver.jcr.transaction.JackrabbitUserTransaction.commit(JackrabbitUserTransaction.java:102)
       at com.oyster.mom.contentserver.jcr.transaction.JrTestDeadlock.run(JrTestDeadlock.java:97)

""IndexMerger"" daemon prio=5 tid=0x030388b8 nid=0x1858 in Object.wait() [34df000..34dfd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x114fd280> (a org.apache.commons.collections.buffer.BlockingBuffer)
       at java.lang.Object.wait(Object.java:429)
       at org.apache.commons.collections.buffer.BlockingBuffer.remove(BlockingBuffer.java:107)
       - locked <0x114fd280> (a org.apache.commons.collections.buffer.BlockingBuffer)
       at org.apache.jackrabbit.core.query.lucene.IndexMerger.run(IndexMerger.java:235)

""Thread-2"" daemon prio=5 tid=0x0303a230 nid=0xe4c in Object.wait() [349f000..349fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x114fd2e0> (a java.util.TaskQueue)
       at java.util.TimerThread.mainLoop(Timer.java:429)
       - locked <0x114fd2e0> (a java.util.TaskQueue)
       at java.util.TimerThread.run(Timer.java:382)

""Thread-1"" daemon prio=5 tid=0x0301b7a0 nid=0x1a00 in Object.wait() [345f000..345fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x114f9058> (a java.util.TaskQueue)
       at java.lang.Object.wait(Object.java:429)
       at java.util.TimerThread.mainLoop(Timer.java:403)
       - locked <0x114f9058> (a java.util.TaskQueue)
       at java.util.TimerThread.run(Timer.java:382)

""ObservationManager"" daemon prio=5 tid=0x02ef6c50 nid=0x10d8 in Object.wait() [341f000..341fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x114f38e0> (a org.apache.commons.collections.buffer.BlockingBuffer)
       at java.lang.Object.wait(Object.java:429)
       at org.apache.commons.collections.buffer.BlockingBuffer.remove(BlockingBuffer.java:107)
       - locked <0x114f38e0> (a org.apache.commons.collections.buffer.BlockingBuffer)
       at org.apache.jackrabbit.core.observation.ObservationManagerFactory.run(ObservationManagerFactory.java:155)
       at java.lang.Thread.run(Thread.java:534)

""Signal Dispatcher"" daemon prio=10 tid=0x00a05590 nid=0x1914 waiting on condition [0..0]

""Finalizer"" daemon prio=9 tid=0x00a027f8 nid=0x17a4 in Object.wait() [2c9f000..2c9fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x113db118> (a java.lang.ref.ReferenceQueue$Lock)
       at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:111)
       - locked <0x113db118> (a java.lang.ref.ReferenceQueue$Lock)
       at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:127)
       at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)

""Reference Handler"" daemon prio=10 tid=0x00a01478 nid=0x16d4 in Object.wait() [2c5f000..2c5fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x113db180> (a java.lang.ref.Reference$Lock)
       at java.lang.Object.wait(Object.java:429)
       at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:115)
       - locked <0x113db180> (a java.lang.ref.Reference$Lock)

""main"" prio=5 tid=0x0003e6f0 nid=0x1470 in Object.wait() [7f000..7fc38]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x11524f10> (a com.oyster.mom.contentserver.jcr.transaction.JrTestDeadlock)
       at java.lang.Thread.join(Thread.java:1001)
       - locked <0x11524f10> (a com.oyster.mom.contentserver.jcr.transaction.JrTestDeadlock)
       at java.lang.Thread.join(Thread.java:1054)
       at com.oyster.mom.contentserver.jcr.transaction.JrTestDeadlock.main(JrTestDeadlock.java:33)

""VM Thread"" prio=5 tid=0x00a42730 nid=0x17d0 runnable

""VM Periodic Task Thread"" prio=10 tid=0x00a45540 nid=0x1928 waiting on condition
""Suspend Checker Thread"" prio=10 tid=0x00a04af8 nid=0x17ac runnable

import javax.jcr.Node;
import javax.jcr.RepositoryException;
import javax.jcr.Session;
import javax.jcr.SimpleCredentials;

import org.apache.jackrabbit.core.RepositoryImpl;
import org.apache.jackrabbit.core.config.RepositoryConfig;

public class JrTestDeadlock extends Thread {

   private static final org.apache.commons.logging.Log log = org.apache.commons.logging.LogFactory.getLog(JrTestDeadlock.class);

   public static String REPOSITORY_HOME = ""d:/repo/jackrabbit/"";

   public static String REPOSITORY_CONFIG = REPOSITORY_HOME + ""repository.xml"";

   public static void main(String[] args) throws Exception {

       JrTestDeadlock test = new JrTestDeadlock(-1);
       test.startup();

       JrTestDeadlock tests[] = new JrTestDeadlock[2];

       for (int i = 0; i < tests.length; i++) {
           JrTestDeadlock x = new JrTestDeadlock(i);
           x.start();
           tests[i] = x;
       }

       for (int i = 0; i < tests.length; i++) {
           tests[i].join();
       }

       test.shutdown();
   }

   private static RepositoryImpl repository;

   private int id;

   public JrTestDeadlock(int i) {
       this.id = i;
   }

   public void startup() throws Exception {
       System.setProperty(""java.security.auth.login.config"", ""c:/jaas.config"");

       RepositoryConfig config = RepositoryConfig.create(REPOSITORY_CONFIG, REPOSITORY_HOME);
       repository = RepositoryImpl.create(config);

       Session session = repository.login(new SimpleCredentials(""admin"", ""admin"".toCharArray()));
       Node rootNode = session.getRootNode();
       if (!rootNode.hasNode(""folder"")) {
           Node folder = rootNode.addNode(""folder"");
           folder.addMixin(""mix:versionable"");
           folder.addMixin(""mix:lockable"");
           rootNode.save();
       }
       session.logout();
   }

   public void shutdown() throws RepositoryException {
       repository.shutdown();
   }

   public Node getFolder(Session session) throws RepositoryException {
       return session.getRootNode().getNode(""folder"");
   }

   public void run() {
       try {
           Session session = repository.login(new SimpleCredentials(""admin"", ""admin"".toCharArray()));
           for (int i = 0; i < 100; i++) {
               log.error(""START id:"" + id + "", i="" + i);

               boolean success = false;

               JackrabbitUserTransaction ut = new JackrabbitUserTransaction(session);
               try {
                   ut.begin();

                   Node folder = getFolder(session);
                   folder.checkout();
                   folder.checkin();

                   success = true;
                   log.info(""SUCCESS id:"" + id + "", i="" + i);
               }
               catch (Exception e) {
                   log.warn(""FAIL:"" + id + "", i="" + i, e);
               }
               finally {
                   try {
                       if (success) {
                           ut.commit();
                       }
                       else {
                           ut.rollback();
                       }
                   }
                   catch (Exception e) {
                       log.fatal(e);
                   }
               }
           }
           session.logout();
       }
       catch (RepositoryException e) {
           e.printStackTrace();
       }
   }
}


13:46 ERROR JrTestDeadlock.run(JrTestDeadlock.java:76) - START id:0, i=0
13:46 ERROR JrTestDeadlock.run(JrTestDeadlock.java:76) - START id:1, i=0
13:46 INFO  JrTestDeadlock.run(JrTestDeadlock.java:89) - SUCCESS id:0, i=0
13:46 INFO  JrTestDeadlock.run(JrTestDeadlock.java:89) - SUCCESS id:1, i=0
13:46 ERROR org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:156) - org.apache.jackrabbit.core.state.StaleItemStateException: 233e656f-79f8-414d-9e37-3fce865b492d/{http://www.jcp.org/jcr/1.0}isCheckedOut has been modified externally
13:46 FATAL JrTestDeadlock.run(JrTestDeadlock.java:104) - javax.transaction.RollbackException: Transaction rolled back: XA_ERR=104
13:46 ERROR JrTestDeadlock.run(JrTestDeadlock.java:76) - START id:1, i=1
13:46 WARN  JrTestDeadlock.run(JrTestDeadlock.java:92) - FAIL:1, i=1
ax.jcr.InvalidItemStateException: f83a830b-abbf-4ab2-8625-b9e2c4802316: the item does not exist anymore
    at org.apache.jackrabbit.core.version.XAVersion.sanityCheck(XAVersion.java:81)
    at org.apache.jackrabbit.core.version.XAVersion.getInternalVersion(XAVersion.java:70)
    at org.apache.jackrabbit.core.version.AbstractVersion.getUUID(AbstractVersion.java:107)
    at org.apache.jackrabbit.core.NodeImpl.checkout(NodeImpl.java:2759)
    at JrTestDeadlock.run(JrTestDeadlock.java:85)
13:46 ERROR JrTestDeadlock.run(JrTestDeadlock.java:76) - START id:1, i=2
13:46 INFO  JrTestDeadlock.run(JrTestDeadlock.java:89) - SUCCESS id:1, i=2
13:51 WARN  org.apache.jackrabbit.core.TransactionContext.run(TransactionContext.java:239) - Transaction rolled back because timeout expired.

"
0,"Test failures when running ""mvn cobertura:check""It looks like the bytecode instrumentation done by Cobertura interferes with the rather complex XPathTokenManager class produced by JavaCC.

The easiest workaround seems to be to simply exclude XPathTokenManager from being instrumented by Cobertura."
0,"Rethink LocalizedTestCaseRunner with JUnit 4 - Clover OOMAs a spinn off from this [conversation|http://www.lucidimagination.com/search/document/ae20885bf5baedc5/build_failed_in_hudson_lucene_3_x_116#7ed351341152ee2d] we should rethink the way how we execute testcases with different locals since glover reports appears to throw OOM errors b/c Junit treats each local as a single test case run.

Here are some options:
* select the local at random only run the test with a single local
* set the local via system property -Dtest.locale=en.EN
* run with the default locale only -Dtest.skiplocale=true
* one from the above but only if instrumented with clover (let common tests run all the locale)

"
1,"URIResolverImpl: use of bitwise instead of logical AND operatorURIResolverImpl, line 111: 

                if (path != null & cache.containsItemId(uuidId)) {
"
0,"User configurable cookie policySome user configurable how cookies are handled.  Emulate cookie options in web
browsers."
0,"FST.BYTE2 should save as fixed 2 byte not as vIntWe currently write BYTE1 as a single byte, but BYTE2/4 as vInt, but I think that's confusing.  Also, for the FST for the new Kuromoji analyzer (LUCENE-3305), writing as 2 bytes instead shrank the FST and ran faster, presumably because more values were >= 16384 than were < 128.

Separately the whole INPUT_TYPE is very confusing... really all it's doing is ""declaring"" the allowed range of the characters of the input alphabet, and then the only thing that uses that is the write/readLabel methods (well and some confusing sugar methods in Builder!).  Not sure how to fix that yet...

It's a simple change but it changes the FST binary format so any users w/ FSTs out there will have to rebuild (FST is marked experimental...).
"
0,WriteLineDocTask should write gzip/bzip2/txt according to the extension of specified output file nameSince the readers behave this way it would be nice and handy if also this line writer would.
1,"Merging implemented by codecs must catch aborted mergesThis is a regression (we lost functionality on landing flex).

When you close IW with ""false"" (meaning abort all running merges), IW asks the merge threads to abort.  The threads are supposed to periodically check if they are aborted and throw an exception if so.

But on the cutover to flex, where the codec can override how merging is done (but a default impl is in the base enum classes), we lost this."
0,"Misleading exception message when re-index failsE.g. the log may say:

19.06.2007 11:25:42 *ERROR* RepositoryImpl: Failed to initialize workspace 'default' (RepositoryImpl.java, line 382)
javax.jcr.RepositoryException: Error indexing root node: 10022d38-c449-4751-b8f0-9d07ac45ead5:
[...]

The mentioned uuid is not the root node and the root cause is missing."
0,"Per socket SOCKS proxiesHttpClient requires a way of allowing a SOCKS proxy to be used on some
connections without requiring that all created Sockets go through the proxy."
0,"MockDirectoryWrapper should track open file handles of IndexOutput tooMockDirectoryWrapper currently tracks open file handles of IndexInput only. Therefore IO files that are not closed do not fail our tests, which can then lead to test directories fail to delete on Windows. We should make sure all open files are tracked and if they are left open, fail the test. I'll attach a patch shortly."
0,"Analysis for IrishAdds analysis for Irish.

The stemmer is generated from a snowball stemmer. I've sent it to Martin Porter, who says it will be added during the week."
0,"Rename BaseMultiReader class to BaseCompositeReader and make publicCurrently the abstract DirectoryReader and MultiReader and ParallelCompositeReader extend a package private class. Users that want to implement a composite reader, should be able to subclass this pkg-private class, as it implements lots of abstract methods, useful for own implementations. In fact MultiReader is a shallow subclass only implementing correct closing&refCounting.

By making it public after the rename, the generics problems (type parameter R is not correctly displayed) in the JavaDocs are solved, too."
0,"Provide Programmatic Access to CheckIndexWould be nice to have programmatic access to the CheckIndex tool, so that it can be used in applications like Solr.  

See SOLR-566"
0,"SQL Server support in clustering moduleI realize the clustering module doesn't specifically support SQL Server yet (there's no mssql.ddl), but I still tried to run the repository against SQL Server with clustering enabled in the hope that the default schema (default.ddl) would suffice. Apparently, it doesn't (unless I'm doing something very wrong), since I kept getting the following error whenever a write operation was attempted:

2007-05-25 14:48:06,757 WARN  [org.apache.jackrabbit.core.journal.DatabaseJournal] Error while rolling back connection: You cannot rollback with autocommit set!
2007-05-25 14:48:06,757 ERROR [org.apache.jackrabbit.core.cluster.ClusterNode] Unable to commit log entry.
org.apache.jackrabbit.core.journal.JournalException: Unable to append revision 1090.
	at org.apache.jackrabbit.core.journal.DatabaseJournal.append
	at org.apache.jackrabbit.core.journal.AppendRecord.update(AppendRecord.java:242)
	at org.apache.jackrabbit.core.cluster.ClusterNode$WorkspaceUpdateChannel.updateCommitted(ClusterNode.java:530)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:725)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:855)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:302)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:306)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1214)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:849)
Caused by: java.sql.DataTruncation: Data truncation
	at net.sourceforge.jtds.jdbc.SQLDiagnostic.addDiagnostic(SQLDiagnostic.java:379)
	at net.sourceforge.jtds.jdbc.TdsCore.tdsErrorToken(TdsCore.java:2781)
	at net.sourceforge.jtds.jdbc.TdsCore.nextToken(TdsCore.java:2224)
	at net.sourceforge.jtds.jdbc.TdsCore.getMoreResults(TdsCore.java:628)
	at net.sourceforge.jtds.jdbc.JtdsStatement.processResults(JtdsStatement.java:525)
	at net.sourceforge.jtds.jdbc.JtdsStatement.executeSQL(JtdsStatement.java:487)
	at net.sourceforge.jtds.jdbc.JtdsPreparedStatement.execute(JtdsPreparedStatement.java:475)
	at org.jboss.resource.adapter.jdbc.WrappedPreparedStatement.execute(WrappedPreparedStatement.java:183)
	at org.apache.jackrabbit.core.journal.DatabaseJournal.append
	... 58 more

However, I think I got things working by using a modified version of default.ddl, with the only change being the type of the REVISION_DATA field (varbinary -> IMAGE)."
1,"Deadlock in DBCP when accessing nodeI found a deadlock situation using JR 2.2.10, the problem is with DBCP 1.2.2 and is fixed in DBCP 1.3, JR trunk also uses DBCP 1.2.2 and should also be updated

The ticket in dbcp is #DBCP-270, related tickets are #DBCP-65 #DBCP-281 #DBCP-271

Stack trace of where my call is stalled:
{code}
main@1, prio=5, in group 'main', status: 'MONITOR'
	 blocks Timer-1@2545
	 waiting for Timer-1@2545 to release lock on {1}
	  at org.apache.commons.pool.impl.GenericObjectPool.addObjectToPool(GenericObjectPool.java:1137)
	  at org.apache.commons.pool.impl.GenericObjectPool.returnObject(GenericObjectPool.java:1076)
	  at org.apache.commons.dbcp.PoolableConnection.close(PoolableConnection.java:87)
	  at org.apache.commons.dbcp.PoolingDataSource$PoolGuardConnectionWrapper.close(PoolingDataSource.java:181)
	  at org.apache.jackrabbit.core.util.db.DbUtility.close(DbUtility.java:75)
	  at org.apache.jackrabbit.core.util.db.ResultSetWrapper.invoke(ResultSetWrapper.java:63)
	  at $Proxy12.close(Unknown Source:-1)
	  at org.apache.jackrabbit.core.persistence.pool.BundleDbPersistenceManager.loadBundle(BundleDbPersistenceManager.java:1042)
	  at org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.getBundle(AbstractBundlePersistenceManager.java:669)
	  at org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.load(AbstractBundlePersistenceManager.java:415)
	  at org.apache.jackrabbit.core.state.SharedItemStateManager.loadItemState(SharedItemStateManager.java:1830)
	  at org.apache.jackrabbit.core.state.SharedItemStateManager.getNonVirtualItemState(SharedItemStateManager.java:1750)
	  at org.apache.jackrabbit.core.state.SharedItemStateManager.getItemState(SharedItemStateManager.java:265)
	  at org.apache.jackrabbit.core.state.LocalItemStateManager.getNodeState(LocalItemStateManager.java:109)
	  at org.apache.jackrabbit.core.state.LocalItemStateManager.getItemState(LocalItemStateManager.java:174)
	  at org.apache.jackrabbit.core.state.XAItemStateManager.getItemState(XAItemStateManager.java:260)
	  at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:161)
	  at org.apache.jackrabbit.core.ItemManager.getItemData(ItemManager.java:382)
	  at org.apache.jackrabbit.core.ItemManager.getNode(ItemManager.java:669)
	  at org.apache.jackrabbit.core.ItemManager.getNode(ItemManager.java:647)
	  at org.apache.jackrabbit.core.LazyItemIterator.prefetchNext(LazyItemIterator.java:120)
	  at org.apache.jackrabbit.core.LazyItemIterator.next(LazyItemIterator.java:257)
	  at info.magnolia.jcr.iterator.DelegatingNodeIterator.next(DelegatingNodeIterator.java:79)
{code}

This is the offending thread:
{code}
Timer-1@2545 daemon, prio=5, in group 'main', status: 'MONITOR'
	 blocks main@1
	 waiting for main@1 to release lock on {1}
	  at org.apache.commons.dbcp.AbandonedTrace.addTrace(AbandonedTrace.java:176)
	  at org.apache.commons.dbcp.AbandonedTrace.init(AbandonedTrace.java:92)
	  at org.apache.commons.dbcp.AbandonedTrace.<init>(AbandonedTrace.java:82)
	  at org.apache.commons.dbcp.DelegatingStatement.<init>(DelegatingStatement.java:61)
	  at org.apache.commons.dbcp.DelegatingConnection.createStatement(DelegatingConnection.java:224)
	  at org.apache.commons.dbcp.PoolableConnectionFactory.validateConnection(PoolableConnectionFactory.java:331)
	  at org.apache.commons.dbcp.PoolableConnectionFactory.validateObject(PoolableConnectionFactory.java:312)
	  at org.apache.commons.pool.impl.GenericObjectPool.evict(GenericObjectPool.java:1217)
	  at org.apache.commons.pool.impl.GenericObjectPool$Evictor.run(GenericObjectPool.java:1341)
	  at java.util.TimerThread.mainLoop(Timer.java:512)
	  at java.util.TimerThread.run(Timer.java:462)
{code}"
1,"Cookie.parse exception when parsing expiry date in single quotesA Netscape-Enterprise/3.6 SP3 server sends a cookie where the parameter expires='Thu, 05-Dec-
2002 12:07:45 GMT'. 
Cookie.parse throws an exception because none of the four built-in formats 
matches - I have tested that the parse code works OK if the single quotes are omitted from the value 
being parsed.

Resolution: If the value of the 'expires' parameter starts and ends with a 
single quote then strip the first and last character before parsing."
0,Create a sample search pageThe web application should have a search page that shows how to use the query features in jackrabbit.
1,"getAllLinearVersions does not return the base versionIt appears that for a given linear version history, getAllLinearVersions returns less versions than getAllVersions -- the root version seems to be missing."
1,"WorkspaceImpl.dispose() might cause ClassNotFoundExceptionWenn using Jackrabbit in an environment, where ClassLoaders may get inactivated in the sense, the loading new classes is not possible anymore, shutting down the repository may result in a ClassNotFoundException during WorkspaceImpl.dispose().

Reason for this is, that in the dispose() method, the ObservationManager is asked for all registered event listeners for them to be removed from the ObservationManager one-by-one. Asking for the listeners results in a new EventListenerIteratorImpl object being created.

If now, this class has never been used during the live time of the repository, this would cause a ClassNotFoundException because the class loader is not laoding classes anymore in the specific environment.

The specific environment is Eclipse, where one plugin is managing different Repository instances provided by separate plugins. When now the Jackrabbit provider plugin has already been stopped while the managing plugin tries to shutdown the Jackrabbit repository, the EventListenerIteratorImpl class cannot be loaded anymore and disposing the WorkspaceImpl in a controlled way fails.

I suggest adding an ObservationManagerImpl.dispose() method, which is called by the WorkspaceImpl like :
    WorkspaceImpl.dispose() {
       if (obsMgr != null) {
         obsMgr.dispose();
         obsMgr = null;
        }
    }

As a side effect of not calling getObservationManager[Impl]() the observation manager would also not be created if not existing yet.

As a side effect to having the dispose method is, that the ObservationManagerImpl class could also do other cleanup work in addition to clearing the listener lists."
0,"Modify confusing javadoc for queryNormSee http://markmail.org/message/arai6silfiktwcer

The javadoc confuses me as well."
1,"Cluster revision entries should be retrieved in orderThe selectRevisionStmtSQL (DatabaseJournal#buildSqlStatements) returns a result set which may not be ordered by REVISION_ID. This has the effect that cluster instances that want to synchronize to the latest revision do not update their local revision appropriately since they assume that the revision result set is ordered (see code in AbstractJournal#doSync). This might cause a lot of unnecessary CPU cycles on these machines with degraded performance as a result. Furthermore, it causes functional issues as well as events may be fired multiple times and in the wrong order."
0,"Access to version history results in reading all versions of versionable nodeInternalVersionHistoryImpl loads all versions at once during initialization. Because of that all versioning operations (incl. checkin, label, restore) are significantly slower when node has many versions.

"
1,"Request/Response race condition when doing multiple requests on the same connection.If one tries to do multiple request over the same socket connection a race 
condition occurs in the input/output streams.
eg. 
-- Some request -->
<- HTTP/1.1 200 OK
<- Some: Headers
<- 
<- The body.

-- Next request -->
<- HTTP/1.1 200 OK
<- More: Headers
<- 
<- Some data.

If the second request is sent, but the second response isn't yet received 
before the client starts to try to read it, it'll get 
a ""org.apache.commons.httpclient.HttpRecoverableException: Error in parsing the 
status  line from the response: unable to find line starting with ""HTTP/"""" 
exception (it will think ""The body."" is part of the second response).

The following code will reproduce the problem:

import java.io.*;
import java.net.*;
import java.util.*;
import org.apache.commons.httpclient.*;
import org.apache.commons.httpclient.methods.*;

public class HttpClientRaceBug {
    public static void main(String[] args) {
        try {
            SimpleHttpServer.listen(8987);
            HttpClient client = new HttpClient();
            client.startSession(""localhost"", 8987);
            client.getState().setCredentials(""Test Realm"",  
                new UsernamePasswordCredentials(""foo"", ""bar""));
            
            for (int i = 0; i < 100; i++) {
                GetMethod meth = new GetMethod();
                client.executeMethod(meth);
            }
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
    
    private static final class SimpleHttpServer implements Runnable {
        private Socket socket;
        public SimpleHttpServer(Socket socket) {
            this.socket = socket;
        }
        public static void listen(final int port) {
            Thread server = new Thread() {
                public void run() {
                    try {
                        ServerSocket ss = new ServerSocket(port);
                        while (true) {
                            new Thread(new 
                                SimpleHttpServer(ss.accept())).start();
                        }
                    } catch (Exception e) {
                        e.printStackTrace();
                    }
                }
            };
            
            server.setDaemon(true);
            server.start();
        }
        public void run() {
            try {
                BufferedReader in = new BufferedReader(new 
                    InputStreamReader(this.socket.getInputStream()));
                
                int len = 0;
                boolean auth = false;
                String line;
                while ((line = in.readLine()) != null) {
                    System.out.println(""> "" + line);
                    
                    if (line.trim().equals("""")) {
                        in.read(new char[len]);
                        doOutput(auth);
                        auth = false;
                        len = 0;
                        
                    } else if (line.indexOf(':') > -1) {
                        StringTokenizer tok = new StringTokenizer(line, "":"");
                        String key = tok.nextToken().toLowerCase();
                        if (key.equals(""content-length"")) {
                            len = Integer.parseInt(tok.nextToken().trim());
                        } else if (key.equals(""authorization"")) {
                            auth = true;
                        }
                    }
                }
            } catch (Exception e) {}
        }
        private static int count = 0;
        public void doOutput(boolean authorized) throws IOException {
            Writer out = new OutputStreamWriter(this.socket.getOutputStream());
            count++;
            
            String id = (count < 100) ? 
                ((count < 10) ? ""00"" + count : ""0"" + count) : """" + count;
            if (authorized) {
                write(out, ""HTTP/1.1 200 OK\r\n"");
            } else {
                write(out, ""HTTP/1.1 401 Unauthorized\r\n"");
            }
            write(out, ""WWW-Authenticate: Basic realm=\""Test Realm\""\r\n"");
            write(out, ""Response-Id: "" + id + ""\r\n"");
            write(out, ""Content-Type: text/html; charset=iso-8859-1\r\n"");
            write(out, ""Content-Length: 17\r\n\r\n"");
            write(out, ""My Response ("" + id + "")"");
            out.close();
        }
        private void write(Writer out, String text) throws IOException {
            System.out.print(""< "" + text);
            out.write(text);
        }
    }
}"
0,"Allow overriding the specification version in MANIFEST.MFThe specification version in MANIFEST.MF should only consist of
digits. When we e. g. build a release candidate with a version like
2.3-rc1 then we have to specify a different specification version.

See related discussion:
http://www.gossamer-threads.com/lists/lucene/java-dev/56611

"
1,"DataStore.close() is never calledI've searched through the jackrabbit-core code and never found a call to DataStore.close(), although the method exists on the DataStore interface"
1,"If you hit the ""max term prefix"" warning when indexing, it never goes awaySilly bug.

If IW's infoStream is on, we warn whenever we hit a ridiculously long term (> 16 KB in length).  The problem is, we never reset this warning, so, once one doc contains such a massive term, we then keep warning over and over about that same term for future docs."
0,"XSLT pretty-printer for JCR document view export filesThe attached XSLT pretty-prints Jackrabbit XML document view export files.

I'm uploading it here so others can use it or improve it.

For now I'm using it standalone to document content structures, later I might create a servlet that applies it live to repository content."
0,"[OCM] rename o.a.j.ocm.persistence.PersistenceManager to avoid confusion with core componentPersistenceManager is a well known and established interface in jackrabbit's architecture. the same-named class in the
jcr-mapping contrib project should IMO be renamed in order to avoid confusion in mailing list threads and jira issues,"
0,Adding a custom location header extractor method for RedirectStrategy.Sometimes Web Servers respond to http requests with non-standard location response headers during a server side redirect. (302)  ADding a convenience method to over come this.
0,"Fix pom.xml in jackrabbit core (small fix, big return)Change the following dependency in pom.xml to match what is really available at the maven repos:
FROM:
    <dependency>
      <groupId>jsr170</groupId>
      <artifactId>jcr</artifactId>
      <version>1.0</version>
    </dependency>


TO:
    <dependency>
      <groupId>javax.jcr</groupId>
      <artifactId>jcr</artifactId>
      <version>1.0</version>
    </dependency>
"
1,JCR2SPI: VersionManagerImpl.getVersionableNodeEntry uses toString() rather than getString() to obtain property valueVersionManagerImpl.getVersionableNodeEntry uses toString() rather than getString() to obtain property value.
1,"TestNRTManager hangdidn't check 3.x yet, just encountered this one running the tests"
0,"Make ""ant -projecthelp"" show the javadocs and docs targets as wellAdded a description to the targets ""javadocs"" and ""docs"".
This makes ant show them when the executes ""ant -projecthelp""
"
0,"Ensure the features.html and index.html adequately give httpclient enough creditSee the email thread started by Eric Johnson.
http://archives.apache.org/eyebrowse/BrowseList?listId=128&by=thread&from=316092

Initial post:
Based on the recent URI discussion, and some other points, it strikes me that we
could take a little more credit for the work that has gone into HttpClient.

On the HttpClient home page
(http://jakarta.apache.org/commons/httpclient/index.html) four RFCs are listed.

Given all the discussion about URIs being thrown around, I think it might be
reasonable to add RFC 2396 - for URI compliance.  Then there is RFC 1867, for
multipart/form-data POST requests (I think I got the right number there).  Are
there RFCs corresponding to our ""cookie"" compliance? Any other RFCs we can claim
credit for conforming to?

With the recent ""Protocol"" changes, I think we've made it relatively
straightforward for clients of HttpClient to plug in their own secure sockets
implementations, making it easier to use third party, non-Sun solutions."
0,"GData Server IndexComponentNew Feature added:

-> Indexcomponent.
-> Content extraction from entries.
-> Custom content ext. strategies added.
-> user defined index schema.
-> extended gdata-config.xml schema (xsd)
-> Indexcomponent UnitTests
-> Spellchecking on some JavaDoc.

##############
New jars included:

nekoHTML.jar 
xercesImpl.jar

@yonik: don't miss the '+' button to add directories :)"
0,"SPI POM improvementsWhile the SPI components were upgraded from the sandbox I didn't pay too much attention to the POM details and thus there still are a number of configuration entries that duplicate stuff from the Jackrabbit parent POM, etc.

I plan to get rid of any such duplication, remove some unneeded dependencies (spi-commons has a compile scope dependency on junit) and generally update the POMs to be in line with the other release components."
0,"Add Japanese filter to replace term attribute with readingsKoji and Robert are working on LUCENE-3888 that allows spell-checkers to do their similarity matching using a different word than its surface form.

This approach is very useful for languages such as Japanese where the surface form and the form we'd like to use for similarity matching is very different.  For Japanese, it's useful to use readings for this -- probably with some normalization."
0,"Change all multi-term querys so that they extend MultiTermQuery and allow for a constant score modeCleans up a bunch of code duplication, closer to how things should be - design wise, gives us constant score for all the multi term queries, and allows us at least the option of highlighting the constant score queries without much further work."
0,"Speedup Startupjackrabbit startup gets slower the more items are in the repository.

possible reasons:
- versioning
- search index"
1,"TestAddIndexes reproducible test failure on turnktrunk: r1133385

{code}
    [junit] Testsuite: org.apache.lucene.index.TestAddIndexes
    [junit] Tests run: 2843, Failures: 1, Errors: 0, Time elapsed: 137.121 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] java.io.FileNotFoundException: _cy.fdx
    [junit]     at org.apache.lucene.store.RAMDirectory.fileLength(RAMDirectory.java:121)
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.fileLength(MockDirectoryWrapper.java:606)
    [junit]     at org.apache.lucene.index.SegmentInfo.sizeInBytes(SegmentInfo.java:294)
    [junit]     at org.apache.lucene.index.TieredMergePolicy.size(TieredMergePolicy.java:633)
    [junit]     at org.apache.lucene.index.TieredMergePolicy.useCompoundFile(TieredMergePolicy.java:611)
    [junit]     at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:2459)
    [junit]     at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes3.doBody(TestAddIndexes.java:847)
    [junit]     at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:675)
    [junit] java.io.FileNotFoundException: _cx.fdx
    [junit]     at org.apache.lucene.store.RAMDirectory.fileLength(RAMDirectory.java:121)
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.fileLength(MockDirectoryWrapper.java:606)
    [junit]     at org.apache.lucene.index.SegmentInfo.sizeInBytes(SegmentInfo.java:294)
    [junit]     at org.apache.lucene.index.TieredMergePolicy.size(TieredMergePolicy.java:633)
    [junit]     at org.apache.lucene.index.TieredMergePolicy.useCompoundFile(TieredMergePolicy.java:611)
    [junit]     at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:2459)
    [junit]     at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes3.doBody(TestAddIndexes.java:847)
    [junit]     at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:675)
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithRollback -Dtests.seed=9026722750295014952:2645762923088581043 -Dtests.multiplier=3
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=SimpleText, content=SimpleText, d=MockRandom, c=SimpleText}, locale=fr, timezone=Africa/Kigali
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestAddIndexes]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=68050392,total=446234624
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAddIndexesWithRollback(org.apache.lucene.index.TestAddIndexes):       FAILED
    [junit]
    [junit] junit.framework.AssertionFailedError:
    [junit]     at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithRollback(TestAddIndexes.java:932)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1362)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1280)
    [junit]
    [junit]
    [junit] Test org.apache.lucene.index.TestAddIndexes FAILED
{code}


Fails randomly in my while(1) test run, and Fails after a few min of running: 
{code}
ant test -Dtestcase=TestAddIndexes -Dtests.seed=9026722750295014952:2645762923088581043 -Dtests.multiplier=3 -Dtests.iter=200 -Dtests.iter.min=1
{code}"
0,"Allow similarity to encode norms other than a single byteLUCENE-3628 cut over norms to docvalues. This removes the long standing limitation that norms are a single byte. Yet, we still need to expose this functionality to Similarity to write / encode norms in a different format. "
1,"ArrayStoreException while reregistering existing node types
class: NodeTypeManagerImpl
method: public NodeType[] registerNodeTypes(InputStream in, String contentType, boolean reregisterExisting)

                ...
                return (NodeType[]) nodeTypes.toArray(new NodeTypeDef[nodeTypes.size()]);
                ...

=> should be (I suppose !)

                return (NodeType[]) nodeTypes.toArray(new NodeType[nodeTypes.size()]);
"
1,"There are a few binary search implmentations in lucene that suffer from a now well known overflow bughttp://googleresearch.blogspot.com/2006/06/extra-extra-read-all-about-it-nearly.html

The places I see it are:

MultiSearcher.subSearcher(int)
TermInfosReader.getIndexOffset(Term)
MultiSegmentReader.readerIndex(int, int[], int)
MergeDocIDRemapper.remap(int)

I havn't taken much time to consider how likely any of these are to overflow. The values being averaged would have to be very large. That would rule out possible problems for at least a couple of these, but how about something like the MergeDocIDRemapper? Is there a document number that could be reached that has a chance of triggering this bug? If not we can close this and have a record of looking into it."
1,"corrupted paths after moving nodeswe just found a bug which corrupts the results of Node.getPath() - it seems to be related to older Jackrabbit bugs (e.g. JCR-768) but still happens in jackrabbit 1.3 and jackrabbit-1.4-SNAPSHOT

Basically we have a node with 3 subnodes (a, b, c), we move all of them to index 1 - this works fine, unless we call getPath() of the third Node before moving it.

The expected paths after moving would be:
a: /pages[37]/page/element[3]
b: /pages[37]/page/element[2]
c: /pages[37]/page/element

But we get these paths:

a: /pages[37]/page/element[3]
b: /pages[37]/page/element
c: /pages[37]/page/element"
0,"Improve reliability of canAddMixinThe current implementation of canAddMixin in JCR2SPI lacks flexibility. It only consults the (SPI) node type registry, checking for (1) whether the mixin exists, and (2) whether it is already present and (3) whether it's consistent with the node's type.

This is fine for stores where any legal mixin can be added anywhere. It doesn't work well for stores that are limited in what they can do; for instance when nt:file nodes can be made mix:versionable, but nt:folder nodes can't.

Proposal: enhance QNodeTypeDefinition with

  public Name[] getSupportedMixins();

where the return value is either null (no constraints or no constraints known), or a list of mixin types that are supported for this node type."
0,Missing log4j.properties fileThe log4j.properties file is missing in the test resources.
0,"Replace deprecated TermAttribute by new CharTermAttributeAfter LUCENE-2302 is merged to trunk with flex, we need to carry over all tokenizers and consumers of the TokenStreams to the new CharTermAttribute.

We should also think about adding a AttributeFactory that creates a subclass of CharTermAttributeImpl that returns collation keys in toBytesRef() accessor. CollationKeyFilter is then obsolete, instead you can simply convert every TokenStream to indexing only CollationKeys by changing the attribute implementation."
0,"Use NativeFSLockFactory as default for new API (direct ctors & FSDir.open)A user requested we add a note in IndexWriter alerting the availability of NativeFSLockFactory (allowing you to avoid retaining locks on abnormal jvm exit). Seems reasonable to me - we want users to be able to easily stumble upon this class. The below code looks like a good spot to add a note - could also improve whats there a bit - opening an IndexWriter does not necessarily create a lock file - that would depend on the LockFactory used.


{code}  <p>Opening an <code>IndexWriter</code> creates a lock file for the directory in use. Trying to open
  another <code>IndexWriter</code> on the same directory will lead to a
  {@link LockObtainFailedException}. The {@link LockObtainFailedException}
  is also thrown if an IndexReader on the same directory is used to delete documents
  from the index.</p>{code}

Anyone remember why NativeFSLockFactory is not the default over SimpleFSLockFactory?"
0,"Configurable actions upon authorizable creation and removali would like to add the possibility to configure custom actions that are executed upon user (and group) creation before 
the operation is persisted. this would allow applications to run custom code without the need of subclassing the
usermanager implementation. e.g.: creating additional mandatory properties, setting up permissions, calculating default 
group membership. the same applies for user/group removal."
0,"Speedup CharArraySet if set is emptyCharArraySet#contains(...) always creates a HashCode of the String, Char[] or CharSequence even if the set is empty. 
contains should return false if set it empty"
0,"Add Bundle Persistence Managerswe (day software) offer our set of bundle persistence managers to the jackrabbit project. those pms combine the node and property states into a single bundle and store them together. this improves performance and reduces storage-memory overhead (no exact numbers available). The bundle pms also have a ""bundle-cache"" that does a memory sensitive caching of the bundles and a negative cache for non-existent bundles. small binary properties are inlined into the bundle rather than stored in the blobstore."
0,"Include generated website in the distributionA user should be able to build the non-api docs as well.

So it would be nice, to include xdocs in the source packages as well ..."
0,"Each TransactionContext creates new threadThe rollback threads are not stopped when the transaction commits, but only when the timeout occurs. This has the effect that lots of threads are created and sleeping when many transactions are committed in a short time frame. The rollback thread should be signaled when the transaction is committed or even better a Timer should be used with a single thread for all transaction contexts."
1,"Deadlocks in ConcurrentVersioningWithTransactionsTestPatch follows for a ConcurrentVersioningWithTransactionsTest, based on the existing ConcurrentVersioningTest but using transactions around the versioning operations.

On my macbook, running the test with CONCURRENCY = 100 and NUM_OPERATIONS = 100 causes a deadlock after a few seconds, thread dumps follow.

Note that I had to ignore StaleItemStateException (which is probably justified, due to not locking stuff IIUC) to let the threads run long enough to show the problem.

Running the test a few times showed the same locking pattern several times: some threads are locked at line 87 (session.save(), no transaction) while others are at line 93 (transaction.commit()), in testConcurrentCheckinInTransaction():

    80    public void testConcurrentCheckinInTransaction() throws RepositoryException {
    81      runTask(new Task() {
    82        public void execute(Session session, Node test) throws RepositoryException {
    83          int i = 0;
    84          try {
    85            Node n = test.addNode(""test"");
    86            n.addMixin(mixVersionable);
    87            session.save();
    88            for (i = 0; i < NUM_OPERATIONS / CONCURRENCY; i++) {
    89              final UserTransaction utx = new UserTransactionImpl(test.getSession());
    90              utx.begin();
    91              n.checkout();
    92              n.checkin();
    93              utx.commit();
    94            }
    95            n.checkout();
    96          } catch (Exception e) {
    97            final String threadName = Thread.currentThread().getName();
    98            final Throwable deepCause = getLevel2Cause(e);
    99            if(deepCause!=null && deepCause instanceof StaleItemStateException) {
   100              // ignore 
   101            } else {
   102              throw new RepositoryException(threadName + "", i="" + i + "":"" + e.getClass().getName(), e);
   103            }
   104          }
   105        }
   106      }, CONCURRENCY);
   107    }"
1,"SpanOrQuery skipTo() doesn't always move forwardsIn SpanOrQuery the skipTo() method is improperly implemented if the target doc is less than or equal to the current doc, since skipTo() may not be called for any of the clauses' spans:

    public boolean skipTo(int target) throws IOException {
          if (queue == null) {
            return initSpanQueue(target);
          }

          while (queue.size() != 0 && top().doc() < target) {
            if (top().skipTo(target)) {
              queue.adjustTop();
            } else {
              queue.pop();
            }
          }
          
        	return queue.size() != 0;
        }

This violates the correct behavior (as described in the Spans interface documentation), that skipTo() should always move forwards, in other words the correct implementation would be:

    public boolean skipTo(int target) throws IOException {
          if (queue == null) {
            return initSpanQueue(target);
          }

          boolean skipCalled = false;
          while (queue.size() != 0 && top().doc() < target) {
            if (top().skipTo(target)) {
              queue.adjustTop();
            } else {
              queue.pop();
            }
            skipCalled = true;
          }
          
          if (skipCalled) {
        	return queue.size() != 0;
          }
          return next();
        }"
1,"TestIndexWriterOnDiskFull.testAddIndexOnDiskFull fails with java.lang.IllegalStateException: CFS has pending open files {noformat}
 Testsuite: org.apache.lucene.index.TestIndexWriterOnDiskFull
    [junit] Testcase: testAddIndexOnDiskFull(org.apache.lucene.index.TestIndexWriterOnDiskFull):	Caused an ERROR
    [junit] CFS has pending open files
    [junit] java.lang.IllegalStateException: CFS has pending open files
    [junit] 	at org.apache.lucene.store.CompoundFileWriter.close(CompoundFileWriter.java:162)
    [junit] 	at org.apache.lucene.store.CompoundFileDirectory.close(CompoundFileDirectory.java:206)
    [junit] 	at org.apache.lucene.index.IndexWriter.createCompoundFile(IndexWriter.java:4099)
    [junit] 	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3661)
    [junit] 	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3260)
    [junit] 	at org.apache.lucene.index.SerialMergeScheduler.merge(SerialMergeScheduler.java:37)
    [junit] 	at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1902)
    [junit] 	at org.apache.lucene.index.IndexWriter.forceMerge(IndexWriter.java:1716)
    [junit] 	at org.apache.lucene.index.IndexWriter.forceMerge(IndexWriter.java:1670)
    [junit] 	at org.apache.lucene.index.TestIndexWriterOnDiskFull.testAddIndexOnDiskFull(TestIndexWriterOnDiskFull.java:304)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:529)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit] 
    [junit] 
    [junit] Tests run: 4, Failures: 0, Errors: 1, Time elapsed: 31.96 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterOnDiskFull -Dtestmethod=testAddIndexOnDiskFull -Dtests.seed=-7dd066d256827211:127c018cbf5b0975:20481cd18a7d8b6e -Dtests.multiplier=3 -Dtests.nightly=true -Dargs=""-Dfile.encoding=ISO8859-1""
    [junit] NOTE: test params are: codec=SimpleText, sim=RandomSimilarityProvider(queryNorm=true,coord=false): {field=DFR GB1, id=DFR I(F)L1, content=IB SPL-D3(800.0), f=DFR G2}, locale=de_AT, timezone=America/Cambridge_Bay
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestAssertions, TestSearchForDuplicates, TestMockAnalyzer, TestDocValues, TestPerFieldPostingsFormat, TestDocument, TestAddIndexes, TestConcurrentMergeScheduler, TestCrashCausesCorruptIndex, TestDocCount, TestDocumentsWriterDeleteQueue, TestFieldInfos, TestFilterIndexReader, TestFlex, TestIndexInput, TestIndexWriter, TestIndexWriterMergePolicy, TestIndexWriterMerging, TestIndexWriterNRTIsCurrent, TestIndexWriterOnDiskFull]
    [junit] NOTE: FreeBSD 8.2-RELEASE amd64/Sun Microsystems Inc. 1.6.0 (64-bit)/cpus=16,threads=1,free=39156976,total=180748288
{noformat}"
0,Some Workspace tests require a second workspaceSome workspace test require a second workspace even though it is not used in the test cases.
1,"Security issue - DigestScheme uses constant nonce count valueThe nonce count value in DigestScheme is static (set to 00000001) and never changes.  (also seen as comment in said file).

This means that it fails against servers that correctly detect man-in-the-middle or replay attacks, leading to additional 401 requests (every second time), or such servers must be configured to turn such checks off (which is either poor security or poor for performance).

I suggest that at minimum, this count is incremented for every call to DigestScheme#createDigest.  It should also be an instance variable instead of a static, as it really relates to the challenge (assuming cases where instances are cached for reuse).  AtomicInteger is a good choice for implementing this counter.

See RFC 2617 chapters 3.2.2 and 3.2.3"
1,"ISOLatin1AccentFilter discards position increments of filtered termsNot sure if this is a bug, but looks like one to me..."
1,"KeywordTokenizer does not set start/end offset of the Token it producesI think just adding these two lines in the next(Token) method is the right fix:

           reusableToken.setStartOffset(0);
           reusableToken.setEndOffset(upto);

I don't think this is a back compat issue because the start/end offset are now meaningless since they will inherit whatever the reusable token had previously been used for."
0,"Update slf4jPlease update slf4j from 1.3.0 to 1.5.2.

jcl104-over-slf4j has been renamed as jcl-over-slf4j, so if one uses a recent version, he has to exclude jcl104-over-slf4j for every jackrabbit dependency, which is quite a pain...

No impact observed.

Best regards,

Stephane Landelle"
1,"JCR-SQL2 : no count when WHERE clause is providedwhenever you provide a where-clause to a sql2 select, jcr/jackrabbit does not provide the hit count.

E.g.:
   select * from [nt:unstructured]
   order by [jcr:score]
returns the hit count (query.execute().getRows().getSize()), 
whereas
  select * from [nt:unstructured]
  where entity = ""customer""
  order by [jcr:score]
doesn't.
"
0,"Factor merge policy out of IndexWriterIf we factor the merge policy out of IndexWriter, we can make it pluggable, making it possible for apps to choose a custom merge policy and for easier experimenting with merge policy variants."
0,"Implementation of Delete methodThe HTTP request method, Delete, had not been implemented. I needed it and created an HttpDelete class modeled after HttpGet."
1,"spi2dav: Accessing moved referenceble nodes results in PathNotFoundExceptionthe following code fragment causes a PathNotFoundException on an existing path
and there seems to be no way to recover the session from this incorrect state:

	// assuming an existing nt:file node at path /apps/foo/bar.txt
	Node n1 = session.getNode(""/apps/foo/bar.txt"");
	Node n2 = n1.getNode(""jcr:content"");
	n2.setProperty(""jcr:data"", new java.io.ByteArrayInputStream(((String)(""blahblah"")).getBytes()));
	n2.save();
	Workspace ws0 = session.getWorkspace();
	ws0.move(""/apps/foo"", ""/apps/foo1"");
	Node n3 = session.getNode(""/apps/foo1/bar.txt"");
	Node n4 = n3.getNode(""jcr:content"");
	n4.refresh(false);
	Node n5 = n3.getNode(""jcr:content"");     // => PathNotFoundException

Please note that the preceeding Node.refresh() call seems to cause the inconsistency.
the problem doesn't occur when omitting this call."
1,"XPath QueryFormat may produce malformed XPath statementWhen the query tree contains select properties *and* an order by clause, then the XPath QueryFormat will produce a malformed XPath statement.

E.g.:

//element(*, foo)/(@a|@b) order by @bar

round trips to:

//element(*, foo) order by @bar/(@a|@b)

"
1,"Setting Query.setOffset() passed the results total returns negative getSize() instead of zero1. Have a query that returns 3 results
2. Now set Query.setOffset(10) (passed the total of 3)
3. Row/NodeIterator.getSize() returns -7 (incorrect)

Expected: getSize() should return 0"
0,"broken test in AddEventListenerHere's the test code, comments inline prefixed with ""reschke""

    /**
     * Tests if {@link javax.jcr.observation.Event#NODE_ADDED} is created only
     * for the specified path if <code>isDeep</code> is <code>false</code>.
     */
    public void testIsDeepFalseNodeAdded() throws RepositoryException {
        EventResult listener = new EventResult(log);

        // reschke: we are listening for changes at testRoot/nodeName1, with isDeep==false 
        obsMgr.addEventListener(listener, Event.NODE_ADDED, testRoot + ""/"" + nodeName1, false, null, null, false);

        // reschke; node at ""testRoot/nodeName1"" being created, the associated parent node for this event is ""testRoot""
        Node n = testRootNode.addNode(nodeName1, testNodeType);

        // reschke: node at ""testRoot/nodeName1/nodeName2"" being created, the associated parent node for this event is ""testRoot/nodeName1""
        n.addNode(nodeName2);
        testRootNode.save();

        Event[] events = listener.getEvents(DEFAULT_WAIT_TIMEOUT);
        obsMgr.removeEventListener(listener);

        // reschke: test case expects event with path ""testRoot/nodeName1""
        checkNodeAdded(events, new String[]{nodeName1});
    }

So, in plain english:

- test case listens for events where the associated parent node equals ""testRoot/nodeName1"", but
- it expects a single event where the Event.getPath() returns ""testRoot/nodeName1"".

This is incorrect (IMHO), because the associated parent node for *that* event is ""testRoot"". 

So the correct test would be to check for:

        checkNodeAdded(events, new String[]{nodeName1 + ""/"" + nodeName2});

Making this change of course leads to a test failure reported against the RI.

Feedback appreciated.
"
0,"[PATCH] Extension to binary Fields that allows fixed byte bufferThis is a very simple patch that supports storing binary values in the index
more efficiently.  A new Field constructor accepts a length argument, allowing a
fixed byte[] to be reused acrossed multiple calls with arguments of different
sizes.  A companion change to FieldsWriter uses this length when storing and/or
compressing the field.

There is one remaining case in Document.  Intentionally, no direct accessor to
the length of a binary field is provided from Document, only from Field.  This
is because Field's created by FieldReader will never have a specified length and
this is usual case for Field's read from Document.  It seems less confusing for
most users.

I don't believe any upward incompatibility is introduced here (e.g., from the
possibility of getting a larger byte[] than actually holds the value from
Document), since no such byte[] values are possible without this patch anyway.

The compression case is still inefficient (much copying), but it is hard to see
how Lucene can do too much better.  However, the application can do the
compression externally and pass in the reused compression-output buffer as a
binary value (which is what I'm doing).  This represents a substantialy
allocation savings for storing large documents bodies (compressed) into the
Lucene index.

Two patch files are attached, both created by svn on 3/17/05."
0,"Ant contrib test can fail if there is a space in path to lucene projectA couple contrib ant tests get the path to test files through a URL object, and so the path is URL encoded. Normally fine, but if you have a space in your path (/svn stuff/lucene/contrib/ant) then it will have %20 for the space and (at least on my Ubuntu system) the test will fail with filenotfound. This patch simply replaces all %20 with "" "". Not sure if we want/need to take it any further."
0,"New feature rich higlighter for Lucene.Well, I refactored (took) some code from two previous highlighters.
This highlighter:
+ use TermPositionVector where available
+ use Analyzer if no TermPositionVector found or is forced to use it.
+ support for all lucene queries (Term, Phrase with slops, Prefix, Wildcard, Range) except Fuzzy Query (can be implemented easly)

- has no support for scoring (yet)
- use same prefix,postfix for accepted terms (yet)

? It's written in Java5

In next release I'd like to add support for Fuzzy, ""coloring"" f.e. diffrent color for terms btw. phrase terms (slops), scoring of fragments

It's apache licensed - I hope so :-) I put licene statement in every file
"
1,"Deprecated Serializer does not properly delegate method calls.The deprecated org.apache.jackrabbit.core.state.util.Serializer class does not actually forward method calls to its replacement. Instead it calls itself repeatedly, leading to infinite recursion. The attached test demonstrates this and yields the following trace:

<<
java.lang.StackOverflowError
	at org.apache.jackrabbit.core.state.util.Serializer.serialize(Serializer.java:39)
>>"
1,"The ""jackrabbit-pool-"" thread prevents the process from stoppingIf the repository is not closed, and a session is still logged in, then the process doesn't terminate because of a non-daemon thread named ""jackrabbit-pool-<n>"". Test case:

public class TestThreadPreventsExit {
    public static void main(String... a) throws Exception {
        new TransientRepository().login(
                new SimpleCredentials("""", new char[0]));
    }
}

This program doesn't stop.

The non-daemon thread was introduces as part of https://issues.apache.org/jira/browse/JCR-2465

The fix is to use a daemon thread."
0,Configurable SimilarityThe similarity implementation for indexing and searching should be configurable.
1,"IllegalStateException: Authentication state already initializedHi,

I am running HttpClient 3.0 RC2 in my application and a user send me a logfile
telling ""IllegalStateException: Authentication state already initialized"". 

He wanted to access a site on SUN.com and is behind a proxy. The site seems to
redirect to a different domain.

I have attached a Debug+Trace HttpClient log.

Ben"
1,"The getOutputStream of the MemoryFileSystem class can replace a folder with a newly created fileIt seems that if the filePath parameter passed to the getOutputStream method of the MemoryFileSystem class points to an  existing folder and not to a file - the folder will be replaced with a newly created file.
The function should probably check whether the passed path points to a file and throw an exception if it points to a folder."
1,"PropertyValue constraint fails with implicit selectorName using JCR-SQL2Compiling a JCR-SQL2 query involving a PropertyValue constraint using a qualified property name fails if selectorName is not explicitly defined.

The following query works:

SELECT * FROM [my:thing] AS thing WHERE thing.[my:property] = 'abc'

the following doesn't:

SELECT * FROM [my:thing] AS thing WHERE [my:property] = 'abc'

(the ""AS thing"" is unecessary here, I can leave it out with the same result).

The second query results in an:
javax.jcr.query.InvalidQueryException: Query:
SELECT * FROM [my:thing] AS thing WHERE [(*)my:property] = 'abc';
expected: NOT, (

The spec final draft however states:

PropertyValue ::= [selectorName'.'] propertyName
   /* If only one selector exists in this query,
      explicit specification of the selectorName is
      optional */"
0,Add simple benchmarking tools for jcr2spi read performance
1,"NullPointerException when using HttpHead and Request/Response interceptorsWhen you try to execute a HttpHead object instead of a HttpGet object while using the add request/response interceptors, you get a nullpointerexception.

I can replicate the exception when using the ClientGZipContentCompression example that can be found at the HttpClient examples. But instead of using the HttpGet object I execute a HttpHead object. When I comment the interceptor parts out, I don't get the exception. 

This is the error stack trace I get when executing the code in netbeans:

Exception in thread ""main"" java.lang.NullPointerException
	at testhttphead.ClientGZipContentCompression$2.process(ClientGZipContentCompression.java:74)
	at org.apache.http.protocol.ImmutableHttpProcessor.process(ImmutableHttpProcessor.java:116)
	at org.apache.http.protocol.HttpRequestExecutor.postProcess(HttpRequestExecutor.java:342)
	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:472)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:820)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:754)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:732)
	at testhttphead.ClientGZipContentCompression.main(ClientGZipContentCompression.java:92)
Java Result: 1

Here is the code that gives me the error:

package testhttphead;

import java.io.IOException;
import java.io.InputStream;
import java.util.zip.GZIPInputStream;
import org.apache.http.*;
import org.apache.http.client.methods.HttpHead;
import org.apache.http.entity.HttpEntityWrapper;
import org.apache.http.impl.client.DefaultHttpClient;
import org.apache.http.protocol.HttpContext;
import org.apache.http.util.EntityUtils;

/**
 * Demonstration of the use of protocol interceptors to transparently modify
 * properties of HTTP messages sent / received by the HTTP client.
 * <p/>
 * In this particular case HTTP client is made capable of transparent content
 * GZIP compression by adding two protocol interceptors: a request interceptor
 * that adds 'Accept-Encoding: gzip' header to all outgoing requests and a
 * response interceptor that automatically expands compressed response entities
 * by wrapping them with a uncompressing decorator class. The use of protocol
 * interceptors makes content compression completely transparent to the consumer
 * of the {@link org.apache.http.client.HttpClient HttpClient} interface.
 */
public class ClientGZipContentCompression {

    public final static void main(String[] args) throws Exception {
        DefaultHttpClient httpclient = new DefaultHttpClient();

        try {
            httpclient.addRequestInterceptor(new HttpRequestInterceptor() {

                public void process(
                        final HttpRequest request,
                        final HttpContext context) throws HttpException, IOException {
                    if (!request.containsHeader(""Accept-Encoding"")) {
                        request.addHeader(""Accept-Encoding"", ""gzip"");
                    }
                }
            });

            httpclient.addResponseInterceptor(new HttpResponseInterceptor() {

                public void process(
                        final HttpResponse response,
                        final HttpContext context) throws HttpException, IOException {
                    HttpEntity entity = response.getEntity();
                    Header ceheader = entity.getContentEncoding();
                    if (ceheader != null) {
                        HeaderElement[] codecs = ceheader.getElements();
                        for (int i = 0; i < codecs.length; i++) {
                            if (codecs[i].getName().equalsIgnoreCase(""gzip"")) {
                                response.setEntity(
                                        new GzipDecompressingEntity(response.getEntity()));
                                return;
                            }
                        }
                    }
                }
            });

            HttpHead httpHead = new HttpHead(""http://www.howest.be"");

            // Execute HTTP request
            System.out.println(""executing request "" + httpHead.getURI());
            HttpResponse response = httpclient.execute(httpHead);

            System.out.println(""----------------------------------------"");
            System.out.println(response.getStatusLine());
            System.out.println(response.getLastHeader(""Content-Encoding""));
            System.out.println(response.getLastHeader(""Content-Length""));
            System.out.println(""----------------------------------------"");

            HttpEntity entity = response.getEntity();

            if (entity != null) {
                String content = EntityUtils.toString(entity);
                System.out.println(content);
                System.out.println(""----------------------------------------"");
                System.out.println(""Uncompressed size: "" + content.length());
            }

        } finally {
            // When HttpClient instance is no longer needed,
            // shut down the connection manager to ensure
            // immediate deallocation of all system resources
            httpclient.getConnectionManager().shutdown();
        }
    }

    static class GzipDecompressingEntity extends HttpEntityWrapper {

        public GzipDecompressingEntity(final HttpEntity entity) {
            super(entity);
        }

        @Override
        public InputStream getContent()
                throws IOException, IllegalStateException {

            // the wrapped entity's getContent() decides about repeatability
            InputStream wrappedin = wrappedEntity.getContent();

            return new GZIPInputStream(wrappedin);
        }

        @Override
        public long getContentLength() {
            // length of ungzipped content is not known
            return -1;
        }
    }
}

With kind regards,

Peter"
1,Incorrect excerpt for index aggregatesIncorrect excerpts may be created when the relevant node has an index aggregate configured and the nodes have properties configured for the node scope index with some of them excluded for use in excerpts.
1,"Two or more writers over NFS can cause index corruptionWhen an index is used over NFS, and, more than one machine can be a
writer such that they swap roles quickly, it's possible for the index
to become corrupt if the NFS client directory cache is stale.

Not all NFS clients will show this.  Very recent versions of Linux's
NFS client do not seem to show the issue, yet, slightly older ones do,
and the latest Mac OS X one does as well.

I've been working with Patrick Kimber, who provided a standalone test
showing the problem (thank you Patrick!).  This came out of this
thread:

  http://www.gossamer-threads.com/lists/engine?do=post_view_flat;post=50680;page=1;sb=post_latest_reply;so=ASC;mh=25;list=lucene

Note that the first issue in that discussion has been resolved
(LUCENE-948).  This is a new issue.
"
0,"Similarity can only be set per index, but I may want to adjust scoring behaviour at a field levelSimilarity can only be set per index, but I may want to adjust scoring behaviour at a field level, to faciliate this could we pass make field name available to all score methods.
Currently it is only passed to some such as lengthNorm() but not others such as tf()"
1,"OutOfMemory problem: HandleMonitor does not release closed input streamsThe class o.a.j.core.fs.local.HandleMonitor does not release closed MonitoredInputStream. There is a close method, but it is never called. The input streams are kept in a hash set / map of HandleMonitor. Eventually, this leads to an OutOfMemory exception after opening / closing many files."
0,"AccessControlImporter does not import repo level ac contentthe implementation of the ProtectedNodeImporter responsible for dealing with access control content should be
adjusted such that it can properly cope with repository level access control that may be stored together with
the root node (by using access control API with null path)."
0,"Avoid ${project.version} in dependenciesAnother one for Jackrabbit 1.5, we should avoid using ${project.version} for our dependencies and override the versions of any transitive dependencies that use ${project.version} (notably the Jetty dependencies in jackrabbit-standalone) to avoid problems with Maven < 2.0.9 caused by MNG-2339 [1].

[1] http://jira.codehaus.org/browse/MNG-2339
"
0,"Replace license headers with new policy textWe need to replace all of the license headers with a new template
that replaces the Copyright and license lines with

---BEGIN PROPOSED SOURCE FILE HEADER---
  Licensed to the Apache Software Foundation (ASF) under one or more
  contributor license agreements.  The ASF licenses this file to You
  under the Apache License, Version 2.0 (the ""License""); you may not
  use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an ""AS IS"" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
---END PROPOSED SOURCE FILE HEADER---

The copyright line is being removed from files due to legal advice from ASF attorneys.
It is replaced with a statement that the copyright owners have licensed it to the ASF.
"
0,"Optimize copies between IndexInput and OutputWe've created an optimized copy of files from Directory to Directory. We've also optimized copyBytes recently. However, we're missing the opposite side of the copy - from IndexInput to Output. I'd like to mimic the FileChannel API by having copyTo on IndexInput and copyFrom on IndexOutput. That way, both sides can optimize the copy process, depending on the type of the IndexInput/Output that they need to copy to/from.

FSIndexInput/Output can use FileChannel if the two are FS types. RAMInput/OutputStream can copy to/from the buffers directly, w/o going through intermediate ones. Actually, for RAMIn/Out this might be a big win, because it doesn't care about the type of IndexInput/Output given - it just needs to copy to its buffer directly.

If we do this, I think we can consolidate all Dir.copy() impls down to one (in Directory), and rely on the In/Out ones to do the optimized copy. Plus, it will enable someone to do optimized copies between In/Out outside the scope of Directory.

If this somehow turns out to be impossible, or won't make sense, then I'd like to optimize RAMDirectory.copy(Dir, src, dest) to not use an intermediate buffer."
0,"FST apis out of sync between trunk/3.xLooks like the offender is LUCENE-3030 :)

Not sure if everything is generally useful but it does change the public API (e.g. you can specify FreezeTail to the super-scary Builder ctor among other things).

Maybe we should sync up for 3.x? "
1,"Range queries fail on large repositoriesAs discussed on the user mailing list, queries on large repositories with date constraints like ""field > constant"" treat the constraint as always true, returning results that should not be returned."
0,"Use POIExtractor wherever possiblePOI scratchpad comes with a couple of text extractor utilities, which makes it easier to extract text. We should rather use those utilities than writing our own extractor code. This helps avoid issues like JCR-1530."
1,"NPE in ObservationManagerImpl.getRegisteredEventListeners() during shutdown after broken startupSee JCR-2378. The variable ""dispatcher"" is passed as null in the constructor."
0,"Performance improvement for TermInfosReaderCurrently we have a bottleneck for multi-term queries: the dictionary lookup is being done
twice for each term. The first time in Similarity.idf(), where searcher.docFreq() is called.
The second time when the posting list is opened (TermDocs or TermPositions).

The dictionary lookup is not cheap, that's why a significant performance improvement is
possible here if we avoid the second lookup. An easy way to do this is to add a small LRU 
cache to TermInfosReader. 

I ran some performance experiments with an LRU cache size of 20, and an mid-size index of
500,000 documents from wikipedia. Here are some test results:

50,000 AND queries with 3 terms each:
old:                  152 secs
new (with LRU cache): 112 secs (26% faster)

50,000 OR queries with 3 terms each:
old:                  175 secs
new (with LRU cache): 133 secs (24% faster)

For bigger indexes this patch will probably have less impact, for smaller once more.

I will attach a patch soon."
1,"Missing sync in IndexWriter.addIndexes(IndexReader[])The 3.x build just hit this:

{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestAddIndexes
    [junit] Testcase: testAddIndexesWithThreads(org.apache.lucene.index.TestAddIndexes):	FAILED
    [junit] expected:<3160> but was:<2701>
    [junit] junit.framework.AssertionFailedError: expected:<3160> but was:<2701>
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:779)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:745)
    [junit] 	at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithThreads(TestAddIndexes.java:708)
    [junit] 
    [junit] 
    [junit] Tests run: 15, Failures: 1, Errors: 0, Time elapsed: 9.28 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] java.lang.AssertionError: RefCount is 0 pre-decrement for file ""_8a.tvf""
    [junit] 	at org.apache.lucene.index.IndexFileDeleter$RefCount.DecRef(IndexFileDeleter.java:608)
    [junit] 	at org.apache.lucene.index.IndexFileDeleter.decRef(IndexFileDeleter.java:505)
    [junit] 	at org.apache.lucene.index.IndexFileDeleter.decRef(IndexFileDeleter.java:496)
    [junit] 	at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:2972)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes.doBody(TestAddIndexes.java:681)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:624)
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithThreads -Dtests.seed=-6912763261803132408:-5575674032550262483 -Dtests.multiplier=3
    [junit] NOTE: test params are: locale=en_AU, timezone=America/Atka
{noformat}

It looks like it's caused by a long-standing missing sync (since at least 2.9.x).  I think likely we hit it just now thanks to adding random Thread.yield()'s in MockDirWrapper!"
1,"Hop 0 sample app doesn't exit because of on-daemon thread pool-1-thread-1When starting the sample app Hop 0 (or any other Hop sample app) if there is no ""repository"" directory, then the application doesn't exit because there is a non-daemon thread named ""pool-1-thread-1""."
0,"Grouping module should allow subclasses to set the group key per documentThe new grouping module can only group by a single-valued indexed field.

But, if we make the 'getGroupKey' a method that a subclass could override, then I think we could refactor Solr over to the module, because it could do function queries and normal queries via subclass (I think).

This also makes the impl more extensible to apps that might have their own interesting group values per document."
1,FALSE predicate always returns trueorg.apache.jackrabbit.spi.commons.iterator.Predicates..FALSE always returns true instead of false.
0,"Nuke SpanFilters and CachingSpanFilter (maybe move to sandbox)SpanFilters are inefficient and OOM easily (they don't scale at all: Create large Lists of Objects for every match, also filtering deleted docs is a pain). Some talks with Grant on Eurocon and also the fact that caching of them is still broken in 3.x (but fixed on trunk) - I assume nobody uses them, so let's nuke them. They are also in wrong package, so standard statement: ""Die, SpanFilters, die!"""
0,"SimilarityDelegator is missing a delegating scorePayload() methodThe handy SimilarityDelegator method is missing a scoreDelegator() delegating method.
The fix is trivial, add the code below at the end of the class:

  public float scorePayload(String fieldName, byte [] payload, int offset, int length)
  {
      return delegee.scorePayload(fieldName, payload, offset, length);
  }
"
0,Return bind variable names on RepositoryService.checkQueryStatement()To properly support JSR 283 bind variables the SPI layer needs to return the names of the bind variables. Otherwise the jcr2spi implementation cannot check for unknown names.
0,"Data store garbage collection: ScanEventListener not workingThe ScanEventListener is currently only called when using the 'scan all nodes recursively' strategy. It is not called when all persistence managers implement IterablePersistenceManager (GarbageCollector.scanPersistenceManagers). The ScanEventListener should be called in every case, otherwise it is not possible to see the progress of the garbage collection.

However there is a problem: IterablePersistenceManager.getAllNodeIds() doesn't return Node objects, and it would make little sense to create real node objects (the performance advantage of scanPersistenceManagers would be lost).

Therefore, I propose a workaround: the ScanEventListener is called using a 'PseudoNode'. This is a class that implements Node but only has meaningful getUUID() and toString() methods. This allows to create a meaningful progress bar (as the UUIDs are returned in order)."
1,"PulsingTermState.clone leaks memoryI looked at the heap dump from the OOME this morning (thank you Uwe
for turning this on!), and I think it's a real memory leak.

Well, not really a leak; rather, the cloned PulsingTermState, which we
cache in the terms dict cache, is hanging onto large byte[]
unnecessarily.
"
0,Automatic upgrade to 2.0Jackrabbit 2.0 contains some changes that are not compatible with repositories created with earlier versions. It would be nice if Jackrabbit would automatically detect and upgrade repositories created with 1.x versions.
1,"Deadlock  on concurrent read & transactional write operationsIsuue has been introduced by resolving JCR-1755 (Transaction-safe versioning). This fixed changed sequence of commits, but at the same time order of acquiring locks has been disturbed.


"
0,"Remove JDOM dependencyProposed by Sylvain Wallez on the dev mailing list.

Replace the JDOM code in the config, nodetype, and xml persistence manager code with equivalent standard DOM code. This change introduces some extra lines of code, but would remove an external dependency and avoid unnecessary deployment problems."
0,Add n-gram tokenizers to contrib/analyzersIt would be nice to have some n-gram-capable tokenizers in contrib/analyzers.  Patch coming shortly.
0,"Add a method public boolean hasNodeType(String name) in NodeTypeManagerImplAs seen in the ML, we plan to add this method and update this class and the interface JackrabbitNodeTypeManager"
0,"'ant javacc' in root project should also properly create contrib/queryparser Java files'ant javacc' in the project root doesn't run javacc in contrib/queryparser
'ant javacc' in contrib/queryparser does not properly create the Java files. What still needs to be done by hand is (partly!) described in contrib/queryparser/README.javacc. I think this process should be automated. Patch provided."
0,Improve Javadoc
0,"Upgrade to Maven 2If you are interested in migrating to maven2 (or adding optional maven 2 build scripts) this is a full maven 2 pom.xml for the main jackrabbit jar.

All the xpath/javacc stuff, previously done in maven.xml, was pretty painfull to reproduce in maven2... the attached pom exactly reproduces the m1 build by using the maven2 javacc plugin + a couple of antrun executions.
Test configuration is not yet complete, I think it will be a lot better to reproduce the previous behaviour (init tests run first) without any customization (maybe using a single junit test suite with setUp tasks). Also custom packaging goals added to maven.xml (that can be esily done in m2 by using the assembly plugin) are not yet reproduced too.

If there is interest, I can also provide poms for the contribution projects (that will be easy, the only complex pom is the main one).
"
1,"IOUtils - getCreated(...) - SimpleDateFormat is not threadsafeSimpleDateFormat is not threadsafe (http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4146524)
See exception in attachment.
IMHO it will be enough to synchronize 'format' method in HttpDateFormat class."
0,RAMDirectory implements SerializableRAMDirectory is for some reason not serializable.
0,"add spanquery support for all multitermqueriesI set fix version: 4.0, but possibly we could do this for 3.x too

Currently, we have a special SpanRegexQuery in contrib, and issues like LUCENE-522 open for SpanFuzzyQuery.
The SpanRegexQuery in contrib is a little messy additionally.

For any arbitrary MultiTermQueries to work as a SpanQuery, there are only 3 requirements:
# The un-rewritten query must extend SpanQuery so it can be included in Span clauses
# The rewritten query should be SpanOrQuery instead of BooleanQuery
# The rewritten term clauses should be SpanTermQueries.

Instead of having logic like this for each query, i suggest adding two rewrite methods:
* ScoringSpanBoolean rewrite
* TopTermsSpanBoolean rewrite

as a start i wrote these up, and added a SpanMultiTermQueryWrapper that can be used to wrap any multitermquery this way.
there are a few kinks, but I think the MTQ policeman can probably help get through them.
"
1,"SnowballFilter loses token position offsetSnowballFilter doesn't set the token position increment (and thus it defaults to 1).
This also affetcs SnowballAnalyzer since it uses SnowballFilter."
0,"Split up IndexInput and IndexOutput into DataInput and DataOutputI'd like to introduce the two new classes DataInput and DataOutput
that contain all methods from IndexInput and IndexOutput that actually
decode or encode data, such as readByte()/writeByte(),
readVInt()/writeVInt().

Methods like getFilePointer(), seek(), close(), etc., which are not
related to data encoding, but to files as input/output source stay in
IndexInput/IndexOutput.

This patch also changes ByteSliceReader/ByteSliceWriter to extend
DataInput/DataOutput. Previously ByteSliceReader implemented the
methods that stay in IndexInput by throwing RuntimeExceptions.

See also LUCENE-2125.

All tests pass."
1,"Cookie.java hashCode method violates contractorg.apache.commons.httpclient.Cookie hashCode() does not meet object.hashCode
() contract.  Cookie.hashCode() returns different values even though data used 
in equals() comparison is the same.

Contract:**Whenever it is invoked on the same object more than once during an 
execution of a Java application, the hashCode method must consistently return 
the same integer, provided no information used in equals comparisons on the 
object is modified.**

Breaks use of cookie within collections such as when using contains().

Traced problem back to parent class NameValuePair.  Cookie.hashCode() calls 
NameValuePair.hashCode() which relies on name/value hashes.  Cookie does not 
rely on value to determine equality."
0,"Implement a way to override or resolve DNS entries defined in the OSWhen working with HttpClient in restrictive environments, where the user doesn't have the permissisions to edit the local /etc/hosts file or the DNS configuration, can be eased with an DNS Overrider capability. 

This can be useful with JMeter which can follow redirects automatically and resolve some of the redirected hosts against its configuration. Another example is a custom forward proxy, written in Java and based on httpclient, which can be deployed is such a restricted environment that would ease the development of various web solutions for some developers. "
0,"test cases relying on Node.equals()Several test cases rely on Node.equals to compare nodes, where instead isSame() should be used:

org.apache.jackrabbit.test.api.NodeTest.testNodeIdentity(NodeTest.java:751)
org.apache.jackrabbit.test.api.NodeTest.testNodeIdentity(NodeTest.java:753)
org.apache.jackrabbit.test.api.version.VersionHistoryTest.testInitallyGetAllVersionsContainsTheRootVersion(VersionHistoryTest.java:126)
org.apache.jackrabbit.test.api.version.VersionHistoryTest.testGetVersion(VersionHistoryTest.java:180)
org.apache.jackrabbit.test.api.version.CheckinTest.testMultipleCheckinHasNoEffect(CheckinTest.java:93)
org.apache.jackrabbit.test.api.version.VersionGraphTest.testInitialBaseVersionPointsToRootVersion(VersionGraphTest.java:47)
org.apache.jackrabbit.test.api.version.RemoveVersionTest.testRemoveVersionAdjustPredecessorSet(RemoveVersionTest.java:120)
org.apache.jackrabbit.test.api.version.RemoveVersionTest.testRemoveVersionAdjustSucessorSet(RemoveVersionTest.java:144)

 "
1,"Concurrent add/remove child node operations in a cluster may corrupt repository.Concurrent add/remove child node operations in a cluster may store an inconsistent list of child node entries, i.e. an entry in the list may appear that has no associated node. This eventually results in an ItemNotFoundException, the next time one of these bogus entries is accessed."
0,"Litmus prophighunicode test failure on JRE 1.5The WebDAV Litmus test suite contains a test case for writing and reading the Unicode character &#x10000; which can't be represented as a single 16-bit char in Java. Instead the character is stored as a surrogate pair of two 16-bit chars. Unfortunately the Xalan XML serializer used by Sun JRE 1.5 incorrectly encodes these as two separate characters in UTF-8, which leads to the following Litmus test failure:

-> running `props':
[...]
17. prophighunicode....... pass
18. propget............... FAIL (PROPFIND on `/default/litmus/prop2': XML parse error at line 1: not well-formed (invalid token))
"
1,"CachingWrapperFilter throws NPE when Filter.getDocIdSet() returns nullFollowup for [http://www.lucidimagination.com/search/document/1014ea92f15677bd/filter_getdocidset_returning_null_and_what_this_means_for_cachingwrapperfilter]:

Daniel Noll is seeing an exception like this:

{noformat}
java.lang.NullPointerException
    at org.apache.lucene.search.CachingWrapperFilter.docIdSetToCache(CachingWrapperFilter.java:84)
    at org.apache.lucene.search.CachingWrapperFilter.getDocIdSet(CachingWrapperFilter.java:112)
    at com.nuix.storage.search.LazyConstantScoreQuery$LazyFilterWrapper.getDocIdSet(SourceFile:91)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.<init>(ConstantScoreQuery.java:116)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:81)
    at org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:297)
    at org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:297)
    at org.apache.lucene.search.QueryWrapperFilter$2.iterator(QueryWrapperFilter.java:75)
{noformat}

The class of our own is just an intermediary which delays creating the Filter object...

{code}
@Override
public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
            if (delegate == null) {
                delegate = factory.createFilter();
            }
            return delegate.getDocIdSet(reader);
}
{code}

Tracing through the code in CachingWrapperFilter, I can see that this NPE would occur if getDocIdSet() were to return null.

The Javadoc on Filter says that null will be returned if no documents will be accepted by the filter, but it doesn't seem that Lucene itself is handling null return values correctly, so which is correct?  The code or the Javadoc?  Supposing that null really is OK, does this cause any problems with how CachingWrapperFilter is implementing the caching?  I notice it's calling get() and then comparing against null so it wouldn't appear that it can distinguish ""the entry isn't in the cache"" from ""the entry is in the cache but it's null""."
0,"Improvement to MultiValueCollectionConverterImpl to Map collections with element class Object.classCurrently MultiValueCollectionConverterImpl  does not support elements of type Object.class.  The type of the contained class has to be specified either through the mapping file or through the Bean annotation.  Even with that flexibility Object.class is specifically excluded (For good reasons.).  

My view is that by definition MultiValueCollectionConverterImpl   should make a best effort to convert and that best effort should include using Undefined UndefinedTypeConverterImpl to convert an object when all the other conversion strategies run out.  To this resolve I have patched the OCM source.  I have test cases also.  I will upload the patch files right after."
1,"TestAddIndexes#testAddIndexesWithThreads fails on RealtimeSelckin reported two failures on LUCENE-3023 which I can unfortunately not reproduce at all. here are the traces

{noformat}
  [junit] Testsuite: org.apache.lucene.index.TestAddIndexes
    [junit] Testcase: testAddIndexesWithThreads(org.apache.lucene.index.TestAddIndexes):	FAILED
    [junit] expected:<3160> but was:<3060>
    [junit] junit.framework.AssertionFailedError: expected:<3160> but was:<3060>
    [junit] 	at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithThreads(TestAddIndexes.java:783)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1226)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1154)
    [junit] 
    [junit] 
    [junit] Tests run: 18, Failures: 1, Errors: 0, Time elapsed: 14.272 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithThreads -Dtests.seed=6128854208955988865:2552774338676281184
    [junit] NOTE: test params are: codec=PreFlex, locale=no_NO_NY, timezone=America/Edmonton
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestToken, TestDateTools, Test2BTerms, TestAddIndexes]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=84731792,total=258080768
    [junit] ------------- ---------------- ---------------
{noformat}
and 
{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestAddIndexes
    [junit] Testcase: testAddIndexesWithThreads(org.apache.lucene.index.TestAddIndexes):	FAILED
    [junit] expected:<3160> but was:<3060>
    [junit] junit.framework.AssertionFailedError: expected:<3160> but was:<3060>
    [junit] 	at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithThreads(TestAddIndexes.java:783)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1226)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1154)
    [junit] 
    [junit] 
    [junit] Tests run: 18, Failures: 1, Errors: 0, Time elapsed: 14.841 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithThreads -Dtests.seed=4502815121171887759:-6764285049309266272
    [junit] NOTE: test params are: codec=PreFlex, locale=tr_TR, timezone=Mexico/BajaNorte
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestToken, TestDateTools, Test2BTerms, TestAddIndexes]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=163663416,total=243335168
    [junit] ------------- ---------------- ---------------
{noformat}"
0,"Explicit management of public APII'd like to start using the Clirr Maven plugin [1] to make sure that we don't accidentally break backwards compatibility in our public APIs, most notably in jackrabbit-api and jackrabbit-jcr-commons.

Also, we should start explicitly managing the API versions exposed as a part of the OSGi package metadata. Currently all our public packages simply get the latest project version as their version number, but it would be better if the version was explicitly managed and only updated if the API actually changes. To do this I propose we use @Version annotations from the bnd tool on the package-info.java files in all packages considered a part of our public API.

The Clirr plugin should flag all changes made in the API, so we have an easy way to tell which packages need to have their version numbers updated.

[1] http://mojo.codehaus.org/clirr-maven-plugin/"
0,"[patch] Support for digest auth MD5-sessI was attempting to access a device that requires Digest authentication using
MD5-sess, which does not seem to be supported."
0,"Add a new TestBackwardsCompatibility index for flex backwards (a 3.0 one with also numeric fields)In flex we change also the encode/decoder for numeric fields (NumericTokenSteam) using BytesRef and also the collation filters. We should add a test index from 3.0 that contains these fields and do some validation, that field contents did not change when read with flex."
0,"hashCode improvementsIt would be nice for all Query classes to implement hashCode and equals to enable them to be used as keys when caching.
"
1,"[httpclient] Incorrect credentials loop infinitelyIf incorrect credentials are assigned to the request, HttpClient will loop 
forever.  It should only try once, and fail with an HttpException if a request 
with credentials set fails.

In org.apache.commons.httpclient.HttpMethodBase.execute(), a check is needed to 
track if credentials have been sent before."
0,"[JCR-RMI] Have ClientItem.isSame throw RepositoryExceptionCurrently the ClientItem.isSame(Item) method wraps a RepositoryException thrown from the path comparison into a RuntimeException and omits an exception declaration on the method. This contrasts with the API specification which allows for a RepositoryExcption to be thrown.

I suggest, to modify ClientItem.isSame(Item) such that the RepositoryException is declared and thrown."
0,JSR 283 Query
1,"WeightedSpanTermExtractor incorrectly treats the same terms occurring in different query typesGiven a BooleanQuery with multiple clauses, if a term occurs both in a Span / Phrase query, and in a TermQuery, the results of term extraction are unpredictable and depend on the order of clauses. Concequently, the result of highlighting are incorrect.

Example text: t1 t2 t3 t4 t2
Example query: t2 t3 ""t1 t2""
Current highlighting: [t1 t2] [t3] t4 t2
Correct highlighting: [t1 t2] [t3] t4 [t2]

The problem comes from the fact that we keep a Map<termText, WeightedSpanTerm>, and if the same term occurs in a Phrase or Span query the resulting WeightedSpanTerm will have a positionSensitive=true, whereas terms added from TermQuery have positionSensitive=false. The end result for this particular term will depend on the order in which the clauses are processed.

My fix is to use a subclass of Map, which on put() always sets the result to the most lax setting, i.e. if we already have a term with positionSensitive=true, and we try to put() a term with positionSensitive=false, we set the result positionSensitive=false, as it will match both cases."
1,upgrade icu jar to 4.8.1.1 / remove lucenetestcase hackThis bug fix release fixes problems with icu under java7: http://bugs.icu-project.org/trac/ticket/8734
1,"When reopen returns a new IndexReader, both IndexReaders may now control the lifecycle of the underlying Directory which is managed by reference countingRough summary. Basically, FSDirectory tracks references to FSDirectory and when IndexReader.reopen shares a Directory with a created IndexReader and closeDirectory is true, FSDirectory's ref management will see two decrements for one increment. You can end up getting an AlreadyClosed exception on the Directory when the IndexReader is open.

I have a test I'll put up. A solution seems fairly straightforward (at least in what needs to be accomplished)."
0,"Add support to provide custom classloader for class instantiation from configurationThe configuration framework is based around a BaseConfig class, which provides functionality to instantiate a class whose name is configured in the repository configuration file. Examples of such classes are the FileSystem or the PersistenceManager elements.

The current implementation of the BeanConfig.newInstance() method is to use the ""default classloader"" to load configured classes. That is, the class loader of the BeanConfig class is actually used. This is - generally - the class loader which loads the repository. In certain environments, classes may be provided from outside the core repository class loader. An example fo such an environment is an OSGi setup where each bundle gets its own class laoder, which is separate from all other class loaders except declared by configuration.

I propose to enhance the BeanConfig class as follows:

public class BeanConfig {
 ...
 // Current default class loader, default is BeanConfig's class loader
 private static ClassLoader defaultClassLoader =
BeanConfig.class.getClassLoader();
 // Current instance class loader
 private ClassLoader classLoader;
 ...
 // Sets the default class loader for new BeanConfig instances
 public static void setDefaultClassLoader(ClassLoader loader);
 // Returns the default class loader for new BeanConfig instances
 public static ClassLoader getClassLoader();
 // Sets the class loader of this BeanConfig instance
 public void setClassLoader(ClassLoader loader);
 // Returns the class loader of this BeanConfig instance
 public ClassLoader getClassLoader();
}

The BeanConfig.newInstance method would then use the following to use the class:

public Object newInstance() throws ConfigurationException {
 Class clazz = Class.forName(getClassName(), true, getClassLoader());
 ...
}


This has also been discussed on the dev list: http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/200607.mbox/%3cae03024e0607272341l52aff9b2h3957131411790bc9@mail.gmail.com%3e"
0,"Reusable Repository access and bind servletsAs discussed in http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/200705.mbox/%3C510143ac0705151453t7a0eb4cam859a40fb106e81f5@mail.gmail.com%3E and JCR-955, it would be useful to have a reusable set of servlet components for accessing and exposing repositories in various configurable ways.

My plan is to refactor the current RepositoryAccessServlet from jackrabbit-webapp and place the resulting servlet components in jackrabbit-jcr-commons, with servlet-api as a new optional (or provided) dependency."
1,"Wrong implementation of DocIdSetIterator.advance Implementations of {{DocIdSetIterator}} behave differently when advanced is called. Taking the following test for {{OpenBitSet}}, {{DocIdBitSet}} and {{SortedVIntList}} only {{SortedVIntList}} passes the test:
{code:title=org.apache.lucene.search.TestDocIdSet.java|borderStyle=solid}
...
	public void testAdvanceWithOpenBitSet() throws IOException {
		DocIdSet idSet = new OpenBitSet( new long[] { 1121 }, 1 );  // bits 0, 5, 6, 10
		assertAdvance( idSet );
	}

	public void testAdvanceDocIdBitSet() throws IOException {
		BitSet bitSet = new BitSet();
		bitSet.set( 0 );
		bitSet.set( 5 );
		bitSet.set( 6 );
		bitSet.set( 10 );
		DocIdSet idSet = new DocIdBitSet(bitSet);
		assertAdvance( idSet );
	}

	public void testAdvanceWithSortedVIntList() throws IOException {
		DocIdSet idSet = new SortedVIntList( 0, 5, 6, 10 );
		assertAdvance( idSet );
	}	

	private void assertAdvance(DocIdSet idSet) throws IOException {
		DocIdSetIterator iter = idSet.iterator();
		int docId = iter.nextDoc();
		assertEquals( ""First doc id should be 0"", 0, docId );

		docId = iter.nextDoc();
		assertEquals( ""Second doc id should be 5"", 5, docId );

		docId = iter.advance( 5 );
		assertEquals( ""Advancing iterator should return the next doc id"", 6, docId );
	}
{code}

The javadoc for {{advance}} says:
{quote}
Advances to the first *beyond* the current whose document number is greater than or equal to _target_.
{quote}
This seems to indicate that {{SortedVIntList}} behaves correctly, whereas the other two don't. 
Just looking at the {{DocIdBitSet}} implementation advance is implemented as:
{code}
bitSet.nextSetBit(target);
{code}
where the docs of {{nextSetBit}} say:
{quote}
Returns the index of the first bit that is set to true that occurs *on or after* the specified starting index
{quote}
"
0,Add parser callback to GQLThe parsing of GQL is currently hidden in the implementation. It would be nice to have a mechanism that allows client code to get callbacks whenever a field/value pair is parsed.
0,"provide a (relatively) simple way to disable anonymous access to the security workspaceAs discussed in this thread: http://sling.markmail.org/thread/st52jejjuxykfxtj, the security workspace is, by default, configured with an AccessControlProvider which provides a fixed access control policy (i.e. o.a.j.core.security.user.UserAccessControlProvider). In order to prevent anonymous access to security-related nodes requires the use of an alternate AccessControlProvider.

The attached patch provides a simpler mechanism. By adding

<param name=""anonymousAccessToSecurityWorkspace"" value=""false"" />

to the configuration of the DefaultSecurityManager, anonymous access to the security workspace is forbidden.
"
0,AbstractRepositoryService should be able to handle GuestCredentialsAbstractRepositoryService.createSessionInfo should handle GuestCredentials. Currently it only handle SimpleCredentials
0,IndexReader.listCommits should return a List and not an abstract CollectionSpinoff from here: http://www.mail-archive.com/dev@lucene.apache.org/msg07509.html
1,toString() causes StackOverflowErrorfurther regressions of JCR-2763...
0,"reorganize xdocs and websitei would like to propose a change to the website and xdocs structure.

the current structure below seems like it could use a brush up.

---
Overview
Architecture Doc
.Overview
..JSR-170 Levels
.Deployment Models
..Application HOWTO
..Model 1 HOWTO
..Model 2 HOWTO
..Model 3 HOWTO
.Core Operations
..Start-up, Initialize
..QueryManager Implementation
First Steps
JCR API Documentation
Layout
Downloads
FAQ
---

i would propose the following, instead:
---
About 
Documentation
.First Steps
.JCR
.API Documentation
.Jackrabbit Architecture
..Start-up, Initialize
..QueryManager Implementation
.Deployment Models
..Application HOWTO
..Model 1 HOWTO
..Model 2 HOWTO
..Model 3 HOWTO
.Nodetype Modelling *new*
Layout
Downloads
FAQ
---

also i would like to introduce a new ""homepage"" with a little bit more attractive content like jackrabbit
news, maybe featured jackrabbit applications, schedules and events.

ideally i would like to re-organize the file structure according to the navigation, which may break
bookmarks and search indexes.

thoughts?"
1,"Indexing configuration not refreshed after node type registrationThe indexing configuration has internal caches that speed up node type matches. Those caches are not updated on new node type registration and newly registered node types are not properly resolved when index-rules are checked.

See also test case in attached patch."
1,"Invalid behavior of StandardTokenizerImplThe following code prints the output of StandardAnalyzer:

        Analyzer analyzer = new StandardAnalyzer();
        TokenStream ts = analyzer.tokenStream(""content"", new StringReader(""<some text>""));
        Token t;
        while ((t = ts.next()) != null) {
            System.out.println(t);
        }

If you pass ""www.abc.com"", the output is (www.abc.com,0,11,type=<HOST>) (which is correct in my opinion).
However, if you pass ""www.abc.com."" (notice the extra '.' at the end), the output is (wwwabccom,0,12,type=<ACRONYM>).

I think the behavior in the second case is incorrect for several reasons:
1. It recognizes the string incorrectly (no argue on that).
2. It kind of prevents you from putting URLs at the end of a sentence, which is perfectly legal.
3. An ACRONYM, at least to the best of my understanding, is of the form A.B.C. and not ABC.DEF.

I looked at StandardTokenizerImpl.jflex and I think the problem comes from this definition:
// acronyms: U.S.A., I.B.M., etc.
// use a post-filter to remove dots
ACRONYM    =  {ALPHA} ""."" ({ALPHA} ""."")+

Notice how the comment relates to acronym as U.S.A., I.B.M. and not something else. I changed the definition to
ACRONYM    =  {LETTER} ""."" ({LETTER} ""."")+
and it solved the problem.

This was also reported here:
http://www.nabble.com/Inconsistent-StandardTokenizer-behaviour-tf596059.html#a1593383
http://www.nabble.com/Standard-Analyzer---Host-and-Acronym-tf3620533.html#a10109926
"
0,"TestExcetions never runIn one of the testcases of HttpClient, TestExcetions, it reads:

    // ------------------------------------------------------------------- Main
    public static void main(String args[]) {
        String[] testCaseName = { TestChallengeParser.class.getName() };
        junit.textui.TestRunner.main(testCaseName);
    }

    // ------------------------------------------------------- TestCase Methods

    public static Test suite() {
        return new TestSuite(TestChallengeParser.class);
    }

Where ""TestChallengeParser"" should be ""TestExcetions""."
1,"Static variables need to be final (or access should be synchronised):Static variables need to be final (or access should be synchronised):

Index: module-client/src/main/java/org/apache/http/conn/params/HttpConnectionManagerParams.java
===================================================================
--- module-client/src/main/java/org/apache/http/conn/params/HttpConnectionManagerParams.java	(revision 652021)
+++ module-client/src/main/java/org/apache/http/conn/params/HttpConnectionManagerParams.java	(working copy)
@@ -53,7 +53,7 @@
     public static final int DEFAULT_MAX_TOTAL_CONNECTIONS = 20;
 
     /** The default maximum number of connections allowed per host */
-    private static ConnPerRoute DEFAULT_CONN_PER_ROUTE = new ConnPerRoute() {
+    private static final ConnPerRoute DEFAULT_CONN_PER_ROUTE = new ConnPerRoute() {
         
         public int getMaxForRoute(HttpRoute route) {
             return ConnPerRouteBean.DEFAULT_MAX_CONNECTIONS_PER_ROUTE;
"
1,"IndexReader overwrites future commits when you open it on a past commitHit this on trying to build up a test index for perf testing...

IndexReader (and Writer) accept an IndexCommit on open.

This is quite powerful, because, if you use a deletion policy that keeps multiple commits around, you can open a not-current commit, make some changes, write a new commit, all without altering the ""future"" commits.

I use this to first build up a big wikipedia index, including one commit w/ multiple segments, then another commit after optimize(), and then I open an writable IR to perform deletions off of both those commits.  This gives me a single test index that has all four combinations (single vs multi segment; deletions vs no deletions).

But IndexReader has a bug whereby it overwrites the segments_N file.  (IndexWriter works correctly)."
0,"add db connection autoConnect for BundleDbPersistenceManager.Since bundled db pm doesn't inherited from database pm, it can't reconnect once database is bounced. it would be nice to add this feature. "
0,"Node type documentation tool (NTDoc)This is the first time I post a contrib here on Jira. Hope I do this the right way :-) 

Some weeks ago I postet a message on the forum about a node type documentation tool I had made. Now, finally I have cleaned up the code and fixed some bugs. It is now useful for the majority out there. I do not guarantee it to be bug-free, but will do my best to fix any bugs that is reported. 

A readme file is included in the distribution. Must build using maven 2. Have not done it maven 1 compliant."
0,Replace TrackingInpuStream with Commons IOThe TrackingInputStream class in jackrabbit-core implements essentially the same functionality as the Commons IO class CountingInputStream.
1,Background text extraction not possible when supportHighlighting is set trueThere is an IndexingQueue that holds nodes that are indexed with a background thread when text extraction takes more time than a configurable limit. When supportHighlighting is set to true the IndexingQueue is never used because the text extract is immediately requested in NodeIndexer. Instead the text extract should be retrieved only when the node is added to the index. 
1,"Typo in the DeltaVConstants class in constant XML_CHECKOUT_CHECKIN valueJust spotted a typo in the http://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-webdav/src/main/java/org/apache/jackrabbit/webdav/version/DeltaVConstants.java
(same is in released 1.4 version)

There's line
    public static final String XML_CHECKOUT_CHECKIN = ""checkin-checkout"";
Probably should be
    public static final String XML_CHECKOUT_CHECKIN = ""checkout-checkin"";
"
0,"Add a variable-sized int block codecWe already have support for fixed block size int block codecs, making it very simple to create a codec from a int encoder algorithms like FOR/PFOR.

But algorithms like Simple9/16 are not fixed -- they encode a variable number of adjacent ints at once, depending on the specific values of those ints."
0,"investigate solr test failures using flexWe have a branch of Solr located here: https://svn.apache.org/repos/asf/lucene/solr/branches/solr

Currently all the tests pass with lucene trunk jars.

I plopped in the flex jars and they do not, so I thought these might be interesting to look at.
"
0,"BaseTestRangeFilter can be extremely slowThe tests that extend BaseTestRangeFilter can sometimes be very slow:
TestFieldCacheRangeFilter, TestMultiTermConstantScore, TestTermRangeFilter

for example, TestFieldCacheRangeFilter just ran for 10 minutes on my computer before I killed it,
but i noticed these tests frequently run for over a minute.

I think at the least we should change these to junit4 so the index is built once in @beforeClass"
1,"Repository lock file is not removed on shutdownThe repository lock file is not removed when Jackrabbit runs on a windows platform:

*ERROR* [Thread-4] RepositoryImpl: Unable to release repository lock (RepositoryImpl.java, line 283)

I assume this problem does not occur on unix based platforms, because they allow to delete a file while another process still uses it."
1,"Duplicate key in DatabasePersistenceManagerHi,

I ran into the exception pasted below. We had 2 threads that both were saving. Maybe it is a race condition?  

Regards,

Martijn Hendriks
<GX> creative online development B.V.
 
t: 024 - 3888 261
f: 024 - 3888 621
e: martijnh@gx.nl
 
Wijchenseweg 111
6538 SW Nijmegen
http://www.gx.nl/ 


Jan 26, 2007 2:23:36 PM org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager store
SEVERE: failed to write property state: e3847bad-f1ee-4adb-a109-e134900935b7/{http://gx.nl}edit_language
ERROR 23505: The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique in dex identified by 'DEFAULT_PROP_IDX' defined on 'DEFAULT_PROP'.
        at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
        at org.apache.derby.impl.sql.execute.IndexChanger.insertAndCheckDups(Unknown Source)
        at org.apache.derby.impl.sql.execute.IndexChanger.doInsert(Unknown Source)
        at org.apache.derby.impl.sql.execute.IndexChanger.insert(Unknown Source)
        at org.apache.derby.impl.sql.execute.IndexSetChanger.insert(Unknown Source)
        at org.apache.derby.impl.sql.execute.RowChangerImpl.insertRow(Unknown Source)
        at org.apache.derby.impl.sql.execute.InsertResultSet.normalInsertCore(Unknown Source)
        at org.apache.derby.impl.sql.execute.InsertResultSet.open(Unknown Source)
        at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
        at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
        at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(Unknown Source)
        at org.apache.derby.impl.jdbc.EmbedPreparedStatement.execute(Unknown Source)
        at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.executeStmt(DatabasePersistenceManager.java:835)
        at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:466)
        at org.apache.jackrabbit.core.persistence.AbstractPersistenceManager.store(AbstractPersistenceManager.java:75)
        at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:274)
        at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:675)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:808)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
        at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:302)
        at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:295)
        at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1210)
"
1,"Unable to create repository using jackrabbit-webapp because a directory called ""jackrabbit"" already existsI mount the jackrabbit-webapp.war in a Jetty installation
* at startup i have the following exception:
ERROR RepositoryStartupServlet: Either create thejackrabbit/bootstrap.properties file or
ERROR RepositoryStartupServlet: use the '/config/index.jsp' for easy configuration.
ERROR RepositoryStartupServlet: RepositoryStartupServlet initializing failed: javax.servlet.ServletException: Repository startup configuration is not valid.
* then when i access http://localhost:8080/ i am forwarded to the page:
 http://localhost:8080/bootstrap/missing.jsp
* creating the repository by clicking on ""Create Content Repository"" button fails complaining that the jackrabbit directory already exists

Indeed, i find a jackrabbit directory in my JETTY_HOME (from where is started Jetty).

A workaround is to delete this ""jackrabbit"" directory and then i can create the repository by clicking on the previous button and therefore access the newly created repository."
0,"Minor typo in org.apache.commons.httpclient.Wire 2.0-rc1Minor typo ""...may noy be null"" in 
public static final void output(final String s) and
public static final void input(final String s)
of org.apache.commons.httpclient.Wire."
0,"jcr-commons: add cnd writer functionalitycurrently jcr-commons only provides an cnd-reader while the writer functionality is only present in spi-commons.
for JCR-2948 a implementation independent cnd-writer would be useful and i would therefore suggest to
add this to jcr-commons based on the code present in spi-commons and let the implementation in spi-commons
extend from the general functionality."
0,"The Field ctors that take byte[] shouldn't take Store, since it must be YESAPI silliness.  Makes you think you can set Store.NO for binary fields.  This used to be meaningful when we also accepted COMPRESS, but now it's an orphan."
0,"increase maxmemory for unit testsWe have some unit tests that require a fair amount of RAM.  But, sometimes the JRE does not give itself a very large max heap size, by default.  EG on a Mac Pro with 6 GB physical RAM, I see JRE 1.6.0 defaulting to max 80 GB and it always then hits this exception during testing:

    [junit] Testcase: testHugeFile(org.apache.lucene.store.TestHugeRamFile):	Caused an ERROR
    [junit] Java heap space
    [junit] java.lang.OutOfMemoryError: Java heap space
    [junit] 	at java.util.Arrays.copyOf(Arrays.java:2760)
    [junit] 	at java.util.Arrays.copyOf(Arrays.java:2734)
    [junit] 	at java.util.ArrayList.ensureCapacity(ArrayList.java:167)
    [junit] 	at java.util.ArrayList.add(ArrayList.java:351)
    [junit] 	at org.apache.lucene.store.RAMFile.addBuffer(RAMFile.java:69)
    [junit] 	at org.apache.lucene.store.RAMOutputStream.switchCurrentBuffer(RAMOutputStream.java:129)
    [junit] 	at org.apache.lucene.store.RAMOutputStream.writeBytes(RAMOutputStream.java:115)
    [junit] 	at org.apache.lucene.store.TestHugeRamFile.testHugeFile(TestHugeRamFile.java:68)

The fix is simple: add maxmemory=512M into common-build.xml.  I'll commit shortly."
1,"[PATCH] Error in GermanStemmer.java,v 1.11GermanStemmer.java,v 1.11 in  lucene-1.4-final
 at the end of a word is not replaced by ss"
0,dists include analyzer contrib in src dist but not binary distdists include analyzer contrib in src dist but not binary dist
0,"BlockJoinQuery/CollectorI created a single-pass Query + Collector to implement nested docs.
The approach is similar to LUCENE-2454, in that the app must index
documents in ""join order"", as a block (IW.add/updateDocuments), with
the parent doc at the end of the block, except that this impl is one
pass.

Once you join at indexing time, you can take any query that matches
child docs and join it up to the parent docID space, using
BlockJoinQuery.  You then use BlockJoinCollector, which sorts parent
docs by provided Sort, to gather results, grouped by parent; this
collector finds any BlockJoinQuerys (using Scorer.visitScorers) and
retains the child docs corresponding to each collected parent doc.

After searching is done, you retrieve the TopGroups from a provided
BlockJoinQuery.

Like LUCENE-2454, this is less general than the arbitrary joins in
Solr (SOLR-2272) or parent/child from ElasticSearch
(https://github.com/elasticsearch/elasticsearch/issues/553), since you
must do the join at indexing time as a doc block, but it should be
able to handle nested joins as well as joins to multiple tables,
though I don't yet have test cases for these.

I put this in a new Join module (modules/join); I think as we
refactor join impls we should put them here.
"
1,"SpellChecker file descriptor leak - no way to close the IndexSearcher used by SpellChecker internallyI can't find any way to close the IndexSearcher (and IndexReader) that
is being used by SpellChecker internally.

I've worked around this issue by keeping a single SpellChecker open
for each index, but I'd really like to be able to close it and
reopen it on demand without leaking file descriptors.

Could we add a close() method to SpellChecker that will close the
IndexSearcher and null the reference to it? And perhaps add some code
that reopens the searcher if the reference to it is null? Or would
that break thread safety of SpellChecker?

The attached patch adds a close method but leaves it to the user to
call setSpellIndex to reopen the searcher if desired."
0,"Implement RepositoryFactory in jcr2davIt's currently a bit cumbersome to set up a spi2dav instance because of the two levels of factories (RepositoryFactory & RepositoryServiceFactory) involved in the process. It would be easier if spi2dav implemented RepositoryFactory directly, so downstream users would only need to provide the server URI parameter instead of specifying also the RepositoryServiceFactory classname.

To do this, spi2dav would need to depend also on jcr2spi. This change would actually simplify downstream projects, that then wouldn't need to depend also to jcr2spi to get JCR -> DAV connectivity."
0,"URI class constructors need revision, optimization1. Currently there's not way to pass an escaped string as a parameter to URI
class. As a result the url parameter in HttpMethodBase#HttpMethodBase(String)
constructor gets converted into an array of char just to be converted back to
string in URI contructor called in that method. 

2. The overall design of URI class contructors does not appear very coherent (at
least to me)"
0,"Changes.html generation improvementsBug fixes for and improvements to changes2html.pl, which generates Changes.html from CHANGES.txt:

# When the current location has a fragment identifier, expand parent sections, so that the linked-to section is visible.
# Properly handle beginning-of-release comments that don't fall under a section heading (previously: some content in release ""1.9 final"" was invisible).
# Auto-linkify SOLR-XXX and INFRA-XXX JIRA issues (previously: only LUCENE-XXX issues).
# Auto-linkify Bugzilla bugs prefaced with ""Issue"" (previously: only ""Bug"" and ""Patch"").
# Auto-linkify Bugzilla bugs in the form ""bugs XXXXX and YYYYY"".
# Auto-linkify issues that follow attributions.
"
0,"XML text extraction in Jackrabbit 1.x accesses external resourcesAs discussed on users@, we should add the following code to the ExtractorHandler class:

   public InputSource resolveEntity(String publicId, String systemId) {
       return new InputSource(new ByteArrayInputStream(new byte[0]));
   }
"
1,"Intermittent thread safety issue with EnwikiDocMakerIntermittent thread safety issue with EnwikiDocMaker

When I run the conf/wikipediaOneRound.alg, sometimes it gets started
OK, other times (about 1/3rd the time) I see this:

     Exception in thread ""Thread-0"" java.lang.RuntimeException: java.io.IOException: Bad file descriptor
     	at org.apache.lucene.benchmark.byTask.feeds.EnwikiDocMaker$Parser.run(EnwikiDocMaker.java:76)
     	at java.lang.Thread.run(Thread.java:595)
     Caused by: java.io.IOException: Bad file descriptor
     	at java.io.FileInputStream.readBytes(Native Method)
     	at java.io.FileInputStream.read(FileInputStream.java:194)
     	at org.apache.xerces.impl.XMLEntityManager$RewindableInputStream.read(Unknown Source)
     	at org.apache.xerces.impl.io.UTF8Reader.read(Unknown Source)
     	at org.apache.xerces.impl.XMLEntityScanner.load(Unknown Source)
     	at org.apache.xerces.impl.XMLEntityScanner.scanQName(Unknown Source)
     	at org.apache.xerces.impl.XMLNSDocumentScannerImpl.scanStartElement(Unknown Source)
     	at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(Unknown Source)
     	at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source)
     	at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
     	at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
     	at org.apache.xerces.parsers.XMLParser.parse(Unknown Source)
     	at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)
     	at org.apache.lucene.benchmark.byTask.feeds.EnwikiDocMaker$Parser.run(EnwikiDocMaker.java:60)
     	... 1 more

The problem is that the thread that pulls the XML docs is started as
soon as EnwikiDocMaker class is instantiated.  When it's started, it
uses the fileIS (FileInputStream) to feed the XML Parser.  But,
openFile is actually called twice on starting the alg, if you use any
task deriving from ResetInputsTask, which closes the original fileIS
that the XML parser may be using.

I changed the thread to instead start on-demand the first time next()
is called.  I also removed a redundant resetInputs() call (which was
opening the file more frequently than needed).  Finally, I added logic
in the thread to detect that the input stream was closed (because
LineDocMaker.resetInputs() was called, eg, if we are not running the
doc maker to exhaustion).

"
0,"Some improvements to contrib/benchmarkI've made some small improvements to the contrib/benchmark, mostly
merging in the ad-hoc benchmarking code I've been using in LUCENE-843:

  - Fixed thread safety of DirDocMaker's usage of SimpleDateFormat

  - Print the props in sorted order

  - Added new config ""autocommit=true|false"" to CreateIndexTask

  - Added new config ""ram.flush.mb=int"" to AddDocTask

  - Added new configs ""doc.term.vector.positions=true|false"" and
    ""doc.term.vector.offsets=true|false"" to BasicDocMaker

  - Added WriteLineDocTask.java, so you can make an alg that uses this
    to build up a single file containing one document per line in a
    single file.  EG this alg converts the reuters-out tree into a
    single file that has ~1000 bytes per body field, saved to
    work/reuters.1000.txt:

      docs.dir=reuters-out
      doc.maker=org.apache.lucene.benchmark.byTask.feeds.DirDocMaker
      line.file.out=work/reuters.1000.txt
      doc.maker.forever=false
      {WriteLineDoc(1000)}: *

    Each line has tab-separted TITLE, DATE, BODY fields.

  - Created feeds/LineDocMaker.java that creates documents read from
    the file created by WriteLineDocTask.java.  EG this alg indexes
    all documents created above:

      analyzer=org.apache.lucene.analysis.SimpleAnalyzer
      directory=FSDirectory
      doc.add.log.step=500

      docs.file=work/reuters.1000.txt
      doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker
      doc.tokenized=true
      doc.maker.forever=false

      ResetSystemErase
      CreateIndex
      {AddDoc}: *
      CloseIndex

      RepSumByPref AddDoc

I'll attach initial patch shortly.
"
0,"Upgrade to Tika 0.6 and PDFBox 1.0.0Tika version 0.6 uses POI 3.6 that's notably smaller (-10MB!) than previous versions. There are also a number of other improvements in Tika 0.6 since the 0.5 release.

While doing the upgrade we should also force the PDFBox version to 1.0.0 from the 0.8.0-incubating version that Tika 0.6 uses. PDFBox 1.0.0 has some nice performance gains (around 30% faster) to text extraction along with other improvements."
0,"Remove unused (and untested) methods from ReaderUtil that are also veeeeery ineffectiveReaderUtil contains two methods that are nowhere used and not even tested. Additionally those are implemented with useless List->array copying; ineffective docStart calculation for a binary search later instead directly returning the reader while scanning -- and I am not sure if they really work as expected. As ReaderUtil is @lucene.internal we should remove them in 3.x and trunk, alternatively the useless array copy / docStarts handling should be removed and tests added:

{code:java}
public static IndexReader subReader(int doc, IndexReader reader)
public static IndexReader subReader(IndexReader reader, int subIndex)
{code}
"
0,"[PATCH] Allow RepositoryAccessServlet to get the Repository from a ServletContext attributeThe attached patch adds a repository.context.attribute.name init parameter to the RepositoryAccessServlet:

        <init-param>
          <param-name>repository.context.attribute.name</param-name>
          <param-value>javax.jcr.Repository</param-value>
          <description>
            If this is set, the RepositoryAccessServlet expects a Repository in the ServletContext 
            attribute having this name. This allows servlets of this module to be used with repositories
            intialized by the jackrabbit-jcr-servlet module utilities.
          </description>
        </init-param>"
0,Add customizable filtering to GQLCurrently GQL is not very flexible because it does not have any hooks that  allows you to modify the query that gets generated from the GQL syntax. As a first step I'd like to introduce a filtering mechanism that can be used to post process the result set and exclude certain rows. This is useful when you cannot express an application constraint in GQL.
0,"outdated information in Analyzer javadocI'm sure you find more ways to improve the javadoc, so feel free to change and extend my patch."
1,"CheckIndex incorrectly sees deletes as index corruptionThere is a silly bug in CheckIndex whereby any segment with deletes is
considered corrupt.

Thanks to Bogdan Ghidireac for reporting this."
1,"TestFSTs.testRandomWords throws AIOBE when ""verbose""=trueSeems like invalid utf-8 sometimes gets passed to Bytesref.utf8ToString() in the verbose ""println""s."
1,ArrayIndexOutOfBoundsException during indexinghttp://search.lucidimagination.com/search/document/f29fc52348ab9b63/arrayindexoutofboundsexception_during_indexing
1,"Persistence data of versioning not cleaned up correctlywhen deleting a version or removing its label, the respective persistence data is not always properly removed."
1,"ClassCastException when updating properties using WebDAVWhen issuing PROPPATCH commands, a ClassCastException is raised.

e.g. 

PROPPATCH /jackrabbit-webapp-1.4/repository/default/test/test_file_v.txt HTTP/1.1
Host: localhost:9000
Connection: TE
TE: trailers, deflate, gzip, compress
User-Agent: UCI DAV Explorer/0.91 RPT-HTTPClient/0.3-3E
Translate: f
Authorization: Basic Y3Jvc3NqYTp0ZXN0
Accept-Encoding: deflate, gzip, x-gzip, compress, x-compress
Content-type: text/xml
Content-length: 170

<A:propertyupdate xmlns:A=""DAV:"">
<A:set>
<A:prop>
<A:auto-version>checkout-checkin</A:auto-version>
</A:prop>
</A:set>
</A:propertyupdate>


results in



24.01.2008 15:38:34 *ERROR* [Webdav]: Servlet.service() for servlet Webdav threw
 exception (StandardWrapperValve.java, line 257)
java.lang.ClassCastException: org.apache.jackrabbit.webdav.property.DefaultDavPr
operty
        at org.apache.jackrabbit.webdav.simple.DavResourceImpl.alterProperties(D
avResourceImpl.java:456)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doPropPatch
(AbstractWebdavServlet.java:457)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(Abs
tractWebdavServlet.java:234)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(Abs
tractWebdavServlet.java:192)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:803)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(Appl
icationFilterChain.java:269)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationF
ilterChain.java:188)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperV
alve.java:210)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextV
alve.java:174)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.j
ava:127)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.j
ava:117)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineVal
ve.java:108)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.jav
a:151)
        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java
:870)
        at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.p
rocessConnection(Http11BaseProtocol.java:665)
        at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpo
int.java:528)
        at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFol
lowerWorkerThread.java:81)
        at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadP
ool.java:685)
        at java.lang.Thread.run(Thread.java:595)
24.01.2008 15:38:34 *ERROR* [Webdav]: Servlet.service() for servlet Webdav threw
 exception (SLF4JLocationAwareLog.java, line 174)
java.lang.ClassCastException: org.apache.jackrabbit.webdav.property.DefaultDavPr
operty
        at org.apache.jackrabbit.webdav.simple.DavResourceImpl.alterProperties(D
avResourceImpl.java:456)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doPropPatch
(AbstractWebdavServlet.java:457)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(Abs
tractWebdavServlet.java:234)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(Abs
tractWebdavServlet.java:192)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:803)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(Appl
icationFilterChain.java:269)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationF
ilterChain.java:188)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperV
alve.java:210)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextV
alve.java:174)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.j
ava:127)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.j
ava:117)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineVal
ve.java:108)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.jav
a:151)
        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java
:870)
        at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.p
rocessConnection(Http11BaseProtocol.java:665)
        at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpo
int.java:528)
        at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFol
lowerWorkerThread.java:81)
        at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadP
ool.java:685)
        at java.lang.Thread.run(Thread.java:595)
24.01.2008 15:53:54 *ERROR* [Webdav]: Servlet.service() for servlet Webdav threw
 exception (StandardWrapperValve.java, line 257)
java.lang.ClassCastException: org.apache.jackrabbit.webdav.property.DefaultDavPr
operty
        at org.apache.jackrabbit.webdav.simple.DavResourceImpl.alterProperties(D
avResourceImpl.java:456)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doPropPatch
(AbstractWebdavServlet.java:457)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(Abs
tractWebdavServlet.java:234)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(Abs
tractWebdavServlet.java:192)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:803)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(Appl
icationFilterChain.java:269)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationF
ilterChain.java:188)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperV
alve.java:210)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextV
alve.java:174)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.j
ava:127)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.j
ava:117)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineVal
ve.java:108)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.jav
a:151)
        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java
:870)
        at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.p
rocessConnection(Http11BaseProtocol.java:665)
        at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpo
int.java:528)
        at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFol
lowerWorkerThread.java:81)
        at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadP
ool.java:685)
        at java.lang.Thread.run(Thread.java:595)
24.01.2008 15:53:54 *ERROR* [Webdav]: Servlet.service() for servlet Webdav threw
 exception (SLF4JLocationAwareLog.java, line 174)
java.lang.ClassCastException: org.apache.jackrabbit.webdav.property.DefaultDavPr
operty
        at org.apache.jackrabbit.webdav.simple.DavResourceImpl.alterProperties(D
avResourceImpl.java:456)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doPropPatch
(AbstractWebdavServlet.java:457)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(Abs
tractWebdavServlet.java:234)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(Abs
tractWebdavServlet.java:192)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:803)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(Appl
icationFilterChain.java:269)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationF
ilterChain.java:188)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperV
alve.java:210)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextV
alve.java:174)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.j
ava:127)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.j
ava:117)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineVal
ve.java:108)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.jav
a:151)
        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java
:870)
        at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.p
rocessConnection(Http11BaseProtocol.java:665)
        at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpo
int.java:528)
        at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFol
lowerWorkerThread.java:81)
        at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadP
ool.java:685)
        at java.lang.Thread.run(Thread.java:595) "
0,"Improve how ConcurrentMergeScheduler handles too-many-merges caseCMS now lets you set ""maxMergeThreads"" to control max # simultaneous
merges.

However, when CMS hits that max, it still allows further merges to
run, by running them in the foreground thread.  So if you set this max
to 1, and use 1 thread to add docs, you can get 2 merges running at
once (which I think is broken).

I think, instead, CMS should pause the foreground thread, waiting
until the number of merge threads drops below the limit.  Then, kick
off the backlog merge in a thread and return control back to primary
thread.
"
0,Publish the jackrabbit-ocm DTDThe jackrabbit-ocm DTD from jackrabbit-ocm/src/dtd should be made available for reference on the Jackrabbit web site.
1,webapp doesn't compile (use of enum keyword)AbstractConfig.java and JNDIConfig.java have local variables named 'enum' that aren't allowed when using JDK5 or later compilers.
0,"HttpClient OSGi Export-Package doesn't specify versionThe ""Export-Package"" manifest entry doesn't specify the version of the package being exported.  This means that packages importing it can't specify a version to import."
0,"waitForResponse is using busy waitIn HttpConnection, the method waitForResponse is using busywait, instead of 
blocking until the response is arriving.

Is this on purpose, or shouldn't it handle this by blocking instead ??"
1,"jcr2spi: Unprocessed ItemInfos call to RepositoryService#getItemInfosstefan reported the following problem:

- batchread config reads with depths infinity
- invalidate tree by calling Node.refresh(false)
- force loading of the tree (e.g. Node.getPath())

afterwards, there may still be invalidated item states indicating that not all ItemInfos were processed.
consequently, there are additional calls to getItemInfos that should have been covered by the loading of the tree.
the problem occuring is not related to limitation of the item-cache size.

problem analysis:

there is a bug in WorkspaceItemStateFactory#createItemStates.
there is a wrapper built around the ItemInfo-Iterator but later on the ItemInfo-Iterator is used instead of the wrapper, which pre-fetches items from the underlying iterator and process them upon hasNext()/next()."
0,"SPI: Get rid of unused method ItemInfo.getParentId()Looking at the various SPI impls in the trunk and in the sandbox reveals that ItemInfo.getParentId is not used at all.
I'd like to suggest to get rid of that method.

Any objections/concerns?
angela

"
0,"Improve reading of cached UUID for given document numberCachingIndexReader.document(int n, FieldSelector fieldSelector) creates a new
Field from the cached UUID. The lucene Field implementation always does a
String.intern() on the field name, which is quite slow. We should probably have
our own implementation for that specific use case where we know that the name
is already interned. e.g. UUIDField implements Fieldable."
0,"Parallelize TestsThe Lucene tests can be parallelized to make for a faster testing system.  

This task from ANT can be used: http://ant.apache.org/manual/CoreTasks/parallel.html

Previous discussion: http://www.gossamer-threads.com/lists/lucene/java-dev/69669

Notes from Mike M.:
{quote}
I'd love to see a clean solution here (the tests are embarrassingly
parallelizable, and we all have machines with good concurrency these
days)... I have a rather hacked up solution now, that uses
""-Dtestpackage=XXX"" to split the tests up.

Ideally I would be able to say ""use N threads"" and it'd do the right
thing... like the -j flag to make.
{quote}"
0,"review TSCCM for spurious wakeupsReview the code of the TSCCM/ConnPoolByRoute for places where spurious wakeups may happen.
Verify that this case is dealt with correctly. Unit test by giving invalid wakeup signals?"
1,"Creating and saving a mix:versionable node creates two VersionHistory nodesSteps:
   - Create a new mix:versionable node
      [ This creates a new VersionHistory node below jcr:persistentVersionStore
        and sets the new node's versionHistory property to the UUID of this
        VersionHistory node. ]
   - Save the session (or alternatively save the parent of the new node)
      [ This creates a new VersionHistory node below jcr:persistentVersionStore
        and sets the node's versionHistory property to the UUID of this
        VersionHistory node. ]

As you can see, you end up with two VersionHistory nodes for the same node, of which the first VersionHistory node is never used again, because the second VersionHistory node is used from now on."
0,Improve FileRevision extensibilityIt'd be nice to make FileRevision more extensible by chaning some of its private variables to protected so it can be extended easier when needed.
1,"CJKTokenizer convert   HALFWIDTH_AND_FULLWIDTH_FORMS wrongCJKTokenizer have these lines..
                if (ub == Character.UnicodeBlock.HALFWIDTH_AND_FULLWIDTH_FORMS) {
                    /** convert  HALFWIDTH_AND_FULLWIDTH_FORMS to BASIC_LATIN */
                    int i = (int) c;
                    i = i - 65248;
                    c = (char) i;
                }

This is wrong. Some character in the block (e.g. U+ff68) have no BASIC_LATIN counterparts.
Only 65281-65374 can be converted this way.

The fix is

             if (ub == Character.UnicodeBlock.HALFWIDTH_AND_FULLWIDTH_FORMS && i <= 65474 && i> 65281) {
                    /** convert  HALFWIDTH_AND_FULLWIDTH_FORMS to BASIC_LATIN */
                    int i = (int) c;
                    i = i - 65248;
                    c = (char) i;
                }"
1,"UserManager: concurrent user creation using same intermediate path failsconcurrently creating users using same intermediate path fails with ""node ... has been modified externally"".

the problem is the intermediate path. if it doesn't exist multiple threads try to create it concurrently: 

o.a.jackrabbit.core.security.user.UserManagerImpl, line 1310ff:


            String[] segmts = defaultPath.split(""/"");
            NodeImpl folder = (NodeImpl) session.getRootNode();
            String authRoot = (isGroup) ? groupsPath : usersPath;

            for (String segment : segmts) {
                if (segment.length() < 1) {
                    continue;
                }
                if (folder.hasNode(segment)) {
                    folder = (NodeImpl) folder.getNode(segment);
                    if (Text.isDescendantOrEqual(authRoot, folder.getPath()) &&
                            !folder.isNodeType(NT_REP_AUTHORIZABLE_FOLDER)) {
                        throw new ConstraintViolationException(""Invalid intermediate path. Must be of type rep:AuthorizableFolder."");
                    }
                } else {
                    Node parent = folder;
                    folder = addNode(folder, session.getQName(segment), NT_REP_AUTHORIZABLE_FOLDER);
                }
            }

the attached test case illustrates this issue/"
0,"Add WaitForMergesTaskWhen building an index, if you just .close the IW, you may leave merges still needing to be done... so a WaitForMerges task lets algs fix this."
0,"Set source and output encoding in POMsModification to POM files to explicitly set the source and report encoding. I've set everything to UTF-8, but this may not be appropriate. However, the encoding properties should be set to ensure that source files are compiled correctly, resources are filtered appropriately and that the reports are using a consistent encoding.

Related info
http://docs.codehaus.org/display/MAVENUSER/POM+Element+for+Source+File+Encoding
http://docs.codehaus.org/display/MAVEN/Reporting+Encoding+Configuration"
0,Bundle cache is not cleared when *BundlePersistenceManager is closedClose method of persistence managers is responsible for releasing all acquired resources. In case of BundlePersistenceManager it should also free memory by clearing the bundle cache.
0,"Setting CONNECTION_TIMEOUT and SO_TIMEOUT on a per-method basisThe capability of setting connection timeout and socket timeout on a per-method
basis should be provided. This would enable different threads, sharing the same
HttpClient, to set different timeouts for their methods executions."
0,tck doesn't compile (use of enum keyword)Use if enum keyword in TestFinder (patch will be attached)
1,"Workspace.clone throws ItemNotFoundException on a referenceable node with childrenAn ItemNotFoundException is thrown when a referenceable node with children is cloned, this happens after the first time the node is cloned.
            
Example:

            Node root = session.getRootNode();   
            Node parent = root.addNode(""parent"");
            parent.addMixin(""mix:referenceable"");
            session.save();
            
// clone parent
            WS2.clone(""default"", ""/parent"", ""/parent"", true);
            
            Node child = parent.addNode(""child"");
// add child
            child.addMixin(""mix:referenceable"");
            session.save();

// clone parent with child            
            WS2.clone(""default"", ""/parent"", ""/parent"", true); 

// clone parent again,   ItemNotFoundException - from now on can't clone parent node.
            WS2.clone(""default"", ""/parent"", ""/parent"", true);


Stacktrace:
javax.jcr.ItemNotFoundException: failed to build path of 229083e5-5f24-4102-b007-785f43be983a: cafebabe-cafe-babe-cafe-babecafebabe has no child entry for 229083e5-5f24-4102-b007-785f43be983a
	at org.apache.jackrabbit.core.HierarchyManagerImpl.buildPath(HierarchyManagerImpl.java:308)
	at org.apache.jackrabbit.core.CachingHierarchyManager.buildPath(CachingHierarchyManager.java:159)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:357)
	at org.apache.jackrabbit.core.CachingHierarchyManager.getPath(CachingHierarchyManager.java:221)
	at org.apache.jackrabbit.core.BatchedItemOperations.checkRemoveNode(BatchedItemOperations.java:700)
	at org.apache.jackrabbit.core.BatchedItemOperations.recursiveRemoveNodeState(BatchedItemOperations.java:1514)
	at org.apache.jackrabbit.core.BatchedItemOperations.removeNodeState(BatchedItemOperations.java:1216)
	at org.apache.jackrabbit.core.BatchedItemOperations.copyNodeState(BatchedItemOperations.java:1642)
	at org.apache.jackrabbit.core.BatchedItemOperations.copy(BatchedItemOperations.java:311)
	at org.apache.jackrabbit.core.WorkspaceImpl.internalCopy(WorkspaceImpl.java:294)
	at org.apache.jackrabbit.core.WorkspaceImpl.clone(WorkspaceImpl.java:401)
	at test.CloneTest.main(CloneTest.java:64)

            "
0,Remove (deprecated) ExtendedFieldCache and Auto/Custom caches and lot's of deprecated sort logicRemove (deprecated) ExtendedFieldCache and Auto/Custom caches and sort
1,"Select * does not return declared properties of node type in FROM clauseThe query only returns the default columns: jcr:primaryType, jcr:score and jcr:path"
0,"the demo application does not work as of 3.0the demo application does not work. QueryParser needs a Version argument.

While I am here, remove @author too"
0,"Better name and path factory exception messagesI've ran across a few cases where the name and path factories throw an exception about an invalid path or name, but fail to include the actual path or name in the exception message. It would be very helpful to have that extra bit of information included."
1,"NPE in OpenOfficeTextExtractorI try to load some Open Office Writer document (see attachment) and receive such exception. 

2008-06-10 17:19:59 <WARN > [btpool0-1] CompositeTextExtractor: Failed to extract text content(92)
java.lang.NullPointerException
    at org.apache.jackrabbit.extractor.OpenOfficeTextExtractor.extractText(OpenOfficeTextExtractor.java:7
8)
    at org.apache.jackrabbit.extractor.CompositeTextExtractor.extractText(CompositeTextExtractor.java:90)
    at org.apache.jackrabbit.core.query.lucene.JackrabbitTextExtractor.extractText(JackrabbitTextExtracto
r.java:195)
    at org.apache.jackrabbit.core.query.lucene.NodeIndexer.addBinaryValue(NodeIndexer.java:393)
    at org.apache.jackrabbit.core.query.lucene.NodeIndexer.addValue(NodeIndexer.java:282)
    at org.apache.jackrabbit.core.query.lucene.NodeIndexer.createDoc(NodeIndexer.java:221)
    at org.apache.jackrabbit.core.query.lucene.SearchIndex.createDocument(SearchIndex.java:892)
    at org.apache.jackrabbit.core.query.lucene.SearchIndex$2.next(SearchIndex.java:543)
    at org.apache.jackrabbit.core.query.lucene.MultiIndex.update(MultiIndex.java:428)
    at org.apache.jackrabbit.core.query.lucene.SearchIndex.updateNodes(SearchIndex.java:527)
    at org.apache.jackrabbit.core.SearchManager.onEvent(SearchManager.java:504)
    at org.apache.jackrabbit.core.observation.EventConsumer.consumeEvents(EventConsumer.java:231)
    at org.apache.jackrabbit.core.observation.ObservationDispatcher.dispatchEvents(ObservationDispatcher.
java:201)
    at org.apache.jackrabbit.core.observation.EventStateCollection.dispatch(EventStateCollection.java:425
)
    at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:737
)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:873)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:334)
    at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:337)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:310)
    at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:317)
    at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1247)
    at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:897)
    at org.apache.jackrabbit.jca.JCASessionHandle.save(JCASessionHandle.java:178)"
0,"DocValues cleanup: constructor & getInnerArray()DocValues constructor taking a numDocs parameter is not very clean.
Get rid of this.

Also, it's optional getInnerArray() method is not very clean.
This is necessary for better testing, but currently tests will fail if it is not implemented.
Modify it to throw UnSupportedOp exception (rather than returning an empty array).
Modify tests to not fail but just warn if the tested iml does not override it.

These changes should make it easier to implement DocValues for other ValueSource's, e.g. above payloads, with or without caching.
"
0,"Add Payload retrieval to SpansIt will be nice to have access to payloads when doing SpanQuerys.

See http://www.gossamer-threads.com/lists/lucene/java-dev/52270 and http://www.gossamer-threads.com/lists/lucene/java-dev/51134

Current API, added to Spans.java is below.  I will try to post a patch as soon as I can figure out how to make it work for unordered spans (I believe I have all the other cases working).

{noformat}
 /**
   * Returns the payload data for the current span.
   * This is invalid until {@link #next()} is called for
   * the first time.
   * This method must not be called more than once after each call
   * of {@link #next()}. However, payloads are loaded lazily,
   * so if the payload data for the current position is not needed,
   * this method may not be called at all for performance reasons.<br>
   * <br>
   * <p><font color=""#FF0000"">
   * WARNING: The status of the <b>Payloads</b> feature is experimental.
   * The APIs introduced here might change in the future and will not be
   * supported anymore in such a case.</font>
   *
   * @return a List of byte arrays containing the data of this payload
   * @throws IOException
   */
  // TODO: Remove warning after API has been finalized
  List/*<byte[]>*/ getPayload() throws IOException;

  /**
   * Checks if a payload can be loaded at this position.
   * <p/>
   * Payloads can only be loaded once per call to
   * {@link #next()}.
   * <p/>
   * <p><font color=""#FF0000"">
   * WARNING: The status of the <b>Payloads</b> feature is experimental.
   * The APIs introduced here might change in the future and will not be
   * supported anymore in such a case.</font>
   *
   * @return true if there is a payload available at this position that can be loaded
   */
  // TODO: Remove warning after API has been finalized
  public boolean isPayloadAvailable();
{noformat}"
0,"AccessControlManager#getApplicablePolicy should check for colliding rep:policy nodewhile AccessControlManager#getApplicablePolicy returns an empty iterator if the target node cannot get the accesscontrollable-mixin set, it does not test if there is a colliding child node that would prevent the policy to be applied calling AccessControlManager#setPolicy. consequently, the setPolicy call fails with ItemExistsException. A simple test upfront could prevent this unexpected failure."
1,CheckIndex overstates how many fields have norms enabledIt simply tells you how many unique fields there are... it should instead only say how many have norms.
0,"HostConfiguration handling requires cleanupAs discussed on the mailing list, the host configuration handling currently
appears faulty:

http://marc.theaimsgroup.com/?t=109644952000001&r=1&w=2

Oleg"
0,"Demo and contrib jars should contain NOTICE.TXT and LICENSE.TXTWe should include NOTICE.TXT and LICENSE.TXT not only in the core jar but also
in the demo and contrib jars."
1,CustomScoreQuery calls weight() where it should call createWeight()Thanks to Uwe for helping me track down this bug after I pulled my hair out for hours on LUCENE-3174.
0,"Backport FilteredQuery/IndexSearcher changes to 3.x: Remove filter logic from IndexSearcher and delegate to FilteredQuerySpinoff from LUCENE-1536: We simplified the code in IndexSearcher to no longer do the filtering there, instead wrap all Query with FilteredQuery, if a non-null filter is given. The conjunction code would then only exist in FilteredQuery which makes it easier to maintain. Currently both implementations differ in 3.x, in trunk we used the more optimized IndexSearcher variant with addition of a simplified in-order conjunction code.

This issue will backport those changes (without random access bits)."
0,"[PATCH] Remove Stutter in NodeStateCode duplicates code for no reason

Index: src/main/java/org/apache/jackrabbit/jcr2spi/state/NodeState.java
===================================================================
--- src/main/java/org/apache/jackrabbit/jcr2spi/state/NodeState.java	(revision 740824)
+++ src/main/java/org/apache/jackrabbit/jcr2spi/state/NodeState.java	(working copy)
@@ -449,7 +449,7 @@
              */
             NodeState parent = getParent();
             NodeId wspId = (NodeId) getWorkspaceId();
-            def = definitionProvider.getQNodeDefinition(getParent().getNodeTypeNames(), getName(), getNodeTypeName(), wspId);
+            def = definitionProvider.getQNodeDefinition(parent.getNodeTypeNames(), getName(), getNodeTypeName(), wspId);
         }
         return def;
     }
"
1,"BitVector.isSparse is sometimes wrongIn working on LUCENE-3246, I found a few problems with
BitVector.isSparse:

  * Its math can overflow int, such that if there are enough deleted
    docs and maxDoc() is largish, isSparse may incorrectly return true

  * It over-estimates the size of the sparse file, since when
    estimating number of bytes for the vInt dgaps it uses bits.length
    instead of bits.length divided by number of set bits (ie, the
    ""average"" gap between set bits)

This is relatively harmless (just affects performance / size of .del
file on disk, not correctness).
"
0,"Calls to SegmentInfos.message should be wrapped w/ infoStream != null checksTo avoid the expensive message creation (which involves the '+' operator on strings, calls to message should be wrapped w/ infoStream != null check, rather than inside message(). I'll attach a patch which does that."
0,"move o.a.l.index.codecs.* -> o.a.l.codecs.*These package names are getting pretty long, e.g.:

org.apache.lucene.index.codecs.lucene40.values.XXXXYYYY

I think we should move it to just the codecs package now while it won't cause anyone any trouble."
0,"SegmentInfos shouldn't blindly increment version on commitSegmentInfos currently increments version on the assumption that there are always changes.

But, both DirReader and IW are more careful about tracking whether there are changes.  DirReader has hasChanges and IW has changeCount.  I think these classes should notify the SIS when there are in fact changes; this will fix the case Simon hit on fixing LUCENE-2082 when the NRT reader thought there were changes, but in fact there weren't because IW simply committed the exact SIS it already had.
"
0,"Build fails on system without XThe failing test is: testFileContains(org.apache.jackrabbit.core.query.FulltextQueryTest)

caused by:

java.lang.InternalError: Can't connect to X11 window server using ':0.0' as the value of the DISPLAY variable.
	at sun.awt.X11GraphicsEnvironment.initDisplay(Native Method)
	at sun.awt.X11GraphicsEnvironment.access$000(X11GraphicsEnvironment.java:53)
	at sun.awt.X11GraphicsEnvironment$1.run(X11GraphicsEnvironment.java:142)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.awt.X11GraphicsEnvironment.<clinit>(X11GraphicsEnvironment.java:131)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:164)
	at java.awt.GraphicsEnvironment.getLocalGraphicsEnvironment(GraphicsEnvironment.java:68)
	at sun.awt.X11.XToolkit.<clinit>(XToolkit.java:96)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:164)
	at java.awt.Toolkit$2.run(Toolkit.java:821)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.awt.Toolkit.getDefaultToolkit(Toolkit.java:804)
	at java.awt.Toolkit.getEventQueue(Toolkit.java:1592)
	at java.awt.EventQueue.isDispatchThread(EventQueue.java:666)
	at javax.swing.SwingUtilities.isEventDispatchThread(SwingUtilities.java:1270)
	at javax.swing.text.StyleContext.reclaim(StyleContext.java:437)
	at javax.swing.text.StyleContext.addAttribute(StyleContext.java:294)
	at javax.swing.text.StyleContext$NamedStyle.addAttribute(StyleContext.java:1486)
	at javax.swing.text.StyleContext$NamedStyle.setName(StyleContext.java:1296)
	at javax.swing.text.StyleContext$NamedStyle.<init>(StyleContext.java:1244)
	at javax.swing.text.StyleContext.addStyle(StyleContext.java:90)
	at javax.swing.text.StyleContext.<init>(StyleContext.java:70)
	at javax.swing.text.DefaultStyledDocument.<init>(DefaultStyledDocument.java:88)
	at org.apache.tika.parser.rtf.RTFParser.parse(RTFParser.java:42)
"
0,Review test cases and cross check with 1.0 specificationThis jira task is meant to collect issues with the TCK test cases.
1,"GData-server storage fix activation depthFixed nullpointer exception while rendering feeds with big amount of extensions. DB4O context.

"
1,"Analysis back compat breakOld and new style token streams don't mix well.
"
0,"Lucene requires ant 1.6?The latest version in CVS as of April 3rd only builds with ant 1.6.   If this is intentional, BUILD.txt should 
be updated.

Here's the error I get with ant 1.5:

BUILD FAILED
file:/Users/skybrian/remote-cvs/jakarta-lucene/build.xml:11: Unexpected element ""tstamp"""
0,Javadoc mistake in SegmentMerger
1,"3.x indexes have the wrong normType set in fieldinfos3.x codec claims the single byte norms are BYTES_VAR_STRAIGHT in FieldInfos,
but the norms implementation itself then has the type as FIXED_INTS_8."
0,"Deprecate NamespaceListener and AbstractNamespaceResolverThe NamespaceListener interface is no longer used with the JSR 283 style namespace handling that avoids lots of the synchronization that was previously to keep the local namespace mappings up to date.

Also, the only (remaining) purpose of the AbstractNamespaceResolver class is to add support for managing NamespaceListeners. Since that functionality is nowhere used anymore, we can make all subclasses use the NamespaceResolver interface directly.

Since NamespaceListener and AbstractNamespaceResolver are public in jackrabbit-spi-commons, I will for now only mark them as deprecated. We can get rid of them in Jackrabbit 2.0."
0,"Text Search Syntax Deviates from SpecOriginal JSR 170 EG Email by David B Victor 2005/03/23:

For Query test XPathQueryLevel2Test.java (src\java\org\apache\jackrabbit\test\api\query) in the TCK, method getFullTextStatement() (used by testFullTextSearch()) uses the word ""AND"" in the syntax in its test that is not in the spec (/*[jcrfnContains(""'quick brown' AND -cat"")]...).  Section ""6.6.4.2 contains function"" of v0.16.3, page 100, outlines the EBNF, which does not include the word ""AND"".  Additionally, the paragraphs here go out of their way to explain that AND is implicit.

At this point, I think it would be best to omit ""AND"" from the TCK method and let it test the implicit AND.

------------------------------------------------------------
David Neuscheler Reply 2005/03/24:

thanks for pointing that out.

i think we should probably track all the tck bugs in jackrabbit jira.
http://issues.apache.org/jira/browse/JCR

could you open a bug for that?

this actually is because we used an non-spec compliant query
parser in the RI, so it actually is even a bug in the RI and the TCK.

thanks again.

regards,
david"
1,"SessionItemStateManager.getIdOfRootTransientNodeState() may cause NPEregression of JCR-2425

in certain scenarios, calling SessionItemStateManager.getIdOfRootTransientNodeState() may cause a NPE.

Test case: 

        Repository repository = new TransientRepository(); 
        Session session = repository.login( 
                new SimpleCredentials(""admin"", ""admin"".toCharArray())); 
        Session session2 = repository.login( 
                new SimpleCredentials(""admin"", ""admin"".toCharArray())); 

        try { 
            while (session.getRootNode().hasNode(""test"")) { 
                session.getRootNode().getNode(""test"").remove(); 
            } 
            Node test = session.getRootNode().addNode(""test""); 
            session.save(); 
            Node x = test.addNode(""x""); 
            session.save(); 

            Node x2 = session2.getRootNode().getNode(""test"").getNode(""x""); 
            x2.remove(); 
            x.addNode(""b""); 
            session2.save(); 
            session.save(); // throws NPE 
        } finally { 
            session.logout(); 
            session2.logout(); 
        }"
1,"fix some more locale problems in lucene/solrset ANT_ARGS=""-Dargs=-Duser.language=tr -Duser.country=TR""
ant clean test

We should make sure this works across all of lucene/solr"
0,"Remove rest of analysis deprecations (Token, CharacterCache)These removes the rest of the deprecations in the analysis package:
- -Token's termText field-- (DONE)
- -eventually un-deprecate ctors of Token taking Strings (they are still useful) -> if yes remove deprec in 2.9.1- (DONE)
- -remove CharacterCache and use Character.valueOf() from Java5- (DONE)
- Stopwords lists
- Remove the backwards settings from analyzers (acronym, posIncr,...). They are deprecated, but we still have the VERSION constants. Do not know, how to proceed. Keep the settings alive for index compatibility? Or remove it together with the version constants (which were undeprecated)."
1,"JCR2SPI: several broken equals() comparisonsDetected by FindBugs:

H C EC: Call to equals() comparing unrelated class and interface in org.apache.jackrabbit.jcr2spi.nodetype.NodeTypeManagerImpl.nodeTypeReRegistered(QName)	xythos-jcr/src/main/java/org/apache/jackrabbit/jcr2spi/nodetype	NodeTypeManagerImpl.java	line 218	1190978573312	1664752
H C EC: Call to equals() comparing unrelated class and interface in org.apache.jackrabbit.jcr2spi.nodetype.NodeTypeManagerImpl.nodeTypeReRegistered(QName)	xythos-jcr/src/main/java/org/apache/jackrabbit/jcr2spi/nodetype	NodeTypeManagerImpl.java	line 227	1190978573312	1664753
H C EC: Call to equals() comparing unrelated class and interface in org.apache.jackrabbit.jcr2spi.nodetype.NodeTypeManagerImpl.nodeTypeUnregistered(QName)	xythos-jcr/src/main/java/org/apache/jackrabbit/jcr2spi/nodetype	NodeTypeManagerImpl.java	line 255	1190978573312	1664754
H C EC: Call to equals() comparing unrelated class and interface in org.apache.jackrabbit.jcr2spi.nodetype.NodeTypeManagerImpl.nodeTypeUnregistered(QName)	xythos-jcr/src/main/java/org/apache/jackrabbit/jcr2spi/nodetype	NodeTypeManagerImpl.java	line 264	1190978573312	1664755
H C EC: org.apache.jackrabbit.jcr2spi.WorkspaceManager.canAccess(String) uses equals to compare an array and nonarray	
"
0,"Explore streaming Viterbi search in KuromojiI've been playing with the idea of changing the Kuromoji viterbi
search to be 2 passes (intersect, backtrace) instead of 4 passes
(break into sentences, intersect, score, backtrace)... this is very
much a work in progress, so I'm just getting my current state up.
It's got tons of nocommits, doesn't properly handle the user dict nor
extended modes yet, etc.

One thing I'm playing with is to add a double backtrace for the long
compound tokens, ie, instead of penalizing these tokens so that
shorter tokens are picked, leave the scores unchanged but on backtrace
take that penalty and use it as a threshold for a 2nd best
segmentation...
"
0,"Cleanup some unused and unnecessary codeSeveral classes in trunk have some unused and unnecessary code. This includes unused fields, unused automatic variables, unused imports and unnecessary assignments. Attached it a patch to clean these up."
1,"TrecContentSource should use a fixed encoding, rather than system dependentTrecContentSource opens InputStreamReader w/o a fixed encoding. On Windows, this means CP1252 (at least on my machine) which is ok. However, when I opened it on a Linux machine w/ a default of UTF-8, it failed to read the files. The patch changes it to use ISO-8859-1, which seems to be the right one (and http://mg4j.dsi.unimi.it/man/manual/ch01s04.html mentions this encoding in its example of a script which reads the data).

Patch to follow shortly."
1,"Versioned node importXML failsWhen importing system-view XML previously exported for a repository, any nodes with a version history cannot be reimported. This appears to be due to the version manager attempting to create a new version history for the node, which fails due to a previous history existing for the same UUID. The behavior occurs with ImportUUIDBehavior.IMPORT_UUID_COLLISION_REPLACE_EXISTING and  ImportUUIDBehavior.IMPORT_UUID_COLLISION_REMOVE_EXISTING, with the following stack trace:

javax.jcr.version.VersionException: History already exists for node a892651d-1688-46cd-bb12-14f2f0b3d886
	at org.apache.jackrabbit.core.version.VersionManagerImpl.createVersionHistory(VersionManagerImpl.java:194)
	at org.apache.jackrabbit.core.ItemImpl.initVersionHistories(ItemImpl.java:900)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1313)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:766)

I am using the 1.0-dev version, revision 209290 obtained on 05 Jul, 2005 at 9:18:02 EST. Attached please find my repository configuration and the test code. Thanks!


"
0,"Current implementation of fuzzy and wildcard queries inappropriately implemented as Boolean query rewritesThe implementation of MultiTermQuery in terms of BooleanQuery introduces several problems:

1) Collisions with maximum clause limit on boolean queries which throws an exception.  This is most problematic because it is difficult to ascertain in advance how many terms a fuzzy query or wildcard query might involve.

2) The boolean disjunctive scoring is not appropriate for either fuzzy or wildcard queries.  In effect the score is divided by the number of terms in the query which has nothing to do with the relevancy of the results.

3) Performance of disjunctive boolean queries for large term sets is quite sub-optimal"
0,"File Formats Documentation is not correct for Term VectorsFrom Samir Abdou on the dev mailing list:

Hi, 

There is an inconsistency between the files format page (from Lucene
website) and the source code. It concerns the positions and offsets of term
vectors. It seems that documentation (website) is not up to date. According
to the file format page, offsets and positions are not stored! Is that
correct?

Many thanks,

Samir
-----
Indeed, in the file formats term vectors section it doesn't talk about the storing of position and offset info.
"
1,"EasyX509TrustManager no longer checks cert expiryEasyX509TrustManager was made even ""easier"" by the last commit:  a socket will
now be created when talking to a server with an expired certificate.

2 commits ago it looked like this (notice ""return false"" on line 107):

102             try {
103                 certificate.checkValidity();
104             }
105             catch (CertificateException e) {
106                 LOG.error(e.toString());
107                 return false;
108             }


Now it looks like this:

102             try {
103                 certificate.checkValidity();
104             }
105             catch (CertificateException e) {
106                 LOG.error(e.toString());
107             }


I'm proposing we just do:

102             certificate.checkValidity();

Now that we're using Java 1.4 in the contrib code, we'll just let the
CertificateException fly up the stack."
0,"Build with JDK 1.4, get many javadoc warningsBuilding httpclient ""dist"" ant target, I get lots of ""warning - The first
sentence is interpreted to be:"".

As the summary says, I get these warnings when I build using JDK 1.4.  Using JDK
1.3.1 yields far fewer problems.  I see this with the latest sources as of this
posting."
0,Reduce calls to RepositoryService.getRepositoryDescriptors()Descriptors do not change and should not be requested for each session.
1,custom sort broken if IS uses executorservice
0,"configurable MultiTermQuery TopTermsScoringBooleanRewrite pq sizeMultiTermQuery has a TopTermsScoringBooleanRewrite, that uses a priority queue to expand the query to the top-N terms.

currently N is hardcoded at BooleanQuery.getMaxClauseCount(), but it would be nice to be able to set this for top-N MultiTermQueries: e.g. expand a fuzzy query to at most only the 50 closest terms.

at a glance it seems one way would be to expose TopTermsScoringBooleanRewrite (it is private right now) and add a ctor to it, so a MultiTermQuery can instantiate one with its own limit."
1,"DEFAULT_HEADERS not added to subsequent requestsDEFAULT_HEADERS are added to the original request only, not to subsequent requests for redirects or authentication."
1,"Search with Filter does not work!See attached JUnitTest, self-explanatory


"
0,"Make MMapDirectory.MAX_BBUF user configureable to support chunking the index files in smaller partsThis is a followup for java-user thred: http://www.lucidimagination.com/search/document/9ba9137bb5d8cb78/oom_with_2_9#9bf3b5b8f3b1fb9b

It is easy to implement, just add a setter method for this parameter to MMapDir."
0,"Consolidate compare behaviour for Value(s) and Comparable(s)There are 2 different implementations of Value comparison (ValueComparator and Util). With the introduction of JCR-2906 which introduces arrays into the mix, I'd like to refactor all of them into one place, namely o.a.j.core.query.lucene.Util.

This will also allow for a wider scope of comparison for Value[], marked as TODO in the ValueComparator class.

Will attach patch shortly"
0,"exceptions from other threads in beforeclass/etc do not fail the testLots of tests create indexes in beforeClass methods, but if an exception is thrown from another thread
it won't fail the test... e.g. this test passes:
{code}
public class TestExc extends LuceneTestCase {
  @BeforeClass
  public static void beforeClass() {
    new Thread() {
      public void run() {
        throw new RuntimeException(""boo!"");
      }  
    }.start();
  }
  
  public void test() { }
}
{code}

this is because the uncaught exception handler is in setup/teardown"
0,"Avoid INFINITE RECURSION when Object Model has cycles.The default ObjectConverterImpl is restricted to acyclic graphs in the object model.

Many Java object models are NOT acyclic.   For instance, I am on your Friends list.   Yoar are on my Friends list.     Java encourages such structures.   Almost any large object model in Java will have hidden cycles.

Saving an Object Model that contains cycles using Graffito causes an infinite recursion.

Clearly, it is important to maintain a 1-to-1 correspondence between Nodes and Objects to prevent this.   In the absence of Multiple Parent Nodes, it will be necessary to use REFERENCE or UNDEFINED Items in place of the 2nd (or greater) Node representing a given Object.   My preference si that the default ObjectConverterImpl should support REFERENCE.,    Failing this, use of UNDEFINED also solves this problem and would  acceptable (as default).  Whether or not REFERENCE is used, both insertion and retrieval must provide a reasonable result.   A custom ojbect converter should be available to switch UNDEFINED to REFERENCE, or vice versa.

Also, it is probably best to keep the targeted, well-defined Nodes close to the Root Node.    This implies that the default ObjectConverterImpl should implement a Breadth-First, rather than a Depth-First, traversal of the Object Model on both insertion and retrieval.   Again, if the default is Depth-First, a custom object converter should be available that implements Breadth-First.

Admittedly, support for (2 representations) X (2 traversals) implies a drastic refactoring and/or rewriting of the ObjectConverterImpl class."
0,"Provide query support  for WEAKREFERENCE reverse lookupthe current implementation of Node.getWeakReferences() and getWeakReferences(String) uses a fulltext query in order to find weak references to a particular node.

this requires the PlainTextExtractor to be enabled in the Search config, e.g. :

    <param name=""textFilterClasses"" value=""org.apache.jackrabbit.extractor.PlainTextExtractor""/>

providing 'native' WEAKREFERNCE reverse lookup in Jackrabbit's QOM implementation would be certainly more efficient.

"
1,"Cluster sync not always done when calling session.refresh(..)Session.refresh(..) is supposed to synchronize cluster changes, but this doesn't always happen, specially if the syncDelay is low. The reason is a wrong assumption in ClusterNode.sync: The code there to avoid duplicate sync calls doesn't always work as expected. The following algorithm is used:

        int count = syncCount;
        syncLock.acquire();
        if (count == syncCount) {
            journalSync();
            syncCount++;
        }
        syncLock.release();

The problem is that the background thread might be at the line ""syncCount++"" when Session.refresh(..) is called, so that the main thread believes journalSync was already called and thus doesn't call it."
0,"Separate NOTICEs and LICENSEs for binary and source packagesBased on recent discussions on sling-dev@ (see [1]) and on legal-discuss@  (see [2]), I'd like to rearrange our NOTICE and LICENSE files so that the root level files refer only to bits included in source releases and that the (in some cases different) files to be included in the binary artifacts would be placed in src/main/resources/META-INF.

See also JCR-1630 for related work.

[1] http://markmail.org/message/2enw6ktxhc4ixmrk
[2] http://markmail.org/message/bttmkavpicxxg7gl
"
0,most tests should use MockRAMDirectory not RAMDirectory
0,"TransientRepository: application doesn't exit quicklyWhen using the TransientRepository, the repository should be closed when the last session logs out. This works, but in some cases there is a very long (60 seconds) delay between closing the last session and closing the repository.

Test case:

    public static void main(String[] args) throws Exception {
        Repository repository = new TransientRepository();
        Session session = repository.login(new SimpleCredentials("""", new char[0]));
        session.getRootNode().setProperty(""a"", ""0"");
        session.save(); // very quick logout without this line
        session.logout();
        System.out.println(""Logout..."");
        final long time = System.currentTimeMillis();
        Runtime.getRuntime().addShutdownHook(new Thread() {
            public void run() {
                System.out.println(""End after: "" + (System.currentTimeMillis() - time));
            }
        });
    }

"
0,"spi2dav Improve performance for large binary propertiesSending large binary properties over spi2dav is slow and requires a lot of heap space in both client and server.
One problematic part is base64 conversion of the property value.

On the contrary, using 'normal' webdav interface (/repository/default/ instead of /server) for uploading a file (through traditional webdav client) it is pretty fast and don't have such impact on heap space.

Some suggestions from the previous discussion:
 - avoid temporary copies of the data, and persist large objects as early as possible. 
 - transfer large objects in blocks from the Jackrabbit SPI client to the server (and back).
 - make usage of the global data store (JCR-926). 
 - straight forward PUT for single-valued properties

Link to discussion: http://www.mail-archive.com/dev@jackrabbit.apache.org/msg09481.html
"
0,Expose directory on IndexReaderIt would be really useful to expose the index directory on the IndexReader class.
0,"Basic refactoring of DocumentsWriterAs a starting point for making DocumentsWriter more understandable,
I've fixed its inner classes to be static, and then broke the classes
out into separate sources, all in org.apache.lucene.index package.

"
1,"Can not instantiate lucene Analyzer in SearchIndexIn the Lucene 3, the there is no default constructor anymore in Analyzer classes


11:46:45.946 [main] WARN  o.a.j.core.query.lucene.SearchIndex - Invalid Analyzer class: org.apache.lucene.analysis.standard.StandardAnalyzer
java.lang.InstantiationException: org.apache.lucene.analysis.standard.StandardAnalyzer
        at java.lang.Class.newInstance0(Class.java:340) ~[na:1.6.0_26]
        at java.lang.Class.newInstance(Class.java:308) ~[na:1.6.0_26]
        at org.apache.jackrabbit.core.query.lucene.SearchIndex.setAnalyzer(SearchIndex.java:1892) ~[jackrabbit-core-2.4.0.jar:2.4.0]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.6.0_26]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) ~[na:1.6.0_26]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) ~[na:1.6.0_26]
        at java.lang.reflect.Method.invoke(Method.java:597) ~[na:1.6.0_26]
        at org.apache.jackrabbit.core.config.BeanConfig.setProperty(BeanConfig.java:255) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.config.BeanConfig.newInstance(BeanConfig.java:203) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$1.getQueryHandler(RepositoryConfigurationParser.java:652) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.config.WorkspaceConfig.getQueryHandler(WorkspaceConfig.java:251) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.SearchManager.<init>(SearchManager.java:171) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getSearchManager(RepositoryImpl.java:1855) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.doPostInitialize(RepositoryImpl.java:2092) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.initialize(RepositoryImpl.java:1997) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.RepositoryImpl.initStartupWorkspaces(RepositoryImpl.java:510) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:318) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:582) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.jndi.BindableRepository.createRepository(BindableRepository.java:141) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.jndi.BindableRepository.init(BindableRepository.java:117) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.jndi.BindableRepository.<init>(BindableRepository.java:106) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.jndi.BindableRepositoryFactory.getObjectInstance(BindableRepositoryFactory.java:52) [jackrabbit-core-2.4.0.jar:2.4.0]
"
0,Add accessor for parent to NodeInfoBuilder/PropertyInfoBuilderNodeInfoBuilder and PropertyInfoBuilder should allow access to its respective parents. I suggest to add a getParent() method to both classes. 
0,"Spatial uses java util logging that causes needless minor work (multiple string concat, a method call) due to not checking log levelNot sure there should be logging here - just used in two spots and looks more for debug - but if its going to be there, should check for isFineEnabled."
0,"Move *.log files to target/The jackrabbit-core component already puts the derby.log file in target/ along with other build  and test artifacts, but many other components don't do that yet. Having all generated files in target/ is good as it makes it very easy to clean things up. Also things like the RAT checks (JCR-1937) are easier when there's no need to worry about such extra files.
"
0,"Change visibility of getComparator method in SortField from protected to publicHi,

Currently I'm using SortField for the creation of FieldComparators, but I ran into an issue.
I cannot invoke SortField.getComparator(...) directly from my code, which forces me to use a  workaround. (subclass SortField and override the getComparator method with visiblity public)
I'm proposing to make this method public. Currently I do not see any problems changing the visibility to public, I do not know if there are any (and the reason why this method is currently protected)
I think that this is a cleaner solution then the workaround I used and also other developers can benefit from it. I will also attach a patch to this issue based on the code in the trunk (26th of May). place). 
Please let me know your thoughts about this.

Cheers,

Martijn

 "
1,"Token of  """" returns in CJKTokenizer + new TestCJKTokenizerThe """" string returns as Token in the boundary of two byte character and one byte character. 

There is no problem in CJKAnalyzer. 
When CJKTokenizer is used with the unit, it becomes a problem. (Use it with 
Solr etc.)"
1,"always apply position increment gap between valuesI'm doing some fancy stuff with span queries that is very sensitive to term positions.  I discovered that the position increment gap on indexing is only applied between values when there are existing terms indexed for the document.  I suspect this logic wasn't deliberate, it's just how its always been for no particular reason.  I think it should always apply the gap between fields.  Reference DocInverterPerField.java line 82:

if (fieldState.length > 0)
          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);

This is checking fieldState.length.  I think the condition should simply be:  if (i > 0).
I don't think this change will affect anyone at all but it will certainly help me.  Presently, I can either change this line in Lucene, or I can put in a hack so that the first value for the document is some dummy value which is wasteful."
1,"Possible concurrency bug with Workspace.copy() Hi,

Enclosed below is a test case that can be used to reproduce a
concurrency bug. This test case uses two con-current threads to
execute Workspace.copy() to copy a node to same destination. The
parent node has set its allowSameNameSiblings to false. According to
the javadoc of Workspace.copy(String srcAbsPath, String destAbsPath) :
""This method copies the node at srcAbsPath to the new location at
destAbsPath. If successful, the change is persisted immediately, there
is no need to call save."".  ""An ItemExistException is thrown if a
property already exists at destAbsPath or a node already exist there,
and same name siblings are not allowed. ""

However in reality this is not the case.  The test case can end up
with two child nodes with same names. Please note, not every run can
reproduce the problem, but generally I can get the problem within 3 to
10 iterations. I also got an InvalidItemStateException once (only
once).  Can someone kindly help to confirm if this is a bug in
Jackrabbit or maybe I am using JackRabbit in a wrong way? The test
case has been tested on Jackrabbit 1.6 branch
(http://svn.apache.org/repos/asf/jackrabbit/tags/1.6.0), Windows
Vista, JDK 1.5.0_14.

The test case is also attached for your convenience.

Thanks,
Jervis Liu

package org.apache.jackrabbit.core;

import org.apache.jackrabbit.test.AbstractJCRTest;
import javax.jcr.ItemExistsException;
import javax.jcr.Node;
import javax.jcr.Session;
import javax.jcr.Value;
import javax.jcr.NodeIterator;
import java.util.Random;
import java.util.ArrayList;
import java.util.Iterator;
import javax.jcr.nodetype.NodeType;

import org.apache.jackrabbit.test.NotExecutableException;
import javax.jcr.RepositoryException;
import javax.jcr.nodetype.NodeTypeManager;


public class ConcurrentCopyTest extends AbstractJCRTest {

    private static final int NUM_ITERATIONS = 40;
    private static final int NUM_SESSIONS = 2;

    String sourcePath;
    String destPath;

    public void testConcurrentCopy() throws Exception {
        for (int n = 0; n < NUM_ITERATIONS; n++) {
            System.out.println(""---Iteration---- "" + n);

            // clean up testRoot first
            if (testRootNode.hasNode(""ConcurrentCopyTestNode"")) {
                Node testNode = testRootNode.getNode(""ConcurrentCopyTestNode"");
                testNode.remove();
                testRootNode.save();
                System.out.println(""---old node removed---"");
            }

            // create a parent node where allowSameNameSiblings is set to false
            Node snsfNode = testRootNode.addNode(""ConcurrentCopyTestNode"",
                    ""nt:folder"");
            testRootNode.save();
            sourcePath = snsfNode.getPath();
            destPath = sourcePath + ""/"" + ""CopiedFromConcurrentCopyTestNode"";
            System.out.println(""---sourcePath-----------------"" + sourcePath);
            System.out.println(""---destPath-----------------"" + destPath);

            // firstly we verify it works with single thread.
            Session rootSession = helper.getSuperuserSession();
            rootSession.getWorkspace().copy(sourcePath, destPath + ""test"");

            // copy again to same destPath, expect an ItemExistsException
            try {
                rootSession.getWorkspace().copy(sourcePath, destPath + ""test"");
                fail(""Node exists below '"" + destPath + ""'. Test should fail."");
            } catch (ItemExistsException e) {
            }

            Thread[] threads = new Thread[NUM_SESSIONS];
            for (int i = 0; i < threads.length; i++) {
                // create new session
                Session session = helper.getSuperuserSession();
                TestSession ts = new TestSession(""s"" + i, session);
                Thread t = new Thread(ts);
                t.setName((NUM_ITERATIONS - n) + ""-s"" + i);
                t.start();
                log.println(""Thread#"" + i + "" started"");
                threads[i] = t;
                // Thread.yield();
                // Thread.sleep(100);
            }
            for (int i = 0; i < threads.length; i++) {
                threads[i].join();
            }

            NodeIterator results = testRootNode.getNode(
                    ""ConcurrentCopyTestNode"").getNodes(
                    ""CopiedFromConcurrentCopyTestNode"");
            while (results.hasNext()) {
                Node node = results.nextNode();
                System.out.println(""--result node- "" + node.getName());
            }

            assertEquals(1, results.getSize());
        }
    }

    // --------------------------------------------------------< inner classes >
    class TestSession implements Runnable {

        Session session;
        String identity;
        Random r;

        TestSession(String identity, Session s) {
            session = s;
            this.identity = identity;
            r = new Random();
        }

        private void randomSleep() {
            long l = r.nextInt(90) + 20;
            try {
                Thread.sleep(l);
            } catch (InterruptedException ie) {
            }
        }

        public void run() {

            log.println(""started."");
            String state = """";
            try {
                this.session.getWorkspace().copy(sourcePath, destPath);
                session.save();
                Node newNode =
testRootNode.getNode(""ConcurrentCopyTestNode/CopiedFromConcurrentCopyTestNode"");
                System.out.println(""--Added node- "" + newNode.getName());

                session.save();
                randomSleep();
            } catch (Exception e) {
                log.println(""Exception while "" + state + "": "" + e.getMessage());
                e.printStackTrace();
            } finally {
                session.logout();
            }

            log.println(""ended."");
        }
    }

}

"
0,"Determination of property state difference should skip binary valueso.a.j.jcr2spi.state.PropertyState.diffPropertyData, PropertyData) should alway consider two binary values to be different. The current implementation compares two binary values with equals(). An implementation will in general have to do a byte by byte comparison of both values. This is most likely always more expensive than considering the values different right from the start. 

"
0,Provide fail-over for multi-home remote servers (if one server in a farm goes down)The HTTP Client does not provide automatic fail-over for multi-home remote servers (web-farm) if one server in a farm goes down
1,"automaton termsenum bug when running with multithreaded searchThis one popped in hudson (with a test that runs the same query against fieldcache, and with a filter rewrite, and compares results)

However, its actually worse and unrelated to the fieldcache: you can set both to filter rewrite and it will still fail.
"
0,"Upgrade benchmark from commons-compress-1.0 to commons-compress-1.1 for 15 times faster gzip decompressionIn LUCENE-1540 TrecContentSource moved from Java's GZipInputStream to common-compress 1.0. 
This slowed down gzip decompression by a factor of 15. 
Upgrading to 1.1 solves this problem.
I verified that the problem is only in GZIP, not in BZIP.
On the way, as 1.1 introduced constants for the compression methods, the code can be made a bit nicer."
0,"replace text from an online collection (used in few test cases) with text that is surely 100% free.Text from an online firstaid collection (firstaid . ie . eu . org)  is used as arbitrary text for test documents creation, in:
   o.a.l.analysis.Analyzer.FunctionTestSetup.DOC_TEXT_LINES
   o.a.l.benchmark.byTask.feeds.SimpleDocMaker.DOC_TEXT

I once got this text from Project Gutenberg and was sure that it is free. But now the referred Web site does not seem to respond, and I can no more find that firstaid eBook in the Project Gutenberg site.

Since it doesn't matter what text we use there, I will just replace that with some of my own words..."
0,"MemcachedHttpCacheStorage should throw IOExceptions instead of Runtime ExceptionsThe MemcachedHttpCacheStorage class implements HttpCacheStorage which defines that methods will throw IOExceptions, but the underlying net.spy.memcached.MemcachedClientIF throws runtime exceptions. These exceptions are not caught in the code where IOExceptions are expected causing these exception bubble up to the calling code. It seems like the MemcachedHttpCacheStorage class should treat at least some of these runtime exceptions as IOExceptions so that normal code execution paths can be followed.  

I'm proposing that MemcachedHttpCacheStorage treat a OperationTimeoutException from the memcached client as an IOException. This would allow the existing CachingHttpClient code to catch and log the exception as a warning, instead of bubbling the exception up the calling code.
"
1,"Node merge method doesnt seems to recurse thru childs of the right source nodeI checked the NodeImpl.merge(...)

it seems the way it process the childs nodes is wrong
as it calls the merge on the childs of the src node that come from the source workspace.
plus in the case srcNode is null it would end on a NullPointerException as

it does  NodeIterator ni = srcNode.getNodes(); in the second statment of the if condition
"
1,"exception during writeRequest leaves the connection un-releasedThe execute method has the following (simplified) flow:
1) get connection
2) write request
3) read result
4) release connection.
The release in step 4 happens when the input is completely read, which works fine.
If an exception occurs between steps 1 and 2, the connection is also released
properly.
However, if an exception occurs during step 2, the connection is never released
back and the connection manager eventually runs out of connections.

The easiest way to test this is to make a simple subclass of PostMethod that
overrides the writeRequest method:

public class TestConnectionReleaseMethod extends PostMethod
{
    protected void writeRequest(HttpState state, HttpConnection conn) throws
IOException, HttpException
    {
         throw new IOException(""for testing"");
    }
}"
0,"Create new method optimize(int maxNumSegments) in IndexWriterSpinning this out from the discussion in LUCENE-847.

I think having a way to ""slightly optimize"" your index would be useful
for many applications.

The current optimize() call is very expensive for large indices
because it always optimizes fully down to 1 segment.  If we add a new
method which instead is allowed to stop optimizing once it has <=
maxNumSegments segments in the index, this would allow applications to
eg optimize down to say <= 10 segments after doing a bunch of updates.
This should be a nice compromise of gaining good speedups of searching
while not spending the full (and typically very high) cost of
optimizing down to a single segment.

Since LUCENE-847 is now formalizing an API for decoupling merge policy
from IndexWriter, if we want to add this new optimize method we need
to take it into account in LUCENE-847.
"
0,"Automatic license header checking using the Apache Rat PluginTo avoid problems with incorrect license headers, we should include some automated header check in the Maven build and have Hudson run the check whenever changes are committed."
1,"Host configuration properties not updated when the method is redirectedthe above uri:

http://www.adobe.com/cgi-bin/redirect?http://lists.w3.org/Archives/Public/www-xsl-fo

generates two 302 responses:

from the original to http://lists.w3.org/Archives/Public/www-xsl-fo
and from that to http://lists.w3.org/Archives/Public/www-xsl-fo/

the client accepts and follows these redirects (a trace of the process shows it's working well) but when 
you ask the getmethod what uri we ended up at using the getURI() method it returns the bastardised 
result:

http://www.adobe.com/Archives/Public/www-xsl-fo/

instead of the correct 

http://lists.w3.org/Archives/Public/www-xsl-fo/

that the client has actually downloaded.

using cvsup'd copy showing version string "" Jakarta Commons-HttpClient/2.1m1"""
0,"contrib/memory: PatternAnalyzerTest is a very, very, VERY, bad unit testwhile working on something else i was started getting consistent IllegalStateExceptions from PatternAnalyzerTest -- but only when running the test from the top level.

Digging into the test, i've found numerous things that are very scary...
* instead of using assertions to test that tokens streams match, it throws an IllegalStateExceptions when they don't, and then logs a bunch of info about the token streams to System.out -- having assertion messages that tell you *exactly* what doens't match would make a lot more sense.
* it builds up a list of files to analyze using patsh thta it evaluates relative to the current working directory -- which means you get different files depending on wether you run the tests fro mthe contrib level, or from the top level build file
* the list of files it looks for include: ""../../*.txt"", ""../../*.html"", ""../../*.xml"" ... so not only do you get different results when you run the tests in the contrib vs at the top level, but different people runing the tests via the top level build file will get different results depending on what types of text, html, and xml files they happen to have two directories above where they checked out lucene.
* the test comments indicates that it's purpose is to show that PatternAnalyzer produces the same tokens as other analyzers - but points out this will fail for WhitespaceAnalyzer because of the 255 character token limit WhitespaceTokenizer imposes -- the test then proceeds to compare PaternAnalyzer to WhitespaceTokenizer, garunteeing a test failure for anyone who happens to have a text file containing more then 255 characters of non-whitespace in a row somewhere in ""../../"" (in my case: my bookmarks.html file, and the hex encoded favicon.gif images)
"
0,"In 3.x branch (starting with 3.4) the IndexFormatTooOldException was backported, but the error message was not modified for 3.xIn 3.x branch (starting with 3.4) the IndexFormatTooOldException was backported, but the error message was not modified for 3.x:

bq. This version of Lucene only supports indexes created with release 3.0 and later.

In 3.x it must be:

bq. This version of Lucene only supports indexes created with release 1.9 and later.

Indexes before 1.9 will throw this exception on reading SegmentInfos (LUCENE-3255)."
0,Add test case support for shard searchingNew test case that helps stress test the APIs to support sharding....
0,"cleanup contrib/demoI don't think we should include optimize in the demo; many people start from the demo and may think you must optimize to do searching, and that's clearly not the case.

I think we should also use a buffered reader in FileDocument?

And... I'm tempted to remove IndexHTML (and the html parser) entirely.  It's ancient, and we now have Tika to extract text from many doc formats."
0,"MMapDirectory speedupsMMapDirectory has some performance problems:
# When the file is larger than Integer.MAX_VALUE, we use MultiMMapIndexInput, 
which does a lot of unnecessary bounds-checks for its buffer-switching etc. 
Instead, like MMapIndexInput, it should rely upon the contract of these operations
in ByteBuffer (which will do a bounds check always and throw BufferUnderflowException).
Our 'buffer' is so large (Integer.MAX_VALUE) that its rare this happens and doing
our own bounds checks just slows things down.
# the readInt()/readLong()/readShort() are slow and should just defer to ByteBuffer.readInt(), etc
This isn't very important since we don't much use these, but I think there's no reason
users (e.g. codec writers) should have to readBytes() + wrap as bytebuffer + get an 
IntBuffer view when readInt() can be almost as fast..."
1,"DefaultHttpRequestRetryHandler must not retry non-idempotent http methods (violates RFC 2616)In DefaultHttpRequestRetryHandler, in case of NoHttpResponseException, the request is retried, without taking into account whether the http method is idempotent or not. This violates RFC 2616 section 8.1.4 which states :
{quote}
This means that clients, servers, and proxies MUST be able to recover
   from asynchronous close events. Client software SHOULD reopen the
   transport connection and retransmit the aborted sequence of requests
   without user interaction so long as the request sequence is
   idempotent (see section 9.1.2). Non-idempotent methods or sequences
   MUST NOT be automatically retried, although user agents MAY offer a
   human operator the choice of retrying the request(s).
{quote}

The fix is simple : at line 94, just remove the {{if (exception instanceof NoHttpResponseException) }} block. This way the idempotency of the method will be taken into account a bit further in the same method."
0,"Jackrabbit web page scroll is slow with FirefoxWhen I visit http://jackrabbit.apache.org/ from my Firefox in Ubuntu  the browser scroll is very slow and make CPU go to 100%. 

The problem seems to be the ""fixed"" attribute in the css background definition, so it should be removed.

body {
  background:white url(bg.png) repeat-x fixed center bottom;
  font-family:Verdana,Helvetica,Arial,sans-serif;
  font-size:small;
  margin:0pt;
  padding:0pt;
}"
1,"Custom similarity is ignored when using MultiSearcherSymptoms:
I am using Searcher.setSimilarity() to provide a custom similarity that turns off tf() factor. However, somewhere along the way the custom similarity is ignored and the DefaultSimilarity is used. I am using MultiSearcher and BooleanQuery.

Problem analysis:
The problem seems to be in MultiSearcher.createWeight(Query) method. It creates an instance of CachedDfSource but does not set the similarity. As the result CachedDfSource provides DefaultSimilarity to queries that use it.

Potential solution:
Adding the following line:
    cacheSim.setSimilarity(getSimilarity());
after creating an instance of CacheDfSource (line 312) seems to fix the problem. However, I don't understand enough of the inner workings of this class to be absolutely sure that this is the right thing to do.

"
0,"Create jackrabbit-parentCurrently the Jackrabbit components use the top-level multi-module POM as their parent POM for sharing many of the common build settings. However, with the planned mixed component release model for 1.5.x we need to be able to increase the version number of the top-level POM without affecting individual components. Thus it is better if we move the shared settings to an explicit jackrabbit-parent component and keep the top-level POM simply as the multi-module container."
0,"WebDAV: pack AbstractWebdavServlet with the jackrabbit-webdav projectsuggestion posted by alan cabrera on the dev list:

""Quite a handy servlet.  Too bad it's in jackrabbit-server.  Would this not be better placed in jackrabbit-webdav?  I'm writing my own server bits under WEBDAV and would prefer not to have JCR/Jackrabbit stuff.  I realize that this is a fussy preference.""

"
1,"SegmentReader.getFieldNames ignores FieldOption.DOC_VALUESwe use this getFieldNames api in segmentmerger if we merge something that isn't a SegmentReader (e.g. FilterIndexReader)

it looks to me that if you use a FilterIndexReader, call addIndexes(Reader...) the docvalues will be simply dropped.

I dont think its enough to just note that the field has docvalues either right? We need to also set the type 
correctly in the merged field infos? This would imply that instead of FieldOption.DOCVALUES, we need to have a 
FieldOption for each ValueType so that we correctly update the type.

But looking at FI.update/setDocValues, it doesn't look like we 'type-promote' here anyway?
"
0,"Use Apache Codec 1.41.4 fixes many bugs and added some nice features: http://commons.apache.org/codec/changes-report.html

It took me a whiel to find out, that this was the main reason my tests failed (MethodNotFoundException).
"
0,Make inspection of BooleanQuery more efficientJust attempting to inspect a BooleanQuery allocates two new arrays.  This could be cheaper.
0,"Change contrib QP API that uses CharSequence as string identifierThere are some API methods on contrib queryparser that expects CharSequence as identifier. This is wrong, since it may lead to incorrect or mislead behavior, as shown on LUCENE-2855. To avoid this problem, these APIs will be changed and enforce the use of String instead of CharSequence on version 4. This patch already deprecate the old API methods and add new substitute methods that uses only String."
0,"Reduce usage of String.intern()String.intern() is used for interning the namespace URI in NameImpl. For some trivial cases the intern() method shouldn't be called but a constant should be
used. E.g. I'm thinking about the empty namespace URI, where calling String.intern() is way more expensive than checking if the length of the URI string is zero. "
0,"Connection pool uses Thread.interrupt()The connection pool for TSCCM uses Thread.interrupt() to wake up waiting threads.
This interferes with application interrupts.

- expose InterruptedException in interface
- change pool implementation to use wait/notify
"
1,"KeywordMarkerFilter resets keyword attribute state to false for tokens not in protwords.txtKeywordMarkerFilter sets true or false for the KeywordAttribute on all tokens. This erases previous state established further up the filter chain, for example in the case where a custom filter wants to prevent a token from being stemmed. 

If a token is already marked as a keyword (KeywordAttribute.isKeyword() == true), perhaps the KeywordMarkerFilterFactory should not re-set the state to false."
0,"Incorrect support for java interfaces in typed collection fieldsIf a typed collection field is defined with an Interface as the type, the following exception is thrown when the main object is inserted : 

org.apache.jackrabbit.ocm.exception.JcrMappingException: Cannot load class interface [name of the interface];

Here is a example : 

@Node
public class EntityA {
       @Field(path=true) String path;
       @Collection List<MyInterface> entityB;
       ....
}

When inserting a new instance of EntityA with a not null entityB, the exception is thrown. 
A workaround is to add the elementClassName on the annotation @Collection. ex. : 

@Collection (elementClassName=MyInterface.class) List<MyInterface> entityB;

elementClassName is used only for untyped collections but if you specify it for a typed collection, the ObjectContentManager will not use reflexion to check the collection class name. 
 
This should be nice to avoid the usage of elementClassName for typed collections. 

"
0,"Replace Maven POM templates with full POMs, and change documentation accordinglyThe current Maven POM templates only contain dependency information, the bare bones necessary for uploading artifacts to the Maven repository.

The full Maven POMs in the attached patch include the information necessary to run a multi-module Maven build, in addition to serving the same purpose as the current POM templates.

Several dependencies are not available through public maven repositories.  A profile in the top-level POM can be activated to install these dependencies from the various {{lib/}} directories into your local repository.  From the top-level directory:

{code}
mvn -N -Pbootstrap install
{code}

Once these non-Maven dependencies have been installed, to run all Lucene/Solr tests via Maven's surefire plugin, and populate your local repository with all artifacts, from the top level directory, run:

{code}
mvn install
{code}

When one Lucene/Solr module depends on another, the dependency is declared on the *artifact(s)* produced by the other module and deposited in your local repository, rather than on the other module's un-jarred compiler output in the {{build/}} directory, so you must run {{mvn install}} on the other module before its changes are visible to the module that depends on it.

To create all the artifacts without running tests:

{code}
mvn -DskipTests install
{code}

I almost always include the {{clean}} phase when I do a build, e.g.:

{code}
mvn -DskipTests clean install
{code}
"
0,"explore using automaton for fuzzyquerywe can optimize fuzzyquery by using AutomatonTermsEnum. The idea is to speed up the core FuzzyQuery in similar fashion to Wildcard and Regex speedups, maintaining all backwards compatibility.

The advantages are:
* we can seek to terms that are useful, instead of brute-forcing the entire terms dict
* we can determine matches faster, as true/false from a DFA is array lookup, don't even need to run levenshtein.

We build Levenshtein DFAs in linear time with respect to the length of the word: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.652

To implement support for 'prefix' length, we simply concatenate two DFAs, which doesn't require us to do NFA->DFA conversion, as the prefix portion is a singleton. the concatenation is also constant time with respect to the size of the fuzzy DFA, it only need examine its start state.

with this algorithm, parametric tables are precomputed so that DFAs can be constructed very quickly.
if the required number of edits is too large (we don't have a table for it), we use ""dumb mode"" at first (no seeking, no DFA, just brute force like now).

As the priority queue fills up during enumeration, the similarity score required to be a competitive term increases, so, the enum gets faster and faster as this happens. This is because terms in core FuzzyQuery are sorted by boost value, then by term (in lexicographic order).

For a large term dictionary with a low minimal similarity, you will fill the pq very quickly since you will match many terms. 
This not only provides a mechanism to switch to more efficient DFAs (edit distance of 2 -> edit distance of 1 -> edit distance of 0) during enumeration, but also to switch from ""dumb mode"" to ""smart mode"".

With this design, we can add more DFAs at any time by adding additional tables. The tradeoff is the tables get rather large, so for very high K, we would start to increase the size of Lucene's jar file. The idea is we don't have include large tables for very high K, by using the 'competitive boost' attribute of the priority queue.

For more information, see http://en.wikipedia.org/wiki/Levenshtein_automaton"
1,"BundleDbPersistenceManager does not work with MySQLIt seems that the bundle persistence manager base does not work with MySQL. A SQLException is thrown on the line ""con.commit();"" in BundleDbPersistenceManager.checkSchema() because autoCommit is set to true in the init method. For some reason, this is ignored by the Oracle and MSSQL drivers. Anyway, commenting out the line fixes the issue, I think."
0,"codec postings api (finishDoc) is inconsistentfinishDoc says:

{noformat}
  /** Called when we are done adding positions & payloads
   *  for each doc.  Not called  when the field omits term
   *  freq and positions. */
   public abstract void finishDoc() throws IOException;
{noformat}

But this is confusing (because a field can omit just positions, is it called then?!),
and wrong (because merging calls it always, even if freq+positions is omitted).

I think we should fix the javadoc and fix FreqProxTermsWriter to always call finish()
"
0,"if the build fails to download JARs for contrib/db, just skip its testsEvery so often our nightly build fails because contrib/db is unable to download the necessary BDB JARs from http://downloads.osafoundation.org.  I think in such cases we should simply skip contrib/db's tests, if it's the nightly build that's running, since it's a false positive failure."
1,"search vs explain - score discrepanciesI'm on a mission to demonstrate (and then hopefully fix) any inconsistencies between the score you get for a doc when executing a search, and the score you get when asking for an explanation of the query for that doc."
0,"Eliminate synchronization contention on initial index reading in TermInfosReader ensureIndexIsRead synchronized method ensureIndexIsRead in TermInfosReader causes contention under heavy load

Simple to reproduce: e.g. Under Solr, with all caches turned off, do a simple range search e.g. id:[0 TO 999999] on even a small index (in my case 28K docs) and under a load/stress test application, and later, examining the Thread dump (kill -3) , many threads are blocked on 'waiting for monitor entry' to this method.

Rather than using Double-Checked Locking which is known to have issues, this implementation uses a state pattern, where only one thread can move the object from IndexNotRead state to IndexRead, and in doing so alters the objects behavior, i.e. once the index is loaded, the index nolonger needs a synchronized method. 

In my particular test, this uncreased throughput at least 30 times.

"
0,"Override method MultipartEntity.addPart so that applications may use FormBodyPartFormBodyPart is similar to Part in HttpClient 3.x in that it couples the form name with the value.  Some applications may find this useful, but cannot really utilize these objects since there is only MultipartEntity.addPart(String name,ContentBody) and FormBodyPart does not have a getContent method:

  entity.addPart(part.getName(), part.getContent()); // Almost but there is no getContent method

How about overriding addPart to take a FormBodyPart object:

  entity.addPart(part);"
1,"BLOB Store: only open a stream when really necessaryCurrently, PropertyImpl.getValue() opens a FileInputStream if the BLOBStore is used.
If the application doesn't use the value, this stream is never closed. 

See also JCR-2067 (FileDataStore)"
0,Move & copy objectsAdd new methods in the persistence manager to move and copy objects
0,"LockInfo.logginOut(SessionImpl): javadoc does not correspond to executed code/**
     * {@inheritDoc}
     * <p/>
     * When the owning session is logging out, we have to perform some
     * operations depending on the lock type.
     * (1) If the lock was session-scoped, we unlock the node.
     * (2) If the lock was open-scoped, we remove the lock token
     *     from the session.
     */
    public void loggingOut(SessionImpl session) {
        if (live) {
            if (sessionScoped) {
                lockMgr.unlock(this);
            } else {
                if (session.equals(lockHolder)) {
                    lockHolder = null;
                }
            }
        }
    } 


if (2) is true, the lockToken is not removed from the session (at least not within the method).

regards
angela"
0,"getPayloadSpans on org.apache.lucene.search.spans.SpanQuery should be abstractI just spent a long time tracking down a bug resulting from upgrading to Lucene 2.4.1 on a project that implements some SpanQuerys of its own and was written against 2.3.  Since the project's SpanQuerys didn't implement getPayloadSpans, the call to that method went to SpanQuery.getPayloadSpans which returned null and caused a NullPointerException in the Lucene code, far away from the actual source of the problem.  

It would be much better for this kind of thing to show up at compile time, I think.

Thanks!"
0,"Make IndexReader.open() always return MSR to simplify (re-)opens.As per discussion in mailing list, I'm making DirectoryIndexReader.open() always return MSR, even for single-segment indexes.
While theoretically valid in the past (if you make sure to keep your index constantly optimized) this feature is made practically obsolete by per-segment collection.

The patch somewhat de-hairies (re-)open logic for MSR/SR.
SR no longer needs an ability to pose as toplevel directory-owning IR.
All related logic is moved from DIR to MSR.
DIR becomes almost empty, and copying two or three remaining fields over to MSR/SR, I remove it.
Lots of tests fail, as they rely on SR returned from IR.open(), I fix by introducing SR.getOnlySegmentReader static package-private method.
Some previous bugs are uncovered, one is fixed in LUCENE-1645, another (partially fixed in LUCENE-1648) is fixed in this patch. "
0,"Issue LUCENE-352 was closed, but the patch there is not applied in the current trunkSee here:
http://issues.apache.org/jira/browse/LUCENE-352

And thanks for making JIRA easier, I noticed the Lucene Java project
was preselected for me.

Regards,
Paul Elschot"
0,"Add TrieRangeFilter to contribAccording to the thread in java-dev (http://www.gossamer-threads.com/lists/lucene/java-dev/67807 and http://www.gossamer-threads.com/lists/lucene/java-dev/67839), I want to include my fast numerical range query implementation into lucene contrib-queries.

I implemented (based on RangeFilter) another approach for faster
RangeQueries, based on longs stored in index in a special format.

The idea behind this is to store the longs in different precision in index
and partition the query range in such a way, that the outer boundaries are
search using terms from the highest precision, but the center of the search
Range with lower precision. The implementation stores the longs in 8
different precisions (using a class called TrieUtils). It also has support
for Doubles, using the IEEE 754 floating-point ""double format"" bit layout
with some bit mappings to make them binary sortable. The approach is used in
rather big indexes, query times are even on low performance desktop
computers <<100 ms (!) for very big ranges on indexes with 500000 docs.

I called this RangeQuery variant and format ""TrieRangeRange"" query because
the idea looks like the well-known Trie structures (but it is not identical
to real tries, but algorithms are related to it).

"
1,Basic Authentification fails with non-ASCII username/password charactershttp://marc.theaimsgroup.com/?t=106866959500001&r=1&w=2
0,"multilingual analyzer based on icuThe standard analyzer in lucene is not exactly unicode-friendly with regards to breaking text into words, especially with respect to non-alphabetic scripts.  This is because it is unaware of unicode bounds properties.

I actually couldn't figure out how the Thai analyzer could possibly be working until i looked at the jflex rules and saw that codepoint range for most of the Thai block was added to the alphanum specification. defining the exact codepoint ranges like this for every language could help with the problem but you'd basically be reimplementing the bounds properties already stated in the unicode standard. 

in general it looks like this kind of behavior is bad in lucene for even latin, for instance, the analyzer will break words around accent marks in decomposed form. While most latin letter + accent combinations have composed forms in unicode, some do not. (this is also an issue for asciifoldingfilter i suppose). 

I've got a partially tested standardanalyzer that uses icu Rule-based BreakIterator instead of jflex. Using this method you can define word boundaries according to the unicode bounds properties. After getting it into some good shape i'd be happy to contribute it for contrib but I wonder if theres a better solution so that out of box lucene will be more friendly to non-ASCII text. Unfortunately it seems jflex does not support use of these properties such as [\p{Word_Break = Extend}] so this is probably the major barrier.

Thanks,
Robert



"
1,"MOVE method returns error 412 Precondition FailedHi, I was trying MacOS X 10.5 Finder's WebDAV client to do testing on Jackrabbit 1.4 which is hosted on Tomcat 5.5.25 on a Windows XP SP2 computer on a LAN. I encounter an error while doing remote editing, I was able to open the text document, but the problem is I couldn't save it.

I tried to find some log on Tomcat but sadly Jackrabbit didn't produces any log files regarding of my problem. So I used Ethereal 0.99.0 to check the packets from the Windows XP computer. The below trace is a summary from the exported text file of the packet analyzer where the problem occur:-

line 11818:-
No.     Time        Source                Destination           Protocol Info
4352 27.629257   10.60.1.90            10.60.1.187           HTTP     MOVE /jackrabbit-webapp-1.4/repository/default/.TemporaryItems/folders.501/TemporaryItems/(A%20Document%20Being%20Saved%20By%20TextEdit)/Copy%20of%20Request_for_GAMS_User_Account.rtf HTTP/1.1

Frame 4352 (592 bytes on wire, 592 bytes captured)
Ethernet II, Src: AppleCom_72:c3:5e (00:0d:93:72:c3:5e), Dst: 00:19:d1:a0:34:f7 (00:19:d1:a0:34:f7)
Internet Protocol, Src: 10.60.1.90 (10.60.1.90), Dst: 10.60.1.187 (10.60.1.187)
Transmission Control Protocol, Src Port: 64970 (64970), Dst Port: 8080 (8080), Seq: 69060, Ack: 90475, Len: 526
    Source port: 64970 (64970)
    Destination port: 8080 (8080)
    Sequence number: 69060    (relative sequence number)
    Next sequence number: 69586    (relative sequence number)
    Acknowledgement number: 90475    (relative ack number)
    Header length: 32 bytes
    Flags: 0x0018 (PSH, ACK)
    Window size: 524280 (scaled)
    Checksum: 0xd4f9 [correct]
    Options: (12 bytes)
Hypertext Transfer Protocol
    MOVE /jackrabbit-webapp-1.4/repository/default/.TemporaryItems/folders.501/TemporaryItems/(A%20Document%20Being%20Saved%20By%20TextEdit)/Copy%20of%20Request_for_GAMS_User_Account.rtf HTTP/1.1\r\n
        Request Method: MOVE
        Request URI: /jackrabbit-webapp-1.4/repository/default/.TemporaryItems/folders.501/TemporaryItems/(A%20Document%20Being%20Saved%20By%20TextEdit)/Copy%20of%20Request_for_GAMS_User_Account.rtf
        Request Version: HTTP/1.1
    User-Agent: WebDAVFS/1.5 (01508000) Darwin/9.1.0 (Power Macintosh)\r\n
    Accept: */*\r\n
    Destination: http://10.60.1.187:8080/jackrabbit-webapp-1.4/repository/default/au/gov/arc/www/rtf/Copy%20of%20Request_for_GAMS_User_Account.rtf\r\n
    Authorization: Basic YWRtaW46YWRtaW4=\r\n
        Credentials: admin:admin
    Content-Length: 0\r\n
    Connection: keep-alive\r\n
    Host: 10.60.1.187:8080\r\n
    \r\n

line 11850 -
No.     Time        Source                Destination           Protocol Info
4353 27.630345   10.60.1.187           10.60.1.90            HTTP     HTTP/1.1 412 Precondition Failed (text/html)

Frame 4353 (1191 bytes on wire, 1191 bytes captured)
Ethernet II, Src: 00:19:d1:a0:34:f7 (00:19:d1:a0:34:f7), Dst: AppleCom_72:c3:5e (00:0d:93:72:c3:5e)
Internet Protocol, Src: 10.60.1.187 (10.60.1.187), Dst: 10.60.1.90 (10.60.1.90)
Transmission Control Protocol, Src Port: 8080 (8080), Dst Port: 64970 (64970), Seq: 90475, Ack: 69586, Len: 1125
    Source port: 8080 (8080)
    Destination port: 64970 (64970)
    Sequence number: 90475    (relative sequence number)
    Next sequence number: 91600    (relative sequence number)
    Acknowledgement number: 69586    (relative ack number)
    Header length: 32 bytes
    Flags: 0x0018 (PSH, ACK)
    Window size: 65535
    Checksum: 0x1c18 [incorrect, should be 0xa2f0]
    Options: (12 bytes)
Hypertext Transfer Protocol
    HTTP/1.1 412 Precondition Failed\r\n
        Request Version: HTTP/1.1
        Response Code: 412
    Server: Apache-Coyote/1.1\r\n
    Content-Type: text/html;charset=utf-8\r\n
    Content-Length: 965\r\n
    Date: Fri, 29 Feb 2008 02:31:01 GMT\r\n
    \r\n
Line-based text data: text/html
    <html><head><title>Apache Tomcat/5.5.25 - Error report</title><style><!--H1 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;font-size:22px;} H2 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#52
"
1,"Large file download over webdav causes exceptionDownloading a large file (>2GB) from webdav causes an exception.

(Note: uploading the file works ok, when jackrabbit is configured to use the filesystem DataStore.)

When trying to retrieve the file with e.g. ""wget"", we get the following error:

Gozer:Desktop greg$ wget --http-user=xxx --http-passwd=xxx http://localhost:8080/jackrabbit/repository/workbench/pkgs/demo/zip/zips/largetest-1.zip
--08:59:50--  http://localhost:8080/jackrabbit/repository/workbench/pkgs/demo/zip/zips/largetest-1.zip
           => `largetest-1.zip'
Resolving localhost... done.
Connecting to localhost[127.0.0.1]:8080... connected.
HTTP request sent, awaiting response... 500 For input string: ""3156213760""
09:04:53 ERROR 500: For input string: ""3156213760"".

In the server log we see this:

06.03.2009 08:59:50 *INFO * RepositoryImpl: SecurityManager = class org.apache.jackrabbit.core.security.simple.SimpleSecurityManager (RepositoryImpl.java, line 432)
2009-03-06 09:04:53.822::WARN:  /jackrabbit/repository/workbench/pkgs/demo/zip/zips/largetest-1.zip
java.lang.NumberFormatException: For input string: ""3156213760""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
	at java.lang.Integer.parseInt(Integer.java:459)
	at java.lang.Integer.parseInt(Integer.java:497)
	at org.apache.jackrabbit.webdav.io.OutputContextImpl.setContentLength(OutputContextImpl.java:60)
	at org.apache.jackrabbit.server.io.ExportContextImpl.informCompleted(ExportContextImpl.java:192)
	at org.apache.jackrabbit.server.io.IOManagerImpl.exportContent(IOManagerImpl.java:157)
	at org.apache.jackrabbit.webdav.simple.DavResourceImpl.spool(DavResourceImpl.java:332)
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.spoolResource(AbstractWebdavServlet.java:422)
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doGet(AbstractWebdavServlet.java:388)
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(AbstractWebdavServlet.java:229)
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(AbstractWebdavServlet.java:196)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:363)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:324)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:864)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:533)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:207)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:403)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)
	at org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:451)


The problem seems to lie in OutputContextImpl.java it makes the mistake of potentially trying to parse a Long as an Integer, here: http://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-webdav/src/main/java/org/apache/jackrabbit/webdav/io/OutputContextImpl.java

in the method setContentLength(long contentLength):

public void setContentLength(long contentLength) {
       int length = Integer.parseInt(contentLength + """");
       if (length >= 0) {
           response.setContentLength(length);
       }
   }

I'm not sure, but a fix might be like this:

public void setContentLength(long contentLength) {
       if(contentLength <= Integer.MAX_VALUE && contentLength >= 0) {
           response.setContentLength((int) contentLength);
       }else if (contentLength >  Integer.MAX_VALUE) {
            response.addHeader(""Content-Length"", Long.toString(contentLength));
       }
   }

This would at least set the Content-Length header, and in some preliminary tests does seem to allow downloading the files."
0,"Upgrade to PDFBox 0.7.3while trying to upload a PDF document (which I can view fine with Acrobat Reader once it is loaded) I get the following exception: 

01.05.2008 12:24:44 *WARN * PdfTextExtractor: Failed to extract PDF text content (PdfTextExtractor.java, line 91)
java.io.IOException: Error: Expected an integer type, actual='%%EOF'
        at org.pdfbox.pdfparser.BaseParser.readInt(BaseParser.java:1159)
        at org.pdfbox.pdfparser.PDFParser.parseObject(PDFParser.java:349)
        at org.pdfbox.pdfparser.PDFParser.parse(PDFParser.java:132)
        at org.apache.jackrabbit.extractor.PdfTextExtractor.extractText(PdfTextExtractor.java:69)
        at org.apache.jackrabbit.extractor.CompositeTextExtractor.extractText(CompositeTextExtractor.java:90)
        at org.apache.jackrabbit.core.query.lucene.JackrabbitTextExtractor.extractText(JackrabbitTextExtractor.java:195)
        at org.apache.jackrabbit.core.query.lucene.NodeIndexer.addBinaryValue(NodeIndexer.java:393)
 ....

I replaced the version of pdfbox (0.6.4) that is bundled with the jackrabbit war file with a more recent version (0.7.3 and fontbox 01.) and it worked fine. The bundled versions should be upgraded.

On the other hand, this software appears to be inactive. Probably a different package should be selected in the long run, but for now, a simple upgrade will do the trick."
0,"Add ability to open prior commits to IndexReaderIf you use a customized DeletionPolicy, which keeps multiple commits
around (instead of the default which is to only preserve the most
recent commit), it's useful to be able to list all such commits and
then open a reader against one of these commits.

I've added this API to list commits:

  public static Collection IndexReader.listCommits(Directory)

and these two new open methods to IndexReader to open a specific commit:

  public static IndexReader open(IndexCommit)
  public static IndexReader open(IndexCommit, IndexDeletionPolicy)

Spinoff from here:

  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200806.mbox/%3c85d3c3b60806161735o207a3238sa2e6c415171a8019@mail.gmail.com%3e

"
0,"Remove circular dependency between VersionManagerImpl and VersionItemStateProviderFrom a architectural perspective the VersionManagerImpl (VMgr) is at a higher level as the VersionItemStateProvider (VISP). While the VMgr deals with Items the VISP deals with ItemState object. Nonetheless the VISP has a reference to the VMgr and also calls the method setNodeReferences(), which violates the rule of a strictly layered system. E.g. one negative effect of this was a deadlock as described in JCR-672. It also makes it hard to solve JCR-962.

The attached patch includes the following changes:

- Move method VersionManagerImpl.setNodeReferences() VersionItemStateManager. The method can operate on ItemStates only and does not need to be in VersionManagerImpl. As can be seen in the current method it directly calls the PeristenceManager, which indicates it should be located in a lower layer.
- Promote the class VersionItemStateManager to a top level class
- Change method VersionManagerImpl.createSharedStateManager to return a VersionItemStateManager
- Remove VersionManagerImpl instance variable from VersionItemStateProvider
- In VersionItemStateProvider.setNodeReferences() call VersionItemStateManager.setNodeReferences()
- Instead of using the PersistenceManager in VersionManagerImpl.getItemReferences() use the ItemStateManager. It also seems that locking is not necessary for this method."
0,"Add javadoc notes about ICUCollationKeyFilter's advantages over CollationKeyFiltercontrib/collation's ICUCollationKeyFilter, which uses ICU4J collation, is faster than CollationKeyFilter, the JVM-provided java.text.Collator implementation in the same package.  The javadocs of these classes should be modified to add a note to this effect.

My curiosity was piqued by [Robert Muir's comment|https://issues.apache.org/jira/browse/LUCENE-1581?focusedCommentId=12720300#action_12720300] on LUCENE-1581, in which he states that ICUCollationKeyFilter is up to 30x faster than CollationKeyFilter.

I timed the operation of these two classes, with Sun JVM versions 1.4.2/32-bit, 1.5.0/32- and 64-bit, and 1.6.0/64-bit, using 90k word lists of 4 languages (taken from the corresponding Debian wordlist packages and truncated to the first 90k words after a fixed random shuffling), using Collators at the default strength, on a Windows Vista 64-bit machine.  I used an analysis pipeline consisting of WhitespaceTokenizer chained to the collation key filter, so to isolate the time taken by the collation key filters, I also timed WhitespaceTokenizer operating alone for each combination.  The rightmost column represents the performance advantage of the ICU4J implemtation (ICU) over the java.text.Collator implementation (JVM), after discounting the WhitespaceTokenizer time (WST): (JVM-ICU) / (ICU-WST). The best times out of 5 runs for each combination, in milliseconds, are as follows:

||Sun JVM||Language||java.text||ICU4J||WhitespaceTokenizer||ICU4J Improvement||
|1.4.2_17 (32 bit)|English|522|212|13|156%|
|1.4.2_17 (32 bit)|French|716|243|14|207%|
|1.4.2_17 (32 bit)|German|669|264|16|163%|
|1.4.2_17 (32 bit)|Ukranian|931|474|25|102%|
|1.5.0_15 (32 bit)|English|604|176|16|268%|
|1.5.0_15 (32 bit)|French|817|209|17|317%|
|1.5.0_15 (32 bit)|German|799|225|20|280%|
|1.5.0_15 (32 bit)|Ukranian|1029|436|26|145%|
|1.5.0_15 (64 bit)|English|431|89|10|433%|
|1.5.0_15 (64 bit)|French|562|112|11|446%|
|1.5.0_15 (64 bit)|German|567|116|13|438%|
|1.5.0_15 (64 bit)|Ukranian|734|281|21|174%|
|1.6.0_13 (64 bit)|English|162|81|9|113%|
|1.6.0_13 (64 bit)|French|192|92|10|122%|
|1.6.0_13 (64 bit)|German|204|99|14|124%|
|1.6.0_13 (64 bit)|Ukranian|273|202|21|39%|
"
0,"TermOrdVal/DocValuesComparator does too much work in compareBottomWe now have logic to fall back to by-value comparison, when the bottom
slot is not from the current reader.

But this is silly, because if the bottom slot is from a different
reader, it means the tie-break case is not possible (since the current
reader didn't have the bottom value), so when the incoming ord equals
the bottom ord we should always return x > 0.

I added a new random string sort test case to TestSort...

I also renamed DocValues.SortedSource.getByValue -> getOrdByValue and
cleaned up some whitespace.
"
1,"Error logged when repository is shut downThis only happens with the bundle DerbyPersistenceManager.

In DerbyPersistenceManager.close() the embedded derby database is shut down and then super.close() is called. There the ConnectionRecoveryManager is closed, which tries to operate on a connection to the already shut down derby database. The log contains entries like:

25.03.2008 13:49:29 *ERROR* [Thread-5] ConnectionRecoveryManager: failed to close connection, reason: No current connection., state/code: 08003/40000 (ConnectionRecoveryManager.java, line 453)"
1,"Problems mapping custom collectionsWhen using a custom list that extends from java.util.AbstractList, ManageableCollectionUtil.getManageableCollection raises a JcrMappingException because it does not consider the custom list to be a java.util.List. This is because it uses ""if (object.getClass().equals(List.class))"" instead of ""if (object instanceof List)"". The same thing will probably happen when using a custom Collection, a custom ArrayList, etc. This is the stack trace:

org.apache.jackrabbit.ocm.exception.JcrMappingException: Unsupported collection 
type : *********** (MyCustomList class) 
        at org.apache.jackrabbit.ocm.manager.collectionconverter.ManageableColle 
ctionUtil.getManageableCollection(ManageableCollectionUtil.java:153) 
        at org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverte 
rImpl.insertCollectionFields(ObjectConverterImpl.java:780) 
        at org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverte 
rImpl.insert(ObjectConverterImpl.java:221) 
        at org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverte 
rImpl.insert(ObjectConverterImpl.java:146) 
        at org.apache.jackrabbit.ocm.manager.impl.ObjectContentManagerImpl.inser 
t(ObjectContentManagerImpl.java:407) 

I have come up to this bug using a MyCustomList<MyClass>, with MyCustomList extending java.util.AbstractList<MyClass>."
1,"SharedFieldCache can cause a memory leakThe SharedFieldCache has some problems with the way it builds the cache:
 - as key is has the IndexReader
 - as value it has a inner cache (another map) that has as a key a static inner class called 'Key'.

This 'Key' holds a reference to the comparator used for in the queries ran.
Assuming this comparator is of any type that extends from AbstractFieldComparator (I think all of the custom JR comparators), then it keeps a reference to all the InderReader instances in order to be able to load the values as Comparable(s).

So the circle is complete and the SharedFieldCache entries never get GC'ed.

One option would have been to implement a 'purge' method on the cache, similar to the lucene mechanism, and when an InderReader gets closed is could call 'purge'. But that is both ugly AND is doesn't seem to work that well :)

A more radical option is to remove the cache completely. Each instance of SimpleFieldComparator (the only client of this cache) already builds an array of the available values, so the cache would only help other instances of the same type. We'll not analyze this further.

The proposed solution (patch will follow shortly) is to remove the Comparator reference from the Key class. 
It looks like it has no real purpose there, just to impact the 'equals' of the key, which makes no sense in the first place as the lucene query does not use the Comparator info at all.
If anything, using the same field and 2 different Comparators we'll get 2 different cache entries based on the same values from the lucene index.

Feedback is appreciated!








"
0,"Deprecate Spatial ContribThe spatial contrib is blighted by bugs.  The latest series, found by Grant and discussed [here|http://search.lucidimagination.com/search/document/c32e81783642df47/spatial_rethinking_cartesian_tiers_implementation] shows that we need to re-think the cartesian tier implementation.

Given the need to create a spatial module containing code taken from both lucene and Solr, it makes sense to deprecate the spatial contrib, and start from scratch in the new module.


"
0,"Small imprecision in Search package JavadocsSearch package Javadocs states that Scorer#score(Collector) will be abstract in Lucene 3.0, which is not accurate anymore."
1,"QueryObjectModel does not generate the corresponding SQL2 Query when dealing with spaces in the pathThis is the original issue:
----------
I tried to get the childnodes of a node names ""/a b"" using the following code
  QueryManager queryManager=session.getWorkspace().getQueryManager();
  QueryObjectModelFactory qomf=queryManager.getQOMFactory();
  Source source1=qomf.selector(NodeType.NT_BASE, ""selector_0"");
  Column[] columns = new Column[]{qomf.column(""selector_0"", null, null)};
  Constraint constraint2 = qomf.childNode(""selector_0"", ""/a b"");
  QueryObjectModel qom = qomf.createQuery(source1, constraint2 , null, columns);

This is not giving any result when the session is acquired through webdav. But when connected using JNDI it is giving the child nodes. 

The sql statement getting created is 
SELECT selector_0.* FROM [nt:base] AS selector_0 WHERE ISCHILDNODE(selector_0, 
[/a b]).

When using webdav If i give this SQL2 query directly along with quotes around 
the path i.e. ['/a b'] then it is working as expected.
----------

this doesn't have anything to do with webdav. the problem is the QueryObjectModel generates an SQL2 query that is not 100% equivalent, it fails to escape paths that have spaces in them.
this way, in the case of davex remoting, the jr client will use the statement generated instead, which is not escaped, and will fail to return the expected nodes. 

This can be seen easily if we do a System.out.println(qom.getStatement())


"
1,"Filters need hashCode() and equals()Filters need to implement hashCode() and equals(), esp since certain query types can contain a filter (FilteredQuery, ConstantScoreQuery)"
0,"HttpMethodBase.getResponseBodyAsString(long limit)Currently HttpMethodBase.getResponseBodyAsString() prints warning in log, and suggests using getResponseStream(). However getResponseBodyAsString() is extremely useful (as it is easy to use). So my wish is to have method

getResponseBodyAsString(long limit)

that should throw HttpException if response size exceeds specified limit.

Same things with getResponseBody(long limit) .

Original methods should be deprecated because of danger, explained in javadoc."
1,"JCR2SPI: NPE when parentId returned by NodeInfo.getParentId does not show up in parent's child node listIn this custom SPI implementation, version history nodes appear as children of jcr:versionStorage, but jcr:versionStorage does not return them as children (which would be impractical for performance reasons - I expect similar approaches used by others...).

getParentId of a NodeInfo of a VersionHistory return the NodeId for jcr:versionStorage. In this case, I get the NPE below:

java.lang.NullPointerException
	at org.apache.jackrabbit.jcr2spi.state.WorkspaceItemStateFactory.createNodeState(WorkspaceItemStateFactory.java:99)
	at org.apache.jackrabbit.jcr2spi.state.CachingItemStateManager.resolve(CachingItemStateManager.java:168)
	at org.apache.jackrabbit.jcr2spi.state.CachingItemStateManager.getItemState(CachingItemStateManager.java:94)
	at org.apache.jackrabbit.jcr2spi.WorkspaceManager.getItemState(WorkspaceManager.java:328)
	at org.apache.jackrabbit.jcr2spi.state.TransientISFactory.createNodeState(TransientISFactory.java:120)
	at org.apache.jackrabbit.jcr2spi.state.CachingItemStateManager.resolve(CachingItemStateManager.java:168)
	at org.apache.jackrabbit.jcr2spi.state.CachingItemStateManager.getItemState(CachingItemStateManager.java:94)
	at org.apache.jackrabbit.jcr2spi.state.TransientItemStateManager.getItemState(TransientItemStateManager.java:209)
	at org.apache.jackrabbit.jcr2spi.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:155)
	at org.apache.jackrabbit.jcr2spi.SessionImpl.getNodeById(SessionImpl.java:271)
	at org.apache.jackrabbit.jcr2spi.SessionImpl.getNodeByUUID(SessionImpl.java:239)

Returning null in this special case fixes the problem over here, but seems to create new problems elsewhere.

Need to clarify the SPI itself, and potentially fix JCR2CPI.
"
0,Add example test case for surround query language
0,"Omit positions but keep termFreqit would be useful to have an option to discard positional information but still keep the term frequency - currently setOmitTermFreqAndPositions discards both. Even though position-dependent queries wouldn't work in such case, still any other queries would work fine and we would get the right scoring."
1,"IndexOutOfBoundsException from FieldsReader after problem reading the indexThere is a situation where there is an IOException reading from Hits, and then the next time you get a NullPointerException instead of an IOException.

Example stack traces:

java.io.IOException: The specified network name is no longer available
	at java.io.RandomAccessFile.readBytes(Native Method)
	at java.io.RandomAccessFile.read(RandomAccessFile.java:322)
	at org.apache.lucene.store.FSIndexInput.readInternal(FSDirectory.java:536)
	at org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:74)
	at org.apache.lucene.index.CompoundFileReader$CSIndexInput.readInternal(CompoundFileReader.java:220)
	at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:93)
	at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:34)
	at org.apache.lucene.store.IndexInput.readVInt(IndexInput.java:57)
	at org.apache.lucene.index.FieldsReader.doc(FieldsReader.java:88)
	at org.apache.lucene.index.SegmentReader.document(SegmentReader.java:344)
	at org.apache.lucene.index.IndexReader.document(IndexReader.java:368)
	at org.apache.lucene.search.IndexSearcher.doc(IndexSearcher.java:84)
	at org.apache.lucene.search.Hits.doc(Hits.java:104)

That error is fine.  The problem is the next call to doc generates:

java.lang.NullPointerException
	at org.apache.lucene.index.FieldsReader.getIndexType(FieldsReader.java:280)
	at org.apache.lucene.index.FieldsReader.addField(FieldsReader.java:216)
	at org.apache.lucene.index.FieldsReader.doc(FieldsReader.java:101)
	at org.apache.lucene.index.SegmentReader.document(SegmentReader.java:344)
	at org.apache.lucene.index.IndexReader.document(IndexReader.java:368)
	at org.apache.lucene.search.IndexSearcher.doc(IndexSearcher.java:84)
	at org.apache.lucene.search.Hits.doc(Hits.java:104)

Presumably FieldsReader is caching partially-initialised data somewhere.  I would normally expect the exact same IOException to be thrown for subsequent calls to the method.
"
0,"clean up obselete information on the websiteWhen searching for information on 'lucene indexing speed' I get back some really out of date stuff:
1. on the features page it proudly proclaims 20MB/minute, on some really old hardware. I think we should
change this to 95GB/hour: http://blog.mikemccandless.com/2010/09/lucenes-indexing-is-fast.html
2. there are ancient benchmarks results from versioned data we link to the website. We list versioned
websites for ancient versions going back to 1.4.3. Also i noticed when just casually googling for
API documentation I tend to get results going to these ancient versions. I think we should remove
stuff for all versions prior to 2.9"
1,"TestSort testParallelMultiSort reproducible seed failuretrunk r1202157
{code}
    [junit] Testsuite: org.apache.lucene.search.TestSort
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.978 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestSort -Dtestmethod=testParallelMultiSort -Dtests.seed=-2996f3e0f5d118c2:32c8e62dd9611f63:7a90f44586ae8263 -Dargs=""-Dfile.encoding=UTF-8""
    [junit] WARNING: test method: 'testParallelMultiSort' left thread running: Thread[pool-1-thread-1,5,main]
    [junit] WARNING: test method: 'testParallelMultiSort' left thread running: Thread[pool-1-thread-2,5,main]
    [junit] WARNING: test method: 'testParallelMultiSort' left thread running: Thread[pool-1-thread-3,5,main]
    [junit] NOTE: test params are: codec=Lucene40: {short=Lucene40(minBlockSize=98 maxBlockSize=214), contents=PostingsFormat(name=MockSep), byte=PostingsFormat(name=SimpleText), int=Pulsing40(freqCutoff=4 minBlockSize=58 maxBlockSize=186), string=PostingsFormat(name=NestedPulsing), i18n=Lucene40(minBlockSize=98 maxBlockSize=214), long=PostingsFormat(name=Memory), double=Pulsing40(freqCutoff=4 minBlockSize=58 maxBlockSize=186), parser=MockVariableIntBlock(baseBlockSize=88), float=Lucene40(minBlockSize=98 maxBlockSize=214), custom=PostingsFormat(name=MockRandom)}, sim=RandomSimilarityProvider(queryNorm=false,coord=false): {short=BM25(k1=1.2,b=0.75), tracer=DFR I(ne)B2, byte=DFR I(ne)B3(800.0), contents=IB LL-LZ(0.3), int=DFR I(n)BZ(0.3), string=IB LL-D3(800.0), i18n=DFR GB2, double=DFR I(ne)B2, long=DFR GB1, parser=DFR GL2, float=BM25(k1=1.2,b=0.75), custom=DFR I(ne)Z(0.3)}, locale=ga_IE, timezone=America/Louisville
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestSort]
    [junit] NOTE: Linux 3.0.6-gentoo amd64/Sun Microsystems Inc. 1.6.0_29 (64-bit)/cpus=8,threads=4,free=78022136,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testParallelMultiSort(org.apache.lucene.search.TestSort): FAILED
    [junit] expected:<[ZJ]I> but was:<[JZ]I>
    [junit] junit.framework.AssertionFailedError: expected:<[ZJ]I> but was:<[JZ]I>
    [junit]     at org.apache.lucene.search.TestSort.assertMatches(TestSort.java:1245)
    [junit]     at org.apache.lucene.search.TestSort.assertMatches(TestSort.java:1216)
    [junit]     at org.apache.lucene.search.TestSort.runMultiSorts(TestSort.java:1202)
    [junit]     at org.apache.lucene.search.TestSort.testParallelMultiSort(TestSort.java:855)
    [junit]     at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:523)
    [junit]     at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:149)
    [junit]     at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:51)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.search.TestSort FAILED
{code}"
1,LuceneQueryFactory should call QueryHits.close() after running a queryLuceneQueryFactory which is responsible for the JCR_SQL2 implementation does not close QueryHits after running a query.
1,"Locking bugIn org.apache.lucene.store.Lock, line 57 (lucene_1_4_final branch):

if (++sleepCount == maxSleepCount)

is incorrect, the sleepCount is incremented before the compare causing it
throwing the exception with out waiting for at least 1 interation.

Should be changed instead to:
if (sleepCount++ == maxSleepCount)

As this is a self-contained simple fix, I am not submitting a patch.

Thanks

-John"
1,Search results not orderedThe query statements in search.jsp do not have an order by.
0,"Upgrade to Java 5 as the base platformAs discussed on the mailing list, Jackrabbit 2.0 will use Java 5 as the base platform.

Now that 1.x has been branched, we can update the build settings in trunk to use Java 5."
0,"Optimize ReadOnlyIndexReader.read(int[] docs, int[] freqs)This method is currently implemented trivially using next(), doc() and freq(). It should read in blocks and filter out deleted docs."
0,JSR 283 NodeType Management
0,"Use only the standard Maven repository for dependenciesThe JCR API jars are now available in the standard Maven repository, see http://jira.codehaus.org/browse/MAVENUPLOAD-1050. We could thus remove the dependency on the Day repository, as requested in http://jira.codehaus.org/browse/MEV-453.
"
0,"Move multipart request to a new RequestEntity typeMultipart posts are currently handled via a separate post method, the MultipartPostMethod.  This 
separate method is unnecessary given the new RequestEntity mechanism."
1,"Prefix fulltext queries with Japanese or Chinese characters fail to matchPrefix fulltext queries with Japanese or Chinese characters do not match because the prefix part is not tokenized. This means, when the prefix length is >1 the sequence of characters is taken as one term to do the index lookup. This will not match anything because on indexing time such characters are always broken into individual tokens."
0,"port url+email tokenizer to standardtokenizerinterface (or similar)We should do this so that we can fix the LUCENE-3358 bug there, and preserve backwards.
We also want this mechanism anyway, for upgrading to new unicode versions in the future.

We can regenerate the new TLD list for 3.4 but, we should ensure the existing one is used for the urlemail33 or whatever,
so that its exactly the same."
0,"JCR2SPI: remove duplicate item statesthe original approach with duplicate item state objects connected to each is not required any more 
and can be simplified."
1,"DefaultRequestDirector converts redirects of PUT/POST to GET for status codes 301, 302, 307The DefaultRequestDirector treats redirect requests created by all redirect status codes (HttpStatus.SC_MOVED_TEMPORARILY: , HttpStatus.SC_MOVED_PERMANENTLY, HttpStatus.SC_SEE_OTHER, HttpStatus.SC_TEMPORARY_REDIRECT) the same, converting PUT/POST methods to GET.  The HttpClient Tutorial even documents this as being in accordance with the specification, but I don't believe that's true.

Per the RFC (http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html), conversion of PUT/POST to GET is appropriate only for 303 (See Other).  The others do not suggest this behavior.  In fact, the following notes attached to them call it out as incorrect.

301 (Moved Permanently) has this note:

      Note: When automatically redirecting a POST request after
      receiving a 301 status code, some existing HTTP/1.0 user agents
      will erroneously change it into a GET request.

And 302 (Found) say this:

      Note: RFC 1945 and RFC 2068 specify that the client is not allowed
      to change the method on the redirected request.  However, most
      existing user agent implementations treat 302 as if it were a 303
      response, performing a GET on the Location field-value regardless
      of the original request method. The status codes 303 and 307 have
      been added for servers that wish to make unambiguously clear which
      kind of reaction is expected of the client.

The currently implemented behavior is causing problems with interacting with Central Authentication Service protected resources, among other things."
0,"Move some *TermsEnum.java from oal.search to oal.indexI think FilteredTermsEnum, SingleTermsEnum should move?

I left TermRangeTermsEnum and FuzzyTermsEnum and PrefixTermsEnum since they seemed search specific."
1,"Preemptive auth flags disregarded during ssl tunnel creationUsing a Squid2.4 proxy, the connection is dropped when trying to connect to a 
ssl site. In order for the connection to remain open, preemptive authorization 
is needed for the proxy. The preemptive authorization flags are not propagated 
down to where the ssl tunnel is created in HttpMethodDirectors executeConnect 
method. A new ConnectMethod object is created for the tunnel but the preemptive 
flags set as parameters are not being set on the new ConnectMethod object.

Here is the code that would replicate the problem using a Squid(2.4) proxy :

HttpClient client = new HttpClient();
client.getHostConfiguration().setProxyHost(new ProxyHost(""someproxy"", 3128));
client.getParams().setAuthenticationPreemptive(true);
client.getState().setProxyCredentials(AuthScope.ANY, new 
UsernamePasswordCredentials(""user"", ""password""));
GetMethod httpget = new GetMethod(""https://www.verisign.com/"");
httpget.getProxyAuthState().setPreemptive();
client.executeMethod(httpget);
httpget.releaseConnection();"
1,"TestParser.testSpanTermXML fails with some simshere is why this test sometimes fails (my explanation in the test i wrote):

{noformat}
  /** make sure all sims work with spanOR(termX, termY) where termY does not exist */
  public void testCrazySpans() throws Exception {
    // The problem: ""normal"" lucene queries create scorers, returning null if terms dont exist
    // This means they never score a term that does not exist.
    // however with spans, there is only one scorer for the whole hierarchy:
    // inner queries are not real queries, their boosts are ignored, etc.
{noformat}

Basically, SpanQueries aren't really queries, you just get one scorer. it calls extractTerms on the whole hierarchy and computes weights (e.g. IDF) on
the whole bag of terms, even if they don't exist.

This is fine, we already have tests that sim's won't bug-out in computeStats() here: however they don't expect to actually score documents based on
these terms that don't exist... however this is exactly what happens in Spans because it doesn't use sub-scorers.

Lucene's sim avoids this with the (docFreq + 1)
"
1,"benchmark cannot parse highlight-vs-vector-highlight.alg, but only on 3.x?!A new test (TestPerfTasksParse.testParseExamples) was added in LUCENE-3768 that 
guarantees all .alg files in the conf/ directory can actually be parsed...

But highlight-vs-vector-highlight.alg cannot be parsed on 3.x (NumberFormatException), 
however it works fine on trunk... and the .alg is exactly the same in both cases.

{noformat}
    [junit] ------------- Standard Error -----------------
    [junit] java.lang.NumberFormatException: For input string: ""maxFrags[3.0],fields[body]""
    [junit] 	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1222)
    [junit] 	at java.lang.Float.parseFloat(Float.java:422)
    [junit] 	at org.apache.lucene.benchmark.byTask.tasks.SearchTravTask.setParams(SearchTravTask.java:76)
    [junit] 	at org.apache.lucene.benchmark.byTask.tasks.SearchTravRetVectorHighlightTask.setParams(SearchTravRetVectorHighlightTask.java:124)
    [junit] 	at org.apache.lucene.benchmark.byTask.utils.Algorithm.<init>(Algorithm.java:112)
    [junit] 	at org.apache.lucene.benchmark.byTask.TestPerfTasksParse.testParseExamples(TestPerfTasksParse.java:132)
{noformat}
"
0,"Occur incompletely implemented for remote use.Occur does not implement readResolve() creating problems for
ParallelMultiSearcher y."
1,"trectopicsreader doesn't properly read descriptions or narrativesTrecTopicsReader does not read these fields correctly, as demonstrated by the test case.
"
1,"Rare thread hazard in IndexWriter.commit()The nightly build 2 nights ago hit this:

{code}
 NOTE: random seed of testcase 'testAtomicUpdates' was: -5065675995121791051
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAtomicUpdates(org.apache.lucene.index.TestAtomicUpdate):	FAILED
    [junit] expected:<100> but was:<91>
    [junit] junit.framework.AssertionFailedError: expected:<100> but was:<91>
    [junit] 	at org.apache.lucene.index.TestAtomicUpdate.runTest(TestAtomicUpdate.java:142)
    [junit] 	at org.apache.lucene.index.TestAtomicUpdate.testAtomicUpdates(TestAtomicUpdate.java:194)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)
{code}

It's an intermittant failure that only happens when multiple threads
are calling commit() at once.  With autoComit=true and
ConcurrentMergeScheduler, this can happen more often because each
merge thread calls commit after it's done.

The problem happens when one thread has already begun the commit
process, but another two or more threads then come along wanting to
also commit after further changes have happened.  Those two or more
threads would wait until the currently committing thread finished, and
then they'd wake up and do their commit.  The problem was, after
waking up they would fail to check whether they had been superseded,
ie whether another thread had already committed more up-to-date
changes.

The fix is simple -- after waking up, check again if your commit has
been superseded, and skip your commit if so.
"
1,"IndexWriter.optimize(boolean doWait) ignores doWait parameter{{IndexWriter.optimize(boolean doWait)}} ignores the doWait parameter and always calls {{optimize(1, true)}}.

That does not seem to be the intended behavior, based on the doc comment."
1,"import must not ignore xml prefixed attributesXML import currently ignores attributes that are in the xml namespace.
e.g., DocViewImportHandler's startElement():

                if (atts.getQName(i).startsWith(""xml:"")) {
                    // skipping xml:space, xml:lang, etc.
                    log.debug(""skipping reserved/system attribute "" + atts.getQName(i));
                    continue;
                }

That is a significant loss of information, since xml:base, xml:lang, and xml:id attributes are critical to the content.  We should register the xml prefix as a reserved namespace (not needing an xmlns declaration) and then treat it like any other attribute.

Here are some useful XML examples:

http://xformsinstitute.com/essentials/browse/ch03s02.php
http://www.zvon.org/HowTo/Output/
http://www.w3.org/Math/testsuite/testsuite/TortureTests/Complexity/complex1.xml
http://intertwingly.net/wiki/pie/EchoExample
http://support.sas.com/onlinedoc/913/getDoc/en/engxml.hlp/a002973381.htm

"
0,Jcr-Server Module: Remove Dependency from Jackrabbit-Core
0,Generate jar containing test classes.The test classes are useful for writing unit tests for code external to the Lucene project. It would be helpful to build a jar of these classes and publish them as a maven dependency.
0,"Create resource sensitive cache for item statesthere is currently a lru-caching strategy for the itemstates in the shared ism, with a hardcoded limit of 1000 entries. the problem is that the size of the states is not respected in the caching strategy; this poses a problem, if the
states are large (i.e. large values in property states, or large number of childnode entries)."
0,"Make collecting group membership information lazyJCR-2710 added a more scalable content model for storing group membership information. To further leverage the new model it would be preferable when group membership collecting where lazy. (i.e. Group#getDeclaredMembers() and Group#getMembers() should not construct the list of all members up front). 
"
0,"Remove code duplication from Token class, just extend TermAttributeImplThis issue removes the code duplication from Token, as it shares the whole char[] buffer handling code with TermAttributeImpl. This issue removes this duplication by just extending TermAttributeImpl.

When the parent issue LUCENE-2302 will extend TermAttribute to support CharSequence and Appendable and also the new BytesRefAttribute gets added, Token will automatically provide this too, so no further code duplication.

This code should also be committed to trunk, as it has nothing to do with flex."
1,Session#move doesn't trigger rebuild of parent node aggregationThe summary says it all.
1,"XercesImpl is missing in WebDav contrib project$ /usr/local/maven/bin/maven
 __  __
|  \/  |__ _Apache__ ___
| |\/| / _` \ V / -_) ' \  ~ intelligent projects ~
|_|  |_\__,_|\_/\___|_||_|  v. 1.0.2

build:start:

multiproject:install:
multiproject:projects-init:
    [echo] Gathering project list
Starting the reactor...
Our processing order:
JCRWebdavServer Webdav Library
JCRWebdavServer Server Library
JCRWebdavServer Client Library
JCRWebdavServer WebApplication
+----------------------------------------
| Gathering project list JCRWebdavServer Webdav Library
| Memory: 3M/4M
+----------------------------------------
+----------------------------------------
| Gathering project list JCRWebdavServer Server Library
| Memory: 3M/4M
+----------------------------------------
+----------------------------------------
| Gathering project list JCRWebdavServer Client Library
| Memory: 3M/4M
+----------------------------------------
+----------------------------------------
| Gathering project list JCRWebdavServer WebApplication
| Memory: 3M/4M
+----------------------------------------
Starting the reactor...
Our processing order:
JCRWebdavServer Webdav Library
JCRWebdavServer Server Library
JCRWebdavServer Client Library
JCRWebdavServer WebApplication
+----------------------------------------
| Executing multiproject:install-callback JCRWebdavServer Webdav Library
| Memory: 3M/4M
+----------------------------------------
Attempting to download jackrabbit-commons-1.0-SNAPSHOT.jar.
Response content length is not known
Artifact /org.apache.jackrabbit/jars/jackrabbit-commons-1.0-SNAPSHOT.jar doesn't exists in remote repository, but it exists locally

multiproject:goal:
build:start:

multiproject:install-callback:
    [echo] Running jar:install for JCRWebdavServer Webdav Library
java:prepare-filesystem:

java:compile:
    [echo] Compiling to /home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/target/classes
    [javac] Compiling 109 source files to /home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/target/classes
/home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/src/java/org/apache/jackrabbit/webdav/WebdavResponseImpl.java:26: package org.apache.xml.serialize does not exist
import org.apache.xml.serialize.OutputFormat;
                                ^
/home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/src/java/org/apache/jackrabbit/webdav/WebdavResponseImpl.java:27: package org.apache.xml.serialize does not exist
import org.apache.xml.serialize.XMLSerializer;
                                ^
/home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/src/java/org/apache/jackrabbit/webdav/WebdavResponseImpl.java:149: cannot resolve symbol
symbol  : class OutputFormat 
location: class org.apache.jackrabbit.webdav.WebdavResponseImpl
                OutputFormat format = new OutputFormat(""xml"", ""UTF-8"", true);
                ^
/home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/src/java/org/apache/jackrabbit/webdav/WebdavResponseImpl.java:149: cannot resolve symbol
symbol  : class OutputFormat 
location: class org.apache.jackrabbit.webdav.WebdavResponseImpl
                OutputFormat format = new OutputFormat(""xml"", ""UTF-8"", true);
                                          ^
/home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/src/java/org/apache/jackrabbit/webdav/WebdavResponseImpl.java:150: cannot resolve symbol
symbol  : class XMLSerializer 
location: class org.apache.jackrabbit.webdav.WebdavResponseImpl
                XMLSerializer serializer = new XMLSerializer(out, format);
                ^
/home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/src/java/org/apache/jackrabbit/webdav/WebdavResponseImpl.java:150: cannot resolve symbol
symbol  : class XMLSerializer 
location: class org.apache.jackrabbit.webdav.WebdavResponseImpl
                XMLSerializer serializer = new XMLSerializer(out, format);
                                               ^
Note: Some input files use or override a deprecated API.
Note: Recompile with -deprecation for details.
6 errors

BUILD FAILED
File...... /home/jeremi/.maven/cache/maven-multiproject-plugin-1.3.1/plugin.jelly
Element... maven:reactor
Line...... 217
Column.... -1
Unable to obtain goal [multiproject:install-callback] -- /home/jeremi/.maven/cache/maven-java-plugin-1.5/plugin.jelly:63:-1: <ant:javac> Compile failed; see the compiler error output for details.
Total time: 8 seconds
Finished at: Fri Jan 27 07:14:37 CET 2006

$"
0,"Flex on non-flex emulation of TermsEnum incorrectly seeks/nexts beyond current fieldSpinoff of LUCENE-2111, where Uwe found this issue with the flex on non-flex emulation."
1,"suggest.fst.Sort.BufferSize should not automatically fail just because of freeMemory()Follow up op dev thread: [FSTCompletionTest failure ""At least 0.5MB RAM buffer is needed"" | http://markmail.org/message/d7ugfo5xof4h5jeh]"
0,"Add Thread-Safety note to IndexWriter JavaDocIndexWriter Javadocs should contain a note about thread-safety. This is already mentioned on the wiki FAQ page but such an essential information should be part of the module documentation too.
"
1,"spi2davex: session-scoped lock tokens not included in if-headerdetected while running API lock tests.
org.apache.jackrabbit.test.api.lock.DeepLockTest#testParentChildDeepLock failed though it used to work with spi2dav.

fix is simple: SessionInfoImpl.getAllLockTokens must be used to populate the if-header as it is done in spi2dav."
0,"add a test for PorterStemFilterThere are no tests for PorterStemFilter, yet svn history reveals some (very minor) cleanups, etc.
The only thing executing its code in tests is a test or two in SmartChinese tests.

This patch runs the StemFilter against Martin Porter's test data set for this stemmer, checking for expected output.

The zip file is 100KB added to src/test, if this is too large I can change it to download the data instead.
"
0,Javadoc improvements for Payload classSome methods in org.apache.lucene.index.Payload don't have javadocs
0,"NTLM implementation lacks support for NTLMv1, NTLMv2, and NTLM2 Session forms of NTLMThe current HttpClient implementation lacks support for all enhancements to NTLM after Windows 95.  That includes NTLMv1, NTLMv2, and NTLM2 Session Response varieties of the protocol.

This seriously impacts the usability of HttpClient in enterprise situations, which has required the Lucene Connector Framework team to extend HttpClient to address the issue.

I've attached a patch which contains the implementation used by LCF.
"
0,"GData-Server - Website sandbox partAdded GData-Server to the sandbox part of the website -- xdocs/sandbox/

Build of website is fine."
1,"jcr:frozenUuid does not contain jcr:contentWhen I store versionable files, I get problems retrieving the jcr:data from a custom node type.

I am storing a node type:

xrc:learningContent
        pd: xrc:Keywords
        pd: xrc:MimeType
        pd: jcr:mixinTypes
        pd: xrc:Description
        pd: xrc:Language
        pd: xrc:Creator
        pd: jcr:created
        pd: xrc:Title
        pd: jcr:primaryType
Extends: nt:resource
        pd: jcr:uuid
        pd: jcr:mixinTypes
        pd: jcr:data
        pd: jcr:encoding
        pd: jcr:mimeType
        pd: jcr:lastModified
        pd: jcr:primaryType

So I commit the changes, then later pull up the version and get it's frozenNode.

Node frozenNode = v.getNode(JcrConstants.JCR_FROZENNODE);

And then I return all of the properties contained within:

PropertyIterator pi = frozenNode.getProperties();
                while (pi.hasNext()) {
                    System.out.println(pi.nextProperty().getName());
}


All that are returned are:

jcr:frozenUuid
jcr:uuid
jcr:frozenPrimaryType
jcr:frozenMixinTypes
jcr:primaryType

Here is the frozen node type:

nt:frozenNode
        pd: *
        pd: *
        pd: jcr:frozenUuid
        pd: jcr:uuid
        pd: jcr:mixinTypes
        pd: jcr:frozenPrimaryType
        pd: jcr:frozenMixinTypes
        pd: jcr:primaryType



So basically it would seem that the recursive copy inside the InternalFrozenNodeImpl is not working. But it seems that is not the case from the code trace I did. Add this to line 368 of InternalFrozenNodeImpl.java

System.out.println(""New node created. Props: "");
        try {
            PropertyState [] ps = node.getProperties();
            for (PropertyState p : ps) {
                System.out.println(p.getName());
                System.out.println(p.toString());
            }
            NodeStateEx [] ns = node.getChildNodes();
            for (NodeStateEx n : ns) {
                System.out.println(n.getName());
                System.out.println(n.toString());
            }
        } catch (ItemStateException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }


And you will get the result:


New node created. Props:
{http://www.jcp.org/jcr/1.0}uuid
org.apache.jackrabbit.core.state.PropertyState@10dd791
{http://www.jcp.org/jcr/1.0}frozenPrimaryType
org.apache.jackrabbit.core.state.PropertyState@1c38291
{http://www.jcp.org/jcr/1.0}frozenMixinTypes
org.apache.jackrabbit.core.state.PropertyState@b12fbb
{http://www.jcp.org/jcr/1.0}baseVersion
org.apache.jackrabbit.core.state.PropertyState@b4d4b6
{http://www.jcp.org/jcr/1.0}primaryType
org.apache.jackrabbit.core.state.PropertyState@1f9045f
{http://www.jcp.org/jcr/1.0}isCheckedOut
org.apache.jackrabbit.core.state.PropertyState@18e16b5
{http://www.jcp.org/jcr/1.0}frozenUuid
org.apache.jackrabbit.core.state.PropertyState@174cb00
{http://www.jcp.org/jcr/1.0}predecessors
org.apache.jackrabbit.core.state.PropertyState@bb7c1b
{http://www.jcp.org/jcr/1.0}data
org.apache.jackrabbit.core.state.PropertyState@d10133
{http://www.jcp.org/jcr/1.0}versionHistory
org.apache.jackrabbit.core.state.PropertyState@1a5f001
{http://www.jcp.org/jcr/1.0}encoding
org.apache.jackrabbit.core.state.PropertyState@12fe3ef
{http://www.jcp.org/jcr/1.0}mimeType
org.apache.jackrabbit.core.state.PropertyState@11d92c8
{http://www.jcp.org/jcr/1.0}lastModified
org.apache.jackrabbit.core.state.PropertyState@8fb83a
New node created. Props:
{http://www.xerceo.com/learn/jcr-1.0}Keywords
org.apache.jackrabbit.core.state.PropertyState@18808f3
{http://www.jcp.org/jcr/1.0}uuid
org.apache.jackrabbit.core.state.PropertyState@397a4
{http://www.jcp.org/jcr/1.0}frozenPrimaryType
org.apache.jackrabbit.core.state.PropertyState@1d88ffd
{http://www.xerceo.com/learn/jcr-1.0}Creator
org.apache.jackrabbit.core.state.PropertyState@d5625b
{http://www.xerceo.com/learn/jcr-1.0}Language
org.apache.jackrabbit.core.state.PropertyState@12c70e6
{http://www.xerceo.com/learn/jcr-1.0}Title
org.apache.jackrabbit.core.state.PropertyState@a836b3
{http://www.jcp.org/jcr/1.0}frozenMixinTypes
org.apache.jackrabbit.core.state.PropertyState@19f273c
{http://www.jcp.org/jcr/1.0}primaryType
org.apache.jackrabbit.core.state.PropertyState@1c8e97d
{http://www.jcp.org/jcr/1.0}frozenUuid
org.apache.jackrabbit.core.state.PropertyState@15915a3
{http://www.jcp.org/jcr/1.0}predecessors
org.apache.jackrabbit.core.state.PropertyState@19ba907
{http://www.xerceo.com/learn/jcr-1.0}MimeType
org.apache.jackrabbit.core.state.PropertyState@763ca1
{http://www.xerceo.com/learn/jcr-1.0}Description
org.apache.jackrabbit.core.state.PropertyState@8687e8
{http://www.jcp.org/jcr/1.0}versionHistory
org.apache.jackrabbit.core.state.PropertyState@44ca0f
{http://www.jcp.org/jcr/1.0}content
org.apache.jackrabbit.core.version.NodeStateEx@2da721

So the new Node definately has these new properties.

Do I have to somehow extend my frozenNode to work with this? Can anyone help me?"
0,"Allow readOnly OpenReader taskI'd like to change OpenReader in contrib/benchmark to open a readOnly reader by default, and take readOnly optional param if for some reason a ""writable IndexReader"" becomes necessary in the future."
0,"Fix incorrect IndexingQueueTest logicThe IndexingQueueTest class assumes that a Session.save() call will push all pending text extraction tasks to the indexing queue, when in fact those can still be kept waiting in the VolatileIndex."
1,"ConcurrentModificationException in QueryStatImplRunning with qurystats enabled the Query#execute can throw ConcurrentModificationException

caused by the iterator which backing collection is changed from another thread

see logQuery method
        Iterator<QueryStatDtoImpl> iterator = popularQueries.iterator();
        while (iterator.hasNext()) {
-->            QueryStatDtoImpl qsdi = iterator.next();
            if (qsdi.equals(qs)) {
                qs.setOccurrenceCount(qsdi.getOccurrenceCount() + 1);
                iterator.remove();
                break;
            }
        }
        popularQueries.offer(qs);
"
1,"NullPointerException in NegotiateScheme- server is configured to allow client to authenticate with kerberos with principal foobar
- client, using httpclient with a registered authscheme SPNEGO set as a NegotiateSchemeFactory

- when the client authenticate with the (correct) principal foobar, it works !
- when the client authenticate with the (wrong) principal fooba, it fails with a NPE below.


Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.commons.codec.binary.Base64.encodeBase64(Base64.java:233)
	at org.apache.commons.codec.binary.Base64.encode(Base64.java:521)
	at org.apache.http.impl.auth.NegotiateScheme.authenticate(NegotiateScheme.java:240)
	at org.apache.http.client.protocol.RequestTargetAuthentication.process(RequestTargetAuthentication.java:99)
	at org.apache.http.protocol.ImmutableHttpProcessor.process(ImmutableHttpProcessor.java:108)
	at org.apache.http.protocol.HttpRequestExecutor.preProcess(HttpRequestExecutor.java:167)
	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:460)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:689)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:624)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:602)
"
0,Codec is not consistently passed in internal APIWhile working on SOLR-1942 I ran into a couple of glitches with codec which is not consistently passed to SegmentsInfo and friends. Codecs should really be consistently passed though. I have fixed the pieces which lead to errors in Solr but I  guess there might be others too. Patch is coming up... 
0,"Add ""tokenize documents only"" task to contrib/benchmarkI've been looking at performance improvements to tokenization by
re-using Tokens, and to help benchmark my changes I've added a new
task called ReadTokens that just steps through all fields in a
document, gets a TokenStream, and reads all the tokens out of it.

EG this alg just reads all Tokens for all docs in Reuters collection:

  doc.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersDocMaker
  doc.maker.forever=false
  {ReadTokens > : *
"
1,"Database Data Store: close result setsThe database data store doesn't close one result sets. This is not a problem for most databases, but anyway should be fixed."
1,"Data Store: UTFDataFormatException when using large minRecordLengthIf using a value larger than 33000 for minRecordLength, and then trying to store a value with 33000 bytes, the following exception is thrown: UTFDataFormatException. The reason is that values are serialized using DataOutputStream.writeUTF. There is size limitation of 65 K when using this method. Small entries are hex encoded, and there is a prefix, so the limitation for minRecordLength should be 32000.

This is a problem for both FileDataStore and DbDataStore.
"
0,"Misleading exception message for jcr:deref()If the type of the second argument in a jcr:deref() function is not a String an InvalidQueryException is thrown with a misleading message: ""Wrong second argument type for jcr:like""

It should be rather something like: ""Second argument for jcr:deref must be a String"""
0,Remove @author tags in jackrabbit-jcr-rmiIt is a recommendation within Apache not to use @author tags or other means to identify source code with individual developers.  The @author tags in jackrabbit-jcr-rmi should therefore be removed.
1,"Core: WEAKREFERENCE properties object have type REFERENCE when being read from the persistent layerit seems to me that WEAKREFERENCE properties are properly created and stored as such but are read as REFERENCE 
properties when built again from the persistent layer.

how to reproduce:

- create a new WEAKREFERENCE property and save the changes
- force reading from the persistent layer  (in my case I used Day's CRX and restartet the server)
- the former WEAKREFERENCE will now be displayed as REFERENCE.

"
0,"NodeTypeRegistry could auto-subtype from nt:basethis is basically a copy of JCR-433, which was fixed but somehow sneaked in again:

when tying to register a (primary) nodetype that does not extend from nt:base the following error is
thrown:

""all primary node types except nt:base itself must be (directly or indirectly) derived from nt:base""

since the registry is able to detect this error, it would be easy to auto-subtype all nodetypes from nt:base. 
imo it's pointless to explicitly add the nt:base to every superclass set. as an analogy, you don't need to 
'extend from java.lang.Object' explicitly - the compiler does that automatically for your."
1,"Jcr2Spi: ExportSysViewTest#testExportSysView_handler_session_saveBinary_* occasionally failingfrom time to time i saw ExportSysViewTest#testExportSysView_handler_session_saveBinary_* test failing. this doesn't occur consistently and i never managed to reproduce it when running the tests in the idea.
"
1,"Workspace operations (copy/clone) do not handle references correctlyREFERENCE properties created through Workspace.copy() or Workspace.clone() are not reflected by 
Node.getReferences() and are as a consequence not enforced."
1,"NPE if you open IW with CREATE on an index with no segments fileI have a simple test case that hits this NPE:

{noformat}
    [junit] java.lang.NullPointerException
    [junit] 	at java.io.File.<init>(File.java:305)
    [junit] 	at org.apache.lucene.store.NIOFSDirectory.openInput(NIOFSDirectory.java:67)
    [junit] 	at org.apache.lucene.store.FSDirectory.openInput(FSDirectory.java:333)
    [junit] 	at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:213)
    [junit] 	at org.apache.lucene.index.IndexFileDeleter.<init>(IndexFileDeleter.java:218)
    [junit] 	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:1113)
    [junit] 	at org.apache.lucene.index.TestIndexWriter.testNoSegmentFile(TestIndexWriter.java:4975)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:277)
{noformat}

It happens if you have an aborted index, ie, there are segment files in there (*.frq, *.tis, etc.) but no segments_N file, and then you try to open an IW with CREATE on that index."
0,"support for DB2 in BundleDbPersistenceManagerBundleDbPersistenceManager doesn't work with DB2, db2.ddl file is missing.I've created the database scheme for DB2."
1,"LuceneTaxonomyReader .decRef() may close the inner IR, renderring the LTR in a limbo.TaxonomyReader which supports ref-counting, has a decRef() method which delegates to an inner IndexReader and calls its .decRef(). The latter may close the reader (if the ref is zeroes) but the taxonomy would remain 'open' which will fail many of its method calls.

Also, the LTR's .close() method does not work in the same manner as IndexReader's - which calls decRef(), and leaves the real closing logic to the decRef(). I believe this should be the right approach for the fix."
0,"Improve the documentation of VersionIn my opinion, we should elaborate more on the effects of changing the Version parameter.
Particularly, changing this value, even if you recompile your code, likely involves reindexing your data.
I do not think this is adequately clear from the current javadocs.
"
0,"Restructure the Jackrabbit source treeReintroduce some of the changes in JCR-157 as a more general restructuring to simplify the Jackrabbit project structure. See http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/9170/ for the rationale and discussion. The main parts of this restructuring would be:

1. Create a Jackrabbit ""super-project"" (artifactId: jackrabbit) in trunk/

2. Use the super-project POM as the parent of all Jackrabbit component POMs

3. Move the contents of trunk/jackrabbit/src/site directly to trunk/src/site, and use the super-project to generate the web site

4. Create independent subprojects for the the jackrabbit-api and jackrabbit-commons components, moving the the corresponding parts of the source tree

5. Move the jcr-server subprojects on level up

6. Rename the subproject directories to match their artifactIds

Note that this restructuring depends on JCR-611 and JCR-332, since the best way to implement this by utilizing a snapshot repository for the component dependencies."
1,TestNRTManager test failurereproduces for me
0,Consolidate ItemDef/QItemDefinitionThere is a great deal of duplicate code in ItemDef (jackrabbit-core) and QItemDefinition (jackrabbit-spi) and their implementing classes.
0,"Database connection poolingJackrabbit should use database connection pools instead of a single connection per persistence manager, cluster journal, or database data store."
1,"Garbage collection deletes temporary files in FileDataStoreIn FileDataStore.addRecord(InputStream), a temporary file is created. The data is written to the file and then it is moved to its final location (based on the contents hash).

If the garbage collector runs whilst this temp file is present, it deletes it (on Solaris 10 at least), and the addRecord fails at the attempt to rename the now non-existent temp file.

I am attaching a minimal patch that prevents these temp files being deleted by deleteOlderRecursive(..), regardless of their lastModified() value.

I have made this a Minor priority, since there is the obvious workaround of disabling the GC.
"
1,"NPE in EventStateCollectionWhen removing a Version with a versionlabel and restoring an other Version from the same containing history within 1 transaction, a NPE occured. When debugging I noticed the method createEventStates was entered with an UUID from a versionLabel. The ChangeLog.get(id) returned null.

Caused by: java.lang.NullPointerException
	at org.apache.jackrabbit.core.observation.EventStateCollection.getNodeType(EventStateCollection.java:614)
	at org.apache.jackrabbit.core.observation.EventStateCollection.createEventStates(EventStateCollection.java:381)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:697)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:1085)
	at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:163)
	at org.apache.jackrabbit.core.version.XAVersionManager.prepare(XAVersionManager.java:509)
	at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:154)
	at org.apache.jackrabbit.core.XASessionImpl.prepare(XASessionImpl.java:331)
	at org.springmodules.jcr.jackrabbit.support.JackRabbitUserTransaction.commit(JackRabbitUserTransaction.java:100)
	at org.springmodules.jcr.jackrabbit.LocalTransactionManager.doCommit(LocalTransactionManager.java:192)"
0,"Allow SnapshotDeletionPolicy to be reused across writer close/openIf you re-use the same instance of SnapshotDeletionPolicy across a
close/open of your writer, and you had a snapshot open, it can still
be removed when the 2nd writer is opened.  This is because SDP is
comparing IndexCommitPoint instances.

The fix is to instead compare segments file names.

I've also changed the inner class IndexFileDeleter.CommitPoint to be
static so an instance of SnapshotDeletionPolicy does not hold
references to IndexFileDeleter, DocumentsWriter, etc.

Spinoff from here:

  http://markmail.org/message/bojgqfgyxkkv4fyb
"
0,"Move Query.weight() to IndexSearcher as protected methodWe had this issue several times, latest in LUCENE-3207.

The method Query.weight() was left in Query for backwards reasons in Lucene 2.9 when we changed Weight class. This method is only to be called on top-level queries - and this is done by IndexSearcher. This method is just a utility method, that has nothing to do with the query itsself (it just combines the createWeight method and calls the normalization afterwards). 

The problem we have is that any query that wraps other queries (like CustomScore, ConstantScore, Boolean) calls Query.weight() instead of Query.createWeight(), it will do normalization two times, leading to strange bugs.

For 3.3 I will make Query.weight() simply delegate to IndexSearcher's replacement method with a big deprecation warning, so user sees this. In IndexSearcher itsself the method will be protected to only be called by itsself or subclasses of IndexSearcher. Delegation for backwards is no problem, as protected is accessible by classes in same package.

I would suggest the method name to be IndexSearcher.createNormalizedWeight(Query q)"
1,"Text.unescape() should should preserve 'unicode' charactersWhen an input to Text.unescape() contains characters > \u00ff, the most significant byte is lost resulting in garbled output. The unescape() function should preserve such characters in order to be useful to decode Internationalized Resource Identifiers (RFC 3987). "
0,"QDefinitionBuilderFactory should auto-subtype from nt:baseSimilar to JCR-2066, the QNodeTypeDefinitions build by QDefinitionBuilderFactory should auto subtype from nt:base. "
0,"filter jcr properties in jcr-serverattached is a patch that implements jcr property filtering in jcr-server in the same way that nodes and resources are filtered. with the default filter configuration, this has the effect of filtering jcr:created, jcr:mixinTypes, and jcr:primaryType from nt:folder and nt:file nodes. 

this is likely the expected default behavior for most webdav servers - they want to return the normal dav properties, live properties defined themselves, and dead properties defined by clients, but not jcr-internal properties which are for all intents and purposes implementation-specific."
0,"ProtocolSocketFactory equals and hashCode don't support subclassingIn the implemenation of equals and hashCode for the classes
org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory
org.apache.commons.httpclient.protocol.SSLProtocolSocketFactory

The implementation of equals and hashCode attempts to make all instances of the classes equal.  However, the manner in which the methods are coded makes it necessary for any subclass to implement equals and hashCode themselves.  A minor change to the methods in these classes will make possible to subclass these factories without re-implementing the equals and hashCode.  The method equals should be written as

        return ((obj != null) && obj.getClass().equals(getClass()));

rather than

        return ((obj != null) && obj.getClass().equals(DefaultProtocolSocketFactory.class));

And similarly, the hashCode method should be

        return getClass().hashCode();

rather than

        return DefaultProtocolSocketFactory.class.hashCode();"
1,"Node.checkin() throws ArrayIndexOutOfBoundsExceptionI get an ArrayIndexOutOfBoundsException for index 0 when checking-in a node. After drilling into the code I found, that during checkin, the jcr:uuid property (defined as OPV INITIALIZE) is not copied from the node to the frozen node.

After checkin though the implementation tries to access the string value of the jcr:uuid property, which is not existing, hence the internal property implementation throws the exception when accessing the first element in the empty value array.

As a workaround I currently the set jcr:uuid property to OPV=COPY in the mixin:referenceable node type. But I could imagine, that this might be incorrect according to the spec, yet it works in my use case."
0,Improve performance of simple path queriesQueries with simple path constraints can be quite slow because of the way they are implemented. The current implementation basically does a hierarchical join with the context nodes and the set of nodes with the name of the next location step. When the specified path is quite selective the implementation should   rather resolve the path expression using the item state manager (similar to how regular paths are resolved in the JCR API).
0,"Rename IndexReader.reopen to make it clear that reopen may not happenSpinoff from LUCENE-3454 where Shai noted this inconsistency.

IR.reopen sounds like an unconditional operation, which has trapped users in the past into always closing the old reader instead of only closing it if the returned reader is new.

I think this hidden maybe-ness is trappy and we should rename it (maybeReopen?  reopenIfNeeded?).

In addition, instead of returning ""this"" when the reopen didn't happen, I think we should return null to enforce proper usage of the maybe-ness of this API."
0,"Preserving UUID and document version history on repository migrationI have been working I an migration utility for OpenKM and I performed some changes in jackrabit-core to enable version import, preserving
the modification date. Also modified org.apache.jackrabbit.core.NodeImpl to preserve UUID in the migration process.

This migration process is needed because there are changes in repository node definition, and Jackrabbit can't deal with this actually.

I've attache a PDF with the changes needed in Jackrabbit-core. It works and there was no problems with the migrated repository."
0,"httpClient does not support installation of different SSLSocketFactoryDescription:

The SSLProtocolSocketFactory class had hard-
coded ""javax.net.ssl.SSLSocketFactory"" as the socket factory.  It does not 
support installation of other socket factory.

Proposed Fix:

We added a setDefaultSSLSocketFactory method to the SSLProtocolSocketFactory 
and modified the code to use the factory it it is set.  The code falls back on 
using ""javax.net.ssl.SSLSocketFactory"" if a default is not set."
1,jackrabbit-jcr-client tests fail (and are disabled in pom)I suggest to enable the tests and fix the issues causing them to fail. 
0,"Document is partially indexed on an unhandled exceptionWith LUCENE-843, it's now possible for a subset of a document's
fields/terms to be indexed or stored when an exception is hit.  This
was not the case in the past (it was ""all or none"").

I plan to make it ""all or none"" again by immediately marking a
document as deleted if any exception is hit while indexing it.

Discussion leading up to this:

  http://www.gossamer-threads.com/lists/lucene/java-dev/56103
"
1,"Bundle cache is not rolled back when the storage of a ChangeLog failsThe bundle cache in the bundle persistence managers is not restored to its old state when the AbstractBundlePersistenceManager.store(ChangeLog changeLog) method throws an exception. If, for instance, the storage of references fails then the AbstractBundlePersistenceManager.putBundle(NodePropBundle bundle) method has already been called for all modified bundles. Because of the connection rollback, the bundle cache will be out-of-sync with the persistent state. As a result, the SharedItemStateManager will have an incorrect view of the persistent state.
Furthermore, if the blockOnConnectionLoss property is set to true, then the BundleDbPersistenceManager can be caught in an infinite loop because of invalid SQL inserts because of an incorrect bundle cache; see attached stacktrace."
0,"PhraseQuery/TermQuery/SpanQuery use IndexReader specific stats in their explainsPhraseQuery uses IndexReader in explainfor top level stats - as mentioned by Mike McCandless in LUCENE-1837.
TermQuery uses IndexReader in explain for top level stats

Always been a bug with MultiSearcher, but per segment search makes it worse.

"
0,Include the WebDAV litmus tests in the Jackrabbit integration testsIt would be great to integrate the litmus tests (http://www.webdav.org/neon/litmus/) to our integration test suite.
0,"Add TopDocs.merge to merge multiple TopDocsIt's not easy today to merge TopDocs, eg produced by multiple shards,
supporting arbitrary Sort.
"
0,"Make JCAManagedConnectionFactory non final, so it can be extendedHello,

Is there a reason why JCAManagedConnectionFactory is final?
I need to build my own one and I'd rather reuse some code of yours.
"
1,"Large distances in Spatial go beyond Prime MEridianhttp://amidev.kaango.com/solr/core0/select?fl=*&json.nl=map&wt=json&radius=5000&rows=20&lat=39.5500507&q=honda&qt=geo&long=-105.7820674

Get an error when using Solr when distance is calculated for the boundary box past 90 degrees.


Aug 4, 2009 1:54:00 PM org.apache.solr.common.SolrException log
SEVERE: java.lang.IllegalArgumentException: Illegal lattitude value 93.1558669413734
        at org.apache.lucene.spatial.geometry.FloatLatLng.<init>(FloatLatLng.java:26)
        at org.apache.lucene.spatial.geometry.shape.LLRect.createBox(LLRect.java:93)
        at org.apache.lucene.spatial.tier.DistanceUtils.getBoundary(DistanceUtils.java:50)
        at org.apache.lucene.spatial.tier.CartesianPolyFilterBuilder.getBoxShape(CartesianPolyFilterBuilder.java:47)
        at org.apache.lucene.spatial.tier.CartesianPolyFilterBuilder.getBoundingArea(CartesianPolyFilterBuilder.java:109)
        at org.apache.lucene.spatial.tier.DistanceQueryBuilder.<init>(DistanceQueryBuilder.java:61)
        at com.pjaol.search.solr.component.LocalSolrQueryComponent.prepare(LocalSolrQueryComponent.java:151)
        at org.apache.solr.handler.component.SearchHandler.handleRequestBody(SearchHandler.java:174)
        at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:131)
        at org.apache.solr.core.SolrCore.execute(SolrCore.java:1328)
        at org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:341)
        at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:244)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:286)
        at org.apache.coyote.http11.Http11AprProcessor.process(Http11AprProcessor.java:857)
        at org.apache.coyote.http11.Http11AprProtocol$Http11ConnectionHandler.process(Http11AprProtocol.java:565)
        at org.apache.tomcat.util.net.AprEndpoint$Worker.run(AprEndpoint.java:1509)
        at java.lang.Thread.run(Thread.java:619)


"
0,"Default KuromojiAnalyzer to use search modeKuromoji supports an option to segment text in a way more suitable for search,
by preventing long compound nouns as indexing terms.

In general 'how you segment' can be important depending on the application 
(see http://nlp.stanford.edu/pubs/acl-wmt08-cws.pdf for some studies on this in chinese)

The current algorithm punishes the cost based on some parameters (SEARCH_MODE_PENALTY, SEARCH_MODE_LENGTH, etc)
for long runs of kanji.

Some questions (these can be separate future issues if any useful ideas come out):
* should these parameters continue to be static-final, or configurable?
* should POS also play a role in the algorithm (can/should we refine exactly what we decompound)?
* is the Tokenizer the best place to do this, or should we do it in a tokenfilter? or both?
  with a tokenfilter, one idea would be to also preserve the original indexing term, overlapping it: e.g. ABCD -> AB, CD, ABCD(posInc=0)
  from my understanding this tends to help with noun compounds in other languages, because IDF of the original term boosts 'exact' compound matches.
  but does a tokenfilter provide the segmenter enough 'context' to do this properly?

Either way, I think as a start we should turn on what we have by default: its likely a very easy win.
"
0,"Signature changes in AttributeSource for better Generics support of AddAttribute/getAttributeThe last update of Attribute API using AttributeImpl as implementation oif Attributes changed the API a little bit. This change leads to the fact, that in Java 1.5 using generics we are no longer able to add Attributes without casting. addAttribute and getAttribute should return the Attribute interface because the implementation of the attribute is not interesting to the caller. By that in 1.5 using generics, one could add a TermAttribute without casting using:
{code}
TermAttribute termAtt = addAttribute(TermAttribute.class);
{code}
The signature to do this is:
{code}
public <T extends Attribute> T addAttribute(Class<T>)
{code}

The attached patch applies the mentioned change to the signature (without generic, only returning Attribute). No other code changes are needed, as current code always casts the result to the requested interface. I also added the 1.5 method signature for all these methods to the javadocs.

All tests pass."
0,"NewAnalyzerTaskNewAnalyzerTask (patch to follow) allows a contrib/benchmark algorithm to change Analyzers during a run.  This is useful when comparing Analyzers

{""NewAnalyzer"" NewAnalyzer(WhitespaceAnalyzer, SimpleAnalyzer, StopAnalyzer, standard.StandardAnalyzer) >

is a sample declaration in an algorithm file."
0,"WindowsDirectoryWe can use Windows' overlapped IO to do pread() and avoid the performance problems of SimpleFS/NIOFSDir.
"
0,"check all tests that use FSDirectory.openIn LUCENE-2471 we were discussing the copyBytes issue, and Shai and I had a discussion about how we could prevent such bugs in the future.

One thing that lead to the bug existing in our code for so long, was that it only happened on windows (e.g. never failed in hudson!)
This was because the bug only happened if you were copying from SimpleFSDirectory, and the test used FSDirectory.open

Today the situation is improving: most tests use newDirectory() which is random by default and never use FSDir.open,
it always uses SimpleFS or NIOFS so that the same random seed will reproduce across both windows and unix.

So I think we need to review all uses of FSDirectory.open in our tests, and minimize these.
In general tests should use newDirectory().
If the test comes with say a zip-file and wants to explicitly open stuff from disk, I think it should open the contents with say SimpleFSDir,
and then call newDirectory(Directory) to copy into a new ""random"" implementation for actual testing. This method already exists:
{noformat}
  /**
   * Returns a new Dictionary instance, with contents copied from the
   * provided directory. See {@link #newDirectory()} for more
   * information.
   */
  public static MockDirectoryWrapper newDirectory(Directory d) throws IOException {
{noformat}
"
1,"Uncaught AbstractMethodError exception in in DomUtil.createFactory()DomUtil.createFactory() throws an uncaught AbstractMethodError exception when xerces is on the classpath and the jackrabbit webdav module is used. 

This can render the class unusable when used in conjunction with the xerces library. 
"
1,"NullPointerException in BooleanFilter BooleanFilter getDISI() method used with QueryWrapperFilter occur NullPointerException,
if any QueryWrapperFilter not match terms in IndexReader.

---------------------------------------------------
java.lang.NullPointerException
	at org.apache.lucene.util.OpenBitSetDISI.inPlaceAnd(OpenBitSetDISI.java:66)
	at org.apache.lucene.search.BooleanFilter.getDocIdSet(BooleanFilter.java:102)
	at org.apache.lucene.search.IndexSearcher.searchWithFilter(IndexSearcher.java:551)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:532)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:463)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:433)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:356)
	at test.BooleanFilterTest.main(BooleanFilterTest.java:50)
---------------------------------------------------

null-check below lines.
---------------------------------------------------
res = new OpenBitSetDISI(getDISI(shouldFilters, i, reader), reader.maxDoc());
res.inPlaceOr(getDISI(shouldFilters, i, reader));
res = new OpenBitSetDISI(getDISI(notFilters, i, reader), reader.maxDoc());
res.inPlaceNot(getDISI(notFilters, i, reader));
res = new OpenBitSetDISI(getDISI(mustFilters, i, reader), reader.maxDoc());
res.inPlaceAnd(getDISI(mustFilters, i, reader));
---------------------------------------------------"
0,Do not log warning when coercing value in query is not possibleThe LuceneQueryBuilder currently logs a warning when a String literal cannot be coerced into a type derived from information provided by the node type manager. The log level should be lowered to debug.
0,"QueryParser support for MatchAllDocsIt seems like there really should be QueryParser support for MatchAllDocsQuery.
I propose *:* (brings back memories of DOS :-)
"
0,Remove deprecated Filter.bits() and make Filter.getDocIdSet() abstract.
1,"Using WildcardQuery with MultiSearcher, and Boolean MUST_NOT clauseWe are searching across multiple indices using a MultiSearcher. There seems to be a problem when we use a WildcardQuery to exclude documents from the result set. I attach a set of unit tests illustrating the problem.

In these tests, we have two indices. Each index contains a set of documents with fields for 'title',  'section' and 'index'. The final aim is to do a keyword search, across both indices, on the title field and be able to exclude documents from certain sections (and their subsections) using a
WildcardQuery on the section field.
 
 e.g. return documents from both indices which have the string 'xyzpqr' in their title but which do not lie
 in the news section or its subsections (section = /news/*).
 
The first unit test (testExcludeSectionsWildCard) fails trying to do this.
 If we relax any of the constraints made above, tests pass:
 
* Don't use WildcardQuery, but pass in the news section and it's child section to exclude explicitly (testExcludeSectionsExplicit)</li>
* Exclude results from just one section, not it's children too i.e. don't use WildcardQuery(testExcludeSingleSection)</li>
* Do use WildcardQuery, and exclude a section and its children, but just use one index thereby using the simple
   IndexReader and IndexSearcher objects (testExcludeSectionsOneIndex).
* Try the boolean MUST clause rather than MUST_NOT using the WildcardQuery i.e. only include results from the /news/ section
   and its children."
1,"Extend mimetype list of text extractorsDo you think it would be possible to extend the mimetype list of the
MsPowerpoint and MsExcel textextractors with ""application/powerpoint"" and
""application/excel""? 

It just took me half an hour to figure out why my
documents didn't turn up in a jackrabbit fulltext-search and maybe other
users might run into the same problem...

I'm not sure if there is some kind of standard which lists the possible
default mimetypes but after a quick google search it seems to me that they
are not that uncommon.
"
1,"Fix and simplify CryptedSimpleCredentialsthe credentials retrieved from UserImpl and used to validate the simplecredentials passed to the repository login is overly complex
and buggy as it tries to match all kind credentials variants with and without hashed password.
in particular it contains the following problems:
- simplecredentials containing the hashed pw are considered valid
- passwords startign with {something} cause inconsistencies and may even prevent the user from login

it should be improved as follows:
- simplecredentials are always expected to contain the plain text password both for creation and
  comparison with the cryptedsimplecredentials.
- creating cryptedsimplecredentials from uid/pw however is left unchanged: the specified pw is
  hashed with the default algorithm if it turns out not to be in the hashed format.
- in addition the pw should also be hashed if it has the form {something}whatever but something
  is an invalid algorithm.
"
0,"Testing for indexable properties should check the default indexable properties firstorg.apache.jackrabbit.core.query.lucene.NodeIndexer#addValue, uses the following condition for a PropertyType.NAME type of property:

if (isIndexed(name)
                    || name.equals(NameConstants.JCR_PRIMARYTYPE)
                    || name.equals(NameConstants.JCR_MIXINTYPES)) {
                addNameValue(doc, fieldName, value.getQName());
}

It'd be more efficient to test the default properties first (which are on every node anyway) than to query the custom indexing rules every time. "
0,"Adding DerbyDataStore to handle proper close of the embedded databaseWhen using embedded Derby in conjunction with DbDataStore, the Derby database is never shutdown, as it requires special code to be executed (creating a Connection with "";shutdown=true"")
We may provide a DerbyDataStore extending standard DbDataStore for handling that."
1,"KeywordTokenizer does not properly set the end offsetKeywordTokenizer sets the Token's term length attribute but appears to omit the end offset. The issue was discovered while using a highlighter with the KeywordAnalyzer. KeywordAnalyzer delegates to KeywordTokenizer propagating the bug. 

Below is a JUnit test (source is also attached) that exercises various analyzers via a Highlighter instance. Every analyzer but the KeywordAnazlyzer successfully wraps the text with the highlight tags, such as ""<b>thetext</b>"". When using KeywordAnalyzer the tags appear before the text, for example: ""<b></b>thetext"". 

Please note NewKeywordAnalyzer and NewKeywordTokenizer classes below. When using NewKeywordAnalyzer the tags are properly placed around the text. The NewKeywordTokenizer overrides the next method of the KeywordTokenizer setting the end offset for the returned Token. NewKeywordAnalyzer utilizes KeywordTokenizer to produce proper token.

Unless there is an objection I will gladly post a patch in the very near future . 

-----------------------------
package lucene;

import java.io.IOException;
import java.io.Reader;

import org.apache.lucene.analysis.Analyzer;
import org.apache.lucene.analysis.KeywordAnalyzer;
import org.apache.lucene.analysis.KeywordTokenizer;
import org.apache.lucene.analysis.SimpleAnalyzer;
import org.apache.lucene.analysis.StopAnalyzer;
import org.apache.lucene.analysis.Token;
import org.apache.lucene.analysis.TokenStream;
import org.apache.lucene.analysis.Tokenizer;
import org.apache.lucene.analysis.WhitespaceAnalyzer;
import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.search.highlight.Highlighter;
import org.apache.lucene.search.highlight.QueryScorer;
import org.apache.lucene.search.highlight.SimpleHTMLFormatter;
import org.apache.lucene.search.highlight.WeightedTerm;
import org.junit.Test;
import static org.junit.Assert.*;

public class AnalyzerBug {

	@Test
	public void testWithHighlighting() throws IOException {
		String text = ""thetext"";
		WeightedTerm[] terms = { new WeightedTerm(1.0f, text) };

		Highlighter highlighter = new Highlighter(new SimpleHTMLFormatter(
				""<b>"", ""</b>""), new QueryScorer(terms));

		Analyzer[] analazers = { new StandardAnalyzer(), new SimpleAnalyzer(),
				new StopAnalyzer(), new WhitespaceAnalyzer(),
				new NewKeywordAnalyzer(), new KeywordAnalyzer() };

		// Analyzers pass except KeywordAnalyzer
		for (Analyzer analazer : analazers) {
			String highighted = highlighter.getBestFragment(analazer,
					""CONTENT"", text);
			assertEquals(""Failed for "" + analazer.getClass().getName(), ""<b>""
					+ text + ""</b>"", highighted);
			System.out.println(analazer.getClass().getName()
					+ "" passed, value highlighted: "" + highighted);
		}
	}
}

class NewKeywordAnalyzer extends KeywordAnalyzer {

	@Override
	public TokenStream reusableTokenStream(String fieldName, Reader reader)
			throws IOException {
		Tokenizer tokenizer = (Tokenizer) getPreviousTokenStream();
		if (tokenizer == null) {
			tokenizer = new NewKeywordTokenizer(reader);
			setPreviousTokenStream(tokenizer);
		} else
			tokenizer.reset(reader);
		return tokenizer;
	}

	@Override
	public TokenStream tokenStream(String fieldName, Reader reader) {
		return new NewKeywordTokenizer(reader);
	}
}

class NewKeywordTokenizer extends KeywordTokenizer {
	public NewKeywordTokenizer(Reader input) {
		super(input);
	}

	@Override
	public Token next(Token t) throws IOException {
		Token result = super.next(t);
		if (result != null) {
			result.setEndOffset(result.termLength());
		}
		return result;
	}
}
"
1,"workspace.copy causes 2 nodes in the same workspace to have the same version historyworkspace.copy creates a copy of a versionable node with a new uuid which share the same version. ""In a given workspace, there is at most one versionable node per version history"" (4.11 spec)"
0,"[PATCH] remove code stutterMethod calls getQName for no reason


public String getName() throws RepositoryException {
        checkStatus();
        Name qName = getQName();
        return session.getNameResolver().getJCRName(getQName());
    }

patch fixes it."
0,"Enforce TokenStream impl / Analyzer finalness by an assertionAs noted in LUCENE-1753 and other issues, TokenStream and Analyzers are based on the decorator pattern. At least all TokenStream and Analyzer implementations in Lucene and Solr should be final.

The attached patch adds an assertion to the ctors of both classes that does the corresponding checks:
- Analyzers must be final or private classes or anonymous inner classes
- TokenStreams must be final or private classes or anonymous inner classes or have a final incrementToken()

I will commit this after robert have fixed solr streams."
0,"Move FuzzyQuery rewrite as separate RewriteMode into MTQ, was: Highlighter fails to highlight FuzzyQueryAs FuzzyQuery does not allow to change the rewrite mode, highlighter fails with UOE in flex since LUCENE-2110, because it changes the rewrite mode to Boolean query. The fix is: Allow MTQ to change rewrite method and make FUZZY_REWRITE public for that.

The rewrite mode will live in MTQ as TOP_TERMS_SCORING_BOOLEAN_REWRITE. Also the code will be refactored to make heavy reuse of term enumeration code and only plug in the PQ for filtering the top terms."
1,"need a test that uses termsenum.seekExact() (which returns true), then calls next()i tried to do some seekExact (where the result must exist) then next()ing in the faceting module,
and it seems like there could be a bug here.

I think we should add a test that mixes seekExact/seekCeil/next like this, to ensure that
if seekExact returns true, that the enum is properly positioned."
0,"changes-to-html: better handling of bulleted lists in CHANGES.txt- bulleted lists
- should be rendered
- as such
- in output HTML"
0,"Add JMX support to register a JCR RMI Server into Jboss I added two classes and one descriptor file to the jcr-rmi project. These files provide support to make the generated jar deployable into a Jboss server. 

 The deployment descriptor contains two parameters, the address of the local repository instance, and the target address where the rmi server should be registered. 

e.g.

<server>
 <mbean code=""org.apache.jackrabbit.rmi.server.jmx.JCRServer""
     name=""Jackrabbit.services:RMIServer = JCR RMI Server"">
    <attribute name=""Local"">java:jcr/local</attribute>
    <attribute name=""Target"">jnp://localhost:1099/jcrServer</attribute>	
<depends>jboss.jca:service=ManagedConnectionFactory,name=jcr/local</depends>					
  </mbean>
</server>	

this configuration registers an RMI server at /jcrServer that wraps the local repository at java:jcr/local.

br,
Edgar"
1,"Authentication fails with proxied SSL ConnectionsWhen connecting through a proxy, using SSL and authentication HttpClient winds 
up sending a GET request to the proxy after the initial auth required response, 
the proxy then obviously responds with a not implemented response since it 
can't handle a GET request to an SSL URL.  In essence the following is 
happening:

1. HttpClient sends Connect response.
2. Proxy responds 200 Connect OK
3. HttpClient uses SSL connection to send the request to the web server.
4. Web server responds with not authorized and closes the connection.
5. HttpClient opens a new connection to the proxy and issues a GET request for 
the SSL URL.
6. Proxy returns 501 not implemented.

I'll attach a full log to this bug.

This is likely to be hard to fix since the retry is performed in HttpMethodBase 
but the Connect method is executed by HttpClient so a fix for this may be best 
waiting for 2.1.  This looks very similar to HTTPCLIENT-195 except that that bug is 
marked as fixed and this one still doesn't work, this also applies to 
authentication schemes other than NTLM (testing NTLM and basic).

My best evaluation is that the web server returns Connection: close when it 
rejects the authorization attempt and then HttpMethodBase is incapable of 
creating a new SSL connection through the proxy.  The only thing I can think of 
that could be done prior to 2.1 to fix this is to send a Connection: keep-alive 
as well as the Proxy-Connection: Keep-Alive we're already sending with the 
original request."
1,"Removal of a node with shared subnodes failsA simple testcase:

Set up (first transaction):
Node a1 = testRootNode.addNode(""a1"");
Node a2 = testRootNode.addNode(""a2"");
a2.addMixin(""mix:shareable"");
session.save();
// now we have a shareable node N with path a2

Workspace workspace = session.getWorkspace();
String path = a1.getPath() + ""/b1"";
workspace.clone(workspace.getName(), a2.getPath(), path, false);
session.save();
// now we have another shareable node N' in the same shared set as N with path a1/b1

Test(second transaction):
testRootNode.remove(""a1"");
session.save();

At least in a transactional repository the node will not be removed, an error will be thrown instead."
0,Favour QValue.getPath() over getString() where appropriateTo avoid extra conversion round trips QValue.getPath() should be used instead of  QValue.getString() where appropriate.
1,"testIWondiskfull unreferenced files failureNOTE: reproduce with: ant test -Dtestcase=TestIndexWriterOnDiskFull -Dtestmethod=testAddDocumentOnDiskFull -Dtests.seed=aff9b14dd518cfb:4d2f112726e2947f:-2b03094a43a947ee -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=ISO8859-1""

Reproduces some of the time..."
1,"save() might create new transient propertiesIt seems that when a new node is saved through the parent node, new properties might get created, which are not saved. To persist those properties the new node must be saved again.

Example:

(Consider a mixin type ""extVer"" extending the standard type mix:versionable.)

      Node node = parent.addNode(""newNode"", ""nt:base"");
      node.addMixin(""extVer"");
      // ""mix:versionable"" properties do not exist here
      
      // save the new node
      parent.save();

      // now ""mix:versionable"" properties like ""jcr:isCheckedOut""
      // exist in the ""node"" but:
      //    node.getProperty(""jcr:isCheckedOut"").isNew() == true
      // fix:
      node.save();

If the last node.save() opertation would not be done, a RepositoryException would result if a node.checkIn() would be done immediately after parent.save().

This seems counterintuitive and seems like an error. I wonder whether the properties should not be added upon ""node.addMixin"" ? At least ""parent.save()"" should (or might I say must ?) not only add the properties but also save them."
0,"ValueFormat should provide method getJCRStringIn order to retrieve the JCR String representation of a QValue currently the following calls are required:

ValueFormat.getJCRValue(QValue, NamePathResolver, ValueFactory)
Value.getString()

This could be simplified if the ValueFormat would provide

ValueFormat.getJCRString(QValue, NamePathResolver)

"
0,"throw exception for fieldcache on a non-atomic readerIn Lucene 4.0, we go through a lot of effort to prevent slow uses of non-atomic readers:

DirectoryReader/MultiReader etc throw exception if you don't try to access postings or docvalues apis per-segment, etc.

But the biggest trap of all is still too easy to fall into, we don't do the same for FieldCache.

I think we should throw exception, forcing the user to either change their code or use a SlowMultiReaderWrapper.
"
0,ConfigurationException constructors are package privateConfigurationException constructors are package private which prevents reusing them in other packages. eg. when extending the configuration.
0,"Contrib queryparser should not use CharSequence as Map keyToday, contrib query parser uses Map<CharSequence,...> in many different places, which may lead to problems, since CharSequence interface does not enforce the implementation of hashcode and equals methods. Today, it's causing a problem with QueryTreeBuilder.setBuilder(CharSequence,QueryBuilder) method, that does not works as expected."
1,"contrib/benchmark assumes Locale.US for parsing dates in Reuters collectionSimpleDateFormat used for parsing dates in Reuters documents is instantiated without specifying a locale. So it is using the default locale. If that happens to be US, it will work. But for another locale a parse exception is likely.

Affects both StandardBenchmarker and ReutersDocMaker.

Fix is trivial - specify Locale.US for SimpleDateFormat's constructor.
"
0,"Jackrabbit utilitiesAttached are two utilities for Jackrabbit:

The first one is a DataStore implementation that uses Amazon S3 for storage.
This is fairly straightforward. It is configured by adding a DataStore
section to the repository.xml file, e.g.:
   <DataStore class=""org.jcrutil.S3DataStore"">
       <param name=""awsAccessKey"" value="""" />
       <param name=""awsSecretKey"" value="""" />
       <param name=""bucketName"" value="""" />
       <param name=""minModifiedDate"" value=""0"" />
       <param name=""minRecordLength"" value=""0"" />
   </DataStore>

The second utility is a JCR based Commons VFS filesystem provider. This
allows you to access a JCR repository (nt:file and nt:folder nodes) using
the Commons VFS API. I've also used this with MINA FTP Server and Dctm VFS
(http://dctmvfs.sourceforge.net/) to provide FTP access to a Jackrabbit
repository.
"
0,"Unnecessary parsing of Name valueWhen a Name value is created for a call like Property.getValue() the internal QName if formatted, parsed and formatted again."
0,"cache entry resource management should be extracted from CachingHttpClientAs we have built in support for stream-based management of cached response bodies, the CachingHttpClient class has its fingers in too many pies and is involved in resource management but not storage of the actual HttpCacheEntries.

I have a patch forthcoming. :)
"
0,"cache should invalidate obsoleted entries mentioned in Content-LocationFrom http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6:

If a cache receives a successful response whose Content-Location field matches that of an existing cache entry for the same Request-URI, whose entity-tag differs from that of the existing entry, and whose Date is more recent than that of the existing entry, the existing entry SHOULD NOT be returned in response to future requests and SHOULD be deleted from the cache.

Current caching module doesn't do this (yet). As this is a recommendation (SHOULD) and not a requirement (MUST) I am marking this as an improvement rather than a bug.
"
1,"Changes from Session.move() to a top-level node aren't seen in a second sessionI'll attach a test case, but basically...

* Create two sessions
* Create a top-level node in the first session and save it.
* Move the top-level node using the first session
* In the second session, try itemExists() for the path of the node. It returns true when it should be false."
0,"Move MemoryJournal from test to mainRunning our tests with the FileJournal implementation on a windows box can be quite slow because of the many FileDescriptor.sync() calls.

I'd like to move the MemoryJournal in jackrabbit-core test to the main sources. That way we can use it in other test setups."
0,"Jackrabbit depends on Oracle driver for BLOB support in Oracle versions previous than 10.2In Oracle versions previous to 10.2, Jackrabbit explicitly uses a class from the Oracle driver to provide BLOB support (see OracleFileSystem.init()). This special handling is no longer necesary for Oracle 10.2+, so we should provide a new implementation. As discussed on the list, we can create a new class for Oracle 10.2+, make it inherit from DbFileSystem, and override the createSchema(), and table space related methods, which are the ones that need special handling. Furthermore, we could refactor the current OracleFileSystem and break it into two clases, one of them to keep the current behavior and a new one to keep the common code (which we could rename to OracleBaseFileSystem or similar, to maintain compatiblity with code that uses OracleFileSystem for versions previous to 10.2). Then we make the Oracle10FileSystem inherit from the latter."
0,"CharFilter - normalize characters before tokenizerThis proposes to import CharFilter that has been introduced in Solr 1.4.

Please see for the details:
- SOLR-822
- http://www.nabble.com/Proposal-for-introducing-CharFilter-to20327007.html"
0,"Pre-analyzed fieldsAdds the possibility to set a TokenStream at Field constrution time, available as tokenStreamValue in addition to stringValue, readerValue and binaryValue.

There might be some problems with mixing stored fields with the same name as a field with tokenStreamValue."
1,"Intermitted failure on DocValues branchI lately ran into two random failures on the CSF branch that seem not to be related to docValues but I can't reproduce them neither on docvalues branch nor on trunk.

{code}
jError Message

IndexFileDeleter doesn't know about file _1e.tvx
Stacktrace

junit.framework.AssertionFailedError: IndexFileDeleter doesn't know about file _1e.tvx
	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:979)
	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:917)
	at org.apache.lucene.index.IndexWriter.filesExist(IndexWriter.java:3633)
	at org.apache.lucene.index.IndexWriter.startCommit(IndexWriter.java:3699)
	at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2407)
	at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2478)
	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2460)
	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2444)
	at org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:213)
Standard Output

NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testRandomExceptionsThreads -Dtests.seed=-6528669668419768890:4860241142852689334 -Dtests.codec=randomPerField -Dtests.multiplier=3
NOTE: test params are: codec=PreFlex, locale=sv, timezone=Atlantic/South_Georgia
Standard Error

NOTE: all tests run in this JVM:
[TestDemo, TestToken, TestBinaryDocument, TestCodecs, TestDirectoryReader, TestIndexInput, TestIndexWriterExceptions]
{code}

and

{code}

[junit] Testsuite: org.apache.lucene.index.TestIndexReaderReopen
    [junit] Testcase: testThreadSafety(org.apache.lucene.index.TestIndexReaderReopen):	Caused an ERROR
    [junit] MockDirectoryWrapper: cannot close: there are still open files: {_4_3.frq=1, _4_3.pos=1, _4_0.frq=1, _4_0.prx=1, _4.pst=1, _4_3.pyl=1, _4_3.skp=1, _4_0.tis=1, _4_3.doc=1, _4_3.tis=1}
    [junit] java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_4_3.frq=1, _4_3.pos=1, _4_0.frq=1, _4_0.prx=1, _4.pst=1, _4_3.pyl=1, _4_3.skp=1, _4_0.tis=1, _4_3.doc=1, _4_3.tis=1}
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:387)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen.testThreadSafety(TestIndexReaderReopen.java:859)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:979)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:917)
    [junit] Caused by: java.lang.RuntimeException: unclosed IndexInput
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:342)
    [junit] 	at org.apache.lucene.store.Directory.openInput(Directory.java:122)
    [junit] 	at org.apache.lucene.index.codecs.standard.StandardPostingsReader.<init>(StandardPostingsReader.java:49)
    [junit] 	at org.apache.lucene.index.codecs.standard.StandardCodec.fieldsProducer(StandardCodec.java:87)
    [junit] 	at org.apache.lucene.index.PerFieldCodecWrapper$FieldsReader.<init>(PerFieldCodecWrapper.java:119)
    [junit] 	at org.apache.lucene.index.PerFieldCodecWrapper.fieldsProducer(PerFieldCodecWrapper.java:211)
    [junit] 	at org.apache.lucene.index.SegmentReader$CoreReaders.<init>(SegmentReader.java:137)
    [junit] 	at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:532)
    [junit] 	at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:509)
    [junit] 	at org.apache.lucene.index.DirectoryReader.<init>(DirectoryReader.java:238)
    [junit] 	at org.apache.lucene.index.DirectoryReader.doReopen(DirectoryReader.java:500)
    [junit] 	at org.apache.lucene.index.DirectoryReader.access$000(DirectoryReader.java:48)
    [junit] 	at org.apache.lucene.index.DirectoryReader$2.doBody(DirectoryReader.java:493)
    [junit] 	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:623)
    [junit] 	at org.apache.lucene.index.DirectoryReader.doReopenNoWriter(DirectoryReader.java:488)
    [junit] 	at org.apache.lucene.index.DirectoryReader.doReopen(DirectoryReader.java:446)
    [junit] 	at org.apache.lucene.index.DirectoryReader.reopen(DirectoryReader.java:406)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$9.run(TestIndexReaderReopen.java:770)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$ReaderThread.run(TestIndexReaderReopen.java:897)
    [junit] 
    [junit] 
    [junit] Tests run: 17, Failures: 0, Errors: 1, Time elapsed: 13.766 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexReaderReopen -Dtestmethod=testThreadSafety -Dtests.seed=-5455993123574190959:-1935535300313439968 -Dtests.codec=randomPerField -Dtests.multiplier=3
    [junit] NOTE: test params are: codec=RandomCodecProvider: {field5=MockVariableIntBlock(baseBlockSize=29), id=Standard, fielda=Standard, field4=MockFixedIntBlock(blockSize=924), field3=Standard, field2=SimpleText, id2=Standard, field6=MockSep, field1=Pulsing(freqCutoff=8)}, locale=zh_CN, timezone=Asia/Hovd
{code}

I haven't seen those before - let me know if you have!"
0,"CacheClient Javadoc and Constants usage cleanupCacheClient has some empty public java doc on methods that are not get/set.  These should have some body.  

Also the HeaderConstants Class has some overlap with the existing HTTP class for header values.  These need cleaning up."
1,"Be consistent about negative vInt/vLongToday, write/readVInt ""allows"" a negative int, in that it will encode and decode correctly, just horribly inefficiently (5 bytes).

However, read/writeVLong fails (trips an assert).

I'd prefer that both vInt/vLong trip an assert if you ever try to write a negative number... it's badly trappy today.  But, unfortunately, we sometimes rely on this... had we had this assert in 'since the beginning' we could have avoided that.

So, if we can't add that assert in today, I think we should at least fix readVLong to handle negative longs... but then you quietly spend 9 bytes (even more trappy!)."
0,"Enable MultiTermQuery's constant score mode to also use BooleanQuery under the hoodWhen MultiTermQuery is used (via one of its subclasses, eg
WildcardQuery, PrefixQuery, FuzzyQuery, etc.), you can ask it to use
""constant score mode"", which pre-builds a filter and then wraps that
filter as a ConstantScoreQuery.

If you don't set that, it instead builds a [potentially massive]
BooleanQuery with one SHOULD clause per term.

There are some limitations of this approach:

  * The scores returned by the BooleanQuery are often quite
    meaningless to the app, so, one should be able to use a
    BooleanQuery yet get constant scores back.  (Though I vaguely
    remember at least one example someone raised where the scores were
    useful...).

  * The resulting BooleanQuery can easily have too many clauses,
    throwing an extremely confusing exception to newish users.

  * It'd be better to have the freedom to pick ""build filter up front""
    vs ""build massive BooleanQuery"", when constant scoring is enabled,
    because they have different performance tradeoffs.

  * In constant score mode, an OpenBitSet is always used, yet for
    sparse bit sets this does not give good performance.

I think we could address these issues by giving BooleanQuery a
constant score mode, then empower MultiTermQuery (when in constant
score mode) to pick & choose whether to use BooleanQuery vs up-front
filter, and finally empower MultiTermQuery to pick the best (sparse vs
dense) bit set impl.
"
0,spi2dav: JSR 283 NodeType Management
0,"TCK: SetValueDateTest compares Calendar objectsSetValueDateTest#testDateSession
SetValueDateTest#testDateParent

Tests compare Calendar objects.  Calendar.equals(Object) is a stronger test than JSR-170 specifies for Value.equals(Object), leading to false failures.  For the purpose of these tests, even Value.equals(Object) is too strong an equality test, since some repositories may normalize date/time values across a save/read roundtrip (for example, converting ""Z"" to ""+00:00"", or adding/removing trailing zeros in fractional seconds).

Proposal: compare the getTimeInMillis() values.

--- SetValueDateTest.java       (revision 422074)
+++ SetValueDateTest.java       (working copy)
@@ -79,7 +80,8 @@
     public void testDateSession() throws RepositoryException {
         property1.setValue(value);
         superuser.save();
-        assertEquals(""Date node property not saved"", value.getDate(), property1.getValue().getDate());
+        assertEquals(""Date node property not saved"",
+          value.getDate().getTimeInMillis(), property1.getDate().getTimeInMillis());
     }
  
     /**
@@ -89,7 +91,8 @@
     public void testDateParent() throws RepositoryException {
         property1.setValue(value.getDate());
         node.save();
-        assertEquals(""Date node property not saved"", value.getDate(), property1.getValue().getDate());
+        assertEquals(""Date node property not saved"",
+          value.getDate().getTimeInMillis(), property1.getDate().getTimeInMillis());
     }
"
0,"Lucene needs to ship the JUnit jar for testingIn order for Hudson builds, etc. to work properly, Lucene needs to ship the JUnit jar and have it made available in the testing classpath.  Our system reqs say 3.8.1, but I have 3.8.2 laying around, so I will update the system requirements, too."
1,"smartcn analyzer throw NullPointer exception when the length of analysed text over 32767That's all because of org.apache.lucene.analysis.cn.smart.hhmm.SegGraph's makeIndex() method:
  public List<SegToken> makeIndex() {
    List<SegToken> result = new ArrayList<SegToken>();
    int s = -1, count = 0, size = tokenListTable.size();
    List<SegToken> tokenList;
    short index = 0;
    while (count < size) {
      if (isStartExist(s)) {
        tokenList = tokenListTable.get(s);
        for (SegToken st : tokenList) {
          st.index = index;
          result.add(st);
          index++;
        }
        count++;
      }
      s++;
    }
    return result;
  }

here 'short index = 0;' should be 'int index = 0;'. And that's reported here http://code.google.com/p/imdict-chinese-analyzer/issues/detail?id=2 and http://code.google.com/p/imdict-chinese-analyzer/issues/detail?id=11, the author XiaoPingGao have already fixed this bug:http://code.google.com/p/imdict-chinese-analyzer/source/browse/trunk/src/org/apache/lucene/analysis/cn/smart/hhmm/SegGraph.java"
0,Move hasVectors() & hasProx() responsibility out of SegmentInfo to FieldInfos Spin-off from LUCENE-2881 which had this change already but due to some random failures related to this change I remove this part of the patch to make it more isolated and easier to test. 
0,"Improve BenchmarkBenchmark can be improved by incorporating recent suggestions posted
on java-dev. M. McCandless' Python scripts that execute multiple
rounds of tests can either be incorporated into the codebase or
converted to Java."
0,"add @experimental javadocs tagThere are a lot of things marked experimental, api subject to change, etc. in lucene.

this patch simply adds a @experimental tag to common-build.xml so that we can use it, for more consistency.
"
0,"Remove query handler idleTimeThe changes included in JCR-415 revealed a synchronization issue with the query handler idle timer task.

See thread on dev-list: http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/10199

We could either fix the synchronization issue in the SearchManager class or remove the functionality all together.

Because the repository also supports a idle time parameter for the whole workspace (maxIdleTime in Workspaces element) the query handler idle time should be removed."
1,"Chunked Stream Encoding Problems Fails to throw ExceptionsUsing the HttpClient 2.0.1 with Sun's JDK 1.4.1_01 and connecting to a site 
that appareantly has problems generating proper chunked output causes the http 
client to catch and log an exception then return null data. Ideally the http 
client should throw the IOException to the calling class so that it can be 
handled by the programmer. It's not a problem that an exception is being 
generated it is a bug that the exception is being trapped in the somewhere in 
the httpclient code.

2004-08-27 21:19:01,013 main HttpMethodBase [ERROR]: I/O failure reading 
response body
java.io.IOException: chunked stream ended unexpectedly
        at 
org.apache.commons.httpclient.ChunkedInputStream.getChunkSizeFromInputStream
(ChunkedInputStream.java:234)
        at org.apache.commons.httpclient.ChunkedInputStream.nextChunk
(ChunkedInputStream.java:205)
        at org.apache.commons.httpclient.ChunkedInputStream.read
(ChunkedInputStream.java:160)
        at java.io.FilterInputStream.read(FilterInputStream.java:111)
        at org.apache.commons.httpclient.AutoCloseInputStream.read
(AutoCloseInputStream.java:110)
        at java.io.FilterInputStream.read(FilterInputStream.java:90)
        at org.apache.commons.httpclient.AutoCloseInputStream.read
(AutoCloseInputStream.java:129)
        at org.apache.commons.httpclient.HttpMethodBase.getResponseBody
(HttpMethodBase.java:685)
        at com.algorim.ei.cets.EmailPreProcessor.processMessage
(EmailPreProcessor.java:565)
        at com.algorim.ei.cets.EmailUpdate.run(EmailUpdate.java:332)
        at com.algorim.ei.cets.EmailUpdate.main(EmailUpdate.java:89)

Request and response that are causing the error:
GET /aeq.aspx?k=32226&k=sb1313@xcorp5.com HTTP/1.1
User-Agent: Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.0; .NET CLR 
1.1.4322)
Host: 38.117.227.56

HTTP/1.1 200 OK
Date: Fri, 27 Aug 2004 20:55:27 GMT
Server: Microsoft-IIS/6.0
X-Powered-By: ASP.NET
X-AspNet-Version: 1.1.4322
Transfer-Encoding: chunked
Cache-Control: private
Content-Type: text/html; charset=utf-8

179
<html><head><META HTTP-EQUIV=Refresh CONTENT=""1; 
URL=http://www.datingresults.com/default.asp?
p=7090&PRM=38664""</head><body><script>win2=win
dow.open('http://m.qmct.com/images/d.html?
a=1', 'newwin','toolbar=0,width=730,height=500');if (win2 != null) win2.blur
();window.focus();wind
ow.location = 'http://www.datingresults.com/default.asp?
p=7090&PRM=38664';</script></body></html>"
1,"IndexWriter can flush too early when flushing by RAM usageThere is a silly bug in how DocumentsWriter tracks its RAM usage:
whenever term vectors are enabled, it incorrectly counts the space
used by term vectors towards flushing, when in fact this space is
recycled per document.

This is not a functionality bug.  All it causes is flushes to happen
too frequently, and, IndexWriter will use less RAM than you asked it
to.  To work around it you can simply give it a bigger RAM buffer.

I will commit a fix shortly."
1,DWPT doesn't see changes to DW#infoStreamDW does not push infostream changes to DWPT since DWPT#infoStream is final and initialized on DWPTPool initialization (at least for initial DWPT) we should push changes to infostream to DWPT too
0,JSR 283: Introduce Event.getDate()JSR 283 adds a method to an Event that returns the date when the change happened that caused the event.
0,"Change BooleanFilter to have only a single clauses ArrayList (so toString() works fine, clauses() method could be added) so it behaves more lik BooleanQueryThis is unrelated to the other BF changes, but should be done"
0,"remove MoreLikeThis's default analyzerMoreLikeThis has the following:

{code}
public static final Analyzer DEFAULT_ANALYZER = new StandardAnalyzer(Version.LUCENE_CURRENT);
{code}"
0,"SQL2 parser: Support CASTSome CAST(...) data conversions are not yet implemented, for example String to Decimal."
1,"NPE doing local sensitive sorting when sort field is missingIf you do a local sensitive sort against a field that is missing from some documents in the index an NPE will get thrown.

Attached is a patch which resolved the issue and updates the sort test case to give coverage to this issue."
0,"WebApp: Ease first access for new users looking for a WebDAV serversuggestion posted by mike oliver in the user list:

> I know that JackRabbit isn't the same as Jakarta Slide and not expecting it to be, but one thing we did right on 
> that project was create a runnable war file that doesn't require any learning curve to get started.  Install the war file, 
> create the network place and login as the root:root user and start creating content folders and documents. 
> If JackRabbit did that, then I think more people would try it and use it and then spend the time to learn how to make 
> it all it can be."
0,"Add private ctors to static utility classesDuring development in 3.x and trunk we added some new classes like IOUtils and CodecUtils that are only providing static methods, but have no ctor at all. This adds the default empty public ctor, which is wrong, the classes should never be instantiated.

We should add private dummy ctors to prevent creating instances."
1,DWFlushControl does not take active DWPT out of the loop on fullFlushWe have seen several OOM on TestNRTThreads and all of them are caused by DWFlushControl missing DWPT that are set as flushPending but can't full due to a full flush going on. Yet that means that those DWPT are filling up in the background while they should actually be checked out and blocked until the full flush finishes. Even further we currently stall on the maxNumThreadStates while we should stall on the num of active thread states. I will attach a patch tomorrow.
0,"Port to Generics - test cases in contrib LUCENE-1257 in Lucene 3.0 addressed porting to generics across public api-s . LUCENE-2065 addressed across src/test . 

This would be a placeholder JIRA for any remaining pending generic conversions across the code base. 

Please keep it open after commiting and we can close it when we are near a 3.1 release , so that this could be a placeholder ticket. 

"
0,"Deprecate and replace SimpleHttpConnection with the SimpleHttpServer based testing frameworkThanks to Christian Kohlschuetter and Odi we now have a very flexible testing
framework, which enables us to emulate pretty much all the aspects of a HTTP
server functionality including non-compliant behavior and various vendor
specific implementation quirks. 

Many, many thanks go to Christian Kohlschuetter for having contributed the
original code. 

I propose SimpleHttpConnection be deprecated and eventually be phased out. I
took the first steps toward this goal by migrating Basic authentication test
cases. I urge all committers and contributors to use SimpleHttpServer for all
the new cases from now on. Ideally in the future we should even be able to get
rid of Tomcat as a dependency for testing.

I also took liberty of tweaking the SimpleHttpServer API a little. I factored
SimpleRequest and SimpleResponse classes out and provided a new interface called
HttpService, which can be used instead of HttpRequestHandler to implement test
cases in a way very similar to writing servlets. 

I'll commit the patch shortly as it does not really touch any _productive_ code. 

Oleg"
0,"CookieIdentityComparator and CookiePathComparator could/should implement SerializableCookieIdentityComparator and CookiePathComparator could/should implement Serializable

As Findbugs suggests:

""Comparator doesn't implement Serializable

This class implements the Comparator interface. You should consider whether or not it should also implement the Serializable interface. If a comparator is used to construct an ordered collection such as a TreeMap, then the TreeMap will be serializable only if the comparator is also serializable. As most comparators have little or no state, making them serializable is generally easy and good defensive programming. ""

Neither class has any state, so implementing Serializable would be trivial.

"
0,"Poor performance race condition in FieldCacheImplA race condition exists in FieldCacheImpl that causes a significant performance degradation if multiple threads concurrently request a value that is not yet cached. The degradation is particularly noticable in large indexes and when there a many concurent requests for the cached value.

For the full discussion see the mailing list thread 'Poor performance ""race condition"" in FieldSortedHitQueue' (http://www.gossamer-threads.com/lists/lucene/java-user/38717)."
0,Precedence query parser using the contrib/queryparser frameworkExtend the current StandardQueryParser on contrib so it supports boolean precedence
1,"relative URIs with internal double-slashes ('//') misparsedURI.parseUriReference()'s heuristic for interpreting URI parts is thrown off by relative URIs which include an internal '//'. As a result, portions of the supplied relative URI (path) can be lost. 

For example:

URI rel = new URI(""foo//bar//baz"");
rel.toString();
(java.lang.String) //bar//baz

The culprit seems to be line 1961 of URI improperly concluding that two slashes later than the beginning of 'tmp' are still indicative the URI is a 'net_path'. 

A possible quick fix might be to add a '!isStartedFromPath &&' to the beginning of the line 1961 test, making the line:

            if (!isStartedFromPath && at + 2 < length && tmp.charAt(at + 1) == '/') {

... and thus preventing the misguided authority-parsing from happening when earlier analysis already identified the current string as a strictly path-oriented URI.

(It also appears the setting of the is_net_path boolean at the end of this if's block may be wrong; this code is run for hier_path URIs that are not net_paths in the 2396 syntax. For example:

URI uri = new URI(""http://www.example.com/some/page"");
uri.isNetPath();
 (boolean) true 

)"
0,"openReaderPassed not populated in CheckIndex.Status.SegmentInfoStatusWhen using CheckIndex programatically, the openReaderPassed flag on the SegmentInfoStatus is never populated (so it always comes back false)

looking at the code, its clear that openReaderPassed is defined, but never used

furthermore, it appears that not all information that is propagated to the ""InfoStream"" is available via SegmentIinfoStatus

All of the following information should be able to be gather from public properties on the SegmentInfoStatus:
test: open reader.........OK
test: fields, norms.......OK [2 fields]
test: terms, freq, prox...OK [101 terms; 133 terms/docs pairs; 133 tokens]
test: stored fields.......OK [100 total field count; avg 1 fields per doc]
test: term vectors........OK [0 total vector count; avg 0 term/freq vector fields per doc]
"
0,Add timing information to event deliveryThere should be debug messages that contain information on how long event listeners spend iterating over the delivered events.
0,java.lang.Iterable support for RangeIteratorsMake javax.jcr.RangeIterator extend java.lang.Iterable in order to enable foreach loops on implementations of RangeIterator.
1,"import of multivalue properties with single value results in incorrect property creationWhen importing a file exported with system view, a value of a multivalued property is stored as a singlevalue property. The bug seems to be that for some reason, even if PropDef.isMultiple() is true for a given property, no ValueFormatException is thrown when setting the property as single value.

Workaround:

It works if I change PropInfo.apply() line 136 to 

if (va.length == 1 && !def.isMultiple()) {
...

"
1,"XML import always throws ItemExistsException when trying to overwrite existing nodesAccording to the JCR-API, it should be possible to govern the import of XML serialized referenceable nodes in case of UUID collision. Unfortunately, the UUID conflict is handled too late during import, an ItemExistsException is always thrown beforehand due to not allowed same-name-siblings.

Simply try to import a previously exported referenceable node twice, providing either

- ImportUUIDBehavior.IMPORT_UUID_COLLISION_REMOVE_EXISTING or
- ImportUUIDBehavior.IMPORT_UUID_COLLISION_REPLACE_EXISTING.

This will fail and result in an ItemExistsException."
0,"encoding of GermanAnalyzer.java and GermanStemmer.java isn't utf-8For PyLucene, the gcj/swig - based python integration of java lucene, it would
be good if java source files didn't use encodings other than utf-8.
On Windows - and systems without iconv support in general - compiling code  
with gcj where the java source text is in another encoding than utf-8 is    
difficult if not impossible.

To change the encoding on these files:

 iconv -f iso-8859-1 -t utf-8 GermanAnalyzer.java > GermanAnalyzer.java.utf-8
 iconv -f iso-8859-1 -t utf-8 GermanStemmer.java > GermanStemmer.java.utf-8"
0,"crank up faceting module testsThe faceting module has a large set of good tests.

lets switch them over to use all of our test infra (randomindexwriter, random iwconfig, mockanalyzer, newDirectory, ...)
I don't want to address multipliers and atLeast() etc on this issue, I think we should follow up with that on a separate issue, that also looks at speed and making sure the nightly build is exhaustive.

for now, lets just get the coverage in, it will be good to do before any refactoring.
"
1,"CJKTokenizer generates tokens with incorrect offsetsIf I index a Japanese *multi-valued* document with CJKTokenizer and highlight a term with FastVectorHighlighter, the output snippets have incorrect highlighted string. I'll attach a program that reproduces the problem soon."
0,"improve automaton performance by running on byte[]Currently, when enumerating terms, automaton must convert entire terms from flex's native utf-8 byte[] to char[] first, then step each char thru the state machine.

we can make this more efficient, by allowing the state machine to run on byte[], so it can return true/false faster."
0,"Patch to JCR-RMI contribution adding Version/VersionHistory supportHi Jukka,

You contributed the famous RMI extension to Jackrabbit. Many thanks. On my way to implement an Eclipse plugin to access repositories this provides great help. Unfortunately your contribution does not include support for versioning yet.

I took the freedom to add this missing piece and provide it to you to add it to your contribution. Thanks."
0,"Restructure codec hierarchySpinoff of LUCENE-2621. (Hoping we can do some of the renaming etc here in a rote way to make progress).

Currently Codec.java only represents a portion of the index, but there are other parts of the index 
(stored fields, term vectors, fieldinfos, ...) that we want under codec control. There is also some 
inconsistency about what a Codec is currently, for example Memory and Pulsing are really just 
PostingsFormats, you might just apply them to a specific field. On the other hand, PreFlex actually
is a Codec: it represents the Lucene 3.x index format (just not all parts yet). I imagine we would
like SimpleText to be the same way.

So, I propose restructuring the classes so that we have something like:
* CodecProvider <-- dead, replaced by java ServiceProvider mechanism. All indexes are 'readable' if codecs are in classpath.
* Codec <-- represents the index format (PostingsFormat + FieldsFormat + ...)
* PostingsFormat: this is what Codec controls today, and Codec will return one of these for a field.
* FieldsFormat: Stored Fields + Term Vectors + FieldInfos?

I think for PreFlex, it doesnt make sense to expose its PostingsFormat as a 'public' class, because preflex
can never be per-field so there is no use in allowing you to configure PreFlex for a specific field.
Similarly, I think in the future we should do the same thing for SimpleText. Nobody needs SimpleText for production, it should
just be a Codec where we try to make as much of the index as plain text and simple as possible for debugging/learning/etc.
So we don't need to expose its PostingsFormat. On the other hand, I don't think we need Pulsing or Memory codecs,
because its pretty silly to make your entire index use one of their PostingsFormats. To parallel with analysis:
PostingsFormat is like Tokenizer and Codec is like Analyzer, and we don't need Analyzers to ""show off"" every Tokenizer.

we can also move the baked in PerFieldCodecWrapper out (it would basically be PerFieldPostingsFormat). Privately it would
write the ids to the file like it does today. in the future, all 3.x hairy backwards code would move to PreflexCodec. 
SimpleTextCodec would get a plain text fieldinfos impl, etc."
0,Convert Batch implementation in spi-rmi from remote object into a local oneThe current implementation of the Batch interface in spi-rmi is very simple and just uses remotes to the server side batch. This should be changed to a local object on the client and only transmit the changes in a single call to the server on save.
1,"StandardCodec sometimes supplies skip pointers past EOFPretty sure this is 4.0-only:
I added an assertion, the test to reproduce is:

ant test-core -Dtestcase=TestPayloadNearQuery -Dtestmethod=testMinFunction -Dtests.seed=4841190615781133892:3888521539169738727 -Dtests.multiplier=3

{noformat}
    [junit] Testcase: testMinFunction(org.apache.lucene.search.payloads.TestPayloadNearQuery):  FAILED
    [junit] invalid skip pointer: 404, length=337
    [junit] junit.framework.AssertionFailedError: invalid skip pointer: 404, length=337
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1127)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1059)
    [junit]     at org.apache.lucene.index.codecs.MultiLevelSkipListReader.init(MultiLevelSkipListReader.java:176)
    [junit]     at org.apache.lucene.index.codecs.standard.DefaultSkipListReader.init(DefaultSkipListReader.java:50)
    [junit]     at org.apache.lucene.index.codecs.standard.StandardPostingsReader$SegmentDocsAndPositionsAndPayloadsEnum.advance(StandardPostingsReader.java:742)
    [junit]     at org.apache.lucene.search.spans.TermSpans.skipTo(TermSpans.java:72)
{noformat}"
1,"HttpConnectionParams.setConnectionTimeout(int) has no effect if host unreachableI have just modified MultiThreadedExample.java by adding
httpClient.getHttpConnectionManager().getParams().setConnectionTimeout(5000); in
order to set a connection timeout on the client side. Then I have added a LAN
url to urisToGet array. The ip of this url (""http://192.168.254.1/"") is not
assigned to any computer.

After running the client, I get the expected message ( error:
org.apache.commons.httpclient.ConnectTimeoutException: The host did not accept
the connection within timeout of 5000 ms) but only after 20 seconds.

I use java version ""1.5.0_04"". This is not a JVM bug since normal connection
procedure times out after 5 seconds as expected:
        SocketAddress addr = new InetSocketAddress(""192.168.254.1"", 80);
        try {
            
            SocketChannel channel = SocketChannel.open();
            channel.socket().connect(addr, 5000);            
            System.out.println(""connected"");
            
        } catch (Exception e) {
            e.printStackTrace();
        }"
0,"Correct copy-paste victim CommentCorrect the doc-comment of FieldsProducer (being a copy-paste victim of FieldsConsumer).
""consumes"" replaced with ""produces"".

One word change to avoid confusion: safe to commit.
"
1,"QueryParser.getFieldQuery(String,String) doesn't set default slop on MultiPhraseQuerythere seems to have been an oversight in calling mph.setSlop(phraseSlop) in QueryParser.getFieldQuery(String,String).  The result being that in some cases, the ""default slop"" value doesnt' get set right (sometimes, ... see below).

when i tried amending TestMultiAnalyzer to demonstrate the problem, I discovered that the grammer aparently always calls getFieldQuery(String,String,int) -- even if no ""~slop"" was specified in the text being parsed, in which case it passes the default as if it were specified.
(just to clarify: i haven't comfirmed this from a detailed reading of the grammer/code, it's just what i've deduced based on observation of the test)

The problem isn't entirely obvious unless you have a subclasses of QueryParser and try to call getFieldQuery(String,String) directly.   

In my case, I had overridden getFieldQuery(String,String) to call super.getFieldQuery(String,String) and wrap the result in a DisjunctionMaxQuery ... I don't care about supporting the ~slop syntax, but i do care about the default slop and i wasn't getting lucky the way QueryParser does, because getFieldQuery(String,String,int) wasn't getting back something it could call setSlop() with the (default) value it got from the javacc generated code.

My description may not make much sense, but hopefull the test patch i'm about to attach will.  The fix is also in the patch, and is fairly trivial.

(disclaimer: i don't have javacc installed, so I tested this patch by manually making the change to both QueryParser.java ... it should only be commited by someone with javacc who can regen the java file and confirm that my jj change doesn't have some weird bug in it)



"
1,"Redirection of a POST methodI execute a PostMethod to an URL which redirects me to a HTML page. If I set 
follow redirects to true the HttpClient wants to execute once more a POST. Of 
course a POST is not allowed to HTML pages. I think the HttpClient should 
exectue a GET method instead. That's also what is in the RFC2616:

10.3 Redirection 3xx

   This class of status code indicates that further action needs to be
   taken by the user agent in order to fulfill the request.  The action
   required MAY be carried out by the user agent without interaction
   with the user if and only if the method used in the second request is
   GET or HEAD. A client SHOULD detect infinite redirection loops, since
   such loops generate network traffic for each redirection.

      Note: previous versions of this specification recommended a
      maximum of five redirections. Content developers should be aware
      that there might be clients that implement such a fixed
      limitation."
0,Promote ItemInfo builder classes from GetItemsTest to top level classesorg.apache.jackrabbit.jcr2spi.GetItem test contains builders for ItemInfo and NodeInfo instances. These should be generalized and promoted to spi-commons. 
0,"Enhance SnapshotDeletionPolicy to allow taking multiple snapshotsA spin off from here: http://www.gossamer-threads.com/lists/lucene/java-dev/99161?do=post_view_threaded#99161

I will:
# Replace snapshot() with snapshot(String), so that one can name/identify the snapshot
# Add some supporting methods, like release(String), getSnapshots() etc.
# Some unit tests of course.

This is mostly written already - I want to contribute it. I've also written a PersistentSDP, which persists the snapshots on stable storage (a Lucene index in this case) to support opening an IW with existing snapshots already, so they don't get deleted. If it's interesting, I can contribute it as well.

Porting my patch to the new API. Should post it soon."
0,"Support DateTools in QueryParserThe QueryParser currently uses the deprecated class DateField to create RangeQueries with date values. However, most users probably use DateTools to store date values in their indexes, because this is the recommended way since DateField has been deprecated. In that case RangeQueries with date values produced by the QueryParser won't work with those indexes.

This patch replaces the use of DateField in QueryParser by DateTools. Because DateTools can produce date values with different resolutions, this patch adds the following methods to QueryParser:

  /**
   * Sets the default date resolution used by RangeQueries for fields for which no
   * specific date resolutions has been set. Field specific resolutions can be set
   * with {@link #setDateResolution(String, DateTools.Resolution)}.
   *  
   * @param dateResolution the default date resolution to set
   */
  public void setDateResolution(DateTools.Resolution dateResolution);
  
  /**
   * Sets the date resolution used by RangeQueries for a specific field.
   *  
   * @param field field for which the date resolution is to be set 
   * @param dateResolution date resolution to set
   */
  public void setDateResolution(String fieldName, DateTools.Resolution dateResolution);

(I also added the corresponding getter methods).

Now the user can set a default date resolution used for all fields or, with the second method, field specific date resolutions.
The initial default resolution, which is used if the user does not set a different resolution, is DateTools.Resolution.DAY. 

Please let me know if you think we should use a different resolution as default.

I extended TestQueryParser to test this new feature.

All unit tests pass.
"
1,"literal plus (+) character in path components of HttpURL is not preserved.When a literal plus character is included in the path component of an URL, it is
not encoded, but get decoded during getPath() to a space.

Reproducible with the following:

HttpURL httpURL = new HttpURL(""http://localhost/test+test"");
System.out.println(httpURL.getPath());

Output:
""test test""

The following path fixes the issue (This patch does not appear to break anything
 else):

Patch against SVN Repo:
URL: http://svn.apache.org/repos/asf/jakarta/commons/proper/httpclient/trunk
Repository UUID: 13f79535-47bb-0310-9956-ffa450edef68
Revision: 405803

Index: src/java/org/apache/commons/httpclient/URI.java
===================================================================
--- src/java/org/apache/commons/httpclient/URI.java (revision 405803)
+++ src/java/org/apache/commons/httpclient/URI.java (working copy)
@@ -1552,6 +1552,7 @@
         allowed_abs_path.or(abs_path);
         // allowed_abs_path.set('/');  // aleady included
         allowed_abs_path.andNot(percent);
+        allowed_abs_path.clear('+');
     }


@@ -1563,6 +1564,7 @@
     static {
         allowed_rel_path.or(rel_path);
         allowed_rel_path.clear('%');
+        allowed_rel_path.clear('+');
     }"
0,"version.propertiesIf we're not going to split it, there should be only one version.properties in module-client.
module-httpmime is currently missing a version.properties file.
"
0,"Multiple tests test for locking instead of versioningMultiple tests claim to check whether versioning is supported, but in reality check for locking.  Patch included."
0,"UUID generation: SecureRandom should be used by defaultCurrently, the UUID generation used the regular java.util.Random implementation to generate random UUIDs. The seed value of Random is initialized using System.currentTimeMillis(); for Windows, the resolution is about 15 milliseconds. That means two computer that start creating UUIDs with Jackrabbit within the same 15 millisecond interval will generate the same UUIDs. In a clustered environment the nodes could be started automatically at the same time (for example after a backup).

Also, the Random class uses a 48-bit seed, which is much less than the number of random bits in UUID (122). This is not secure. See also:

http://en.wikipedia.org/wiki/UUID
Random UUID probability of duplicates
""The probability [of duplicates] also depends on the quality of the random number generator. A cryptographically secure pseudorandom number generator must be used to generate the values, otherwise the probability of duplicates may be significantly higher.""

Therefore, I suggest to change VersionFourGenerator to use the SecureRandom implementation in by default."
0,"Cached filter for a single term fieldThese classes implement inexpensive range filtering over a field containing a single term. They do this by building an integer array of term numbers (storing the term->number mapping in a TreeMap) and then implementing a fast integer comparison based DocSetIdIterator.

This code is currently being used to do age range filtering, but could also be used to do other date filtering or in any application where there need to be multiple filters based on the same single term field. I have an untested implementation of single term filtering and have considered but not yet implemented term set filtering (useful for location based searches) as well. 

The code here is fairly rough; it works but lacks javadocs and toString() and hashCode() methods etc. I'm posting it here to discover if there is other interest in this feature; I don't mind fixing it up but would hate to go to the effort if it's not going to make it into Lucene.

"
0,"[PATCH] FSDirectory create() method deletes all fileshi all,

the current implementation of FSDirectory.create(...) silently deletes all files
(even empty directories) within the index directory when setting up a new index
with create option enabled. Lucene doesn't care when deleting files in the index
directory if they  belong to lucene or not. I don't think that this is a real
bug, but it can be a pain if somebody whants to store some private information
in the lucene index directory, e.g some configuration files.

Therefore i implemented a FileFilter which knows about the internal lucene file
extensions, so that all other files would never get touched when creating a new
index. The current patch is an enhancement in FSDirectory only. I don't think
that there is a need to make it available in the Directory class and change all
it's depending classes.

regards
Bernhard"
1,"TestSimpleExplanations failure{noformat}
ant test -Dtestcase=TestSimpleExplanations -Dtestmethod=testDMQ8 -Dtests.seed=-7e984babece66153:3e3298ae627b33a9:3093059db62bcc71
{noformat}

fails w/ this on current trunk... looks like silly floating point precision issue:

{noformat}

    [junit] Testsuite: org.apache.lucene.search.TestSimpleExplanations
    [junit]   1.4508595 = (MATCH) sum of:
    [junit]     1.4508595 = (MATCH) weight(field:yy in 2) [DefaultSimilarity], result of:
    [junit]       1.4508595 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]         1.287682 = queryWeight, product of:
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           1.0 = queryNorm
    [junit]         1.1267219 = fieldWeight in 2, product of:
    [junit]           1.0 = tf(freq=1.0), with freq of:
    [junit]             1.0 = termFreq=1
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           0.875 = fieldNorm(doc=2)
    [junit]   145085.95 = (MATCH) weight(field:xx^100000.0 in 2) [DefaultSimilarity], result of:
    [junit]     145085.95 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]       128768.2 = queryWeight, product of:
    [junit]         100000.0 = boost
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         1.0 = queryNorm
    [junit]       1.1267219 = fieldWeight in 2, product of:
    [junit]         1.0 = tf(freq=1.0), with freq of:
    [junit]           1.0 = termFreq=1
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         0.875 = fieldNorm(doc=2)
    [junit]  expected:<145086.66> but was:<145086.69>)
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.544 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestSimpleExplanations -Dtestmethod=testDMQ8 -Dtests.seed=144152895b276837:eb7ba4953db943f:33373b79a971db02
    [junit] NOTE: test params are: codec=PreFlex, sim=RandomSimilarityProvider(queryNorm=false,coord=false): {field=DefaultSimilarity, alt=DFR I(ne)LZ(0.3), KEY=IB LL-D2}, locale=en_IN, timezone=Pacific/Samoa
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestSimpleExplanations]
    [junit] NOTE: Linux 2.6.33.6-147.fc13.x86_64 amd64/Sun Microsystems Inc. 1.6.0_21 (64-bit)/cpus=24,threads=1,free=130426744,total=189988864
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testDMQ8(org.apache.lucene.search.TestSimpleExplanations):	FAILED
    [junit] ((field:yy field:w5^100.0) | field:xx^100000.0)~0.5: score(doc=2)=145086.66 != explanationScore=145086.69 Explanation: 145086.69 = (MATCH) max plus 0.5 times others of:
    [junit]   1.4508595 = (MATCH) sum of:
    [junit]     1.4508595 = (MATCH) weight(field:yy in 2) [DefaultSimilarity], result of:
    [junit]       1.4508595 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]         1.287682 = queryWeight, product of:
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           1.0 = queryNorm
    [junit]         1.1267219 = fieldWeight in 2, product of:
    [junit]           1.0 = tf(freq=1.0), with freq of:
    [junit]             1.0 = termFreq=1
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           0.875 = fieldNorm(doc=2)
    [junit]   145085.95 = (MATCH) weight(field:xx^100000.0 in 2) [DefaultSimilarity], result of:
    [junit]     145085.95 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]       128768.2 = queryWeight, product of:
    [junit]         100000.0 = boost
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         1.0 = queryNorm
    [junit]       1.1267219 = fieldWeight in 2, product of:
    [junit]         1.0 = tf(freq=1.0), with freq of:
    [junit]           1.0 = termFreq=1
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         0.875 = fieldNorm(doc=2)
    [junit]  expected:<145086.66> but was:<145086.69>
    [junit] junit.framework.AssertionFailedError: ((field:yy field:w5^100.0) | field:xx^100000.0)~0.5: score(doc=2)=145086.66 != explanationScore=145086.69 Explanation: 145086.69 = (MATCH) max plus 0.5 times others of:
    [junit]   1.4508595 = (MATCH) sum of:
    [junit]     1.4508595 = (MATCH) weight(field:yy in 2) [DefaultSimilarity], result of:
    [junit]       1.4508595 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]         1.287682 = queryWeight, product of:
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           1.0 = queryNorm
    [junit]         1.1267219 = fieldWeight in 2, product of:
    [junit]           1.0 = tf(freq=1.0), with freq of:
    [junit]             1.0 = termFreq=1
    [junit]           1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]           0.875 = fieldNorm(doc=2)
    [junit]   145085.95 = (MATCH) weight(field:xx^100000.0 in 2) [DefaultSimilarity], result of:
    [junit]     145085.95 = score(doc=2,freq=1.0 = termFreq=1
    [junit] ), product of:
    [junit]       128768.2 = queryWeight, product of:
    [junit]         100000.0 = boost
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         1.0 = queryNorm
    [junit]       1.1267219 = fieldWeight in 2, product of:
    [junit]         1.0 = tf(freq=1.0), with freq of:
    [junit]           1.0 = termFreq=1
    [junit]         1.287682 = idf(docFreq=2, maxDocs=4)
    [junit]         0.875 = fieldNorm(doc=2)
    [junit]  expected:<145086.66> but was:<145086.69>
    [junit] 	at org.apache.lucene.search.CheckHits.verifyExplanation(CheckHits.java:324)
    [junit] 	at org.apache.lucene.search.CheckHits$ExplanationAsserter.collect(CheckHits.java:494)
    [junit] 	at org.apache.lucene.search.Scorer.score(Scorer.java:60)
    [junit] 	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:580)
    [junit] 	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:363)
    [junit] 	at org.apache.lucene.search.CheckHits.checkExplanations(CheckHits.java:302)
    [junit] 	at org.apache.lucene.search.QueryUtils.checkExplanations(QueryUtils.java:92)
    [junit] 	at org.apache.lucene.search.QueryUtils.check(QueryUtils.java:126)
    [junit] 	at org.apache.lucene.search.QueryUtils.check(QueryUtils.java:122)
    [junit] 	at org.apache.lucene.search.QueryUtils.check(QueryUtils.java:106)
    [junit] 	at org.apache.lucene.search.CheckHits.checkHitCollector(CheckHits.java:89)
    [junit] 	at org.apache.lucene.search.TestExplanations.qtest(TestExplanations.java:99)
    [junit] 	at org.apache.lucene.search.TestSimpleExplanations.testDMQ8(TestSimpleExplanations.java:224)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.search.TestSimpleExplanations FAILED
{noformat}"
1,"XMLTextFilter does not extract text elementsXMLTextFilter only returns the text from attributes, not the content of text elements,"
1,"Jenkins builds hang quite often in TestIndexWriterWithThreads.testCloseWithThreadsLast hung test run: [https://builds.apache.org/job/Lucene-Solr-tests-only-trunk/10638/console]

{noformat}
[junit] ""main"" prio=5 tid=0x0000000801ef3800 nid=0x1965c waiting on condition [0x00007fffffbfd000]
[junit]    java.lang.Thread.State: WAITING (parking)
[junit] 	at sun.misc.Unsafe.park(Native Method)
[junit] 	- parking to wait for  <0x0000000825d853a8> (a java.util.concurrent.locks.ReentrantLock$NonfairSync)
[junit] 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
[junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:838)
[junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:871)
[junit] 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1201)
[junit] 	at java.util.concurrent.locks.ReentrantLock$NonfairSync.lock(ReentrantLock.java:214)
[junit] 	at java.util.concurrent.locks.ReentrantLock.lock(ReentrantLock.java:290)
[junit] 	at org.apache.lucene.index.DocumentsWriterFlushControl.markForFullFlush(DocumentsWriterFlushControl.java:403)
[junit] 	at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:557)
[junit] 	- locked <0x0000000825d81998> (a org.apache.lucene.index.DocumentsWriter)
[junit] 	at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2776)
[junit] 	- locked <0x0000000825d7d840> (a java.lang.Object)
[junit] 	at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2904)
[junit] 	- locked <0x0000000825d7d830> (a java.lang.Object)
[junit] 	at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:1156)
[junit] 	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1099)
[junit] 	at org.apache.lucene.index.TestIndexWriterWithThreads.testCloseWithThreads(TestIndexWriterWithThreads.java:200)
[junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[junit] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
[junit] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[junit] 	at java.lang.reflect.Method.invoke(Method.java:616)
[junit] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
[junit] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
[junit] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
[junit] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
[junit] 	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
[junit] 	at org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)
[junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
[junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
[junit] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
[junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)
[junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)
[junit] 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
[junit] 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
[junit] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
[junit] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
[junit] 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
[junit] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
[junit] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
[junit] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
[junit] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
[junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
[junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
[junit] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)
{noformat}"
0,"deprecate ChineseAnalyzerThe ChineseAnalyzer, ChineseTokenizer, and ChineseFilter (not the smart one, or CJK) indexes chinese text as individual characters and removes english stopwords, etc.

In my opinion we should simply deprecate all of this in favor of StandardAnalyzer, StandardTokenizer, and StopFilter, which does the same thing."
0,"Stored-only fields automatically enable norms and tf when added to documentDuring updating my internal components to the new TrieAPI, I have seen the following:

I index a lot of numeric fields with trie encoding omitting norms and term frequency. This works great. Luke shows that both is omitted.

As I sometimes also want to have the components of the field stored and want to use the same field name for it. So I add additionally the field again to the document, but stored only (as the Field c'tor using a TokenStream cannot additionally store the field). As it is stored only, I thought, that I can left out explicit setting of omitNorms and omitTermFreqAndPositions. After adding the stored-only-without-omits field, Luke shows all fields with norms enabled. I am not sure, if the norms/tf were really added to the index, but Luke shows a value for the norms and FieldInfo has it enabled.

In my opinion, this is not intuitive, o.a.l.document.Field  should switch both omit* options on when storing fields only (and also disable other indexing-only options). Alternatively the internal FieldInfo.update(boolean isIndexed, boolean storeTermVector, boolean storePositionWithTermVector, boolean storeOffsetWithTermVector, boolean omitNorms, boolean storePayloads, boolean omitTermFreqAndPositions) should only change the omit* and other options, if the isIndexed parameter (not this.isIndexed) is also true, elsewhere leave it as it is.

In principle, when adding a stored-only field, any indexing-specific options should not be changed in FieldInfo. If the field was indexed with norms before, norms should stay enabled (but this would be the default as it is)."
1,"Cluster Node ID should be trimmedIf the cluster node ID is not configured in repository.xml, it is read from the file cluster_node.id instead. In case this file is edited by hand, some editors (e.g. vi) insert a trailing newline character (""\n""). This leads to the cluster node ID to contain a blank character. While I don't expect this to cause any issues, it is inconvenient for debugging and also introduces line-breaks in log files. I suggest to trim the cluster node ID, so only non-blank characters are used."
0,"Searchability settings in PropertyDefinitionRelated to JCR-1591, the new JCR 2.0 property definitions contain settings for searchability of properties.

I'm not sure how deeply we want to implement these settings (perhaps we should just hard-code the values), but in any case the relevant methods need to be implemented."
0,"Move text extraction into a background threadEven though text extraction is not done right on save() most of the extraction work is later done by a client thread. There is a mechanism in place that commits the deferred work in a background thread. But the background thread is only triggered by a timer and does not constantly write back pending index changes. For regular index changes this is done on purpose and should not be changed. However text extraction work should be moved completely into a background thread because it often takes a fair amount of time to index a large document.

Outline of a possible solution:
- all text filtering is tasks are put into a work queue
- the work queue is processed by a background thread
- basic indexing of nt:resource without text filtering takes place
- the background thread updates the index when text filtering completed for a nt:resource

There should be a configuration parameter that allows to execute text filtering without the background thread. This way it is possible to get the existing behaviour of Jackrabbit: the fulltext index is always up-to-date and can be used.
With the background process this is no longer the case."
1,"JCA will not compile with J2EE1.3 classesIn JCAManagedConnectionFactory, the constructor invoked to throw ResourceException does not exist under J2EE1.3 / JCA1.1 classes.
 throw new ResourceException(e)  -  line 136 and line 277.

Instead the code needs to do something like:

            ResourceException exception = new ResourceException(""Failed to create session"");
            exception.setLinkedException(e);
            throw exception;

This will allow it to compile/run under J2EE1.3
"
0,Add DataInput/DataOutput subclasses that delegate to an InputStream/OutputStream.Such classes would be handy for FST serialization/deserialization.
0,"IndexOutput.writeString() should write length in bytesWe should change the format of strings written to indexes so that the length of the string is in bytes, not Java characters.  This issue has been discussed at:

http://www.mail-archive.com/java-dev@lucene.apache.org/msg01970.html

We must increment the file format number to indicate this change.  At least the format number in the segments file should change.

I'm targetting this for 2.1, i.e., we shouldn't commit it to trunk until after 2.0 is released, to minimize incompatible changes between 1.9 and 2.0 (other than removal of deprecated features)."
0,"Coarser granularity of node type unregistration notificationsWhen unregistering multiple node types at a time, the internal notification methods are called separately for each type. This causes some problems as the first notifications triggers the regeneration of the full virtual node type tree, and later notification calls will fail (logging an error) in VirtualNodeTypeStateManager because the removed type is no longer there. A better approach would be to send the names of all the unregistered node types as a collection."
1,"session.move() throws ItemExistsException despite same name siblingscode to reproduce:

            Session session = r.login(new SimpleCredentials(""johndoe"", """".toCharArray()), wspName);
            Node root = session.getRootNode();

            // setup test case
            if (!root.hasNode(""foo"")) {
                root.addNode(""foo"");
                root.save();
            }
            if (!root.hasNode(""bar"")) {
                root.addNode(""bar"");
                root.save();
            }

            session.move(""/foo"", ""/bar"");    // ==> ItemExistsException
"
0,Use type StaticOperand for fullTextSearchExpressionSee: https://jsr-283.dev.java.net/issues/show_bug.cgi?id=691
0,"Simplified Repository URI format for JNDI lookupsThe JndiRepositoryFactory class (together with JcrUtils) currently supports the following repository URI formats:

    JcrUtils.getRepository(""jndi:name-of-repository"");
    JcrUtils.getRepository(""jndi://ignored?org.apache.jackrabbit.repository.jndi.name=name-of-repository&other-parameters"");

The first uri formats allows no extra JNDI environment settings to be passed in, and the second one is pretty verbose and simply ignores the authority and path parts of the URI.

I'd like to add support for the following simplified format that makes it easy to provide the repository name along with the initial context factory from which the name is to be looked up:

    JcrUtils.getRepository(""jndi://initial-context-factory/name-of-repository"");

Extra JNDI environment settings could still be included as additional query parameters. Backwards compatibility with the previous formats would be guaranteed based on the presence or absence of the org.apache.jackrabbit.repository.jndi.name parameter in hierarchical URIs."
1,"NullPointerException when accessing the about.jsp page because of missing /META-INF/NOTICE.TXTAccessing http://localhost:8080/about.jsp produces:

java.lang.NullPointerException
	at org.apache.jsp.about_jsp.output(org.apache.jsp.about_jsp:39)
	at org.apache.jsp.about_jsp._jspService(org.apache.jsp.about_jsp:103)
	at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:109)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:389)
	at org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:486)
	at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:380)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:363)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.handler.RequestLogHandler.handle(RequestLogHandler.java:49)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:324)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:864)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:533)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:207)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:403)
	at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:522)

This is because the jar misses the following file:
/META-INF/NOTICE.TXT
but there is /NOTICE.TXT and /META-INF/NOTICE

This problem is not reproducible with jackrabbit-webapp-2.0-beta1.war.
"
0,"Enable setting hits queue size in Search*Task in contrib/benchmarkIn testing for LUCENE-1483, I'd like to try different collector queue
sizes during benchmarking.  But currently contrib/benchmark uses
deprecated Hits with hardwired ""top 100"" queue size.  I'll switch it to
the TopDocs APIs."
0,"LocalNamespaceMappings does not make use of NameCache in NamespaceRegistryImplThis basically means that the NameCache in NamespaceRegistryImpl is never used.

The LocalNamespaceMappings should also implement NameCache and forward calls to the NamespaceRegistryImpl for names
with namespace URIs that are not locally remapped. See proposed patch."
1,"DocumentsWriter blocks flushes when applyDeletes takes forever - memory not releasedIn DocumentsWriter we have a safety check that applies all deletes if the deletes consume too much RAM to prevent too-frequent flushing of a long tail of tiny segments. If we enter applyAllDeletes we essentially lock on IW -> BufferedDeletes which is fine since this usually doesn't take long and doesn't keep DWPTs from indexing. Yet, if that takes long and at the same time a semgent is flushed and subsequently published to the IW we take the lock on the ticket queue and the IW. Now this prevents all other threads to append to the ticketQueue which is done BEFORE we actually flush the segment concurrently and free up the RAM.

Essentially its ok to block on the IW lock but we should not keep concurrent flushed from execution just because we apply deletes. The threads will block once they try to execute maybeMerge after the segment is flushed so we don't pile up subsequent memory but we should actually allow the DWPT to be flushed since we actually try to get rid of memory.

I ran into this by accident due to a coding bug using delete queries instead of terms for each document. This thread dump show the problem:

{noformat}
""Application Worker Thread"" prio=10 tid=0x00007fdda0238000 nid=0x3256 waiting for
monitor entry [0x00007fddad3c2000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:424)
       - waiting to lock <0x00007fddb74ff990> (a
org.apache.lucene.index.DocumentsWriter$TicketQueue)
       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)
       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

""Application Worker Thread"" prio=10 tid=0x00007fdda0236000 nid=0x3255 waiting for
monitor entry [0x00007fddad4c3000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.IndexWriter.updatePendingMerges(IndexWriter.java:1854)
       - waiting to lock <0x00007fddb74fe350> (a
org.apache.solr.update.SolrIndexWriter)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1848)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1843)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1493)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

""Application Worker Thread"" prio=10 tid=0x00007fdda0234000 nid=0x3254 waiting for
monitor entry [0x00007fddad5c4000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.IndexWriter.updatePendingMerges(IndexWriter.java:1854)
       - waiting to lock <0x00007fddb74fe350> (a
org.apache.solr.update.SolrIndexWriter)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1848)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1843)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1493)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

""Application Worker Thread"" prio=10 tid=0x00007fdda0232000 nid=0x3253 waiting for
monitor entry [0x00007fddad6c5000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:424)
       - waiting to lock <0x00007fddb74ff990> (a
org.apache.lucene.index.DocumentsWriter$TicketQueue)
       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)
       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

""Application Worker Thread"" prio=10 tid=0x00007fdda0230800 nid=0x3252 waiting for
monitor entry [0x00007fddad7c6000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.IndexWriter.updatePendingMerges(IndexWriter.java:1854)
       - waiting to lock <0x00007fddb74fe350> (a
org.apache.solr.update.SolrIndexWriter)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1848)
       at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1843)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1493)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

""Application Worker Thread"" prio=10 tid=0x00007fdda022e800 nid=0x3251 runnable
[0x00007fddad8c6000]
  java.lang.Thread.State: RUNNABLE
       at java.nio.Bits.copyToArray(Bits.java:715)
       at java.nio.DirectByteBuffer.get(DirectByteBuffer.java:233)
       at org.apache.lucene.store.MMapDirectory$MMapIndexInput.readBytes(MMapDirectory.java:319)
       at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum$Frame.loadBlock(BlockTreeTermsReader.java:2283)
       at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$SegmentTermsEnum.seekExact(BlockTreeTermsReader.java:1600)
       at org.apache.lucene.util.TermContext.build(TermContext.java:97)
       at org.apache.lucene.search.TermQuery.createWeight(TermQuery.java:180)
       at org.apache.lucene.search.BooleanQuery$BooleanWeight.<init>(BooleanQuery.java:186)
       at org.apache.lucene.search.BooleanQuery.createWeight(BooleanQuery.java:423)
       at org.apache.lucene.search.IndexSearcher.createNormalizedWeight(IndexSearcher.java:583)
       at org.apache.lucene.search.QueryWrapperFilter.getDocIdSet(QueryWrapperFilter.java:55)
       at org.apache.lucene.index.BufferedDeletesStream.applyQueryDeletes(BufferedDeletesStream.java:431)
       at org.apache.lucene.index.BufferedDeletesStream.applyDeletes(BufferedDeletesStream.java:268)
       - locked <0x00007fddb751e1e8> (a
org.apache.lucene.index.BufferedDeletesStream)
       at org.apache.lucene.index.IndexWriter.applyAllDeletes(IndexWriter.java:2852)
       - locked <0x00007fddb74fe350> (a org.apache.solr.update.SolrIndexWriter)
       at org.apache.lucene.index.DocumentsWriter.applyAllDeletes(DocumentsWriter.java:188)
       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:470)
       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)
       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)
       

""Application Worker Thread"" prio=10 tid=0x00007fdda022d800 nid=0x3250 waiting for
monitor entry [0x00007fddad9c8000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:424)
       - waiting to lock <0x00007fddb74ff990> (a
org.apache.lucene.index.DocumentsWriter$TicketQueue)
       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)
       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)
   
""Application Worker Thread"" prio=10 tid=0x00007fdda022d000 nid=0x324f waiting for
monitor entry [0x00007fddadac9000]
  java.lang.Thread.State: BLOCKED (on object monitor)
       at org.apache.lucene.index.IndexWriter.useCompoundFile(IndexWriter.java:2274)
       - waiting to lock <0x00007fddb74fe350> (a
org.apache.solr.update.SolrIndexWriter)
       at org.apache.lucene.index.IndexWriter.prepareFlushedSegment(IndexWriter.java:2156)
       at org.apache.lucene.index.DocumentsWriter.publishFlushedSegment(DocumentsWriter.java:526)
       at org.apache.lucene.index.DocumentsWriter.finishFlush(DocumentsWriter.java:506)
       at org.apache.lucene.index.DocumentsWriter.applyFlushTickets(DocumentsWriter.java:483)
       - locked <0x00007fddb74ff990> (a
org.apache.lucene.index.DocumentsWriter$TicketQueue)
       at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:449)
       at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:320)
       at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:393)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1484)
       at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1456)
       at org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:160)

{noformat}"
1,"Lower-Case Search-Function works with Upper-Case Searchstringif you perform a query like this
testroot/*[jcr:like(fn:lower-case(@prop1), 'FO%')]
you get valid results even though the value in the property has the ""foo"" value
The search works with lower and upper-case search strings."
0,Add args to test-macroAdd passing args to JUnit.  (Like Solr and mainly for debugging).  
1,"TestFSTs.testRandomWords failureWas running some while(1) tests on the docvalues branch (r1103705) and the following test failed:

{code}
    [junit] Testsuite: org.apache.lucene.util.automaton.fst.TestFSTs
    [junit] Testcase: testRandomWords(org.apache.lucene.util.automaton.fst.TestFSTs):	FAILED
    [junit] expected:<771> but was:<TwoLongs:771,771>
    [junit] junit.framework.AssertionFailedError: expected:<771> but was:<TwoLongs:771,771>
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs$FSTTester.verifyUnPruned(TestFSTs.java:540)
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs$FSTTester.doTest(TestFSTs.java:496)
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs$FSTTester.doTest(TestFSTs.java:359)
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs.doTest(TestFSTs.java:319)
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs.testRandomWords(TestFSTs.java:940)
    [junit] 	at org.apache.lucene.util.automaton.fst.TestFSTs.testRandomWords(TestFSTs.java:915)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1282)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1211)
    [junit] 
    [junit] 
    [junit] Tests run: 7, Failures: 1, Errors: 0, Time elapsed: 7.628 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: Ignoring nightly-only test method 'testBigSet'
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestFSTs -Dtestmethod=testRandomWords -Dtests.seed=-269475578956012681:0
    [junit] NOTE: test params are: codec=PreFlex, locale=ar, timezone=America/Blanc-Sablon
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestToken, TestCodecs, TestIndexReaderReopen, TestIndexWriterMerging, TestNoDeletionPolicy, TestParallelReaderEmptyIndex, TestParallelTermEnum, TestPerSegmentDeletes, TestSegmentReader, TestSegmentTermDocs, TestStressAdvance, TestTermVectorsReader, TestSurrogates, TestMultiFieldQueryParser, TestAutomatonQuery, TestBooleanScorer, TestFuzzyQuery, TestMultiTermConstantScore, TestNumericRangeQuery64, TestPositiveScoresOnlyCollector, TestPrefixFilter, TestQueryTermVector, TestScorerPerf, TestSloppyPhraseQuery, TestSpansAdvanced, TestWindowsMMap, TestRamUsageEstimator, TestSmallFloat, TestUnicodeUtil, TestFSTs]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=137329960,total=208207872
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.util.automaton.fst.TestFSTs FAILED
{code}

I am not able to reproduce"
0,"Initial size of ConcurrentCache depends on number of segments (available processors)This causes a build failure on my machine. Tests run into an OOME because the initial memory footprint of a ConcurrentCache on my machine is 8k. Many of the tests keep references to some kind of repository objects (node, session, x-manager), which means ConcurrentCache instances  cannot be garbage collected immediately after a test run.

I think the overall initial size of the cache should be independent of the number of segments. See proposed patch."
1,"CachingHierarchyManager synchronization problemThe method CachingHierarchyManager.resolveNodePath(..) does not synchronize on the cacheMonitor object. This can result in an endless loop in cache(), in NullPointerException or in other unexpected behavior in CachingHierarchyManager."
0,"Add a ContextAwareAuthScheme that has access to the HttpContext in the authenticate methodThe interface to be added would be:

/**
 * This interface represents an extended  authentication scheme
 * that requires access to {@link HttpContext} in order to
 * generate an authorization string.
 *
 * @since 4.1
 */

public interface ContextAwareAuthScheme extends AuthScheme {

    /**
     * Produces an authorization string for the given set of
     * {@link Credentials}.
     *
     * @param credentials The set of credentials to be used for athentication
     * @param request The request being authenticated
     * @param context HTTP context
     * @throws AuthenticationException if authorization string cannot
     *   be generated due to an authentication failure
     *
     * @return the authorization string
     */
    Header authenticate(
            Credentials credentials,
            HttpRequest request,
            HttpContext context) throws AuthenticationException;

}

Binary compatibility can be maintained by doing an instanceof check at the location where AuthScheme.authenticate() is called at the moment, and calling the context aware version if available.

This interface is necessary for the NegotiateScheme authentication scheme because the service names for the authentication tickets are based on the hostname of the target host or proxy host, depending on whether it's normal or proxy authentication, and this information is only available from the HttpContext.

Without the HttpContext there is a workaround that works most of the time, which looks like this:

	String host;
	if (isProxy()) {
		// FIXME this should actually taken from the HttpContext.
		HttpHost proxy = ConnRouteParams.getDefaultProxy(request.getParams());
		host = proxy.getHostName();
	} else {
		host = request.getLastHeader(""Host"").getValue();
	}

"
0,"Allow query results with unknown sizeTo further optimize certain queries the query implementation should be changed to allow for unknown result sizes. Currently there is only one query ( //* ) where the query result returns an unknown size and a special query result implementation is returned. At the same time, this should be fixed that only one implementation is used."
0,"XMLReader logs fatal error to system outSome test cases check if an appropriate exception is thrown when invalid XML is supplied, in that case the build in XMLReader in Java 1.5 logs a fatal error to system out.

This seems to be caused by a missing error handler on the XMLReader."
0,"DbDataStore: delete temporary files using finalize()Currently, reading from the DbDataStore creates a temporary file by default. If the application doesn't fully read or close the input stream, the file is not deleted. The best solution is to use finally { in.close() } in the application, but this is easily forgotten.

I suggest to delete the temp file using finalize(). There is a small performance penalty when creating the temporary object, but compared to I/O it is very small. Note that FileInputStream and FileOutputStream also use finalize()."
1,RepositoryConfig created by Jcr2spiRepositoryFactory should always return same RepositoryService instanceThe Jcr2spiRepositoryFactory uses a default implementation of RepositoryConfig if none is passed to it by the user. Currently this default implementation returns a new RepositoryService instance on each call to getRepositoryService(). This is not correct since the consumer of the RepositoryConfig instance expects the same RepositoryService instance on every call. 
1,"TokenStream.next(Token) reuse 'policy': calling Token.clear() should be responsibility of producer.Tokenizers which implement the reuse form of the next method:
    next(Token result) 
should reset the postionIncrement of the returned token to 1."
1,"SQL2 Left Outer JoinCreate this nodes.
def n1 = root.addNode(""node1"", ""sling:SamplePage"");
n1.setProperty(""n1prop1"", ""page1"");
def n2 = n1.addNode(""node2"", ""sling:SampleContent"");
n2.setProperty(""n2prop1"", ""content1"");

Execute this Query:
Select * from [sling:SamplePage] as page left outer join [sling:SampleContent] as content on ISDESCENDANTNODE(content,page) where page.n1prop1 = 'page1' and content.n2prop1 = 'content1';
The resultset have 1 row with 2 Nodes. This OK.

Then execute this:
Select * from [sling:SamplePage] as page left outer join [sling:SampleContent] as content on ISDESCENDANTNODE(content,page) where page.n1prop1 = 'page1' and content.n2prop1 = 'XXXXX';

The resultset has 1 row with 1 node.
This wrong. The result should be 0 rows.

Old Versions, prior 2.2.2 have also 0 rows as result.

Also, if nodes ""n2"" not exists, jackrabbit reports 1 row as result.

"
0,"Configure the maven build for IDE project generation for IDEA and EclipseCan we add a plugin configuration for the maven-idea-plugin and maven-eclipse-plugin, with JDK version set for IDEA and configured source download of dependencies?

Simplifies project regeneration and working with IDEA or Eclipse.

I'll add a patch.

Thanks!


 "
0,"Things to be done now that Filter is independent from BitSet(Aside: where is the documentation on how to mark up text in jira comments?)

The following things are left over after LUCENE-584 :

For Lucene 3.0  Filter.bits() will have to be removed.

There is a CHECKME in IndexSearcher about using ConjunctionScorer to have the boolean behaviour of a Filter.

I have not looked into Filter caching yet, but I suppose there will be some room for improvement there.
Iirc the current core has moved to use OpenBitSetFilter and that is probably what is being cached.
In some cases it might be better to cache a SortedVIntList instead.

Boolean logic on DocIdSetIterator is already available for Scorers (that inherit from DocIdSetIterator) in the search package. This is currently implemented by ConjunctionScorer, DisjunctionSumScorer,
ReqOptSumScorer and ReqExclScorer.
Boolean logic on BitSets is available in contrib/misc and contrib/queries

DisjunctionSumScorer calls score() on its subscorers before the score value actually needed.
This could be a reason to introduce a DisjunctionDocIdSetIterator, perhaps as a superclass of DisjunctionSumScorer.

To fully implement non scoring queries a TermDocIdSetIterator will be needed, perhaps as a superclass of TermScorer.

The javadocs in org.apache.lucene.search using matching vs non-zero score:
I'll investigate this soon, and provide a patch when necessary.

An early version of the patches of LUCENE-584 contained a class Matcher,
that differs from the current DocIdSet in that Matcher has an explain() method.
It remains to be seen whether such a Matcher could be useful between
DocIdSet and Scorer.

The semantics of scorer.skipTo(scorer.doc()) was discussed briefly.
This was also discussed at another issue recently, so perhaps it is wortwhile to open a separate issue for this.

Skipping on a SortedVIntList is done using linear search, this could be improved by adding multilevel skiplist info much like in the Lucene index for documents containing a term.

One comment by me of 3 Dec 2008:

A few complete (test) classes are deprecated, it might be good to add the target release for removal there.
"
0,"Improve exception handling in observation (ChangePolling)Currently when an (internal) event listener throws an exception, all further event listeners are skipped. This happens for move events where the HierarchyListener throws an UnsupportedOperationException. I suggest to move the exception handler up the call chain such that exceptions are caught and logged per listener instead of for all listeners. "
0,"Lazy field loading breaks backward compatDocument.getField() and Document.getFields() have changed in a non backward compatible manner.
Simple code like the following no longer compiles:
 Field x = mydoc.getField(""x"");"
0,"java.util.logging configuration examples does not work as intendedjava.util.logging configuration examples do not work as intended. Those can be found here: http://jakarta.apache.org/httpcomponents/httpclient-3.x/logging.html

Steps to reproduce:
1. Create a simple project using HttpClient (see listing below) and JDK 1.6 (I suppose it is JDK 1.4 or higher, but I did not test anything other than 1.6). Without log4j in the classpath and without any commons-logging system properties set, java.util.logging is automatically selected by commons-logging.
2. Create logging.properties file as shown in any of the java.util.logging examples
3. Run a program, passing -Djava.util.logging.config.file=logging.properties argument to the JVM

Expected results:
Quite a few log messages should be sent to System.err

Actual results:
Unless there is an I/O error encountered, no log messages are sent to System.err

The problem, as far as I can see, is caused by the default logging level of java.util.logging.ConsoleHandler, which is set to INFO. In order for any log messages to go through, the log hadler log level needs to be lower than logged messages log level. Adding the following line to all java.util.logging examples should fix the problem:

java.util.logging.ConsoleHandler.level = ALL

--- Get.java -----------------------------------------

import java.io.IOException;
import java.io.InputStreamReader;
import java.io.Reader;

import org.apache.commons.httpclient.HostConfiguration;
import org.apache.commons.httpclient.HttpClient;
import org.apache.commons.httpclient.HttpException;
import org.apache.commons.httpclient.HttpMethodBase;
import org.apache.commons.httpclient.methods.GetMethod;


public class Get {

	/**
	 * @param args
	 */
	public static void main(String[] args) {
		
 		HttpClient client = new HttpClient();
		HttpMethodBase get = new GetMethod(""http://www.apache.org"");
		
		try {
			int code = client.executeMethod(get);
			System.out.println(""Status code: "" + code);
			
			String csn = get.getResponseCharSet();
			System.out.println(""Charset is: "" + csn);
			
			long len = get.getResponseContentLength();
			System.out.println(""Length is: "" + len);
			len = len < 0 ? 200 : len;
			
			StringBuilder buf = new StringBuilder((int)len);
			Reader r = new InputStreamReader(get.getResponseBodyAsStream(), csn);
			for (int c = r.read(); c >= 0; c = r.read()) {
				buf.append((char)c);
			}
			
			System.out.println(""Body:"");
			System.out.println(buf.toString());			
			
		} catch (HttpException e) {
			e.printStackTrace();
		} catch (IOException e) {
			e.printStackTrace();
		} finally {
			get.releaseConnection();
		}				
	}
}

--- logging.properties ----- From examples ----------------------

.level=INFO

handlers=java.util.logging.ConsoleHandler
java.util.logging.ConsoleHandler.formatter = java.util.logging.SimpleFormatter

httpclient.wire.header.level=FINEST
org.apache.commons.httpclient.level=FINEST

--------------------------------------------------------------------------------
"
0,Perform random operation testsAs discussed on the mailing list and in other jira issues it makes sense to execute tests that perform random operations on the repository. This helps us detect concurrency issues in jackrabbit and increase code coverage.
0,"NodeTypeRegistry.registerNodetypes(Collection) should not register a partial setthe javadoc says:

     * Note that in the case an exception is thrown, some node types might have
     * been nevertheless successfully registered.

the problem hereby is, that it cannot be determined easily, what nodetypes could be registered, and which couldnt. i would rather prefer a all-or-nothing behaviour."
1,"SpellChecker not working because of stale IndexSearcherThe SpellChecker unit test did not work, because of a stale IndexReader and IndexSearcher instance after calling indexDictionary(Dictionary)."
1,"SessionImpl.createSession uses same Subject/LoginContextSessionImpl.createSession(String) uses the same loginctx/subject to create a new session.
this will cause problems if Session.logout() is called on the original instance.

i suggest to fix that by creating a new subject for the new session instance."
1,"XATest error: commit from different thread but same XID must not blockI'm seeing the following test error quite often in the CI server at work:

testDistributedThreadAccess(org.apache.jackrabbit.core.XATest)  Time elapsed: 0.213 sec  <<< ERROR!
javax.transaction.SystemException: commit from different thread but same XID must not block
	at org.apache.jackrabbit.core.UserTransactionImpl.commit(UserTransactionImpl.java:147)
	at org.apache.jackrabbit.core.XATest.testDistributedThreadAccess(XATest.java:1637)

It seems to be a system-specific issue, as I've never seen the same error locally or on Hudson."
0,"jcr:like() does not scale well on large value rangesThere are two major issues with the current WildcardQuery implementation:

1) A wildcard expression is restricted to match at most 1024 terms, otherwise a TooManyClauses exception is thrown. Similar to the RangeQuery issue: JCR-111
2) The enumeration over the terms that match the wildcard pattern is slow"
0,"TCK: NodeOrderableChildNodesTest tests node order even if node type doesn't support child node orderingNodeOrderableChildNodesTest# testOrderBeforeUnsupportedRepositoryOperationException

This test calls prepareTest, which requires getNodes() to return the child nodes in the order added, even if the node type doesn't support child node ordering.  JSR-170 (Section 4.4.2) imposes no such requirement.

Proposal: do not check child node order in this test case.
"
1,jcr:encoding not respected in NodeIndexerThe value of the jcr:encoding property is not passed to the TextFilter instances.
0,"TCK: NodeMixinUtil should exclude for mix:shareableThe mixin test NodeRemoveMixinTest#testRemoveSuccessfully tries to remove a mixin:

what it does: retrieve an addable mixin (NodeTypeUtil#getAddableMixinName), adds it and tries to remove it later on.
the addable mixins are retrieve from the complete set of mixin, testing node.canAddMixin.

However: with jackrabbit-core ""mix:shareable"" can be added but not removed.
if the test by chance gets exactly that mixin the test fails with exception (there is an explicit check the core for exactly that case).

the tck should exclude that special case, shouldn't it?

((michael found the issue)) "
0,release is not signedthere are no signatures & checksums available for your latest release
0,Configure occurrence of property value in excerptJackrabbit currently includes all indexed property values as potential content in an excerpt. This is not always desirable because there may be properties that need to be full-text indexed but should not show up in an excerpt.
1,"TestDocValuesIndexing reproducible  test failuredocvalues branch: r1131275

{code}
    [junit] Testsuite: org.apache.lucene.index.values.TestDocValuesIndexing
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.81 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestDocValuesIndexing -Dtestmethod=testAddIndexes -Dtests.seed=-3253978684351194958:-8331223747763543724
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=Standard, BYTES_VAR_STRAIGHT=Pulsing(freqCutoff=12), BYTES_FIXED_SORTED=MockRandom}, locale=es_MX, timezone=Pacific/Chatham
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDocValuesIndexing]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=89168480,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAddIndexes(org.apache.lucene.index.values.TestDocValuesIndexing):     FAILED
    [junit] [first=BYTES_FIXED_SORTED, second=BYTES_VAR_STRAIGHT] expected:<9> but was:<10>
    [junit] junit.framework.AssertionFailedError: [first=BYTES_FIXED_SORTED, second=BYTES_VAR_STRAIGHT] expected:<9> but was:<10>
    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.testAddIndexes(TestDocValuesIndexing.java:208)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1348)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1266)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.values.TestDocValuesIndexing FAILED
{code}"
0,"Improve StandardTokenizer's understanding of non ASCII punctuation and quotesIn the vein of LUCENE-1126 and LUCENE-1390, StandardTokenizerImpl.jflex should do a better job at understanding non-ASCII punctuation characters.

For example, its understanding of the single-quote character ""'"" is currently limited to that character only. It will set a token's type to APOSTROPHE only if the ""'"" was used.
In the patch attached, I added all the characters that ASCIIFoldingFilter would change into ""'"".

I'm not sure that this is the right approach so I didn't write a complete patch for all the other hardcoded characters used in jflex rules such as ""."", ""-"" which have some variants in ASCIIFoldingFilter that could be used as well.

Maybe a better approach would be to make it possible to have an ASCIIFoldingFilter-like reader as a character filter that could be in inserted in front of StandardTokenizer ?"
0,"handle multivalue headers correctlySome times, web servers send back multiple headers with the same key. e.g.

WWW-Authenticate: Negotiate
WWW-Authenticate: NTLM
WWW-Authenticate: Basic realm=""kmdc5""

To handle this correctly, we should add a method 

public java.util.Iterator getResponseHeaders(java.lang.String name)

just as in javax.servlet.http.HttpServletRequest."
0,"removing properties through SPI: two ways to do itBatch currently provides two ways to delete a property, similarly to JCR:

- Batch.remove()
- Batch.setValue(..., null)

JCR2SPI currently uses (AFAIK) Batch.remove().

Proposal:

- clarify that the QValue argument in setValue must be non-null (same for setValues)

"
1,"TermVectors corruption case when autoCommit=falseI took Yonik's awesome test case (TestStressIndexing2) and extended it to also compare term vectors, and, it's failing.

I still need to track down why, but it seems likely a separate issue."
0,"populate.jsp uses Java 1.5 methodThe method is URLConnection.setReadTimeout()
"
0,"SimpleSelectionTest assumes RowIterator.getSize() not to return -1Test case ""testSingleProperty"" assumes that RowIterator.getSize() will not return -1. This is an incorrect assumption, according to the JavaDoc for RangeIterator.

Suggested change:

        long size = result.getRows().getSize();
        if (size != -1) {
            assertEquals(""Should have only 1 result"", 1, size);
        }
"
0,"Support lower and upper case functions in ""order by"" clauseThe query languages should support lower- and upper-case functions within the ""order by"" clause.  This would provide case-insensitive ordering of query results.

Example:  Find all ""nt:base"" nodes ordered by the ""foo"" property, but ignoring case

In XPath:

//element(*,nt:base) order by fn:lower-case(@foo)

In SQL:

SELECT * FROM nt:base ORDER BY lower(foo)

"
0,Limit fields read from indexReading a lucene document from the index should be limited to only those fields that are necessary.
0,"Make shutdown hooks in TransientFileFactory removableTransientFileFactory class always registers shutdown hook. So, if jackrabbit classes were loaded by web-app classloader, they will not be released when web-app is undeployed (if jackrabbit-jcr-commons JAR is inside WAR). This causes classloader leak.
It seems to be useful to have ability to cancel TransientFileFactory's shutdown hook when application is going to be unloaded to avoid classloader leak."
0,"Revise Weight#scorer & Filter#getDocIdSet API to pass Readers contextSpinoff from LUCENE-2694 - instead of passing a reader into Weight#scorer(IR, boolean, boolean) we should / could revise the API and pass in a struct that has parent reader, sub reader, ord of that sub. The ord mapping plus the context with its parent would make several issues way easier. See LUCENE-2694, LUCENE-2348 and LUCENE-2829 to name some.

"
0,"Wrapup flexible indexingSpinoff from LUCENE-1458.

The flex branch is in fairly good shape -- all tests pass, initial search performance testing looks good, it survived several visits from the Unicode policeman ;)

But it still has a number of nocommits, could use some more scrutiny especially on the ""emulate old API on flex index"" and vice/versa code paths, and still needs some more performance testing.  I'll do these under this issue, and we should open separate issues for other self contained fixes.

The end is in sight!"
0,"RepositoryFactory implementation for jcr2spiThere should be a RepositoryFactory implementation in jcr2spi that also covers acquiring the underlying RepositoryService.

For this purpose I suggest to create:
-  a RepositoryServiceFactory in jackrabbit-spi, which encapsulates the spi implementation specifc instantiation of the RepositoryService. the factory probably just needs a single method that takes a parameters map.
- a RepositoryFactory implementation in jcr2spi, which works with a URI that contains all required information to connect/acquired the RepositoryService.

To use jcr2spi/spi2jcr/jackrabbit-core:
- jcr+file://path/to/repository/home?config=repository.xml

To use jcr2spi/spi2dav:
- jcr+dav://localhost:8080/server/repository/?br=4

To use jcr2spi/spi2davex:
- jcr+davex://localhost:8080/server/repository/

An implementation of RepositoryServiceFactory must check the scheme and decide if it can handle it and create a RepositoryService instance with it, otherwise it must return null. This means there would be a single name for the connect URI for all RepositoryServiceFactory implementations.

"
0,"Code depends on Log4J directlyThe code is written against the Log4J APIs, which forces all users of Jackarabbit to pick up log4J dependency and to juggle with JDK logging and Log4J configuration if other components of the project uses JDK 1.4 logging.
If the code is move to depend on Apache commons-logging this issue will be resolved. Also this should be a minor fix."
0,JSR 283: Binary interfaces The Binary interface replaces the deprecated methods for getting/setting the InputStream of a given JCR value and the method to create binary value (ValueFactory).
1,"index corruption autoCommit=falseIn both Lucene 2.3 and trunk, the index becomes corrupted when autoCommit=false"
1,"CloseableThreadLocal should allow null ObjectsCloseableThreadLocal does not allow null Objects in its get() method, but does nothing to prevent them in set(Object). The comment in get() before assert v != null is irrelevant - the application might have passed null.

Null is an important value for Analyzers. Since tokenStreams (a ThreadLocal private member in Analyzer) is not accessible by extending classes, the only way for an Analyzer to reset the tokenStreams is by calling setPreviousTokenStream(null).

I will post a patch w/ a test"
0,"intermittent failures of  TestTimeLimitedCollector.testTimeoutMultiThreaded in nightly testsOccasionly TestTimeLimitedCollector.testTimeoutMultiThreaded fails. e.g. with this output:

{noformat}
   [junit] ------------- Standard Error -----------------
   [junit] Exception in thread ""Thread-97"" junit.framework.AssertionFailedError: no hits found!
   [junit]     at junit.framework.Assert.fail(Assert.java:47)
   [junit]     at junit.framework.Assert.assertTrue(Assert.java:20)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestTimeout(TestTimeLimitedCollector.java:152)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.access$100(TestTimeLimitedCollector.java:38)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector$1.run(TestTimeLimitedCollector.java:231)
   [junit] Exception in thread ""Thread-85"" junit.framework.AssertionFailedError: no hits found!
   [junit]     at junit.framework.Assert.fail(Assert.java:47)
   [junit]     at junit.framework.Assert.assertTrue(Assert.java:20)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestTimeout(TestTimeLimitedCollector.java:152)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.access$100(TestTimeLimitedCollector.java:38)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector$1.run(TestTimeLimitedCollector.java:231)
   [junit] ------------- ---------------- ---------------
   [junit] Testcase: testTimeoutMultiThreaded(org.apache.lucene.search.TestTimeLimitedCollector):      FAILED
   [junit] some threads failed! expected:<50> but was:<48>
   [junit] junit.framework.AssertionFailedError: some threads failed! expected:<50> but was:<48>
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.doTestMultiThreads(TestTimeLimitedCollector.java:255)
   [junit]     at org.apache.lucene.search.TestTimeLimitedCollector.testTimeoutMultiThreaded(TestTimeLimitedCollector.java:220)
   [junit]
{noformat}

Problem either in test or in TimeLimitedCollector."
1,"Passing a null fieldname to MemoryFields#terms in MemoryIndex throws a NPEI found this when querying a MemoryIndex using a RegexpQuery wrapped by a SpanMultiTermQueryWrapper.  If the regexp doesn't match anything in the index, it gets rewritten to an empty SpanOrQuery with a null field value, which then triggers the NPE."
1,"Thread safety issue can cause index corruption when autoCommit=true and multiple threads are committingThis is only present in 2.9 trunk, but has been there since
LUCENE-1516 was committed I believe.

It's rare to hit: it only happens if multiple calls to commit() are in
flight (from different threads) and where at least one of those calls
is due to a merge calling commit (because autoCommit is true).

When it strikes, it leaves the index corrupt because it incorrectly
removes an active segment.  It causes exceptions like this:
{code}
java.io.FileNotFoundException: _1e.fnm
	at org.apache.lucene.store.MockRAMDirectory.openInput(MockRAMDirectory.java:246)
	at org.apache.lucene.index.FieldInfos.<init>(FieldInfos.java:67)
	at org.apache.lucene.index.SegmentReader.initialize(SegmentReader.java:536)
	at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:468)
	at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:414)
	at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:641)
	at org.apache.lucene.index.IndexWriter$ReaderPool.get(IndexWriter.java:627)
	at org.apache.lucene.index.DocumentsWriter.applyDeletes(DocumentsWriter.java:923)
	at org.apache.lucene.index.IndexWriter.applyDeletes(IndexWriter.java:4987)
	at org.apache.lucene.index.IndexWriter.doFlushInternal(IndexWriter.java:4165)
	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:4025)
	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:4016)
	at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:2077)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:2040)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:2004)
	at org.apache.lucene.index.TestStressIndexing2.indexRandom(TestStressIndexing2.java:210)
	at org.apache.lucene.index.TestStressIndexing2.testMultiConfig(TestStressIndexing2.java:104)
{code}

It's caused by failing to increment changeCount inside the same
synchronized block where segmentInfos was changed, in commitMerge.
The fix is simple -- I plan to commit shortly.
"
0,"Support for transactions when using JCR over RMI.At this time, the sessions obtained from o.a.j.rmi.client.LocalAdapterFactory do not implement the methods for the XASession.  Therefor the RMI access layer does not support a transactional session."
0,"BrowserCompatHostnameVerifier and StrictHostnameVerifier should handle wildcards in SSL certificates betterI ran into a problem with SSL wildcard certificates in the class BrowserCompatHostnameVerifier. It handles ""*.example.org"" fine but ""server*.example.org"" fails to work correctly. The javadoc claims that it should behave the same way as curl and FireFox. In Firefox an SSL certificate for ""server*.example.org"" works fine for the host ""server.example.org"", using HttpClient it throws an exception.

Here is an example test (JUnit4):

package org.example.hb;

import javax.net.ssl.SSLException;

import org.apache.http.conn.ssl.BrowserCompatHostnameVerifier;
import org.junit.Test;

public class BrowserCompatHostnameVerifierTest {

	/**
	 * Should not throw an exeption in the verify method.
	 * @throws SSLException
	 */
	@Test
	public void testVerifyStringStringArrayStringArray() throws SSLException
	{
		BrowserCompatHostnameVerifier hv = new BrowserCompatHostnameVerifier();
		String host = ""www.example.org"";
		String[] cns = {""www*.example.org""};
		
		hv.verify(host, cns, cns);
	}

}"
0,"Improve BaseTokenStreamTestCase to uses a fake attribute to check if clearAttributes() was called correctly - found bugs in contrib/analyzersRobert had the idea to use a fake attribute inside BaseTokenStreamTestCase that records if its clear() method was called. If this is not the case after incrementToken(), asserTokenStreamContents fails. It also uses the attribute in TeeSinkTokenFilter, because there a lot of copying, captureState and restoreState() is used. By the attribute, you can track wonderful, if save/restore and clearAttributes is correctly implemented. It also verifies that *before* a captureState() it was also cleared (as the state will also contain the clear call). Because if you consume tokens in a filter, capture the consumed tokens and insert them, the capturedStates must also be cleared before.

In contrib analyzers are some test that fail to pass this additional assertion. They are not fixed in the attached patch."
0,"FileDataStore should check for lastModified error resultAccording to javadoc for File.lastModified(), the return value may indicate error: ""...or 0L if the file does not exist or if an I/O error occurs"".

Accordingly, FileDataStore should be checking for this return value, rather than treating it as an actual modification time of ""0"".

Patch to follow.
"
1,"Issue while loading list of classes at that path itself.Hi,

I cannot retrieve list of objects that are directly under the path that they were saved in. I did not know where to simulate this issue and hence I have used DigesterSimpleQueryTest. I have attached the path for the newly added test case testObjectListRetrievalAtBasePath. In case the patch is not up to the mark I have attached the modified file too.

Instead of creating Page in /test if I create it in /sample/test and search in /sample/test it returns nothing but if I search in /sample it would return the object.

Another important point here is that it is causing issues while retrieving Page class, the other test cases that are retrieving Paragraph class (embedded inside Page class) are still working fine!

Regards,

Kaizer"
0,"Configuration of CacheManager memory sizes(I already posted this as comments under JCR-619.)

The maximum size for all caches in CacheManager is hardcoded to 16 megabytes and there's no way to change that. It would be nice if this as well as other CacheManager parameters were configurable. It's just a waste running Jackrabbit on a server with gigabytes of memory and only using 16 megabytes for cache...

I have created a really simple and straightforward patch (jackrabbit-cachemanager-config.patch) which enables reaching the CacheManager instance through RepositoryImpl object and setting all three of its memory parameters. The memory parameters are no longer static constants, but instance fields getting initial values from constants (so the default behavior of the class remains the same).

(It would be even nicer if these parameters were configurable via configuration files, but that should probably be implemented by someone close to the project.)"
1,"FilteredDocIdSet does not handle a case where the inner set iterator is nullDocIdSet#iterator is allowed to return null, when used in FilteredDocIdSet, if null is returned from the inner set, the FilteredDocIdSetIterator fails since it does not allow for nulls to be passed to it.

The fix is simple, return null in FilteredDocIdSet in the iterator method is the iterator is null."
0,"Change DateTools to not create a Calendar in every call to dateToString or timeToStringDateTools creates a Calendar instance on every call to dateToString and timeToString. Specifically:

# timeToString calls Calendar.getInstance on every call.
# dateToString calls timeToString(date.getTime()), which then instantiates a new Date(). I think we should change the order of the calls, or not have each call the other.
# round(), which is called from timeToString (after creating a Calendar instance) creates another (!) Calendar instance ...

Seems that if we synchronize the methods and create the Calendar instance once (static), it should solve it."
0,"Add Warnlog on Extraction FailureIt will be fine to have a feedback if a exception occurs in the TextExtractors.
At the moment only a empty StringReader will be returned.
We had the issue that we updated the content and in the textextractor a exception occured
so the index was not updated and the document was searchable by its old content."
0,"Open up org.apache.commons.httpclient.Base64 pleaseI have had several problems lately where I needed to truck backwards and
forwards between bytes and base64. As I am using HttpClient, I know I have the
code in my proect, but I need to duplicate it into my own heirarchy to get
access rights. 

Please make appropriate changes to Base64 (can be as simple as marking the
encode and decode methods public) to allow outside use of Base64.

Would be nice to extend Base64 to deal with multi-line Base64 content too - but
I know this is outside of Base64 original intended use. But it would be useful. :)"
0,"Transparent Content Coding supportI would like to see HttpClient features brought up to parity with other libraries, both in Java and other languages. c.f. Python's httplib2 (not yet in the standard library, but many would like to see it in there). That library transparently handles gzip and compress content codings.

This issue is to capture possible solutions to providing this sort of innate functionality in HttpClient, so that users aren't required to know RFC2616 intimately. The HttpClient library should do the right thing and use the network in the most efficient manner possible."
1,"IndexOutOfBoundsException at ShingleMatrixFilter's Iterator#hasNext methodI tried to use the ShingleMatrixFilter within Solr. To test the functionality etc., I first used the built-in field analysis view.The filter was configured to be used only at query time analysis with ""_"" as spacer character and a min. and max. shingle size of 2. The generation of the shingles for query strings with this filter seems to work at this view, but by turn on the highlighting of indexed terms that will match the query terms, the exception was thrown. Also, each time I tried to query the index the exception was immediately thrown.

Stacktrace:
{code}
java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
	at java.util.ArrayList.RangeCheck(Unknown Source)
	at java.util.ArrayList.get(Unknown Source)
	at org.apache.lucene.analysis.shingle.ShingleMatrixFilter$Matrix$1.hasNext(ShingleMatrixFilter.java:729)
	at org.apache.lucene.analysis.shingle.ShingleMatrixFilter.next(ShingleMatrixFilter.java:380)
	at org.apache.lucene.analysis.StopFilter.next(StopFilter.java:120)
	at org.apache.lucene.analysis.TokenStream.next(TokenStream.java:47)
	...
{code}

Within the hasNext method, there is the {{s-1}}-th Column from the ArrayList {{columns}} requested, but there isn't this entry within columns.

I created a patch that checks, if {{columns}} contains enough entries."
1,"MatchAllDocsQuery doesn't honor boost or queryNormMatchAllDocsQuery doesn't pay attention to either it's own boost, or lucene's query normalization factor."
1,"removing source parent node after session move throws on savethe following code fragment illustrates the problem:

        /**
         * create the following node tree:
         *     
         *       + A
         *         + B
         *            + C
         *         + D
         */
        Node A;
        if (root.hasNode(""A"")) {
            A = root.getNode(""A"");
        } else {
            A = root.addNode(""A"");
        }
        Node B = A.addNode(""B"");
        Node C = B.addNode(""C"");
        Node D = A.addNode(""D"");
        root.save();

        // move C under D
        session.move(""/A/B/C"", ""/A/D/C"");
        // remove B
        A.getNode(""B"").remove();
        /**
         * the expected resulting node tree:
         *     
         *       + A
         *         + D
         *            + C
         */
        A.save();


==> the last save() will throw 
javax.jcr.RepositoryException: inconsistency: failed to retrieve transient state for ...
 "
1,"303 Redirects are not handled properlyWhen the server spits back a 303 (See Other), the redirect is not handled. 
Looking at the code, I saw that the processRedirectResponse method in
HttpMethodBase does not check for SC_SEE_OTHER in the case statement. 
SC_SEE_OTHER is a redirect and should be handled appropriately.

Here is a trace from the output of the client and server.

GET http://172.30.229.75/CGI/Screenshot HTTP/1.1 
Authorization: Basic c3VwZXJ1c2VyOnJvb3Q= 
Host: 172.30.229.75 
User-Agent: Jakarta Commons-HttpClient/2.0M1 

HTTP/1.1 303 See Other 
Location: http://172.30.229.75/FS/CIP_0_5842
Content-Length: 0 
Server: *snip*"
0,"Add offsets to postings (D&PEnum)I think should explore making start/end offsets a first-class attr in the
postings APIs, and fixing the indexer to index them into postings.

This will make term vector access cleaner (we now have to jump through
hoops w/ non-first-class offset attr).  It can also enable efficient
highlighting without term vectors / reanalyzing, if the app indexes
offsets into the postings.
"
0,"replace UUID strings by UUID classes in NodeId, etc..Currently the UUIDs of the nodes are stored as Strings in the ItemIds and ItemStates and cause alot of overhead throughout jackrabbit. they should be replaced by a fast implementation of a UUID class."
1,"MSSqlFileSystem - JNDI & several configuration issuesthere are several configuration issues using the org.apache.jackrabbit.core.fs.db.MSSqlFileSystem
my (working) configuration (repository.xml) looks like:

<FileSystem class=""org.apache.jackrabbit.core.fs.db.MSSqlFileSystem"">
 <param name=""driver"" value=""javax.naming.InitialContext""/>
 <param name=""url"" value=""java:MYDatasource""/>
 <param name=""schema"" value=""mssql""/>
 <param name=""schemaObjectPrefix"" value=""MYPREFIX_""/>
 <param name=""user"" value=""MYUSERNAME""/> 
 <param name=""password"" value=""MYPASSWORD""/>
 <param name=""tableSpace"" value=""""/>
</FileSystem>

i have to unnecessarily specify username & password, because the MSSqlFileSystem presets them to an empty string instead of null. funnily enough  the tableSpace is preset to null, which leads to a nullpointer in createSchemaSql



"
0,"Remove benchmark/lib/xml-apis.jar - JVM 1.5 already contains the required JAXP 1.3 implementationOn [LUCENE-2957|https://issues.apache.org/jira/browse/LUCENE-2957?focusedCommentId=13004991&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13004991], Uwe wrote:
{quote}
xml-apis.jar is not needed with xerces-2.9 and Java 5, as Java 5 already has these interface classes (JAXP 1.3). Xerces 2.11 needs it, because it ships with Java 6's JAXP release (containing STAX & Co. not available in Java 5).
{quote}

On the #lucene IRC channel, Uwe also wrote:
{noformat}
since we are on java 5 since 3.0
we have the javax APIs already available in the JVM
xerces until 2.9.x only needs JAXP 1.3
so the only thing you need is xercesImpl.jar
and serializer.jar
serializer.jar is shared between all apache xml projects, dont know the exact version number
ok you dont need it whan you only parse xml
as soon as you want to serialize a dom tree or result of an xsl transformation you need it
[...]
but if we upgrade to latest xerces we need it [the xml-apis jar] again unless we are on java 6
so the one shipped with xerces 2.11 is the 1.4 one
because xerces 2.11 supports Stax
{noformat}"
1,"Test failures in jcr-rmi and jcr2davIntegration testing currently fails for jcr-rmi:
  testCloneNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceCloneTest)
  testCopyNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceCopyBetweenWorkspacesTest)
  testCopyNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceCopyTest)
  testMoveNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceMoveTest)
  testImpersonate(org.apache.jackrabbit.test.api.ImpersonateTest)
  testCheckPermission(org.apache.jackrabbit.test.api.CheckPermissionTest)
  testRemoveItem4(org.apache.jackrabbit.test.api.SessionRemoveItemTest)
  testReadOnlyPermission(org.apache.jackrabbit.test.api.HasPermissionTest)
  testGetPrivileges(org.apache.jackrabbit.test.api.security.RSessionAccessControlDiscoveryTest)
  testNotHasPrivileges(org.apache.jackrabbit.test.api.security.RSessionAccessControlDiscoveryTest)
  testGetApplicablePolicies(org.apache.jackrabbit.test.api.security.RSessionAccessControlPolicyTest)
  testGetPolicy(org.apache.jackrabbit.test.api.security.RSessionAccessControlPolicyTest)
  testGetEffectivePolicy(org.apache.jackrabbit.test.api.security.RSessionAccessControlPolicyTest)
  testMoveNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testSetProperty(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testSetValue(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testDeleteNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testDeleteProperty(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testWorkspaceMoveNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testCopyNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)

and jcr2dav:
  testCloneNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceCloneTest)
  testCopyNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceCopyBetweenWorkspacesTest)
  testCopyNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceCopyTest)
  testMoveNodesAccessDenied(org.apache.jackrabbit.test.api.WorkspaceMoveTest)
  testCheckPermission(org.apache.jackrabbit.test.api.CheckPermissionTest)
  testRemoveItem4(org.apache.jackrabbit.test.api.SessionRemoveItemTest)
  testReadOnlyPermission(org.apache.jackrabbit.test.api.HasPermissionTest)
  testMoveNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testSetProperty(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testSetValue(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testDeleteNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testDeleteProperty(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testWorkspaceMoveNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
  testCopyNode(org.apache.jackrabbit.test.api.security.RSessionAccessControlTest)
"
0,"Jcr-Server Contrib: Remove JDOM dependenciesJDOM has been replace throught the jackrabbit project except the jcr-server contrib.

"
0,"optimizer for n-gram PhraseQueryIf 2-gram is used and the length of query string is 4, for example q=""ABCD"", QueryParser generates (when autoGeneratePhraseQueries is true) PhraseQuery(""AB BC CD"") with slop 0. But it can be optimized PhraseQuery(""AB CD"") with appropriate positions.

The idea came from the Japanese paper ""N.M-gram: Implementation of Inverted Index Using N-gram with Hash Values"" by Mikio Hirabayashi, et al. (The main theme of the paper is different from the idea that I'm using here, though)"
1,"DbInputStream does not support mark()/reset() when exhausted.The DbDataStore implementation uses a DbInputStream to read binary properties from the database. When a new binary property is created, Jackrabbit attempts to index it. Tika's CharsetDetector is used in the process, which marks the input stream, reads the first 8000 bytes and then resets the stream.

This results in the stacktrace shown at the end of the issue, if the following two conditions hold true:
* the property is larger than the minRecordLength configuration of the Datastore and
* the property is smaller than 8000 bytes

The DbInputStream needs to have the following properties:
1. lazy instantiation of the underlying stream
2. auto-close underlying stream when EOF is reached
3. fully support mark()/reset() even if  the underlying stream is auto-closed due to 2.


12.03.2010 15:53:28 *WARN * LazyTextExtractorField: Failed to extract text from a binary property (LazyTextExtractorField.java, line 165)
java.io.EOFException
        at org.apache.jackrabbit.core.data.db.DbInputStream.reset(DbInputStream.java:180)
        at org.apache.tika.io.ProxyInputStream.reset(ProxyInputStream.java:156)
        at org.apache.tika.io.ProxyInputStream.reset(ProxyInputStream.java:156)
        at org.apache.tika.parser.txt.CharsetDetector.setText(CharsetDetector.java:131)
        at org.apache.tika.parser.txt.TXTParser.parse(TXTParser.java:77)
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:120)
        at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:101)
        at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:114)
        at org.apache.jackrabbit.core.query.lucene.LazyTextExtractorField$ParsingTask.run(LazyTextExtractorField.java:160)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:207)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
"
0,"TCK: SerializationTest.helpTestSaxException casts ContentHandler to DefaultHandlerthe JSR170 defines import with ContentHandler (see Session.getImportContentHandler, Workspace.getImportContentHandler)

the mentioned helper method within the TCK casts the ContentHandler returned by those methods to DefaultHandler without testing if the contenthandler is a DefaultHandler (line 273)"
0,"[PATCH] BitSetQuery, FastPrefixQuery, FastWildcardQuery and FastQueryParserFastPrefixQuery and FastWildcardQuery rewrites to BitSetQuery instead of OR'ed
BooleanQuery's.  A BitSetQuery contains a BitSet that desginates which document
should be included in the search result.  BitSetQuery cannot be used by itself
with MultiSearcher as of now."
1,"Do not launch new merges if IndexWriter has hit OOMEif IndexWriter has hit OOME, it defends itself by refusing to commit changes to the index, including merges.  But this can lead to infinite merge attempts because we fail to prevent starting a merge.

Spinoff from http://www.nabble.com/semi-infinite-loop-during-merging-td23036156.html."
0,"allow SPI implementation to compute default values for autocreated propertiesCurrently, when creating nodes in transient space, JCR2SPI uses hard-wired logic trying to populate system generated properties such as jcr:created, jcr;uuid and so on.

This is problematic as

- it doesn't scale -- it fails for autocreated properties not known to JCR2SPI, and

- the syntax for the defaults may be dependant on the back end, such as legal syntax for (UU)IDs.

Proposal:

- extend QValueFactory with something like

  QValue computeDefaultValue(QPropertyDefinition)

- use that in JCR2SPI, getting rid of the currently hard-wired logic.
"
0,IntParser and FloatParser unused by FieldCacheImplFieldCacheImpl doesn't use IntParser or FloatParser to parse values
1,"Node.orderBefore and JackrabbitNode.rename should check for ability to modify children-collection on parent nodecurrently the implementation of Node.orderBefore and JackrabbitNode.rename perform the same validation that is executed
for a move operation which includes removal of the original node. however, the methods mentioned above only include
a manipulation on the child-node-collection of the parent (subset of the current check). therefore the permission check should be 
adjusted accordingly."
0,"Fixed Spelling mailinglist.xmlJust fixed some spelling in the mailinglist.xml in /java/trunk/xdocs



"
0,"References to old repository-1.x.dtdSome components still reference old version of the repository-1.x.dtd.
All components should be upgraded to repository-1.6.dtd"
1,"NullPointerException in IndexModifier.close()We upgraded from Lucene 2.0.0. to 2.3.1 hoping this would resolve this issue.

http://jira.codehaus.org/browse/MRM-715

Trace is as below for Lucene 2.3.1:
java.lang.NullPointerException
at org.apache.lucene.index.IndexModifier.close(IndexModifier.java:576)
at org.apache.maven.archiva.indexer.lucene.LuceneRepositoryContentIndex.closeQuietly(LuceneRepositoryContentIndex.java:416)
at org.apache.maven.archiva.indexer.lucene.LuceneRepositoryContentIndex.modifyRecord(LuceneRepositoryContentIndex.java:152)
at org.apache.maven.archiva.consumers.lucene.IndexContentConsumer.processFile(IndexContentConsumer.java:169)
at org.apache.maven.archiva.repository.scanner.functors.ConsumerProcessFileClosure.execute(ConsumerProcessFileClosure.java:51)
at org.apache.commons.collections.functors.IfClosure.execute(IfClosure.java:117)
at org.apache.commons.collections.CollectionUtils.forAllDo(CollectionUtils.java:388)
at org.apache.maven.archiva.repository.scanner.RepositoryContentConsumers.executeConsumers(RepositoryContentConsumers.java:283)
at org.apache.maven.archiva.proxy.DefaultRepositoryProxyConnectors.transferFile(DefaultRepositoryProxyConnectors.java:597)
at org.apache.maven.archiva.proxy.DefaultRepositoryProxyConnectors.fetchFromProxies(DefaultRepositoryProxyConnectors.java:157)
at org.apache.maven.archiva.web.repository.ProxiedDavServer.applyServerSideRelocation(ProxiedDavServer.java:447)
at org.apache.maven.archiva.web.repository.ProxiedDavServer.fetchContentFromProxies(ProxiedDavServer.java:354)
at org.apache.maven.archiva.web.repository.ProxiedDavServer.process(ProxiedDavServer.java:189)
at org.codehaus.plexus.webdav.servlet.multiplexed.MultiplexedWebDavServlet.service(MultiplexedWebDavServlet.java:119)
at org.apache.maven.archiva.web.repository.RepositoryServlet.service(RepositoryServlet.java:155)
at javax.servlet.http.HttpServlet.service(HttpServlet.java:803)"
0,"jcr-rmi maven ""site"" target failsthe target ""site"" in jcr rmi fails because org.apache.jackrabbit.rmi.remote.SerialValue.java is empty"
1,"WorkspaceAccessManager defined with SecurityManager that keeps users per workspace must test if user existsthe WorkspaceAccessManager defined with the security manager keeping users per workspace currently returns true upon calls to grant(Set, String) if
a workspace with the given name exists.

while this is fine for the initial check upon session creation, it obviously isn't for all method calls that test for accessible workspace names, such as
Workspace#getAccessibleWorkspaceName, Workspace#clone and copy across workspaces.

instead it should test if any of the specified principals corresponds to a valid user within the workspace identified by the given workspaceName."
0,"When using QueryWrapperFilter with CachingWrapperFilter, QueryWrapperFilter returns a DocIdSet that creates a Scorer, which gets cached rather than a bit setthere is a large performance cost to this.

The old impl for this type of thing, QueryFilter, recommends :

@deprecated use a CachingWrapperFilter with QueryWrapperFilter

The deprecated QueryFilter itself also suffers from the problem because its now implemented using a CachingWrapperFilter and QueryWrapperFilter.

see http://search.lucidimagination.com/search/document/7f54715f14b8b7a/lucene_2_9_0rc4_slower_than_2_4_1"
0,"Append-only index updatesCurrently index updates modify some existing files. This is troublesome in scenarios like a backup or when an index will be shared in a cluster (though this is not yet the case).

Requirements are:

- index segments need a custom (lucene) IndexDeletionPolicy to keep index commits for a given time.
- index segments are not only referenced by their name, but also with their generation
- the segments file must now also record the generation of a segment. the file itself must be generational itself.
- purging of outdated index segment commits
"
1,"GzipDecompressingEntity (and therefore ContentEncodingHttpClient) not consistent with EntityUtils.consumeEntityInvoking EntityUtils.consume( entity ) after a previous call to entity.getContent (and subsequent processing of the content) throws a java.io.EOFException when gzip decompression support is enabled via ContentEncodingHttpClient or some similar mechanism.  I invoke EntityUtils.consume in a 'finally' block - maybe I'm not using the API correctly ... ?  

java.io.EOFException
	at java.util.zip.GZIPInputStream.readUByte(GZIPInputStream.java:207)
	at java.util.zip.GZIPInputStream.readUShort(GZIPInputStream.java:197)
	at java.util.zip.GZIPInputStream.readHeader(GZIPInputStream.java:136)
	at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:58)
	at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:68)
	at org.apache.http.client.entity.GzipDecompressingEntity.getContent(GzipDecompressingEntity.java:63)
	at org.apache.http.conn.BasicManagedEntity.getContent(BasicManagedEntity.java:88)
	at org.apache.http.util.EntityUtils.consume(EntityUtils.java:65)

I believe the problem is that the underlying DecompressingEntity allocates a new GzipInputStream for each call to getContent, rather than caching the stream created by the first getContent call.  
       http://svn.apache.org/repos/asf/httpcomponents/httpclient/trunk/httpclient/src/main/java/org/apache/http/client/entity/DecompressingEntity.java
The ""CustomProtocolInterceptors"" example has the same bug:  http://hc.apache.org/httpcomponents-client-ga/examples.html

I worked around the problem implementing the example with my own GzipDecompressingEntity (scala code - lazy value not evaluated till accessed):

  class GzipDecompressingEntity( entity:http.HttpEntity) extends http.entity.HttpEntityWrapper(entity) {
    private lazy val gzipStream = new GZIPInputStream( entity.getContent() )
    
    /** 
     * Wrap entity stream in GZIPInputStream
     */
    override def getContent():java.io.InputStream = gzipStream

    /**
     * Return -1 - don't know unzipped content size
     */
    override def getContentLength():Long = -1L
  }

"
0,"Minimize calls to PersistenceManagerIn some situations the PersistenceManager is called even though it is not necessary.

E.g. when new items are created the method NodeImpl.getOrCreateProperty() will always check if there is an already existing property state. If the node is new the call will always go down the full item state stack and ask the PersistenceManager if it knows the property id. This is unnessessary because there will never exist properties in the persistence manager for a new node that has not been saved yet.

I propose to add a check to the method to see if  the node is new and does not yet have a property with the given name. In that case the property can be created without further checks.

With the patch applied the time to transiently create 1000 nodes with 4 properties each drops from 1485 ms to 422 ms."
0,"Analysis package calls Java 1.5 APII found compile errors when I tried to compile trunk with 1.4 JVM.
org.apache.lucene.analysis.NormalizeCharMap
org.apache.lucene.analysis.MappingCharFilter

uses Character.valueOf() which has been added in 1.5.
I added a CharacterCache (+ testcase) with a valueOf method as a replacement for that quite useful method.

org.apache.lucene.analysis.BaseTokenTestCase

uses StringBuilder instead of the synchronized version StringBuffer (available in 1.4)

I will attach a patch shortly."
1,"Occasional NullPointerException in ItemManagerFrom time to time I see a NullPointerException in ItemManager when running ConcurrentReadWriteTest. The exception is probably caused by another session that removes the property, which has the effect that the ItemState in ItemData is set to null.

Exception in thread ""Thread-11"" java.lang.NullPointerException
	at org.apache.jackrabbit.core.ItemManager.canRead(ItemManager.java:313)
	at org.apache.jackrabbit.core.ItemManager.getItemData(ItemManager.java:293)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:226)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:486)
	at org.apache.jackrabbit.core.LazyItemIterator.prefetchNext(LazyItemIterator.java:111)
	at org.apache.jackrabbit.core.LazyItemIterator.<init>(LazyItemIterator.java:93)
	at org.apache.jackrabbit.core.LazyItemIterator.<init>(LazyItemIterator.java:75)
	at org.apache.jackrabbit.core.ItemManager.getChildProperties(ItemManager.java:658)
	at org.apache.jackrabbit.core.NodeImpl.getProperties(NodeImpl.java:2663)
	at org.apache.jackrabbit.core.ConcurrentReadWriteTest$1$1.execute(ConcurrentReadWriteTest.java:65)
	at org.apache.jackrabbit.core.AbstractConcurrencyTest$Executor.run(AbstractConcurrencyTest.java:206)
	at java.lang.Thread.run(Thread.java:595)

This issue does not occur in a release but only in trunk."
0,"Testcase for StandardAnalyzerAs per our discussion on lucene-user, I'm attaching a unit test for 
StandardAnalyzer.  I wrote most of the tests from reading the comments in the 
StandardTokenizer.jj grammar.  Someone familiar with the grammar (and its 
intent) should review the tests."
1,"URI.normalize() errorcode:

----------------------------
import org.apache.commons.httpclient.URI;

class Main {
  publi static void main(String[] args) throws Exception {
    URI uri = new URI(""http"", null, ""host"", -1, ""/tmp/../yo"", null, null);
    uri.normalize();
    System.out.println(uri);
  }
} /// end of Main
----------------------------

prints:

http://host/tmp/../yo

instead of

http://host/yo"
0,"Enable maven-source-pluginCurrently the maven-source-plugin is enabled by default in jackrabbit-jcr-rmi, but it would be good to enable it globally for all Jackrabbit components."
0,"Avoid unnecessary index reader calls when using aggregate definitionsSearchIndex.retrieveAggregateRoot(Set<NodeId> removedIds, Map<NodeId, NodeState> map) identifies aggregate root nodes based on removed nodes and aggregate rules defined in the indexing configuration. This process requires index lookups. The method can be optimized for the case when no nodes are removed and an unnecessary call to the index reader can be avoided."
1,RMI reference not automatically bound by the standalone serverThe RMI servlet in the 1.5.0 standalone server is only initialized (and the remote reference bound to the RMI registry) when the http://.../rmi URL is first accessed. The RMI binding should be made as soon as the standalone server starts.
0,"Contrib analyzers need testsThe analyzers in contrib need tests, preferably ones that test the behavior of all the Token 'attributes' involved (offsets, type, etc) and not just what they do with token text.

This way, they can be converted to the new api without breakage."
1,"IndexWriter.updateDocument is no longer atomicSpinoff from LUCENE-847.

Ning caught that as of LUCENE-843, we lost the atomicity of the delete
+ add in IndexWriter.updateDocument.

Ning suggested a simple fix: move the buffered deletes into
DocumentsWriter and let it do the delete + add atomically.  This has a
nice side effect of also consolidating the ""time to flush"" logic in
DocumentsWriter.

"
0,"Add ReverseStringFilteradd ReverseStringFilter and ReverseStringAnalyzer that can be used for backword much. For Example, ""*ry"", ""*ing"", ""*ber""."
1,"Concurrent Session.move() operations failurePerforming concurrent move operations may cause failures similar to the following:

javax.jcr.RepositoryException: Unable to update item: node /
       at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1147)
       at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:920)
       at ConcurrentMoveTest$1.execute(ConcurrentMoveTest.java:30)
       at org.apache.jackrabbit.core.AbstractConcurrencyTest$Executor.run(AbstractConcurrencyTest.java:209)
       at java.lang.Thread.run(Thread.java:637)
Caused by: org.apache.jackrabbit.core.state.ItemStateException: Unable
to resolve path for item: 79a0fbdb-49fd-4830-a842-5ab11842cd17
       at org.apache.jackrabbit.core.observation.EventStateCollection.getPath(EventStateCollection.java:683)
       at org.apache.jackrabbit.core.observation.EventStateCollection.createEventStates(EventStateCollection.java:268)
       at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:702)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:1110)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:1140)
       at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:351)
       at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:354)
       at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
       at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:328)
       at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1141)
       ... 4 more
Caused by: javax.jcr.ItemNotFoundException: failed to build path of
79a0fbdb-49fd-4830-a842-5ab11842cd17:
826f0c19-9956-402a-9c0d-93089eedcc1c has no child entry for
79a0fbdb-49fd-4830-a842-5ab11842cd17
       at org.apache.jackrabbit.core.HierarchyManagerImpl.buildPath(HierarchyManagerImpl.java:291)
       at org.apache.jackrabbit.core.CachingHierarchyManager.buildPath(CachingHierarchyManager.java:198)
       at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:395)
       at org.apache.jackrabbit.core.CachingHierarchyManager.getPath(CachingHierarchyManager.java:232)
       at org.apache.jackrabbit.core.observation.EventStateCollection.getPath(EventStateCollection.java:678)
       ... 13 more
"
0,"Specialize BooleanQuery if all clauses are TermQueriesDuring work on LUCENE-3319 I ran into issues with BooleanQuery compared to PhraseQuery in the exact case. If I disable scoring on PhraseQuery and bypass the position matching, essentially doing a conjunction match, ExactPhraseScorer beats plain boolean scorer by 40% which is a sizeable gain. I converted a ConjunctionScorer to use DocsEnum directly but still didn't get all the 40% from PhraseQuery. Yet, it turned out with further optimizations this gets very close to PhraseQuery. The biggest gain here came from converting the hand crafted loop in ConjunctionScorer#doNext to a for loop which seems to be less confusing to hotspot. In this particular case I think code specialization makes lots of sense since BQ with TQ is by far one of the most common queries.

I will upload a patch shortly"
0,JSR 283: Evaluate Capabilities Exposed by Session.hasCapability
0,Make WeightedSpanTermExtractor extensible to handle custom query implemenationsCurrently if I have a custom query which subclasses query directly I can't use the QueryScorer for highlighting since it does explicit instanceof checks. In some cases its is possible to rewrite the query before passing it to the highlighter to obtain a primitive query. However I had the usecase where this was not possible ie. the original index was not available on the machine which highlights the results. To still use the highlighter I had to copy a bunch of code due to visibility issues in those classes. I think we can make this extensible with minor effort to allow this usecase without massive code duplication.
0,"BasicResponseHandler Javadoc Needs ClarificationThe class-level javadoc for BasicResponseHandler indicates that it reads the response body before throwing an Exception for responses with status code >= 300, which is not the case."
0,"Rename Analyzer.reusableTokenStream() to tokenStream()All Analysis consumers now use reusableTokenStream().  To finally make reuse mandatory, lets rename resuableTokenStream() to tokenStream() (removing the old tokenStream() method)."
1,"AccessControlManager#setPolicy may fail for new applicable policy despite jcr:modifyAccessControl privilege being grantedthe sequence AccessControlManager.getApplicablePolicies -> modify -> AccessControlManager#setPolicy fails
due to bug in AC evaluation if target is an AC-item but not yet existing. in this case the
wrong parent node is used for AC-evaluation."
1,"Unclosed files when aggregated property states are indexedThis is a regression caused by JCR-1990.

The lucene document for the node that contains the aggregated property may contain an extractor job that has an open file handle to the jcr:data binary property. The document must be disposed after the properties are transferred."
1,"JCR2SPI: lockmgr isn't aware about external unlock (CacheBehavior.OBSERVATION)issue occurring with CacheBehavior.OBSERVATION only:

the lock manager expects the jcr:lockIsDeep property to be created upon successful lock.
this however isn't the case since the time, we changed the Operation.persisted method to invalidate the affected states. consequently the mgr never started to listen on changes made to the jcr:lockIsDeep property and consequently wasn't aware of an external removal.

suggested fix:
force re-loading of the lock holding node."
0,"Document Vector->ArrayListDocument Vector should be changed to ArrayList.
Document is not advertised to be thread safe, and it's doubtful that anyone modifies a Document from multiple threads."
0,"Term improvementTerm is designed for reuse of the supplied filter, to minimize intern().

One of the common use patterns is to create a Term with the txt field being an empty string.

To simplify this pattern and to document it's usefulness, I suggest adding a constructor:
public Term(String fld)
with the obvious implementation
and use it throughout core and contrib as a replacement.

"
1,"org.apache.commons.httpclient.HeaderElement fail to parse cookie headerif Set-Cookie header has value such ""expires=Mon, .."".
org.apache.commons.httpclient.HeaderElement will fail to parse this header.

Cause:
In the source cord:
-------------
            try {
                /*
                 * Following to RFC 2109 and 2965, in order not to conflict
                 * with the next header element, make it sure to parse tokens.
                 * the expires date format is ""Wdy, DD-Mon-YY HH:MM:SS GMT"".
                 * Notice that there is always comma(',') sign.
                 * For the general cases, rfc1123-date, rfc850-date.
                 */
                if (tokenizer.hasMoreTokens()) {
                    String s = nextToken.toLowerCase();
                    if (nextToken.endsWith(""mon"") 
                        || s.endsWith(""tue"")
                        || s.endsWith(""wed"") 
                        || s.endsWith(""thu"")
                        || s.endsWith(""fri"")
                        || s.endsWith(""sat"")
                        || s.endsWith(""sun"")
                        || s.endsWith(""monday"") 
                        || s.endsWith(""tuesday"") 
---- snip ---
 
""if (nextToken.endsWith(""mon"") "" is wrong.
this must be ""if (s.endsWith(""mon"") "".

Source cord version:
 * $Header:
/home/cvspublic/jakarta-commons/httpclient/src/java/org/apache/commons/httpclient/HeaderElement.java,v
1.17 2003/03/08 21:30:02 olegk Exp $
 * $Revision: 1.17 $
 * $Date: 2003/03/08 21:30:02 $"
1,"replace invalid U+FFFF character during indexingIf the invalid U+FFFF character is embedded in a token, it actually causes indexing to silently corrupt the index by writing duplicate terms into the terms dict.  CheckIndex will catch the error, and merging will hit exceptions (I think).

We already replace invalid surrogate pairs with the replacement character U+FFFD, so I'll just do the same with U+FFFF."
0,"SPI: Testsuite for the SPI Interfacesnow that people start writing SPI implementations we should provide a test-suite that runs on the SPI directly in order to provide the developers a way to assert basic compliance of their implementation without having the JCR api in between.
"
0,"Use covariant clone() return types*Paul Cowan wrote in LUCENE-1257:*

OK, thought I'd jump in and help out here with one of my Java 5 favourites. Haven't seen anyone discuss this, and don't believe any of the patches address this, so thought I'd throw a patch out there (against SVN HEAD @ revision 827821) which uses Java 5 covariant return types for (almost) all of the Object#clone() implementations in core. 
i.e. this:

public Object clone() {
changes to:
public SpanNotQuery clone() {

which lets us get rid of a whole bunch of now-unnecessary casts, so e.g.

if (clone == null) clone = (SpanNotQuery) this.clone();
becomes
if (clone == null) clone = this.clone();

Almost everything has been done and all downcasts removed, in core, with the exception of

Some SpanQuery stuff, where it's assumed that it's safe to cast the clone() of a SpanQuery to a SpanQuery - this can't be made covariant without declaring ""abstract SpanQuery clone()"" in SpanQuery itself, which breaks those SpanQuerys that don't declare their own clone() 
Some IndexReaders, e.g. DirectoryReader - we can't be more specific than changing .clone() to return IndexReader, because it returns the result of IndexReader.clone(boolean). We could use covariant types for THAT, which would work fine, but that didn't follow the pattern of the others so that could be a later commit. 
Two changes were also made in contrib/, where not making the changes would have broken code by trying to widen IndexInput#clone() back out to returning Object, which is not permitted. contrib/ was otherwise left untouched.

Let me know what you think, or if you have any other questions."
0,"[PATCH] jackrabbit-webapp pom.xml patch to create an additional jar artifactModifies the jackrabbit-webapp pom.xml to create a jar artifact in addition to the existing war artifact, to allow the jackrabbit-webapp utility servlets to be reused in other modules.

The right way would be to create a separate jar module for the servlets (or move them to jackrabbit-jcr-commons?), and reuse that jar as a dependency in the jackrabbit-webapp. So I'm not sure if this patch deserves to be applied to the trunk, but it can be useful as a workaround before a cleaner solution is implemented.

See also http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/200705.mbox/%3C510143ac0705151453t7a0eb4cam859a40fb106e81f5@mail.gmail.com%3E which discusses possible improvements to these jackrabbit-webapp utility servlets.

"
0,'ant javacc' in root project should also properly create contrib/surround Java filesFor consistency after LUCENE-1829 which did the same for contrib/queryparser
0,Add the new DataSource element to the repository DTDThe connection pooling feature from JCR-1456 introduced a new DataSource configuration element to Jackrabbit. It should be added to the repository config DTD.
0,"Add a way to locate full text extraction problemsFull text indexing of a binary document can fail for various reasons. Currently we just log a generic error message in such cases, which makes it difficult for the user to locate such problems for review and reindexing. We should improve this by making the logs more informative or by adding some other mechanism for locating troublesome documents."
1,"DataStore: garbage collection fails if a workspace is not initializedThe test case GCEventListenerTest fails with the following exception:

testEventListener(org.apache.jackrabbit.core.data.GCEventListenerTest)  Time elapsed: 10.235 sec  <<< ERROR!
java.lang.IllegalStateException: workspace 'test' not initialized
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getPersistenceManager(RepositoryImpl.java:1703)
	at org.apache.jackrabbit.core.SessionImpl.createDataStoreGarbageCollector(SessionImpl.java:694)
	at org.apache.jackrabbit.core.data.GCEventListenerTest.doTestEventListener(GCEventListenerTest.java:75)
	at org.apache.jackrabbit.core.data.GCEventListenerTest.testEventListener(GCEventListenerTest.java:49)

"
0,"Change value for SearchIndex#DEFAULT_EXTRACTOR_BACK_LOGThe value is currently 100. This means that once 100 extractor jobs are pending in the indexing queue additional extractor jobs are executed with the current thread. I think it would be more useful to change this value to Integer.MAX_VALUE (or in other words: unbounded).

If the backlog is filled up then this indicates that the repository is very busy and we should not put additional burden on the current thread in that case."
0,fix for Document.getBoost() documentationThe attached patch fixes the javadoc to make clear that getBoost() will never return a useful value in most cases. I will commit this unless someone has a better wording or a real fix.
1,"QueryManager.createQuery() exception handlingQuery q = this.superuser.getWorkspace().getQueryManager()
                .createQuery(""SELECT * FROM nt:base"", Query.XPATH);

produces:
org.apache.jackrabbit.core.query.xpath.TokenMgrError: Lexical error at line 1, column 28.  Encountered: ""b"" (98), after : "":""
	at org.apache.jackrabbit.core.query.xpath.XPathTokenManager.getNextToken(XPathTokenManager.java:14546)
	at org.apache.jackrabbit.core.query.xpath.XPath.jj_ntk(XPath.java:9187)
	at org.apache.jackrabbit.core.query.xpath.XPath.PredicateList(XPath.java:5195)
	at org.apache.jackrabbit.core.query.xpath.XPath.AxisStep(XPath.java:4707)
	at org.apache.jackrabbit.core.query.xpath.XPath.StepExpr(XPath.java:4597)
	at org.apache.jackrabbit.core.query.xpath.XPath.RelativePathExpr(XPath.java:4511)
	at org.apache.jackrabbit.core.query.xpath.XPath.PathExpr(XPath.java:4482)
	at org.apache.jackrabbit.core.query.xpath.XPath.ValueExpr(XPath.java:4125)
	at org.apache.jackrabbit.core.query.xpath.XPath.UnaryExpr(XPath.java:4032)
	at org.apache.jackrabbit.core.query.xpath.XPath.CastExpr(XPath.java:3935)
	at org.apache.jackrabbit.core.query.xpath.XPath.CastableExpr(XPath.java:3898)
	at org.apache.jackrabbit.core.query.xpath.XPath.TreatExpr(XPath.java:3861)
	at org.apache.jackrabbit.core.query.xpath.XPath.InstanceofExpr(XPath.java:3824)
	at org.apache.jackrabbit.core.query.xpath.XPath.IntersectExceptExpr(XPath.java:3748)
	at org.apache.jackrabbit.core.query.xpath.XPath.UnionExpr(XPath.java:3672)
	at org.apache.jackrabbit.core.query.xpath.XPath.MultiplicativeExpr(XPath.java:3622)
	at org.apache.jackrabbit.core.query.xpath.XPath.AdditiveExpr(XPath.java:3510)
	at org.apache.jackrabbit.core.query.xpath.XPath.RangeExpr(XPath.java:3451)
	at org.apache.jackrabbit.core.query.xpath.XPath.ComparisonExpr(XPath.java:3353)
	at org.apache.jackrabbit.core.query.xpath.XPath.AndExpr(XPath.java:3290)
	at org.apache.jackrabbit.core.query.xpath.XPath.OrExpr(XPath.java:3227)
	at org.apache.jackrabbit.core.query.xpath.XPath.ExprSingle(XPath.java:2214)
	at org.apache.jackrabbit.core.query.xpath.XPath.ForClause(XPath.java:2337)
	at org.apache.jackrabbit.core.query.xpath.XPath.FLWORExpr(XPath.java:2233)
	at org.apache.jackrabbit.core.query.xpath.XPath.ExprSingle(XPath.java:2133)
	at org.apache.jackrabbit.core.query.xpath.XPath.Expr(XPath.java:2094)
	at org.apache.jackrabbit.core.query.xpath.XPath.QueryBody(XPath.java:2066)
	at org.apache.jackrabbit.core.query.xpath.XPath.MainModule(XPath.java:512)
	at org.apache.jackrabbit.core.query.xpath.XPath.Module(XPath.java:387)
	at org.apache.jackrabbit.core.query.xpath.XPath.QueryList(XPath.java:151)
	at org.apache.jackrabbit.core.query.xpath.XPath.XPath2(XPath.java:118)
	at org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder.<init>(XPathQueryBuilder.java:224)
	at org.apache.jackrabbit.core.query.xpath.XPathQueryBuilder.createQuery(XPathQueryBuilder.java:255)
	at org.apache.jackrabbit.core.query.QueryParser.parse(QueryParser.java:57)
	at org.apache.jackrabbit.core.query.lucene.QueryImpl.<init>(QueryImpl.java:119)
	at org.apache.jackrabbit.core.query.lucene.SearchIndex.createExecutableQuery(SearchIndex.java:158)
	at org.apache.jackrabbit.core.query.QueryImpl.<init>(QueryImpl.java:90)
	at org.apache.jackrabbit.core.SearchManager.createQuery(SearchManager.java:192)
	at org.apache.jackrabbit.core.query.QueryManagerImpl.createQuery(QueryManagerImpl.java:87)
	at org.apache.jackrabbit.test.api.query.IllegalXPathTest.testIllegalStatement(IllegalXPathTest.java:45)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:324)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:401)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:474)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:342)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:194)
"
1,"rep:similar in xpath does not workThis query //*[rep:similar(., '/content/en')] produces an exception:

24.09.2009 16:56:48.156 *ERROR* [0:0:0:0:0:0:0:1%0 [1253804208093] GET /libs/cq/search/content/querydebug.html HTTP/1.1] org.apache.sling.engine.impl.SlingMainServlet service: Uncaught SlingException java.lang.ArrayIndexOutOfBoundsException: -1
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.visit(LuceneQueryBuilder.java:612)
	at org.apache.jackrabbit.spi.commons.query.RelationQueryNode.accept(RelationQueryNode.java:115)
	at org.apache.jackrabbit.spi.commons.query.NAryQueryNode.acceptOperands(NAryQueryNode.java:143)
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.visit(LuceneQueryBuilder.java:489)
	at org.apache.jackrabbit.spi.commons.query.LocationStepQueryNode.accept(LocationStepQueryNode.java:166)
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.visit(LuceneQueryBuilder.java:468)
	at org.apache.jackrabbit.spi.commons.query.PathQueryNode.accept(PathQueryNode.java:74)
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.visit(LuceneQueryBuilder.java:257)
	at org.apache.jackrabbit.spi.commons.query.QueryRootNode.accept(QueryRootNode.java:115)
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.createLuceneQuery(LuceneQueryBuilder.java:247)
	at org.apache.jackrabbit.core.query.lucene.LuceneQueryBuilder.createQuery(LuceneQueryBuilder.java:227)
	at org.apache.jackrabbit.core.query.lucene.QueryImpl.execute(QueryImpl.java:111)
	at org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:127)
"
0,"Add Highlighting benchmark support to contrib/benchmarkI would like to be able to test the performance (speed, initially) of the Highlighter in a standard way.  Patch to follow that adds the Highlighter as a dependency benchmark and adds in tasks extending the ReadTask to perform highlighting on retrieved documents."
0,Remove excessive dependencies from jcr-client module
0,"Lock-less commitsThis is a patch based on discussion a while back on lucene-dev:

    http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200608.mbox/%3c44E5B16D.4010805@mikemccandless.com%3e

The approach is a small modification over the original discussion (see
Retry Logic below).  It works correctly in all my cross-machine test
case, but I want to open it up for feedback, testing by
users/developers in more diverse environments, etc.

This is a small change to how lucene stores its index that enables
elimination of the commit lock entirely.  The write lock still
remains.

Of the two, the commit lock has been more troublesome for users since
it typically serves an active role in production.  Whereas the write
lock is usually more of a design check to make sure you only have one
writer against the index at a time.

The basic idea is that filenames are never reused (""write once""),
meaning, a writer never writes to a file that a reader may be reading
(there is one exception: the segments.gen file; see ""RETRY LOGIC""
below).  Instead it writes to generational files, ie, segments_1, then
segments_2, etc.  Besides the segments file, the .del files and norm
files (.sX suffix) are also now generational.  A generation is stored
as an ""_N"" suffix before the file extension (eg, _p_4.s0 is the
separate norms file for segment ""p"", generation 4).

One important benefit of this is it avoids files contents caching
entirely (the likely cause of errors when readers open an index
mounted on NFS) since the file is always a new file.

With this patch I can reliably instantiate readers over NFS when a
writer is writing to the index.  However, with NFS, you are still forced to
refresh your reader once a writer has committed because ""point in
time"" searching doesn't work over NFS (see LUCENE-673 ).

The changes are fully backwards compatible: you can open an old index
for searching, or to add/delete docs, etc.  I've added a new unit test
to test these cases.

All units test pass, and I've added a number of additional unit tests,
some of which fail on WIN32 in the current lucene but pass with this
patch.  The ""fileformats.xml"" has been updated to describe the changes
to the files (but XXX references need to be fixed before committing).

There are some other important benefits:

  * Readers are now entirely read-only.

  * Readers no longer block one another (false contention) on
    initialization.

  * On hitting contention, we immediately retry instead of a fixed
    (default 1.0 second now) pause.

  * No file renaming is ever done.  File renaming has caused sneaky
    access denied errors on WIN32 (see LUCENE-665 ).  (Yonik, I used
    your approach here to not rename the segments_N file(try
    segments_(N-1) on hitting IOException on segments_N): the separate
    "".done"" file did not work reliably under very high stress testing
    when a directory listing was not ""point in time"").

  * On WIN32, you can now call IndexReader.setNorm() even if other
    readers have the index open (fixes a pre-existing minor bug in
    Lucene).

  * On WIN32, You can now create an IndexWriter with create=true even
    if readers have the index open (eg see
    www.gossamer-threads.com/lists/lucene/java-user/39265) .


Here's an overview of the changes:

  * Every commit writes to the next segments_(N+1).

  * Loading the segments_N file (& opening the segments) now requires
    retry logic.  I've captured this logic into a new static class:
    SegmentInfos.FindSegmentsFile.  All places that need to do
    something on the current segments file now use this class.

  * No more deletable file.  Instead, the writer computes what's
    deletable on instantiation and updates this in memory whenever
    files can be deleted (ie, when it commits).  Created a common
    class index.IndexFileDeleter shared by reader & writer, to manage
    deletes.

  * Storing more information into segments info file: whether it has
    separate deletes (and which generation), whether it has separate
    norms, per field (and which generation), whether it's compound or
    not.  This is instead of relying on IO operations (file exists
    calls).  Note that this fixes the current misleading
    FileNotFoundException users now see when an _X.cfs file is missing
    (eg http://www.nabble.com/FileNotFound-Exception-t6987.html).

  * Fixed some small things about RAMDirectory that were not
    filesystem-like (eg opening a non-existent IndexInput failed to
    raise IOException; renames were not atomic).  I added a stress
    test against a RAMDirectory (1 writer thread & 2 reader threads)
    that uncovered these.

  * Added option to not remove old files when create=true on creating
    FSDirectory; this is so the writer can do its own [more
    sophisticated because it retries on errors] removal.

  * Removed all references to commit lock, COMMIT_LOCK_TIMEOUT, etc.
    (This is an API change).

  * Extended index/IndexFileNames.java and index/IndexFileNameFilter.java
    with logic for computing generational file names.

  * Changed index/IndexFileNameFilter.java to use a HashSet to check
    file extentsions for better performance.

  * Fixed the test case TestIndexReader.testLastModified: it was
    incorrectly (I think?) comparing lastModified to version, of the
    index.  I fixed that and then added a new test case for version.


Retry Logic (in index/SegmentInfos.java)

If a reader tries to load the segments just as a writer is committing,
it may hit an IOException.  This is just normal contention.  In
current Lucene contention causes a [default] 1.0 second pause then
retry.  With lock-less the contention causes no added delay beyond the
time to retry.

When this happens, we first try segments_(N-1) if present, because it
could be segments_N is still being written.  If that fails, we
re-check to see if there is now a newer segments_M where M > N and
advance if so.  Else we retry segments_N once more (since it could be
it was in process previously but must now be complete since
segments_(N-1) did not load).

In order to find the current segments_N file, I list the directory and
take the biggest segments_N that exists.

However, under extreme stress testing (5 threads just opening &
closing readers over and over), on one platform (OS X) I found that
the directory listing can be incorrect (stale) by up to 1.0 seconds.
This means the listing will show a segments_N file but that file does
not exist (fileExists() returns false).

In order to handle this (and other such platforms), I switched to a
hybrid approach (originally proposed by Doron Cohen in the original
thread): on committing, the writer writes to a file ""segments.gen"" the
generation it just committed.  It writes 2 identical longs into this
file.  The retry logic, on detecting that the directory listing is
stale falls back to the contents of this file.  If that file is
consistent (the two longs are identical), and, the generation is
indeed newer than the dir listing, it will use that.

Finally, if this approach is also stale, we fallback to stepping
through sequential generations (up to a maximum # tries).  If all 3
methods fail, we throw the original exception we hit.

I added a static method SegmentInfos.setInfoStream() which will print
details of retry attempts.  In the patch it's set to System.out right
now (we should turn off before a real commit) so if there are problems
we can see what retry logic had done.
"
0,"CustomScoreQuery (function query) is broken (due to per-segment searching)Spinoff from here:

  http://lucene.markmail.org/message/psw2m3adzibaixbq

With the cutover to per-segment searching, CustomScoreQuery is not really usable anymore, because the per-doc custom scoring method (customScore) receives a per-segment docID, yet there is no way to figure out which segment you are currently searching.

I think to fix this we must also notify the subclass whenever a new segment is switched to.  I think if we copy Collector.setNextReader, that would be sufficient.  It would by default do nothing in CustomScoreQuery, but a subclass could override."
1,"BooleanQuery explain with boost==0BooleanWeight.explain() uses the returned score of subweights to determine if a clause matched.
If any required clause has boost==0, the returned score will be zero and the explain for the entire BooleanWeight will be simply  Explanation(0.0f, ""match required"").

I'm not sure what the correct fix is here.  I don't think it can be done based on score alone, since that isn't how scorers work.   Perhaps we need a new method ""boolean Explain.matched()"" that returns true on a match, regardless of what the score may be? 

Related to the problem above, even if no boosts are zero, it it sometimes nice to know *why* a particular query failed to match.  It would mean a longer explanation, but maybe we should include non matching explains too?"
1,"Brazilian Analyzer doesn't remove stopwords when uppercase is givenThe order of filters matter here, just need to apply lowercase token filter before removing stopwords

	result = new StopFilter( result, stoptable );
		result = new BrazilianStemFilter( result, excltable );
		// Convert to lowercase after stemming!
		result = new LowerCaseFilter( result );

Lowercase must come before BrazilianStemFilter

At the end of day I'll attach a patch, it's straightforward"
1,Benchmark deletes.alg failsBenchmark deletes.alg fails because the index reader defaults to open readonly.  
1,"Lucene fails to close file handles under certain situationsAs a followon to LUCENE-820, I've added a further check in
MockRAMDirectory to assert that there are no open files when the
directory is closed.

That check caused a few unit tests to fail, and in digging into the
reason I uncovered these cases where Lucene fails to close file
handles:

  * TermInfosReader.close() was setting its ThreadLocal enumerators to
    null without first closing the SegmentTermEnum in there.  It looks
    like this was part of the fix for LUCENE-436.  I just added the
    call to close.

    This is somewhat severe since we could leak many file handles for
    use cases that burn through threads and/or indexes.  Though,
    FSIndexInput does have a finalize() to close itself.

  * Flushing of deletes in IndexWriter opens SegmentReader to do the
    flushing, and it correctly calls close() to close the reader.  But
    if an exception is hit during commit and before actually closing,
    it will leave open those handles.  I fixed this first calling
    doCommit() and then doClose() in a finally.  The ""disk full"" tests
    we now have were hitting this.

  * IndexWriter's addIndexes(IndexReader[]) method was opening a
    reader but not closing it with a try/finally.  I just put a
    try/finally in.

I've also changed some unit tests to use MockRAMDirectory instead of
RAMDirectory to increase testing coverage of ""leaking open file
handles"".
"
0,"OCM:Add the ability to specify name of a Collection Element through XML Mapping files.Collection elements get mapped to a node ""collection-element"" when the mappings are specified through XML config files.  We need the ability to control this name through configuration.  Without that feature querying object structures is painful.  For example I have structure as below :

class Foo{
String id;
 List<Foo> children
 List<Foo> friends
}

And I have a need to query a Foo with id : 100 .  If I am interested only in child nodes with id = 110 , I could specify through the Filter that look at only node names , ""childFoo"" ; If I have the flexibility of adding a child node name."
1,"Unreferenced sessions should get garbage collectedIf an application opens many sessions and doesn't close them, they are never garbage collected. After some time, the virtual machine will run out of memory. This code will run out of memory after a few thousand logins:

Repository rep = new TransientRepository();
for (int i = 0; ; i++) {
  rep.login(new SimpleCredentials("""", new char[0]));
}

Using a finalizer to close SessionImpl doesn't work, because it seems there are references from the (hard referenced part of the cache) to the SessionImpl objects. Maybe it is possible to remove those references, or change them to weak references.
"
1,"IndexReader.termDocs() retrieves no documentsTermDocs object returned by indexReader.termDocs() retrieves no documents, howerver, the documents are retrieved correctly when using indexReader.termDocs(Term), indexReader.termDocs(null) and indexSearcher.search(Query)."
0,Change contrib tests to use the special LuceneTestCase(J4) constant for the current version used a matchVersion parameterSub issue for contrib changes
0,New MsOutlook Message ExtractorSinse we are using poi 3.0.2 it will be useful to have a outlook message extractor
1,CompactNodeTypeDefWriter does not escaped names properlyCompactNodeTypeDefWriter does not escaped names properly. If the name includes a '-' or a '+' the names must be surrounded by single quotes.
0,HttpClient 'ParamBeans' for easier configurationAs I did for a 'core' here I would like to contribute for 'client' part as few 'ParamBeans' for easier external configuration... Any comment or improvement is very welcome...
0,"user need a way to control cookie policyUser need a way to control what cookie can be accepted and what should be 
rejected. It would be nice to provide a cookie filter interface, so the user 
can change the cookie policy by implementing his own filter."
1,"CartesianPolyFilterBuilder doesn't handle edge case around the 180 meridianTest case:  
Points all around the globe, plus two points at 0, 179.9 and 0,-179.9 (on each side of the meridian).  Then, do a Cartesian Tier filter on a point right near those two.  It will return all the points when it should just return those two.

The flawed logic is in the else clause below:
{code}
if (longX2 != 0.0) {
		//We are around the prime meridian
		if (longX == 0.0) {
			longX = longX2;
			longY = 0.0;
        	shape = getShapeLoop(shape,ctp,latX,longX,latY,longY);
		} else {//we are around the 180th longitude
			longX = longX2;
			longY = -180.0;
			shape = getShapeLoop(shape,ctp,latY,longY,latX,longX);
	}
{code}

Basically, the Y and X values are transposed.  This currently says go from longY (-180) all the way around  to longX which is the lower left longitude of the box formed.  Instead, it should go from the lower left long to -180."
1,"Escaped wildcard character in wildcard term not handled correctlyIf an escaped wildcard character is specified in a wildcard query, it is treated as a wildcard instead of a literal.
e.g., t\??t is converted by the QueryParser to t??t - the escape character is discarded."
0,"Support multiple proxiesHttpClient supports one proxy currently.
Our requirement is to suppport more than one proxy. We may need to connect more than one proxies before connects to target resource. 
I found that HttpMethodDirector creates tunnelled socket and there is no easy way to plugin our custom HttpMethodDirector class with HttpClient other than extending HttpClient to override ""public int executeMethod(HostConfiguration hostconfig, final HttpMethod method, final HttpState state"" method.


"
0,Remove deprecated DocIdSetIterator methods
0,"Introduce similarity functionThe query handler should support a similarity function that allows one to find nodes that are similar to a given existing one.

Example:

//*[rep:similar(""/foo/bar"")]

Finds nodes that are similar to node /foo/bar."
0,"SimpleWebdavServlet: avoid 404 for the root collectionin order to avoid strange 404 error when accessing the root collection in simple webdav servlet (thus missing workspace name), that request should either be redirected or handled by a create fake root that has no correspondance in the jsr170 repository."
0,Avoid using BitSets in ChildAxisQuery to minimize memory usageWhen doing ChildAxisQueries on large indexes the internal BitSet instance (hits) may consume a lot of memory because the BitSet is always as large as IndexReader.maxDoc(). In our case we had a query consisting of 7 ChildAxisQueries which combined to a total of 14MB. Since we have multiple users executing this query simultaneously this caused an out of memory error.
0,"Not getting random-seed/reproduce-with if a test fails from another threadSee https://builds.apache.org/job/Lucene-Solr-tests-only-trunk/12822/console as an example.

This is at least affecting 4.0, maybe 3.x too"
0,"move intblock/sep codecs into testThe intblock and sep codecs in core exist to make it easy for people to try different low-level algos for encoding ints.

Sep breaks docs, freqs, pos, skip data, payloads into 5 separate files (vs 2 files that standard codec uses).

Intblock further enables the docs, freqs, pos files to encode fixed-sized blocks of ints at a time.

So an app can easily ""subclass"" these codecs, using their own int encoder.

But these codecs are now concrete, and they use dummy low-level block int encoder (eg encoding 128 ints as separate vints).

I'd like to change these to be abstract, and move these dummy codecs into test.

The tests would still test these dummy codecs, by rotating them in randomly for all tests.

I'd also like to rename IntBlock -> FixedIntBlock, because I'm trying to get a VariableIntBlock working well (for int encoders like Simple9, Simple16, whose block size varies depending on the particular values).
"
0,"More Fine grained Permission FlagsIt would be fine to have one more Permission Flag on node add.
At the moment there are 3 flags. We need to know if a node will be updated or created.
This is not possible with the current implementation because on node add the permission flag 
AccessManager.WRITE will be used. This is a Problem in a  WebDav Scenario with Microsoft-Word because if i open a Node and 
try to save it i need write permissions on the parent node. this is ok. If a user trys to save the file with a other name
he can because the same PermissionFlag will be used.
Maybe there is a other solution for this problem ?
BR,
claus"
0,"Change Log-Level in DefaultIOListenerPlease change loglevel for method onEnd(IOHandler handler, IOContext ioContext, boolean success) to debug"
1,Index recovery may fail when redo log contains nodes that are part of an index aggregateSearchIndex.mergeAggregatedNodeIndexes() will throw a NullPointerException because index is not yet set. The call is made from the recovery code that is triggered in the MultiIndex constructor.
1,"LockTest.testLogout fails to refresh session before checking lock from other sessionLockTest.testLogout() fails to refresh the session before checking the lock state of a node that was locked by another session.

Proposal:

Insert 

  n1.refresh(false);

before 

  assertTrue(""node must be locked"", n1.isLocked());

"
0,Remove System.out left in SpanHighlighter codeA System.out debug was left in the code when a Query is not supported by the SpanHighlighter. This issue simply removes it.
1,"JCAResourceAdapter must implement SerializableWe are running Weblogic 10.0 servers in cluster environment.   When deploying the rar, we always get this warning from weblogic stdout.log: 

<Jan 15, 2009 2:42:10 AM PST> <Warning> <Connector> <BEA-190155> <Compliance checking/validation of the resource adapter /home/user/jackrabbit_rar/jackrabbit-jca-1.5.0.rar resulted in the following warnings:  The ra.xml <resourceadapter-class> class 'org.apache.jackrabbit.jca.JCAResourceAdapter' should implement java.io.Serializable but does not.> 

When trying to do the JNDI lookup the repository, we got the error ""No Object found: jackrabbit|null"".   The jackrabbit entry in the jndi tree is visible only as a javax.naming.reference and not as the JCARepositoryHandle due to the above warning.  Due to that, we can't deploy jackrabbit-jca in Test/Production environment.  

I'm no expert in JCA, but feel it is fairly easy to implement Serializable for  JCAResourceAdapter.  Please help us out.
"
0,"spi2davex: clear uri-lookup after removing node identified with uniqueIDsome test cases of DocumentViewImportTest fail in the setup (line 325 of AbstractImportXmlTest) since the uri resolved from the specified
nodeID still refers to the node removed during the initial import before. this would equally cause problems whenever a referenceable node was
replaced by another node and can easily be fixed by clearing the uri-cache after the removal as it was already done for move."
1,"Inflater.end() method not always called in FieldsReader
We've just found an insidious memory leak in our own application as we did not always call Deflater.end() and Inflater.end(). As documented here;

http://bugs.sun.com/view_bug.do?bug_id=4797189

The non-heap memory that the native zlib code uses is not freed in a timely manner.

FieldsWriter appears safe as no exception can be thrown between the Deflater's creation and end() as it uses a ByteArrayOutputStream

FieldsReader, however, is not safe. In the event of a DataFormatException the call to end() will not occur."
1,"Clustering: re-registration of nodetypes is not  synchronizedThe re-registration of nodetypes is not yet synchronized between clusternodes, although re-registration is already (partially) implemented in the NodeTypeRegistry."
1,"MS Excel Mime Type missing in MsExcelTextExtractor The MsExcelTextExtractor listens to mime type ""application/vnd.ms-excel"", but storing excels will result in mime type ""application/msexcel"", too. Such tagged files will not be indexed by the MsExcelTextExtractor. The class should register itself to both mime types like the MsWordTextExtractor does. "
0,"reopen support for SegmentReaderReopen for SegmentReader can be supported simply as the following:

  @Override
  public synchronized IndexReader reopen() throws CorruptIndexException,
		IOException {
	return reopenSegment(this.si,false,readOnly);
  }

  @Override
  public synchronized IndexReader reopen(boolean openReadOnly)
		throws CorruptIndexException, IOException {
	return reopenSegment(this.si,false,openReadOnly);
  }
"
1,"bogus positions create a corrumpt indexIts pretty common that positionIncrement can overflow, this happens really easily 
if people write analyzers that don't clearAttributes().

It used to be the case that if this happened (and perhaps still is in 3.x, i didnt check),
that IW would throw an exception.

But i couldnt find the code checking this, I wrote a test and it makes a corrumpt index..."
0,"loadURI compile error with Maven 1.0.2As reported on the mailing list by Ashley Martens:

----
C:\apache\jackrabbit-contrib\nt-ns-util>maven
 __  __
|  \/  |__ _Apache__ ___
| |\/| / _` \ V / -_) ' \  ~ intelligent projects ~
|_|  |_\__,_|\_/\___|_||_|  v. 1.0.2

Attempting to download jackrabbit-1.0-SNAPSHOT.jar.
Artifact /org.apache.jackrabbit/jars/jackrabbit-1.0-SNAPSHOT.jar doesn't exists in remote repository, but it exists locally
Attempting to download jackrabbit-commons-1.0-SNAPSHOT.jar.
Artifact /org.apache.jackrabbit/jars/jackrabbit-commons-1.0-SNAPSHOT.jar doesn't exists in remote repository, but it exists locally
build:start:

java:prepare-filesystem:

java:compile:
   [echo] Compiling to C:\apache\jackrabbit-contrib\nt-ns-util/target/classes
   [javac] Compiling 1 source file to C:\apache\jackrabbit-contrib\nt-ns-util\target\classes
C:\apache\jackrabbit-contrib\nt-ns-util\src\main\java\org\apache\jackrabbit\util\nodetype\SchemaConverter.java:71: cannot resolve symbol
symbol  : method loadURI (java.lang.String)
location: class org.apache.xerces.impl.xs.XMLSchemaLoader
       XSModel xsModel = loader.loadURI(uri);
                               ^
1 error

BUILD FAILED
File...... C:\Documents and Settings\ashleym\.maven\cache\maven-java-plugin-1.5\plugin.jelly
Element... ant:javac
Line...... 63
Column.... 48
Compile failed; see the compiler error output for details.
Total time: 8 seconds
Finished at: Mon Jan 02 10:40:47 EST 2006
----

Peeter Piegaze found out the problem:

----
I was able to build it without a problem using maven-1.1-beta-2 and JDK 1.4.2.

However, it sounds to me like in your case maven has set up its
on-build classpath so that it sees the older xerces-2.4.0.jar before
the new xerxesImpl.-2.6.2.jar. Maven seems to download the old
xerces-2.4.0 into its repository for internal use, while my code uses
the newer xerxesImpl-2.6.2.jar. The old jar overlaps class-wise with
the new one, but the new one implements the additional loadURI method
(among others).

I am not sure exactly why your maven build process is looking in the
wrong jar. But that is what is doing, almost certainly.
----
"
0,"Add a MBean method to programatically create a new Workspace.Would be useful to have a mbean method to create a new workspace to use if with a jmx console.

"
1,"RFC4918IfHeaderTest.testPutIfLockToken could fail with 412 Precondition FailedIn org.apache.jackrabbit.webdav.server.RFC4918IfHeaderTest:110 (webdav-test), 
the lock request is initialized with a timeout of 1800 milliseconds, which is rounded as Timeout: Second-1 (at org.apache.jackrabbit.webdav.header.TimeoutHeader:46).

The assertion in the finally block MUST fail (412, Precondition Failed) if the lock has expired (cf. RFC 4918, Section 10.4.10).

The lock request should be initialized with a higher timeout, at least several seconds."
0,"New Gump projects for HttpComponents 4.0Create new Gump definitions for the 4.0 code base, both core and client.
There are other Maven-based projects in Gump to learn from, for example Apollo and Excalibur.

"
1,"More Locale problems in LuceneThis is a followup to LUCENE-1836: I found some more Locale problems in Lucene with Date Formats. Even for simple date formats only consisting of numbers (like ISO dates), you should always give the US locale. Because the dates in DateTools should sort according to String.compare(), it is important, that the decimal digits are western ones. In some strange locales, this may be different. Whenever you want to format dates for internal formats you exspect to behave somehow, you should at least set the locale to US, which uses ASCII. Dates entered by users and displayed to users, should be formatted according to the default or a custom specified locale.
I also looked for DecimalFormat (especially used for padding numbers), but found no problems."
0,"FormBodyPart code does not agree with ContentDescriptor Javadoc wrt nullability of mimeType and transferEncodingThe FormBodyPart does not agree with ContentDescriptor Javadoc wrt nullability of mimeType and transferEncoding:

The code in FormBodyPart explicitly allows mimeType and transferEncoding to be null, in which case the relevant header is not generated.
This is useful behaviour, as the headers are not necessaruly needed.

However the bahaviour disagrees with the Javadoc in the ContentDescriptor interface - null is not allowed.
Also, AbstractContentBody does not allow mime-type to be null."
0,"Enable setting the terms index divisor used by IndexWriter whenever it opens internal readersOpening a place holder issue... if all the refactoring being discussed don't make this possible, then we should add a setting to IWC to do so.

Apps with very large numbers of unique terms must set the terms index divisor to control RAM usage.

(NOTE: flex's RAM terms dict index RAM usage is more efficient, so this will help such apps).

But, when IW resolves deletes internally it always uses default 1 terms index divisor, and the app cannot change that.  Though one workaround is to call getReader(termInfosIndexDivisor) which will pool the reader with the right divisor."
0,"Get javadoc for the similarities package in shape1. Create a package.html in the similarities package.
2. Update the javadoc of the search package (package.html mentions Similarity)?
3. Compile the javadoc to see if there are any warnings."
0,"httpclient build requires jdk 1.4 or jce in classpathCurrently when a 'ant dist'
is performed httpclient is looking for javax.crypt.* which is in jce.jar

The build.xml and build.properties.sample need to be patched
so they allow the jce.jar file to be specified
just like the jsse.jar is specified.

will attach two patch files made from todays cvs"
0,"Review pck names in the others ocm subprojectsReview package structure and graffito references in the other OCM subprojects : jcr-nodemanagement & spring. 
"
0,"improve termquery ""pk lookup"" performanceFor things that are like primary keys and don't exist in some segments (worst case is primary/unique key that only exists in 1)
we do wasted seeks.

While LUCENE-2694 tries to solve some of this issue with TermState, I'm concerned we could every backport that to 3.1 for example.

This is a simpler solution here just to solve this one problem in termquery... we could just revert it in trunk when we resolve LUCENE-2694,
but I don't think we should leave things as they are in 3.x
"
0,"Update overview example codeSee http://lucene.apache.org/java/2_4_1/api/core/overview-summary.html - need to update for non-deprecated best-practices/recommended API usage.

Also, double-check that the demo app works as documented."
1,BoostingNearQuery doesn't have hashCode/equals
1,"preflex codec returns wrong terms if you use an empty field namespinoff from LUCENE-3473.

I have a standalone test for this... the termsenum is returning a bogus extra empty-term (I assume it has no postings, i didnt try).

This causes the checkindex test in LUCENE-3473 to fail, because there are 4 terms instead of 3. 

"
0,"smartcn HHMM doc translationMy coworker Patricia Peng translated the documentation and code comments for smartcn HHMM package.
"
1,"Oracle JDBC Class Cast ExceptionWhen utilizing the OraclePersistenceManager (package org.apache.jackrabbit.core.persistence.db) (I realize this is marked as deprecated) we noticed during our migration from Jackrabbit 1.6.1 to 2.2.10/11 that when starting the application server an error message is displayed to us that indicates that the Connection object passed to the createTemporaryBlob method of the BLOB class can't be cast to oracle.jdbc.OracleConnection

Here the interesting lines from our log:
2012-03-15 17:15:47,926 ERROR [org.apache.jackrabbit.core.persistence.db.OraclePersistenceManager] failed to write node state: cafebabe-cafe-babe-cafe-babecafebabe
java.lang.ClassCastException: org.apache.commons.dbcp.PoolingDataSource$PoolGuardConnectionWrapper cannot be cast to oracle.jdbc.OracleConnection
	at oracle.sql.BLOB.createTemporary(BLOB.java:708)
	at org.apache.jackrabbit.core.persistence.db.OraclePersistenceManager.createTemporaryBlob(OraclePersistenceManager.java:375)

I want to highlight at this point that the do not see the issue when using the Oracle Bundled persistence manager, however due to the fact that we haven't used the bundled version in the past we have a lot of customers with repo layouts that can not be used by the bundled persistence manager - we ran some tests and noticed that the consistency check fails.
-> At the moment there is no good upgrade path to move a repo to the bundled structure, the paths provided thus far are shaky at best.

I did find a solution to the problem that has shown no issues thus far and wanted to share this with you:

It is a one line change that can be made before the wrapped connection is passed to the Oracle driver:
org.apache.jackrabbit.core.util.db.ConnectionFactory.unwrap(con);

This then solves the problem, I also wanted to share that we are using an XA datasource."
0,jcr:like on node nameUntil now it is only possible to do an exact match on the node name. It would be useful to also use jcr:like on the node name.
1,"Paths not correct after reordering childrenReordered, unsaved children of a node do not have the correct path. In the test case attached, the following operation is attempted with three SNS children named b[1], b[2], b[3]: the last element is ordered before the first three times, which should result in the initial children order.
"
0,HttpState should have methods for clearing all cookies and credentials 
1,"The repeats mechanism in SloppyPhraseScorer is broken when doc has tokens at same positionIn LUCENE-736 we made fixes to SloppyPhraseScorer, because it was
matching docs that it shouldn't; but I think those changes caused it
to fail to match docs that it should, specifically when the doc itself
has tokens at the same position.
"
0,"allow ResourceType dav property to have multiple valuesattached is a patch that allows the ResourceType dav property to have multiple values (useful for dav protocol extensions such as caldav). 

it is not a perfect patch, in that subclasses of ResourceType do not know about each others' resource types, but it is a decent start. one way to address this issue might be to have subclasses register extended resource types and their corresponding xml representations with ResourceType, removing the need for them to override resourceTypeToXml() and isValidResourceType().
"
0,"Default merge policy should take deletions into accountLUCENE-1634 added a calibrateSizeByDeletes; we had a TODO to default this to true as of 3.0 but we missed it.  I'll fix it now for 3.1 and 4.0.  While this is technically a change in back-compat (for 3.x), I think it's fine to make an exception here; this should be a big win for indices that have high doc turnover with time."
0,"Failures during contrib builds, when classes in core were changed without ant cleanFrom java-dev by Shai Erera:

{quote}
I've noticed that sometimes, after I run test-core and test-contrib, and then change core code, test-contrib fail on NoSuchMethodError and stuff like that. I've noticed that core.jar exists under build, and I assumed it's used by test-contrib, and probably is not recreated after core code has changed.

I verified it when looking in contrib-build.xml, which defines a property lucene.jar.present which is set to true if the jar is ... well, present. Which I believe is the reason for these failures. I've been thinking how to resolve that, and I can think of two ways:

(1) have test-core always delete that file, but that has two issues:
(1.1) It's redundant if the code hasn't changed.
(1.2) It forces you to either jar-core or test-core before you test-contrib, if you want to make sure you run w/ the latest jar.

or

(2) have test-contrib always call jar-core, which will first delete the file and then re-create it by compiling first. Compiling should not do anything if the code hasn't changed. So the only waste would be to create the .jar, but I think that's quite fast?

Does anyone, with more Ant skills than me, know of a better way to detect from test-contrib that core code has changed and only then rebuild the jar?
{quote}"
0,Only load root node definition when requiredThe root node definition is currently loaded whenever a session logs in.
0,"Change project names to start with jackrabbitAll the released projects should have artifactId's starting with ""jackrabbit""."
0,"Deploy JCA JAR file to maven repositoryPlease deploy the JCA JAR file to the maven repository (ibiblio) whenever deploying the RAR artifact.

The JAR is need for non managed usage of the Jackrabbit JCA, eg. for embedding the resource adapter in your application with Spring JCA in order to use XA for Jackrabbit.

It would be nice if this could be done starting at the current 1.3 version (and for future versions, too).

Thanks!"
1,"in trunk if you switch up omitNorms while indexing, you get a corrumpt norms filedocument 1 has 
  body: norms=true
  title: norms=true
document 2 has 
  body: norms=false
  title: norms=true

when seeing 'body' for the first time, normswriterperfield gets 'initial fieldinfo' and 
saves it away, which says norms=true

however, at flush time we dont check, so we write the norms happily anyway.
then SegmentReader reads the norms later: it skips ""body"" since it omits norms
and if you ask for the norms of 'title' it instead returns the bogus ""body"" norms.

asserting that SegmentReader ""plans to"" read the whole .nrm file exposes the bug."
1,"DefaultMethodRetryHandler bugDefaultMethodRetryHandler does not seem to test correctly for the number of
attempts to retry a given method. It seems to bail out one attempt too early:

if (executionCount >= this.retryCount) {
  // Do not retry if over max retry count
  return false;
}

For example, if I set the retryCount to 1, HttpClient does not retry the method
at all. At least that's what I'm seeing when I step through it with a debugger."
0,Reorganize Jackrabbit into 'core' 'api' and 'commons'
1,"ManageableCollectionUtil doesn't support MapsManageableCollectionUtil has two getManageableCollection methods, which do not currently return a ManageableCollection which wraps Maps. 

ManagedHashMap already exists in the codebase which I assume was created for this purpose, so both getManageableCollection methods could be modified so that they do something like:

            if (object instanceof Map){
                return new ManagedHashMap((Map)object);
            }


An alternative solution might be to modify the JCR mapping to support explicitly defining the 'ManagedXXX' class."
0,"Errors in character entities in Javadoc for HttpVersionThere are some errors in the Javadoc for the HttpVersion class. This is the
class comment:

 *  <p>HTTP version, as specified in RFC 2616.</p>
 *  <p>
 *  HTTP uses a ""&ltmajor&gt.&ltminor&gt"" numbering scheme to indicate versions
 *  of the protocol. The protocol versioning policy is intended to allow
 *  the sender to indicate the format of a message and its capacity for
 *  understanding further HTTP communication, rather than the features
 *  obtained via that communication. No change is made to the version
 *  number for the addition of message components which do not affect
 *  communication behavior or which only add to extensible field values.
 *  The &ltminor&gt number is incremented when the changes made to the
 *  protocol add features which do not change the general message parsing
 *  algorithm, but which may add to the message semantics and imply
 *  additional capabilities of the sender. The &ltmajor&gt number is
 *  incremented when the format of a message within the protocol is
 *  changed. See RFC 2145 [36] for a fuller explanation.
 *  </p>
 *  <p>
 *  The version of an HTTP message is indicated by an HTTP-Version field
 *  in the first line of the message.
 *  </p>
 *  <pre>
 *     HTTP-Version   = ""HTTP"" ""/"" 1*DIGIT ""."" 1*DIGIT
 *  </pre>
 *  <p>
 *   Note that the major and minor numbers MUST be treated as separate
 *   integers and that each MAY be incremented higher than a single digit.
 *   Thus, HTTP/2.4 is a lower version than HTTP/2.13, which in turn is
 *   lower than HTTP/12.3. Leading zeros MUST be ignored by recipients and
 *   MUST NOT be sent.
 *  </p>

Note that the character entities for less-than and greater-than are not properly
ended with a semi-colon.

I will attach a proposed fix."
0,"Default lock timeouts should have static setter/getters
We recently stopped using Java system properties to derive defaults for things like the write/commit lock timeout, and switched to getter/setter's across all classes.  See here:

    http://www.gossamer-threads.com/lists/lucene/java-dev/27447

But, in the case at least of the write lock timeout, because it's marked ""public final static"", a consumer of this API can no longer change this value before instantiating the IndexWriter.  This is because the getter/setter for this is not static, which generally makes sense so you can change the timeout for each instance of IndexWriter.  But because IndexWriter on construction uses the timeout value, some uses cases need to change the value before getting an instance of IndexWriter.

This was actually a regression, in that Lucene users lost functionality they previously had, on upgrading.

I would propose that that we add getter/setter for the default value of this timeout, which would be static.  I'll attach a patch file.

See this thread for context that led to this issue:

   http://www.gossamer-threads.com/lists/lucene/java-dev/37421"
0,"Windows specific implementation of the Digest auth schemeMicrosoft Windows 2003 implementation of digest auth scheme is essentially a
superset of RFC 2617 with Windows specific aspects:
http://www.microsoft.com/technet/prodtechnol/windowsserver2003/library/TechRef/717b450c-f4a0-4cc9-86f4-cc0633aae5f9.mspx

Provide a super class of DigestScheme with Windows 2003 specific extensions,
which can be plugged in instead of the standard Digest impl

For details see PR #34909"
1,"LuceneDictionary skips first word in enumerationThe current code for LuceneDictionary will always skip the first word of the TermEnum. The reason is that it doesn't initially retrieve TermEnum.term - its first call is to TermEnum.next, which moves it past the first term (line 76).
To see this problem cause a failure, add this test to TestSpellChecker:
similar = spellChecker.suggestSimilar(""eihgt"",2);
      assertEquals(1, similar.length);
      assertEquals(similar[0], ""eight"");

Because ""eight"" is the first word in the index, it will fail.
"
1,NotQuery does not implement extractTerms()If the not() function is used in query together with the rep:excerpt() function an UnsupportedOperationException is thrown.
0,"Mark Fieldable as allowing some changes in 2.x future releasesSee http://lucene.markmail.org/message/4k2gqs3n7coh4lmd?q=Fieldable

1. We mark Fieldable as being subject to change. We heavily advertise (on java-dev and java-user and maybe general) that in the next minor release of Lucene (2.4), Fieldable will be changing. It is also marked at the top of CHANGES.txt very clearly for all the world to see. Since 2.4 is probably at least a month away, I think this gives anyone with a pulse enough time to react. "
1,"jcr-server: DefaultItemCollection#unlock does not call DavSession#removeReferenceDefaultItemCollection#unlock does not remove the token-reference from the DavSession that has been added
before upon creating the lock. This causes pending lock-references thus the cache entries in JCRWebdavServer
will not be cleared filling up the cache although the locks have been properly released. 
"
1,"spi2davex: Batch fails to create/modify properties with non-ascii characters namesthe spi2davex batch implementation fails upon creation/modification of all property types that have their value sent as
separate stringpart or binarypart AND contain non-ascii characters in their property name.

from what i've seen this is due to a limitation in HttpClient 3.x Part#sendDispositionHeader that always writes the part name
as ascii-bytes. in a related discussion [1] specification compliance and usability were addressed.

looking at the server-side part revealed that org.apache.commons.fileupload.FileUploadBase#FileItemIteratorImpl
is prepared to receive non-ascii characters in a header value.
a simple test also showed that curl is perfectly able to send utf-8 part names.

based on this information and given the fact that spi2dav and the server-sided part are intended to communicate
with one other rather than with any kind of custom clients, i suggest to add a simple fix by patching the parts used
within spi2davex.

btw: in HttpClient 4.x there seems to be a workaround for this problem [2]

[1] http://www.mail-archive.com/httpclient-dev@jakarta.apache.org/msg04637.html
[2] https://issues.apache.org/jira/browse/HTTPCLIENT-293"
0,"Clean up old JIRA issues in component ""Other""A list of all JIRA issues in component ""Other"" that haven't been updated in 2007:

   *	 LUCENE-746  	 Incorrect error message in AnalyzingQueryParser.getPrefixQuery   
   *	LUCENE-644 	Contrib: another highlighter approach 
   *	LUCENE-574 	support for vjc java compiler, also known as J# 
   *	LUCENE-471 	gcj ant target doesn't work on windows 
   *	LUCENE-434 	Lucene database bindings 
   *	LUCENE-254 	[PATCH] pseudo-relevance feedback enhancement 
   *	LUCENE-180 	[PATCH] Language guesser contribution 
"
0,"Improve performance of CharTermAttribute(Impl) and also fully implement AppendableThe Appendable.append(CharSequence) method in CharTermAttributes is good for general use. But like StringBuilder has for some common use cases specialized methods, this does the same and adds separate append methods for String, StringBuilder and CharTermAttribute itsself. This methods enable the compiler to directly link the specialized methods and don't use the instanceof checks. The unspecialized method only does the instanceof checks for longer CharSequences (>8 chars), else it simply iterates.

This patch also fixes the required special ""null"" handling. append() methods are required by Appendable to append ""null"", if the argument is null. I dont like this, but its required. Maybe we should document, that we dont dont support it. Otherwise, JDK's formatter fails with formatting null."
0,"Restore top level disjunction performanceThis patch restores the performance of top level disjunctions. 
The introduction of BooleanScorer2 had impacted this as reported
on java-user on 21 Nov 2006 by Stanislav Jordanov.
"
1,"FileInputStream never closed in HTMLParserHTMLParser.java contains this code: 
 
  public HTMLParser(File file) throws FileNotFoundException { 
    this(new FileInputStream(file)); 
  } 
 
This FileInputStream should be closed with the close() method, as there's no 
guarantee that the garbage collection will run and do this for you. I don't 
know how to fix this without changing the API to take a FileInputStream 
instead of a File, as the call to this() must be the first thing in the 
constructor, i.e. you cannot create the stream, call this(...), and then close 
the stream."
0,Fix Hits deprecation noticeJust needs to be committed to 2.9 branch since hits is now removed.
0,"Hindi AnalyzerAn analyzer for hindi.

below are MAP values on the FIRE 2008 test collection.
QE means expansion with morelikethis, all defaults, on top 5 docs.

||setup||T||T(QE)||TD||TD(QE)||TDN||TDN(QE)||
|words only|0.1646|0.1979|0.2241|0.2513|0.2468|0.2735|
|HindiAnalyzer|0.2875|0.3071|0.3387|*0.3791**|0.3837|0.3810|
|improvement|74.67%|55.18%|51.14%|50.86%|55.47%|39.31%|

* TD was the official measurement, highest score for this collection in FIRE 2008 was 0.3487: http://www.isical.ac.in/~fire/paper/mcnamee-jhu-fire2008.pdf

needs a bit of cleanup and more tests"
0,"Change access to internal maps of HttpState to protected.To be able to serialize the conversational state of a http session access to the internal maps of HttpState is required. Currently they are all ""private"", so subclasses cannot access them. Changing the access to ""protected"" will allow any subclass to access those maps."
0,"Use a separate JFlex generated Unicode 4 by Java 5 compatible StandardTokenizerThe current trunk version of StandardTokenizerImpl was generated by Java 1.4 (according to the warning). In Java 3.0 we switch to Java 1.5, so we should regenerate the file.

After regeneration the Tokenizer behaves different for some characters. Because of that we should only use the new TokenizerImpl when Version.LUCENE_30 or LUCENE_31 is used as matchVersion."
0,"Add files generated by eclipse or maven to svn:ignoreTo make life easier for eclipse and maven users please add the following files to svn:ingore:

jackrabbit-jcr-rmi:
-------------------
.settings
.classpath
.project
jackrabbit-jcr-rmi-pom-snapshot-version
project.xml.md5

jackrabbit-core:
----------------
.settings
.classpath
.project
jackrabbit-core-pom-snapshot-version
project.xml.md5

Maybe there some files missing in this list that could help developers using IDEA?"
1,"WorkspaceManager.dispose() should wait until change feed thread is stoppedThe WorkspaceManager currently only interrupts the change feed thread, but does not wait until it stops."
0,"Data store garbage collectionCurrently the data store garbage collection needs to be run manually. It should be simpler to use (maybe tool based), or automatic."
0,"need tests to guarantee transparency of caching module on end-to-end headers""A transparent proxy SHOULD NOT modify an end-to-end header unless the definition of that header requires or specifically allows that.""

http://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.5.2

This is already true of our implementation, but we should have tests to preserve that behavior.
"
1,"SSL does not seem to work at allWhenever I try to request content via https I get this exception:


Exception in thread ""main"" javax.net.ssl.SSLException: hostname in certificate didn't match: <140.211.11.131> != <*.apache.org>
	at org.apache.http.conn.ssl.AbstractVerifier.verify(AbstractVerifier.java:220)
	at org.apache.http.conn.ssl.BrowserCompatHostnameVerifier.verify(BrowserCompatHostnameVerifier.java:54)
	at org.apache.http.conn.ssl.AbstractVerifier.verify(AbstractVerifier.java:149)
	at org.apache.http.conn.ssl.AbstractVerifier.verify(AbstractVerifier.java:130)
	at org.apache.http.conn.ssl.SSLSocketFactory.createSocket(SSLSocketFactory.java:399)
	at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:143)
	at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:149)
	at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:108)
	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:415)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:641)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:576)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:554)
	at HttpsTest.fails(HttpsTest.java:25)
	at HttpsTest.main(HttpsTest.java:12)


I can reproduce this whith the following code:


import org.apache.http.client.HttpClient;
import org.apache.http.client.methods.HttpGet;
import org.apache.http.impl.client.DefaultHttpClient;

public class HttpsTest {

    public static void main(final String[] args) throws Exception {
        final HttpClient client = new DefaultHttpClient();
        final HttpGet req = new HttpGet(""https://www.apache.org"");
        client.execute(req);
    }

}
"
0,"StringBody has incorrect default for charactersetStringBody defaults to Charset.defaultCharset() if the charset is not provided.

This means that the default depends on the current host.

The default should be US-ASCII (as was the case with StringPart in Commons HttpClient 3.1)."
0,DescendantSelfAxisQuery creates too many object instancesIn DescendantSelfAxisQuery.DescendantSelfAxisScorer.isValid() there is an ArrayList and an Integer instance created on every call. Since this method gets called really often during queries the object creation/gc affects performance.
0,"Possible Memory Leak in StoredFieldsWriterStoredFieldsWriter creates a pool of PerDoc instances

this pool will grow but never be reclaimed by any mechanism

furthermore, each PerDoc instance contains a RAMFile.
this RAMFile will also never be truncated (and will only ever grow) (as far as i can tell)

When feeding documents with large number of stored fields (or one large dominating stored field) this can result in memory being consumed in the RAMFile but never reclaimed. Eventually, each pooled PerDoc could grow very large, even if large documents are rare.

Seems like there should be some attempt to reclaim memory from the PerDoc[] instance pool (or otherwise limit the size of RAMFiles that are cached) etc
"
0,"refactor spatial contrib ""Filter"" ""Query"" classesFrom erik's comments in LUCENE-1387

    * DistanceQuery is awkwardly named. It's not an (extends) Query.... it's a POJO with helpers. Maybe DistanceQueryFactory? (but it creates a Filter also)

    * CartesianPolyFilter is not a Filter (but CartesianShapeFilter is)
"
0,Use jackrabbit-jcr-commons in jackrabbit-jcr-rmiThe jackrabbit-jcr-rmi component should leverage the general-purpose classes in jackrabbit-jcr-commons even at the expense of introducing an extra dependency.
0,"Contrib/Module-uptodate assume name matches path and jarWith adding a new 'queries' module, I am trying to change the project name of contrib/queries to queries-contrib.  However currently the contrib-uptodate assumes that the name property is used in the path and in the jar name.

By using the name in the path, I must set the value to 'queries' (since the path is contrib/queries).  However because the project name is now queries-contrib, the actual jar file will be lucene-queries-contrib-${version}.jar, not lucene-queries-${version}.jar, as is expected.

Consequently I think we need to separate the path name from the jar name properties.  For simplicity I think adding a new jar-name property will suffice, which can be optional and if omitted, is filled in with the name property."
0,"contrib/Highlighter javadoc example needs to be updatedThe Javadoc package.html example code is outdated, as it still uses QueryParser.parse.  

http://lucene.zones.apache.org:8080/hudson/job/Lucene-Nightly/javadoc/contrib-highlighter/index.html"
0,"Modify ParallelMultiSearcher to use a CompletionService instead of slowly polling for resultsRight now, the parallel multi searcher creates an array/list of Future<V> representing each of the searchables that's being concurrently searched (and its corresponding search task).

As it stands, once the tasks are all submitted to the executor, the array is iterated over, FIFO, and Future.get() is called iteratively.  This obviously works, but isn't ideal.  It's entirely possible (a situation I've run into) where one of the first searchables represents a large index that takes a long time to search, so the results of the other searchables can't be processed until the large index is done searching.  In my case, we have two indexes with several million records that get searched in front of some other indexes, the smallest of which has only a few ten thousand entries and I didn't think it was ideal for the results of the other indexes to wait.

I've modified ParallelMultiSearcher to use CompletionServices instead, so that results are processed in the order they are completed, rather than the order that they are submitted.  All the tests still pass, and to the best of my knowledge this won't break anything.  This have several advantages:
1) Speed - the thread owning the executor doesn't have to wait for the first submitted task to finish in order to process the results of the other tasks, which may have finished first
2) Removed several warnings (even if they are annotated away) due to the ugliness of typecasting generic arrays.
3) Decreased the complexity of the code in some cases, usually by removing the necessity of allocating and filling arrays.

With a primed ""cache"" of searchables, I was getting 700-1200 ms per search, and using the same phrases, with this patch, I am now getting 400-500ms per search :)

Patch is attached."
0,"package org.apache.xml.utils does not exist (JDK 1.5.0)Executing ""maven jar"" on a freshly checked out source tree fails with the following error messages when using JDK 1.5.0:

/home/jukkaz/src/jackrabbit/src/java/org/apache/jackrabbit/core/xml/DocViewSAXEventGenerator.java:24: package org.apache.xml.utils does not exist
import org.apache.xml.utils.XMLChar;
                            ^
/home/jukkaz/src/jackrabbit/src/java/org/apache/jackrabbit/core/xml/DocViewSAXEventGenerator.java:142: cannot find symbol
symbol  : variable XMLChar
location: class org.apache.jackrabbit.core.xml.DocViewSAXEventGenerator
            if (!XMLChar.isValidName(elemName)) {
                 ^
/home/jukkaz/src/jackrabbit/src/java/org/apache/jackrabbit/core/xml/DocViewSAXEventGenerator.java:162: cannot find symbol
symbol  : variable XMLChar
location: class org.apache.jackrabbit.core.xml.DocViewSAXEventGenerator
                if (!XMLChar.isValidName(attrName)) {

The same build succeeds without problems on JDK 1.4.2_06.

I found some reports about similar problems after upgrading from JDK 1.4 to 1.5. It seems that the org.apache.xml.utils.XMLChar was a part (undocumented?) of the standard JDK classpath, but that it has been dropped from JDK 1.5.

A similar (the same?) XMLChar utility class can be found in the org.apache.xerces.utils package, which is automatically included by the Xerces dependency. The following change fixes the problem on JDK 1.5.0 and seems to work fine also on JDK 1.4.2_06.

Index: src/java/org/apache/jackrabbit/core/xml/DocViewSAXEventGenerator.java
===================================================================
--- src/java/org/apache/jackrabbit/core/xml/DocViewSAXEventGenerator.java      (revision 57540)
+++ src/java/org/apache/jackrabbit/core/xml/DocViewSAXEventGenerator.java      (working copy)
@@ -21,7 +21,7 @@
 import org.apache.jackrabbit.core.state.PropertyState;
 import org.apache.jackrabbit.core.util.Base64;
 import org.apache.log4j.Logger;
-import org.apache.xml.utils.XMLChar;
+import org.apache.xerces.util.XMLChar;
 import org.xml.sax.ContentHandler;
 import org.xml.sax.SAXException;
 import org.xml.sax.helpers.AttributesImpl;

"
0,"Speedup merging of stored fields when field mapping ""matches""Robert Engels suggested the following idea, here:

  http://www.gossamer-threads.com/lists/lucene/java-dev/54217

When merging in the stored fields from a segment, if the field name ->
number mapping is identical then we can simply bulk copy the entire
entry for the document rather than re-interpreting and then re-writing
the actual stored fields.

I've pulled the code from the above thread and got it working on the
current trunk."
0,"wordlistloader is inefficientWordListLoader is basically used for loading up stopwords lists, stem dictionaries, etc.
Unfortunately the api returns Set<String> and sometimes even HashSet<String> or HashMap<String,String>

I think we should break it and return CharArraySets and CharArrayMaps (but leave the return value as generic Set,Map).

If someone objects to breaking it in 3.1, then we can do this only in 4.0, but i think it would be good to fix it both places.
The reason is that if someone does new FooAnalyzer() a lot (probably not uncommon) i think its doing a bunch of useless copying.

I think we should slap @lucene.internal on this API too, since thats mostly how its being used.
"
0,"Allow access to journal inside ClusterNodeJCR-757 added support multiple consumers/producers to be attached to the same journal. In order to access this journal, however, o.a.j.core.cluster.ClusterNode has to allow access to the journal it has created."
0,"Disable SearchManagerIn previous versions (e.g. SVN tag 0.1-spec0.14) it was possible to disable the SearchManagers by not configuring a search index path. In the current revision, a NullPointerException is thrown, if the search index configuration is missing, tough the rest of the system would support missing search index configuration as before.

I suggest to extend search index configuration interpretation in WorkspaceCfg.init as follows:

        Element srchConfig = wspElem.getChild(SEARCH_INDEX_ELEMENT);
        if (srchConfig != null) {
            String pathAttr = srchConfig.getAttributeValue(PATH_ATTRIB);
            if (pathAttr != null && pathAttr.length() > 0) {
                searchIndexDir = replaceVars(pathAttr, vars);
            }
        }

This only reads search index configuration if available.

The reason to switch of the SearchManager is, that in my use case enabling the SearchManager yields a performance degradation of a factor of 10 ! Instead of taking around 500ms (which is still too long :-) to save 3 nodes and 15 properties, it would take around 5 seconds to save the same amount of data. And I do not need the SearchManager in my use case."
1,"TestFSTs.testRealTerms produces a corrupt indexseems to be prox/skip related: the test passes, but the checkindex upon closing fails.

ant test-core -Dtestcase=TestFSTs -Dtests.seed=-4012305283315171209:0 -Dtests.multiplier=3 -Dtests.nightly=true -Dtests.linedocsfile=c:/data/enwiki.random.lines.txt.gz

Note: to get the enwiki.random.lines.txt.gz you have to fetch it from hudson (warning 1 gigabyte file).
you also have to run the test a few times to trigger it.

ill upload the index this thing makes to this issue.
"
0,"allow tests to use different Directory implsNow that all tests use MockRAMDirectory instead of RAMDirectory, they are all picky like windows and force our tests to
close readers etc before closing the directory.

I think we should do the following:
# change new MockRAMDIrectory() in tests to .newDirectory(random)
# LuceneTestCase[J4] tracks if all dirs are closed at tearDown and also cleans up temp dirs like solr.
# factor out the Mockish stuff from MockRAMDirectory into MockDirectoryWrapper
# allow a -Dtests.directoryImpl or simpler to specify the default Directory to use for tests: default being ""random""

i think theres a chance we might find some bugs that havent yet surfaced because they are easier to trigger with FSDir
Furthermore, this would be beneficial to Directory-implementors as they could run the entire testsuite against their Directory impl, just like codec-implementors can do now.
"
0,Grouping collector that computes grouped facet countsSpinoff from issue SOLR-2898. 
1,"Highlighter doesn't support NumericRangeQuery or deprecated RangeQuerySucks. Will throw a NullPointer exception. 

Only NumericRangeQuery will throw the exception.
RangeQuery just won't highlight."
1,"SampleComparable doesn't work well in contrib/remote testsAs discovered in LUCENE-1749, when using identical instances of a SortComparator you get multiple entries in the FieldCache.

demonstrating this bug currently requires the patches in LUCENE-1749.

See markmiller's comment here...
https://issues.apache.org/jira/browse/LUCENE-1749?focusedCommentId=12735190#action_12735190"
0,Consolidate Solr  & Lucene FunctionQuery into modulesSpin-off from the [dev list | http://www.mail-archive.com/dev@lucene.apache.org/msg13261.html]  
0,"Elision filter for simple french analyzingIf you don't wont to use stemming, StandardAnalyzer miss some french strangeness like elision.
""l'avion"" wich means ""the plane"" must be tokenized as ""avion"" (plane).
This filter could be used with other latin language if elision exists."
0,"comparator API for segment versionsSee LUCENE-3012 for an example.

Things get ugly if you want to use SegmentInfo.getVersion()

For example, what if we committed my patch, release 3.2, but later released 3.1.1 (will ""3.1.1"" this be whats written and returned by this function?)
Then suddenly we broke the index format because we are using Strings here without a reasonable comparator API.

In this case one should be able to compute if the version is < 3.2 safely.

If we don't do this, and we rely upon this version information internally in lucene, I think we are going to break something."
0,"Use Maven dependency managementMany of the Jackrabbit components have dependencies to each other and to external libraries,
whose versions should ideally be the same for all the Jackrabbit components. To guarantee
the use of same depedency versions and to simplify overall depedency management we should
start using the Maven depedencyManagement feature in the Jackrabbit parent pom."
0,"add TCK test for Info map of NODE_MOVED event on node reorderingadd the TCK test for this problem, and mark this as known test failure for now"
1,"contrib-spatial java.lang.UnsupportedOperationException on QueryWrapperFilter.getDocIdSetWe use in our Project (which is in the devel phase) the latest Snapshot release of lucene. After i updated to the latest Snapshot a few days ago one of our JUnit tests fails and throws the following error:

java.lang.UnsupportedOperationException
	at org.apache.lucene.search.Query.createWeight(Query.java:91)
	at org.apache.lucene.search.QueryWrapperFilter.getDocIdSet(QueryWrapperFilter.java:72)
	at org.apache.lucene.misc.ChainedFilter.getDISI(ChainedFilter.java:150)
	at org.apache.lucene.misc.ChainedFilter.initialResult(ChainedFilter.java:173)
	at org.apache.lucene.misc.ChainedFilter.getDocIdSet(ChainedFilter.java:211)
	at org.apache.lucene.misc.ChainedFilter.getDocIdSet(ChainedFilter.java:141)
	at org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.<init>(ConstantScoreQuery.java:116)
	at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:81)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:244)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:172)
	at org.apache.lucene.search.Searcher.search(Searcher.java:183)
	at org.hibernate.search.query.QueryHits.updateTopDocs(QueryHits.java:100)
	at org.hibernate.search.query.QueryHits.<init>(QueryHits.java:61)
	at org.hibernate.search.query.QueryHits.<init>(QueryHits.java:51)
	at org.hibernate.search.query.FullTextQueryImpl.getQueryHits(FullTextQueryImpl.java:373)
	at org.hibernate.search.query.FullTextQueryImpl.list(FullTextQueryImpl.java:293)
	...

I think it appeared after the Hudson build 917... and the following commit of the Query.java http://hudson.zones.apache.org/hudson/job/Lucene-trunk/917/changes#detail4 and is in connection with this JIRA issue: LUCENE-1771
I hope i'm at the right place and that you can fix it. Thanks!"
1,"CloseableThreadLocal does not work well with Tomcat thread poolingWe tracked down a large memory leak (effectively a leak anyway) caused
by how Analyzer users CloseableThreadLocal.
CloseableThreadLocal.hardRefs holds references to Thread objects as
keys.  The problem is that it only frees these references in the set()
method, and SnowballAnalyzer will only call set() when it is used by a
NEW thread.

The problem scenario is as follows:

The server experiences a spike in usage (say by robots or whatever)
and many threads are created and referenced by
CloseableThreadLocal.hardRefs.  The server quiesces and lets many of
these threads expire normally.  Now we have a smaller, but adequate
thread pool.  So CloseableThreadLocal.set() may not be called by
SnowBallAnalyzer (via Analyzer) for a _long_ time.  The purge code is
never called, and these threads along with their thread local storage
(lucene related or not) is never cleaned up.

I think calling the purge code in both get() and set() would have
avoided this problem, but is potentially expensive.  Perhaps using 
WeakHashMap instead of HashMap may also have helped.  WeakHashMap 
purges on get() and set().  So this might be an efficient way to
clean up threads in get(), while set() might do the more expensive
Map.keySet() iteration.

Our current work around is to not share SnowBallAnalyzer instances
among HTTP searcher threads.  We open and close one on every request.

Thanks,
Matt"
0,"Configure Maximum Connection LifetimesProvide a means of configuring a maximum lifetime for HttpClient connections.  Currently, it would appear as long as a connection is used it may persist indefinitely.

This would be useful for situations where HttpClient needs to react to DNS changes, such as the following situation that may occur when using DNS load balancing:
 - HttpClient maintains connections to example.com which resolves to IP A
 - Machine at IP A fails, and example.com now resolves to backup machine at IP B
 - Since IP A is failing, connections are destroyed, and new connections are made to IP B
 - Machine at IP A recovers, but HttpClient maintains connections to IP B since the connections are still healthy

The desired behavior would be that connections to IP B will reach their connection lifetime, and new connections could be created back to IP A according to the updated DNS settings."
0,Wrong javadoc on LowerCaseTokenizer.normalizeThe javadoc on LowerCaseTokenizer.normalize seems to be copy/paste from LetterTokenizer.isTokenChar.
0,"Handle date values in the far future or prevent these from being persistedSetting a date property with a value in the far future (e.g., the year 20009) and saving the session causes the index component to throw an exception (see the DateField#timeToString method). Furthermore, when the repository is restarted, the properties' value cannot be retrieved anymore because of a ValueFormatException caused by an empty value. Restarting the repository with an empty search index does not work because indexing fails. I haven't looked into the effect on queries.
"
1,"JCR-SQL2 query with multiple columns in result only returns last column when using Row.getValues()When running a query like below on an in-process repository (via TransientRepository) or via RMI access, a call to Row.getValues() only returns the last column selected:

       SELECT property1, property2 FROM [nodetype]

QueryResult.getColumnNames() returns the right set of columns.

Stepping through the code shows that org.apache.jackrabbit.core.query.lucene.join.AbstractRow has the implementation of getValues() - this creates a new Values array, then overwrites it multiple times in a for loop that iterates once per column. That doesn't sound like the desired behaviour.

Getting values via individual calls to Row.getValue(""property1"") gives the correct results.

"
0,"Little improvement for SimpleHTMLEncoderThe SimpleHTMLEncoder could be improved slightly: all characters with code >=
128 should be encoded as character entities. The reason is, that the encoder
does not know the encoding that is used for the response. Therefore it is safer
to encode all characters beyond ASCII as character entities.

Here is the necessary modification of SimpleHTMLEncoder:

       default:
         if (c < 128) {
           result.append(c);
         } else {
           result.append(""&#"").append((int)c).append("";"");
         }"
0,"Make the Payload Boosting Queries consistentBoostingFunctionTermQuery should be consistent with BoostingNearQuery -

Renaming to PayloadNearQuery and PayloadTermQuery"
0,"https should check CN of x509 certhttps should check CN of x509 cert

Since we're essentially rolling our own ""HttpsURLConnection"",  the checking provided by ""javax.net.ssl.HostnameVerifier"" is no longer in place.

I have a patch I'm about to attach which caused both createSocket() methods on o.a.h.conn.ssl.SSLSocketFactory to blowup:

test1: javax.net.ssl.SSLException: hostname in certificate didn't match: <vancity.com> != <www.vancity.com>
test2: javax.net.ssl.SSLException: hostname in certificate didn't match: <vancity.com> != <www.vancity.com>

Hopefully people agree that this is desirable.
"
0,"TCK: SessionReadMethodsTest#testIsLive calls logout() more than onceSessionReadMethodsTest#testIsLive calls logout more than once in a session (once in the test, once in tearDown).  JSR-170 doesn't prohibit an implementation from throwing an unchecked exception (such as IllegalStateException) if logout is called more than once.

Proposal: change tearDown to test isLive before calling logout.

--- SessionReadMethodsTest.java (revision 422074)
+++ SessionReadMethodsTest.java (working copy)
@@ -57,7 +57,7 @@
      * Releases the session aquired in {@link #setUp()}.
      */
     protected void tearDown() throws Exception {
-        if (session != null) {
+        if (session != null && session.isLive()) {
             session.logout();
         }
         super.tearDown();
"
1,"document view: importXML() fails on protected property jcr:primaryTypewhen trying to import an xml document where elements contain the attribute jcr:primaryType the import fails with:

javax.jcr.nodetype.ConstraintViolationException: cannot set the value of a protected property /testroot/docviewimport/doc/jcr:primaryType
	at org.apache.jackrabbit.core.PropertyImpl.setValue(PropertyImpl.java:907)
	at org.apache.jackrabbit.core.NodeImpl.setProperty(NodeImpl.java:1044)
	at org.apache.jackrabbit.core.xml.DocViewImportHandler.startElement(DocViewImportHandler.java:124)
	at org.apache.jackrabbit.core.xml.ImportHandler.startElement(ImportHandler.java:164)
	at org.apache.xerces.parsers.AbstractSAXParser.startElement(Unknown Source)
	at org.apache.xerces.impl.XMLNSDocumentScannerImpl.scanStartElement(Unknown Source)
	at org.apache.xerces.impl.XMLNSDocumentScannerImpl$NSContentDispatcher.scanRootElementHook(Unknown Source)
	at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(Unknown Source)
	at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source)
	at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
	at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
	at org.apache.xerces.parsers.XMLParser.parse(Unknown Source)
	at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)
	at org.apache.jackrabbit.core.SessionImpl.importXML(SessionImpl.java:836)
	at org.apache.jackrabbit.test.api.DocViewImportTest.setUp(DocViewImportTest.java:92)
	at com.intellij.rt.execution.junit2.JUnitStarter.main(JUnitStarter.java:31)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:78)

if i understand the spec correctly, the import process should take care of this attribute and determine the node type of the new nodes based on it."
0,allow AbstractFileSystemTest.getFileSystem to throw an Exception
0,"reusing connections is unreliableHttpConnection reuse is unreliable. Because of the following:

1) There is currently no way to determine if a connection is still open on the
server side.
2) If an IOException occurs while writing to a connection it cannot be reused."
0,"configurable User-Agent stringUser configurable item to set the user agent without haveing to set it on a per
HttpMethod basis."
0,"[PATCH] new method expungeDeleted() added to IndexWriterWe make use the docIDs in lucene. I need a way to compact the docIDs in segments
to remove the ""holes"" created from doing deletes. The only way to do this is by
calling IndexWriter.optimize(). This is a very heavy call, for the cases where
the index is large but with very small number of deleted docs, calling optimize
is not practical.

I need a new method: expungeDeleted(), which finds all the segments that have
delete documents and merge only those segments.

I have implemented this method and have discussed with Otis about submitting a
patch. I don't see where I can attached the patch. I will do according to the
patch guidleine and email the lucene mailing list.

Thanks

-John

I don't see a place where I can"
0,"Add oal.util.Version ctor to QueryParserThis is a followup of LUCENE-1987:

If somebody uses StandardAnalyzer with Version.LUCENE_CURRENT and then uses QueryParser, phrase queries will not work, because the StopFilter enables position Increments for stop words, but QueryParser ignores them per default. The user has to explicitely enable them.

This issue would add a ctor taking the Version constant and automatically enable this setting. The same applies to the contrib queryparser. Eventually also StopAnalyzer should add this version ctor.

To be able to remove the default ctor for 3.0 (to remove a possible trap for users of QueryParser), it must be deprecated and the new one also added to 2.9.1."
1,"DefaultLoginModule/SimpleLoginModule don't support custom PrincipalProviderWhen configuring a custom PrincipalProvider for the SimpleLoginModule or DefaultLoginModule, inside of a repository.xml file with configuration such as the following:

    <Security appName=""Jackrabbit"">
        <AccessManager
            class=""org.apache.jackrabbit.core.security.DefaultAccessManager"">
        </AccessManager>
        <LoginModule
            class=""org.apache.jackrabbit.core.security.authentication.DefaultLoginModule"">
            <param name=""principalprovider"" value=""com.foo.jcr.BasicPrincipalProvider""/>
        </LoginModule>
      <SecurityManager class=""org.apache.jackrabbit.core.DefaultSecurityManager"">         
      </SecurityManager>    
    </Security>

And that yields the following stacktrace:

javax.jcr.LoginException: org.apache.jackrabbit.core.security.authentication.DefaultLoginModule does not support 'principalprovider: org.apache.jackrabbit.core.security.authentication.DefaultLoginModule does not support 'principalprovider: org.apache.jackrabbit.core.security.authentication.DefaultLoginModule does not support 'principalprovider
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1353)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:53)
	at com.foo.jcr.PrincipalProviderTest.testPrincipalProvider(PrincipalProviderTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:59)
	at org.junit.internal.runners.MethodRoadie.runTestMethod(MethodRoadie.java:98)
	at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:79)
	at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:87)
	at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77)
	at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42)
	at org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88)
	at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51)
	at org.junit.internal.runners.JUnit4ClassRunner$1.run(JUnit4ClassRunner.java:44)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37)
	at org.junit.internal.runners.JUnit4ClassRunner.run(JUnit4ClassRunner.java:42)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:45)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: javax.security.auth.login.LoginException: org.apache.jackrabbit.core.security.authentication.DefaultLoginModule does not support 'principalprovider
	at org.apache.jackrabbit.core.security.authentication.LocalAuthContext.login(LocalAuthContext.java:68)
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1346)
	... 24 more
javax.security.auth.login.LoginException: org.apache.jackrabbit.core.security.authentication.DefaultLoginModule does not support 'principalprovider
	at org.apache.jackrabbit.core.security.authentication.LocalAuthContext.login(LocalAuthContext.java:68)
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1346)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:53)
	at com.foo.jcr.PrincipalProviderTest.testPrincipalProvider(PrincipalProviderTest.java:24)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:59)
	at org.junit.internal.runners.MethodRoadie.runTestMethod(MethodRoadie.java:98)
	at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:79)
	at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:87)
	at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77)
	at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42)
	at org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88)
	at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51)
	at org.junit.internal.runners.JUnit4ClassRunner$1.run(JUnit4ClassRunner.java:44)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37)
	at org.junit.internal.runners.JUnit4ClassRunner.run(JUnit4ClassRunner.java:42)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:45)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)"
1,"DBDataStore doesn't support concurrent readsMy understanding is that setting parameter copyWhenReading to true should allow concurrent reads by spooling binary property to temporary file and free database resources (connection) immediately to make it available for other threads.

After applying patch for JCR-1388, DBDataStore doesn't support concurrent reads anymore, resultSet is kept open and db connection is blocked until the stream is read and closed. When copyWhenReading is set to true db connection should be released immediately, this is the reason i guess why temporary file is used."
0,"New Analyzer for buffering tokensIn some cases, it would be handy to have Analyzer/Tokenizer/TokenFilters that could siphon off certain tokens and store them in a buffer to be used later in the processing pipeline.

For example, if you want to have two fields, one lowercased and one not, but all the other analysis is the same, then you could save off the tokens to be output for a different field.

Patch to follow, but I am still not sure about a couple of things, mostly how it plays with the new reuse API.

See http://www.gossamer-threads.com/lists/lucene/java-dev/54397?search_string=BufferingAnalyzer;#54397"
0,"back-compat tests (""ant test-tag"") should test JAR drop-in-ability
We now test back-compat with ""ant test-tag"", which is very useful for
catching breaks in back compat before committing.

However, that currently checks out ""src/test"" sources and then
compiles them against the trunk JAR, and runs the tests.  Whereas our
back compat policy:

  http://wiki.apache.org/lucene-java/BackwardsCompatibility

states that no recompilation is required on upgrading to a new JAR.
Ie you should be able to drop in the new JAR in place of your old one
and things should work fine.

So... we should fix ""ant test-tag"" to:

  * Do full checkout of core sources & tests from the back-compat-tag

  * Compile the JAR from the back-compat sources

  * Compile the tests against that back-compat JAR

  * Swap in the trunk JAR

  * Run the tests

"
0,Expose BootstrapConfig in Servletsthe RepostitoryStartup and RepositroyAccess servlets use a bootstrap config object for initialization. in order to generate diagnostics reports it would be very useful to be able to access them.
1,"The CredentialsWrapper should use a empty String as userId if custom Credentials are usedIf custom Credentials are used we get a IllegalArgumentException from the AbstractQValueFactory while executing SessionItemStateManager.computeSystemGeneratedPropertyValues().
The 2 Properties jcr:createdBy and jcr:lastModified could not be created."
0,New QueryParser should not allow leading wildcard by defaultThe current QueryParser disallows leading wildcard characters by default.
1,ItemManager registers itself as listener too earlyThis is similar to JCR-2168 but for ItemManager and SessionItemStateManager.
1,"Path returned by FileSystemBLOBStore.createId() is not absoluteHi,

I have developed my own FileSystem in which I call FileSystemPathUtil.checkFormat(path) for every operation on the file system.
When the file system is called to store a BLOB value, the path I get is always relative, resulting in a ""not an absolute path"" FileSystemException.

The problem has been traced back to org.apache.jackrabbit.core.state.util.FileSystemBLOBStore.creatId().
I think there should be a:
   sb.append(FileSystem.SEPARATOR_CHAR);
before the for loop.

Thanks."
1,"cache module produces improperly formatted Warning header when revalidation failsThe warning header currently attached to a stale response by the caching module when validation with the origin server fails is not a properly-formatted Warning header.

http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.46"
0,"ChineseFilter is inefficienttrivial patch to use CharArraySet, so it can use termBuffer() instead of term()
"
0,"Use StringBuilder instead of StringBuffer in benchmarkMinor change - use StringBuilder instead of StringBuffer in benchmark's code. We don't need the synchronization of StringBuffer in all the places that I've checked.

The only place where it _could_ be a problem is in HtmlParser's API - one method accepts a StringBuffer and it's an interface. But I think it's ok to change benchmark's API, back-compat wise and so I'd like to either change it to accept a String, or remove the method altogether -- no code in benchmark uses it, and if anyone needs it, he can pass StringReader to the other method."
0,"Optimize usage of normsThere is a very significant potential for optimizing the size of the search index.

We have seen a case where there were multiple segments with about the same number of nodes (roughly 10 million), but the size on disk was very different.
One segment was 19 GB while all others where around 3 GB. The major difference was the number of fields indexed. The large segment had significantly more fields, which resulted in a large norms file.

We should go through our implementation and see where norms are really necessary and disable tracking of norms wherever possible."
0,"Upgrade JUnit to 4.10, refactor state-machine of detecting setUp/tearDown call chaining.Both Lucene and Solr use JUnit 4.7. I suggest we move forward and upgrade to JUnit 4.10 which provides several infrastructural changes (serializable Description objects, class-level rules, various tweaks). JUnit 4.10 also changes (or fixes, depends how you look at it) the order in which @Before/@After hooks and @Rules are applied. This makes the old state-machine in LuceneTestCase fail (because the order is changed).

I rewrote the state machine and used a different, I think simpler, although Uwe may disagree :), mechanism in which the hook methods setUp/ tearDown are still there, but they are empty at the top level and serve only to detect whether subclasses chain super.setUp/tearDown properly (if they override anything).

In the long term, I would love to just get rid of public setup/teardown methods and make them private (so that they cannot be overriden or even seen by subclasses) but this will require changes to the runner itself."
0,"better handling of files inside/outside CFS by codecSince norms and deletes were moved under Codec (LUCENE-3606, LUCENE-3661),
we never really properly addressed the issue of how Codec.files() should work,
considering these files are always stored outside of CFS.

LUCENE-3606 added a hack, LUCENE-3661 cleaned up the hack a little bit more,
but its still a hack.

Currently the logic in SegmentInfo.files() is:
{code}
clearCache()

if (compoundFile) {
  // don't call Codec.files(), hardcoded CFS extensions, etc
} else {
  Codec.files()
}

// always add files stored outside CFS regardless of CFS setting
Codec.separateFiles()

if (sharedDocStores) {
  // hardcoded shared doc store extensions, etc
}
{code}

Also various codec methods take a Directory parameter, but its inconsistent
what this Directory is in the case of CFS: for some parts of the index its
the CFS directory, for others (deletes, separate norms) its not.

I wonder if instead we could restructure this so that SegmentInfo.files() logic is:
{code}
clearCache()
Codec.files()
{code}

and so that Codec is instead responsible.

instead Codec.files logic by default would do the if (compoundFile) thing, and
Lucene3x codec itself would only have the if (sharedDocStores) thing, and any
part of the codec that wants to put stuff always outside of CFS (e.g. Lucene3x separate norms, deletes) 
could just use SegmentInfo.dir. Directory parameters in the case of CFS would always
consistently be the CFSDirectory.

I haven't totally tested if this will work but there is definitely some cleanups 
we can do either way, and I think it would be a good step to try to clean this up
and simplify it.
"
1,"TestGrouping failure{noformat}
ant test -Dtestcase=TestGrouping -Dtestmethod=testRandom -Dtests.seed=295cdb78b4a442d4:-4c5d64ef4d698c27:-425d4c1eb87211ba
{noformat}

fails with this on current trunk:

{noformat}

    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestGrouping -Dtestmethod=testRandom -Dtests.seed=295cdb78b4a442d4:-4c5d64ef4d698c27:-425d4c1eb87211ba
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=MockRandom, content=MockSep, sort2=SimpleText, groupend=Pulsing(freqCutoff=3 minBlockSize=65 maxBlockSize=132), sort1=Memory, group=Memory}, sim=RandomSimilarityProvider(queryNorm=true,coord=false): {id=DFR I(F)L2, content=DFR BeL3(800.0), sort2=DFR GL3(800.0), groupend=DFR G2, sort1=DFR GB3(800.0), group=LM Jelinek-Mercer(0.700000)}, locale=zh_TW, timezone=America/Indiana/Indianapolis
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestGrouping]
    [junit] NOTE: Linux 2.6.33.6-147.fc13.x86_64 amd64/Sun Microsystems Inc. 1.6.0_21 (64-bit)/cpus=24,threads=1,free=143246344,total=281804800
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testRandom(org.apache.lucene.search.grouping.TestGrouping):	FAILED
    [junit] expected:<11> but was:<7>
    [junit] junit.framework.AssertionFailedError: expected:<11> but was:<7>
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)
    [junit] 	at org.apache.lucene.search.grouping.TestGrouping.assertEquals(TestGrouping.java:980)
    [junit] 	at org.apache.lucene.search.grouping.TestGrouping.testRandom(TestGrouping.java:865)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)
    [junit] 
    [junit] 
{noformat}

I dug for a while... the test is a bit sneaky because it compares sorted docs (by score) across 2 indexes.  Index #1 has no deletions; Index #2 has same docs, but organized into doc blocks by group, and has some deletions.  In theory (I think) even though the deletions will cause scores to differ across the two indices, it should not alter the sort order of the docs.  Here is the explain output of the docs that sorted differently:

{noformat}
#1: top hit in the ""has deletes doc-block"" index (id=239):

explain: 2.394486 = (MATCH) weight(content:real1 in 292)
[DFRSimilarity], result of:
 2.394486 = score(DFRSimilarity, doc=292, freq=1.0), computed from:
   1.0 = termFreq=1
   41.944084 = NormalizationH3, computed from:
     1.0 = tf
     5.3102274 = avgFieldLength
     2.56 = len
   102.829 = BasicModelBE, computed from:
     41.944084 = tfn
     880.0 = numberOfDocuments
     239.0 = totalTermFreq
   0.023286095 = AfterEffectL, computed from:
     41.944084 = tfn


#2: hit in the ""no deletes normal index"" (id=229)

ID=229 explain=2.382285 = (MATCH) weight(content:real1 in 225)
[DFRSimilarity], result of:
 2.382285 = score(DFRSimilarity, doc=225, freq=1.0), computed from:
   1.0 = termFreq=1
   41.765594 = NormalizationH3, computed from:
     1.0 = tf
     5.3218827 = avgFieldLength
     10.24 = len
   101.879845 = BasicModelBE, computed from:
     41.765594 = tfn
     786.0 = numberOfDocuments
     215.0 = totalTermFreq
   0.023383282 = AfterEffectL, computed from:
     41.765594 = tfn

Then I went and called explain on the ""no deletes normal index"" for
the top doc (id=239):

explain: 2.3822558 = (MATCH) weight(content:real1 in 17)
[DFRSimilarity], result of:
 2.3822558 = score(DFRSimilarity, doc=17, freq=1.0), computed from:
   1.0 = termFreq=1
   42.165264 = NormalizationH3, computed from:
     1.0 = tf
     5.3218827 = avgFieldLength
     2.56 = len
   102.8307 = BasicModelBE, computed from:
     42.165264 = tfn
     786.0 = numberOfDocuments
     215.0 = totalTermFreq
   0.023166776 = AfterEffectL, computed from:
     42.165264 = tfn
{noformat}"
1,"EventFilter misses Events for same NodetypeIf an ObservationListener registers with a NodeType-filter, 
it only gets informed about events on Sub-NodeTypes of the ones specified in the filter but not on the NodeType itself.

Example:
========
ObservationManager om = wsp.getObservationManager();
om.addEventListener(listener, Event.PROPERTY_ADDED, ""/"", true, null, new String[]{""nt:unstructured""}, true);

would receive notifications on nodes of type ""rep:root"", which is based on ""nt:unstructured"" but not of ""nt:unstructured""


"
1,"Deadlock inside XASession on WeblogicIn one of our client deployments on WebLogic 9.2 we observed JackRabbit sessions going stale in a load test. This was observed against release 1.6.1 (to which we migrated due to concurrency related issues JCR-2081 and JCR-2237). Same effect with 2.0.0.
 
I could finally reproduce this issue locally. And it seems to boil down to WLS invoking the sequence of <prepare> ... <release> ... <commit> on one XA session from multiple threads, as it seems breaking assumptions of the thread-bound java.util.concurrent-RWLock based DefaultISMLocking class.
Effectively the setActiveXid(..) method on DefaultISMLocking$RWLock fails as the old active XID was not yet cleared. With the result of more and more sessions deadlocking in below's invocation stack.

{code}
""[ACTIVE] ExecuteThread: '27' for queue: 'weblogic.kernel.Default (self-tuning)'"" daemon prio=1 tid=0x33fc3ec0 nid=0x2324 in Object.wait() [0x2156a000..0x2156beb0] at java.lang.Object.wait(Native Method) - waiting on <0x68a54698> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock) at java.lang.Object.wait(Object.java:474) at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock.acquire(Unknown Source) - locked <0x68a54698> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock) at org.apache.jackrabbit.core.state.DefaultISMLocking$1.<init>(DefaultISMLocking.java:64) at org.apache.jackrabbit.core.state.DefaultISMLocking.acquireWriteLock(DefaultISMLocking.java:61) at org.apache.jackrabbit.core.version.AbstractVersionManager.acquireWriteLock(AbstractVersionManager.java:146) at org.apache.jackrabbit.core.version.XAVersionManager$1.prepare(XAVersionManager.java:562) at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:154) - locked <0x6dc2ad88> (a org.apache.jackrabbit.core.TransactionContext) at org.apache.jackrabbit.core.XASessionImpl.prepare(XASessionImpl.java:331) at org.apache.jackrabbit.jca.TransactionBoundXAResource.prepare(TransactionBoundXAResource.java:68) at weblogic.connector.security.layer.AdapterLayer.prepare(AdapterLayer.java:397) at weblogic.connector.transaction.outbound.XAWrapper.prepare(XAWrapper.java:297) at weblogic.transaction.internal.XAServerResourceInfo.prepare(XAServerResourceInfo.java:1276) at weblogic.transaction.internal.XAServerResourceInfo.prepare(XAServerResourceInfo.java:499) at weblogic.transaction.internal.ServerSCInfo$1.execute(ServerSCInfo.java:335) at weblogic.kernel.Kernel.executeIfIdle(Kernel.java:243) at weblogic.transaction.internal.ServerSCInfo.startPrepare(ServerSCInfo.java:326) at weblogic.transaction.internal.ServerTransactionImpl.localPrepare(ServerTransactionImpl.java:2516) at weblogic.transaction.internal.ServerTransactionImpl.globalPrepare(ServerTransactionImpl.java:2211) at weblogic.transaction.internal.ServerTransactionImpl.internalCommit(ServerTransactionImpl.java:266) at weblogic.transaction.internal.ServerTransactionImpl.commit(ServerTransactionImpl.java:227) at weblogic.transaction.internal.TransactionManagerImpl.commit(TransactionManagerImpl.java:283) at org.springframework.transaction.jta.JtaTransactionManager.doCommit(JtaTransactionManager.java:1028) at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:709) at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:678)
{code}"
1,"Workspace is shut down while creating initial indexThis only happens when a maxIdleTime is configured for the workspaces in the repository.xml and the workspace to index is not the default workspace.

The idle check considers a workspace as idle when there only a system session is open and the configured idle time elapsed. This is also the case when the workspace is initializing.

The repository should either check if a workspace is still initializing or we need to move the search manager initialization into the WorkspaceInfo.doInitialize() method.

"
0,"Database persistence managers: log database and driver name and versionDatabase related problems can be solved more easily when we know what database and driver version is used. Sometimes multiple database drivers are installed in an app server environment, and the user may not even know it. 

Currently the driver class name is logged. I suggest to log the driver and database name and version as well."
1,"ClassCastException bei unregisterNodeTypeI have a NodeType with various childnodes which I want to unregister. If I call:

    
      NodeTypeManager ndmg = session.getWorkspace().getNodeTypeManager();
      NodeTypeRegistry ntReg = ((NodeTypeManagerImpl) ndmg).getNodeTypeRegistry();
      ntReg.unregisterNodeType(new QName(""testURI"",""Page""));


I get a 

java.lang.ClassCastException
 at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.getDependentNodeTypes(NodeTypeRegistry.java:1242)
 at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.unregisterNodeType(NodeTypeRegistry.java:1120)
 at de.freaquac.test.JCRTest.main(JCRTest.java:80)

It looks to me like there are QNames in the Iterator but NodeTyeDefs are expected.
"
0,Fix SnowballAnalyzer casing behavior for Turkish LanguageLUCENE-2102 added a new TokenFilter to handle Turkish unique casing behavior correctly. We should fix the casing behavior in SnowballAnalyzer too as it supports a TurkishStemmer.
0,"Deployment of webdav servlet on Jboss problem - loggingTested two different installs of JBoss to verify problem is not related to a specific version. There is a problem with the jackrabbit-server.war when deploying on jboss.  Here are the details during deployment:

=======================
13:20:48,654 INFO  [TomcatDeployer] deploy, ctxPath=/jackrabbit-server, warUrl=.../deploy/jackrabbit-server.war/
13:20:48,857 INFO  [STDOUT] log4j:ERROR A ""org.jboss.logging.util.OnlyOnceErrorHandler"" object is not assignable to a ""o rg.apache.log4j.spi.ErrorHandler"" variable.
13:20:48,857 INFO  [STDOUT] log4j:ERROR The class ""org.apache.log4j.spi.ErrorHandler"" was loaded by
13:20:48,857 INFO  [STDOUT] log4j:ERROR [WebappClassLoader
  delegate: false
  repositories:
    /WEB-INF/classes/
----------> Parent Classloader:
java.net.FactoryURLClassLoader@19d277e
] whereas object of type
13:20:48,857 INFO  [STDOUT] log4j:ERROR ""org.jboss.logging.util.OnlyOnceErrorHandler"" was loaded by [org.jboss.system.se rver.NoAnnotationURLClassLoader@ab95e6].
13:20:48,904 INFO  [STDOUT] log4j:ERROR Could not create an Appender. Reported error follows.
13:20:48,904 INFO  [STDOUT] java.lang.ClassCastException: org.jboss.logging.appender.DailyRollingFileAppender
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.parseAppender(DOMConfigurator.java:165)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.findAppenderByName(DOMConfigurator.java:140)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.findAppenderByReference(DOMConfigurator.java:153
)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.parseChildrenOfLoggerElement(DOMConfigurator.jav
a:415)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.parseRoot(DOMConfigurator.java:384)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.parse(DOMConfigurator.java:783)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:666)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:616)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:602)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:460)

13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.LogManager.<clinit>(LogManager.java:113)
13:20:48,904 INFO  [STDOUT]     at org.apache.log4j.xml.DOMConfigurator.configure(DOMConfigurator.java:543)
13:20:48,904 INFO  [STDOUT]     at org.apache.jackrabbit.j2ee.LoggingServlet.configureXML(LoggingServlet.java:148)
13:20:48,904 INFO  [STDOUT]     at org.apache.jackrabbit.j2ee.LoggingServlet.configure(LoggingServlet.java:115)
13:20:48,904 INFO  [STDOUT]     at org.apache.jackrabbit.j2ee.LoggingServlet.init(LoggingServlet.java:86)
======================
 
Unlike most logging problems, this is having an impact during runtime - when trying to do DASL searches, will return a 500 error as the server was unable to log correctly.
"
1,"Removed version is not invalidatedwhen a version is removed, it's internal represenation is not evicted from the cache. this can leed to unexpected behaviours. XATest.removeVersion() tests this. this also happens in a non-transactional environment."
0,"Add QueryParser.newFieldQueryNote: this patch changes no behavior, just makes QP more subclassable.

Currently we have Query getFieldQuery(String field, String queryText, boolean quoted)
This contains very hairy methods for producing a query from QP's analyzer.

I propose we factor this into newFieldQuery(Analyzer analyzer, String field, String queryText, boolean quoted)
Then getFieldQuery just calls newFieldQuery(this.analyzer, field, queryText, quoted);

The reasoning is: it can be quite useful to consider the double quote as more than phrases, but a ""more exact"" search.
In the case the user quoted the terms, you might want to analyze the text with an alternate analyzer that:
doesn't produce synonyms, doesnt decompose compounds, doesn't use WordDelimiterFilter 
(you would need to be using preserveOriginal=true at index time for the WDF one), etc etc.

This is similar to the way google's double quote operator works, its not defined as phrase but ""this exact wording or phrase"".
For example compare results to a query of tests versus ""tests"".

Currently you can do this without heavy code duplication, but really only if you make a separate field (which is wasteful),
and make your custom QP lie about its field... in the examples I listed above you can do this with a single field, yet still
have a more exact phrase search.
"
1,"Cookies with ',' in the value string is not parsed correctly in some casesThis version extracts the ""Set-Cookie"" statementes of the following
HTTP response headers incorrectly.

The HTTP response is sent when executing GET method on --->
""http://my.taishinbank.com.tw/netbank/nbslogin.asp?
subFunID=https://my.taishinbank.com.tw/netbank/AccountQuery/QAccbyID.asp""

After the HttpClient extracts Set-Cookie from the response, it generates a wrong
cookie statement---->

  [INFO] wire - ->> ""Cookie: $Version=0; _mysite=520163500; 1027657033=null; 
   1027787539=null; 0=null; $Path=/; cata=11; $Path=/;   
   ASPSESSIONIDGGGQQXEU=ADLCDAGAJLKEBJEKBOMMAMOB; 
   $Path=/""

, where it shall 
be ""_mysite=520163500,1027657033,1027787539,1027787539,0;"" ,but 
not ""_mysite=520163500; 1027657033=null; 1027787539=null; 0=null;""
 

Thank you"
1,"TestStressIndexing has intermittent failuresSee http://www.gossamer-threads.com/lists/lucene/java-dev/55092 copied below:

 OK, I have seen this twice in the last two days:
Testsuite: org.apache.lucene.index.TestStressIndexing
[junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 18.58
sec
[junit]
[junit] ------------- Standard Output ---------------
[junit] java.lang.NullPointerException
[junit] at
org.apache.lucene.store.RAMInputStream.readByte(RAMInputStream.java:67)
[junit] at
org.apache.lucene.store.IndexInput.readInt(IndexInput.java:66)
[junit] at org.apache.lucene.index.SegmentInfos
$FindSegmentsFile.run(SegmentInfos.java:544)
[junit] at
org
.apache
.lucene.index.DirectoryIndexReader.open(DirectoryIndexReader.java:63)
[junit] at
org.apache.lucene.index.IndexReader.open(IndexReader.java:209)
[junit] at
org.apache.lucene.index.IndexReader.open(IndexReader.java:192)
[junit] at
org.apache.lucene.search.IndexSearcher.<init>(IndexSearcher.java:56)
[junit] at org.apache.lucene.index.TestStressIndexing
$SearcherThread.doWork(TestStressIndexing.java:111)
[junit] at org.apache.lucene.index.TestStressIndexing
$TimedThread.run(TestStressIndexing.java:55)
[junit] ------------- ---------------- ---------------
[junit] Testcase:
testStressIndexAndSearching
(org.apache.lucene.index.TestStressIndexing): FAILED
[junit] hit unexpected exception in search1
[junit] junit.framework.AssertionFailedError: hit unexpected
exception in search1
[junit] at
org
.apache
.lucene.index.TestStressIndexing.runStressTest(TestStressIndexing.java:
159)
[junit] at
org
.apache
.lucene
.index
.TestStressIndexing
.testStressIndexAndSearching(TestStressIndexing.java:187)
[junit]
[junit]
[junit] Test org.apache.lucene.index.TestStressIndexing FAILED

Subsequent runs have, however passed. Has anyone else hit this on
trunk?

I am running using ""ant clean test""

I'm on a Mac Pro 4 core, 4GB machine, if that helps at all. Not sure
how to reproduce at this point, but strikes me as a threading issue.
Oh joy!

I'll try to investigate more tomorrow to see if I can dream up a test
case.

-Grant 

"
1,"MMapDirectory chunking is buggyMMapDirectory uses chunking with MultiMMapIndexInput.
 
Because Java's ByteBuffer uses an int to address the
values, it's necessary to access a file >
Integer.MAX_VALUE in size using multiple byte buffers.

But i noticed from the clover report the entire MultiMMapIndexInput class is completely untested: no surprise since all tests make tiny indexes.
"
0,Wrong link in javadoc of QNodeTypeDefinitionThe javadoc of QNodeTypeDefinition links to javax.jcr.nodetype.NodeDefinition instead of javax.jcr.nodetype.NodeType
0,"Add API for selective bundle consistency check (Jackrabbit-specific)Add a jackrabbit-specific API for doing a selective consistencyCheck, ie. on single nodes. The current entire-workspace check can be very slow if there workspace is large enough. Also it should be easy to write a tool to invoke that feature programmatically rather than by configuration + restart (see below).

Existing Implementation:
The current bundle consistencyCheck feature is enabled by setting a bundle PM parameter and restarting Jackrabbit, it will then run upon startup (see JCR-972 for the only issue regarding bundle consistency check). This check looks for broken parent-child relationships, ie. it will remove any child node entries that reference non-existing parent nodes. For non-existing parent UUIDs and other problems in bundles it will log those.

Outlook:
An advanced consistencyCheck could also check for non-existing version nodes and vice-versa (see JCR-630), but this is not the focus of this issue and could be a later addition to the API."
0,"JSR 283: adopt CND syntax changesthe CND syntax has changed from Public Review Draft to Public Final Draft.

old and new syntax are incompatible."
0,"Demo: DeleteFiles doesn't delete files by their path namesIt appears that delete(term) fails to delete the last document containing term, which
for a unique match means that you can't remove an individual document.

Code attempting to remove document with specific 'path' (slightly modified version of demo code):

Directory directory = FSDirectory.getDirectory(""index"", false);
IndexReader reader = IndexReader.open(directory);
Term term = new Term(""path"", args[0]);  // path passed via command line arg
int deleted = reader.delete(term);
reader.close();
directory.close();

System.out.println(""deleted "" + deleted + "" documents containing "" + term);

Executing this always returns ""deleted 0 documents containing <path entered>""

In IndexReader.java, delete() has:

public final int delete(Term term) throws IOException {
  TermDocs docs = termDocs(term);
  if (docs == null) return 0;
  int n = 0;
  try {
    while (docs.next()) {
      delete(docs.doc());
      n++;
    }
  } finally {
    docs.close();
  }
  return n;
}

It appears that docs.next() always returns false when there is only one doc, hence
delete() is never called and 0 is always returned.  I assume that this also means that
if there are multiple matches, the last doc will not be deleted either, but I have not tested
that.

I modified the code as follows:

    boolean more = true;
    try {
      docs.next();
      while (more) {
        delete(docs.doc());
        n++;
        more = docs.next();
      }
    } finally {
      docs.close();
    }

and then it worked as expected (at least attempts to delete a single document from the
index succeeded whereas previously they did not)."
1,"Lock expires almost immediatelyWhen a timeoutHint other than Long.MAX_VALUE is given to the javax.jcr.lock.LockManager API:

   lock(String absPath, boolean isDeep, boolean isSessionScoped, long timeoutHint, String ownerInfo)

a timeoutTime in seconds will be computed as follows (o.a.j.core.lock.LockInfo#updateTimeoutTime):

   long now = (System.currentTimeMillis() + 999) / 1000; // round up
   this.timeoutTime = now + timeoutHint;

the TimeoutHandler in o.a.j.core.lock.LockManagerImpl running every second will then check whether the timeout has expired (o.a.j.core.lock.LockInfo#isExpired):

    public boolean isExpired() {
        return timeoutTime != Long.MAX_VALUE
            && timeoutTime * 1000 > System.currentTimeMillis();
    }

Obviously, the latter condition is true from the very beginning. Replacing '>' with '<' or '<=' should do the trick."
0,"Initializing SeededSecureRandom may be slowFor systems where reading from /dev/random is very slow (so that the alternative seed algorithm is used), initializing the org.apache.jackrabbit.core.id.SeededSecureRandom singleton may be very slow, because it is not synchronized. Each thread that calls SeededSecureRandom.getInstance() will wait up to 1 second until the singleton is initialized.

At the same time, I would like to add more entropy to the alternative seed algorithm.
"
0,"Remove remaining @author references$ find . -name \*.java | xargs grep '@author' | cut -d':' -f1 | xargs perl -pi -e 's/ \@author.*//'
"
0,"DocId.UUIDDocId should not have a string attr uuidAfter JCR-1213 will be solved, lots of DocId.UUIDDocId can be cached, and not being cleaned after every gc(). The number of cached UUIDDocId can grow very large, depending on the size of the repository.  Therefor, instead of storing the private String uuid; we can make it more memory efficient by storing 2 long's, the lsb and msb of the uuid.  Storing 1.000.000 of parent UUIDDocId might differ about 100Mb of memory. 

I even did test by removing the entire uuid string, and not use msb or lsb, because, when everything works properly (with references to index reader segments (See JCR-1213)), the uuid is never needed again: in 

UUIDDocId getDocumentNumber(IndexReader reader) throws IOException {

we could set uuid = null just before the return. It works perfectly well, because when an index reader is recreated, the CachingIndexReader will be recreated, hence DocId[] parents will be recreated. 

So, IMO, I think we might be able to remove the uuid entirely when the docNumber is found in DocId.UUIDDocId (obviously after JCR-1213)

WDOT?

"
0,"Various inner classes maintain references to owning class for no reasonVarious inner classes maintain references to their owning classes for no reason, as they are independent classes. This issue will change these classes to be static inner classes, so that their footprint decreases, they ease gc work, and potentially reduce the lifetime of the owning classes if they outlive their owner."
1,I/O exceptions can cause loss of buffered deletesSome I/O exceptions that result in segmentInfos rollback operations can cause buffered deletes that existed before the rollback creation point to be incorrectly lost when the IOException triggers a rollback.
1,"Deadlock case in IndexWriter on exception just before flushIf a document hits a non-aborting exception, eg something goes wrong
in tokenStream.next(), and, that document had triggered a flush
(due to RAM or doc count) then DocumentsWriter will deadlock because
that thread marks the flush as pending but fails to clear it on
exception.

I have a simple test case showing this, and a fix fixing it."
0,Cut Norms over to DocValuessince IR is now fully R/O and norms are inside codecs we can cut over to use a IDV impl for writing norms. LUCENE-3606 has some [ideas|https://issues.apache.org/jira/browse/LUCENE-3606?focusedCommentId=13160559&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13160559] about how this could be implemented
0,"Implement ""ignoreCookies"" CookieSpecIt would be useful to Implement an ""ignoreCookies"" CookieSpec, as was done in Commons HC 3.1

This should be registered by DefaultHttpClient.createCookieSpecRegistry().

Patch to follow."
0,"Child Axis support in order by clauseHi, 

since child axis is supported in XPath predicates, it would be nice to support it in order by clause as well

Queries of type

//element(*, type) [ foo/@bar ]  order by foo/@bar asc

can become very useful

BR, 

Savvas"
0,"Provide a Method getCredentialsProvider to the SimpleWebdavServletIt will be useful to provide a easy way to change the default CredentialsProvider (BasicCredentialsProvider) when the SessionProvider will be created.
It makes sense to let the SessionProvider return a other CredentialsProvider so that no BasicAuthentication wil be prompt.
thanks
claus"
1,"ZombieHierarchyManager can return wrong child node entries for replaced nodesThe ZombieHierarchyManager currently implements the two getChildNodeEntry methods like this:

1) look up child node in old, overlayed state, which might contain removed child nodes
2) if not found, ask the super implementation (ie. get the child node from the up-to-date list)

The purpose of the ZombieHM is to be able to return removed item ids from the attic. However, the behavior above is IMO wrong, as it should first find an existing child node with the given name (or id):

1) look up child node in super implementation (ie. get the child node from the up-to-date list)
2) if not found, look in the old, overlayed state if it might have been removed

I was able to reproduce this issue when replacing a node (but note the custom access manager in 1.4.x used as explained below): create /replaced/subnode structure, save the session, remove the replaced node and add /replaced and then /replaced/subnode again:

        Node rootNode = session.getRootNode();
        
        // 1. create structure /replaced/subnode
        Node test = rootNode.addNode(""replaced"", NT);
        test.addNode(""subnode"", NT);
        // 2. persist changes
        session.save();

        // 3. remove node and recreate it
        test.remove();
        test = rootNode.addNode(""replaced"", NT);
        
        // 4. create previous child with same name
        test.addNode(""subnode"", NT);
        
        // 5. => gives exception
        test.getNode(""subnode"").getNodes();

To complicate things further, this was only triggered by a custom access manager, and all based upon Jackrabbit 1.4.x. Back then (pre-1.5 and new security stuff era), the access manager would get a ZombieHM as its hierarchy manager. If its implementation called resolvePath() on the HM for checking read-access in the final getNodes() call, where the tree will be traversed using the getChildNdeEntry(NodeState, Name, int) method, it would get the old node id and hence fail if it would try to retrieve it from the real item state manager.

Thus with a Jackrabbit >= 1.5 and 2.0 the above code will work fine, because the ZombieHM is not used.

However, we might want to fix it for 1.4.x and also check the other uses of the ZombieHM in the current trunk, which I couldn't test. These are (explicit and implicit): ChangeLogBasedHierarchyMgr, SessionItemStateManager.getDescendantTransientItemStates(NodeId), ItemImpl.validateTransientItems(Iterable<ItemState>, Iterable<ItemState>) and SessionItemStateManager.getDescendantTransientItemStatesInAttic(NodeId).
"
1,"Wildcard query with no wildcard characters in the term throws StringIndexOutOfBounds exception
Query q1 = new WildcardQuery(new Term(""Text"", ""a""));
Hits hits = searcher.search(q1);


Caught Exception
java.lang.StringIndexOutOfBoundsException : String index out of range: -1
    at java.lang.String.substring(Unknown Source)
    at org.apache.lucene.search.WildcardTermEnum.<init>(WildcardTermEnum.java:65)
    at org.apache.lucene.search.WildcardQuery.getEnum (WildcardQuery.java:38)
    at org.apache.lucene.search.MultiTermQuery.rewrite(MultiTermQuery.java:54)
    at org.apache.lucene.search.IndexSearcher.rewrite(IndexSearcher.java:137)
    at org.apache.lucene.search.Query.weight (Query.java:92)
    at org.apache.lucene.search.Hits.<init>(Hits.java:41)
    at org.apache.lucene.search.Searcher.search(Searcher.java:44)
    at org.apache.lucene.search.Searcher.search(Searcher.java:36)
    at QuickTest.main(QuickTest.java:45)


From Erik Hatcher

Feel free to log this as a bug report in our JIRA issue tracker.  It
seems like a reasonable change to make, such that a WildcardQuery
without a wildcard character would behave like TermQuery."
1,"Http Authentication with invalid credentials causes infinite loopAt HttpMethodBase(460), a break statement is executed only if
log.isInfoEnabled(). The break statement needs to be moved outside of the if
statement so that it breaks if realms already contains foo. Patch submitted on
mailing list as per Apache site guidelines."
0,"Add reopen(IndexCommit) methods to IndexReaderAdd reopen(IndexCommit) methods to IndexReader to be able to reopen an index on any previously saved commit points with all advantages of LUCENE-1483.

Similar to open(IndexCommit) & company available in 2.4.0.
"
1,"OverlappingFileLockException with JRE 1.6Per email discussion:
On Mon, 2007-02-26 at 10:26 +0100, Marcel Reutegger wrote:
> > just my 2c, I didn't really investigated this issue in more detail...
> >
> > according to the javadoc of FileChannel.tryLock() the
> > OverlappingFileLockException is thrown if the JVM already holds a lock on the
> > channel.
> >
> > in contrast, the current check in the repository startup method primarily
> > focuses on the situation where *two* JVMs start a repository on the same home
> > directory.
> >
> > I'd say the OverlappingFileLockException is thrown because two repository
> > instances are startup within the *same* JVM using the same repository home
> > directory.
> >
> > I suggest we add a catch clause, which also covers OverlappingFileLockException
> > in addition to IOException.
> >
> > regards
> >   marcel
> >
> > Stefan Guggisberg wrote:
> > > btw, afaik OverlappingFileLockException is only thrown on linux,
> > > FileChannel#getLock on windows e.g. returns null in the same situation.
> > >
> > > you might want to test on a different platform to further isolate the
> > > issue.
> > > you could also place a breakpoint at the top of the
> > > RepositoryImpl#acquireRepositoryLock
> > > method, step through the code, verify the contents of your fs etc.
> >
>


=== Original email

On 2/19/07, Patrick Haggood <codezilla@> wrote:
I'm using Linux, Sun Java 6 and Jackrabbit 1.3 with Derby persistance.
I have a putNode(object) function that's giving the above error in unit
tests.  It always fails after the second update, even when I swap tests
(i.e. save user doc then save user).  Prior to each test, I delete the
repository directory.

Do I need to set explicit locks before/after each session.save()?

*********** Unit Test
DBConn dbc;

    public SessionUtilTest(String testName) {
        super(testName);
        dbc = new DBConn();
    }

// Note - putUser and putDocument both use putNode after determining
which rootnode will be used

   /**
     * Test of putUnityUser method, of class unityjsr170.jr.SessionUtil.
     */
    public void testPutUnityUser() {
        System.out.println(""putUnityUser"");
        UnityUser usr = usr1;
        SessionUtil instance = dbc.getSutil();
        String result = instance.putUnityUser(usr1);
        assertNotNull(result);
        usr = (UnityUser) instance.getUnityUserByID(result);
        assertEquals(usr1.getName(),usr.getName());
    }
       
    /**
     * Test of putUnityDocument method, of class
unityjsr170.jr.SessionUtil.
     */
    public void testPutUnityDocument() {
        System.out.println(""putUnityDocument"");
        UnityDocument udoc = adr1;
        SessionUtil instance = dbc.getSutil();
        String result = instance.putUnityDocument(udoc);   <---- File
Lock Error
        assertNotNull(result);
        udoc = (UnityDocument) instance.getUnityDocumentByID(result);
        assertEquals(adr1.getName(),udoc.getName());
    }


********* Here's where I setup my repository connection

    public DBConn(){
        sutil = null;
        try {
            rp = new TransientRepository();
            sutil= new SessionUtil(rp);
        } catch (IOException ex) {
            ex.printStackTrace();
        }
    }
    
    public void shutdown(){
        sutil.closeAll();
    }
    
    public SessionUtil getSutil(){
        return sutil;
    }

****************  SessionUtil

    public SessionUtil(Repository rp){
        try {
            session = rp.login(new
SimpleCredentials(""username"",""password"".toCharArray()));
            
        } catch (LoginException ex) {
            ex.printStackTrace();
        } catch (RepositoryException ex) {
            ex.printStackTrace();
        } 
        
    }
    
    public void closeAll(){
        try {
            session.logout();
        } catch (Exception ex) {
            ex.printStackTrace();
            System.out.println(""Error closing repository"");
        }
    }
    
 // Put a node on the tree under the root node, return the uuid of the
new or updated node
    private String putNode(String nodetype, UnityBaseObject ubo){
        String resultuuid =null;
        String uname = ubo.getName();
        String utype = ubo.getType();
        String objectuid = ubo.getId();
        Node pnode; //  node to add or update
        Session ses = null;
        try {
            ses = getSession();
            // Does updateable node already have node Id?
            if (objectuid==null) {
                Node rn = ses.getRootNode();
                pnode = rn.addNode(utype);
                pnode.addMixin(""mix:referenceable"");
            } else{
                // grab existing node by uuid
                pnode = ses.getNodeByUUID(objectuid);
            }
            // Did we get an updateable node?
            if (pnode!=null){
                ubo.setId(pnode.getUUID());
                String unityXML =
utrans.getXMLStringFromUnityBaseObject(ubo);
                // update all the properties
                pnode.setProperty(""name"",ubo.getName());
                pnode.setProperty(""type"",ubo.getType());
                pnode.setProperty(""xmldata"",unityXML);
                ses.save();
                resultuuid = ubo.getId();
            }
        } catch(Exception e) {
            e.printStackTrace();
        } 
        return resultuuid;
    }

    private Session getSession(){
        return session;
    }
    

************  repository.xml

 <Workspace name=""${wsp.name}"">
        <FileSystem
class=""org.apache.jackrabbit.core.fs.local.LocalFileSystem"">
            <param name=""path"" value=""${wsp.home}""/>
        </FileSystem>
        <PersistenceManager
class=""org.apache.jackrabbit.core.state.db.DerbyPersistenceManager"">
            <param name=""url"" value=""jdbc:derby:
${wsp.home}/db;create=true""/>
            <param name=""schemaObjectPrefix"" value=""${wsp.name}_""/>
        </PersistenceManager>
        <SearchIndex
class=""org.apache.jackrabbit.core.query.lucene.SearchIndex"">
            <param name=""path"" value=""${wsp.home}/index""/>
            <param name=""useCompoundFile"" value=""true""/>
            <param name=""minMergeDocs"" value=""100""/>
            <param name=""volatileIdleTime"" value=""3""/>
            <param name=""maxMergeDocs"" value=""100000""/>
            <param name=""mergeFactor"" value=""10""/>
            <param name=""bufferSize"" value=""10""/>
            <param name=""cacheSize"" value=""1000""/>
            <param name=""forceConsistencyCheck"" value=""false""/>
            <param name=""autoRepair"" value=""true""/>
            <param name=""analyzer""
value=""org.apache.lucene.analysis.standard.StandardAnalyzer""/>
        </SearchIndex>
    </Workspace>

"
0,"Enable flexible scoringThis is a first step (nowhere near committable!), implementing the
design iterated to in the recent ""Baby steps towards making Lucene's
scoring more flexible"" java-dev thread.

The idea is (if you turn it on for your Field; it's off by default) to
store full stats in the index, into a new _X.sts file, per doc (X
field) in the index.

And then have FieldSimilarityProvider impls that compute doc's boost
bytes (norms) from these stats.

The patch is able to index the stats, merge them when segments are
merged, and provides an iterator-only API.  It also has starting point
for per-field Sims that use the stats iterator API to compute boost
bytes.  But it's not at all tied into actual searching!  There's still
tons left to do, eg, how does one configure via Field/FieldType which
stats one wants indexed.

All tests pass, and I added one new TestStats unit test.

The stats I record now are:

  - field's boost

  - field's unique term count (a b c a a b --> 3)

  - field's total term count (a b c a a b --> 6)

  - total term count per-term (sum of total term count for all docs
    that have this term)

Still need at least the total term count for each field.
"
0,"[PATCH] tests use 12 for month which is invalidtests create calendar with 12 as a month, which is invalid. December is 11, so use Calendar.DECEMBER instead. - patch fixes this."
1,"Intermittent failure in TestIndexWriter.testCommitThreadSafetyMark's while(1) hudson box found this failure (and I can repro it too):

{noformat}
Error Message

MockRAMDirectory: cannot close: there are still open files: {_1m.cfs=1,
_1k.cfs=1, _14.cfs=1, _1g.cfs=1, _1h.cfs=1, _1f.cfs=1, _1n.cfs=1,
_1i.cfs=1, _1j.cfs=1, _1l.cfs=1}

Stacktrace

java.lang.RuntimeException: MockRAMDirectory: cannot close: there are
still open files: {_1m.cfs=1, _1k.cfs=1, _14.cfs=1, _1g.cfs=1,
_1h.cfs=1, _1f.cfs=1, _1n.cfs=1, _1i.cfs=1, _1j.cfs=1, _1l.cfs=1}
       at
org.apache.lucene.store.MockRAMDirectory.close(MockRAMDirectory.java:282)
       at
org.apache.lucene.index.TestIndexWriter.testCommitThreadSafety(TestIndexWriter.java:4616)
       at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:328)

Standard Output

NOTE: random codec of testcase 'testCommitThreadSafety' was: Sep

Standard Error

The following exceptions were thrown by threads:
*** Thread: Thread-1784 ***
java.lang.RuntimeException: junit.framework.AssertionFailedError: null
       at org.apache.lucene.index.TestIndexWriter$9.run(TestIndexWriter.java:4606)
Caused by: junit.framework.AssertionFailedError: null
       at junit.framework.Assert.fail(Assert.java:47)
       at junit.framework.Assert.assertTrue(Assert.java:20)
       at junit.framework.Assert.assertTrue(Assert.java:27)
       at org.apache.lucene.index.TestIndexWriter$9.run(TestIndexWriter.java:4597)
{noformat}"
1,"Unmatched right parentheses truncates queryThe query processor truncates a query when right parentheses are unmatched.
E.g.:

 secret AND illegal) AND access:confidential

will not result in a ParseException instead will run as:

 secret AND illegal"
0,"Exclude tests instead skipping themjcr2spi tests run with the spi2jcr module by default so they are configured to be skipped when jcr2spi is built. Manually running a jcr2spi test like this

mvn -Dtest=MyTest -Dmaven.test.skip=false test

does not work however. The pom configuration seems to take precedence here. 

To fix this I propose to exclude all test instead of skipping them making it possible to manually execute tests like this

mvn -Dtest=MyTest test
"
0,"Highlighter wraps caching token filters that are not CachingTokenFilter in CachingTokenFilterI figured this was fine and a rare case that you would have another caching tokenstream to feed the highlighter with - but I guess if its happening to you, especially depending on what you are doing - its not an ideal situation."
1,"new QueryParser over-increment position for MultiPhraseQueryIf the new QP is parsing a phrase, and when the analyzer runs on the text within the phrase it produces some tokens with posIncr=0, a MultiPhraseQuery is produced.  But, the positions of the added terms are over-incremented, and don't match what the current QueryParser does."
1,"CheckIndex should allow term position = -1
Spinoff from this discussion:

    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200803.mbox/%3CPine.LNX.4.62.0803292323350.16762@radix.cryptio.net%3E

Right now CheckIndex claims the index is corrupt if you index a Token with -1 position, which happens if your first token has positionIncrementGap set to 0.

But, as far as I can tell, Lucene doesn't ""mind"" when this happens.

So I plan to fix CheckIndex to allow this case.  I'll backport to 2.3.2 as well.

LUCENE-1253 is one example where Lucene's core analyzers could do this."
1,queries with zero boosts don't workQueries consisting of only zero boosts result in incorrect results.
1,"JCR Server has concurrency issues on JcrWebdavServer.SessionCache internal HashMap cachesAfter doing the davex remoting performance work outlined in JCR-3026, the increased concurrency on my jcr server exposed a lot of errors related to getting and putting from the JcrWebdavServer.SessionCache's internal HashMap's.  This problem with HashMap's is a well known concurrency error and was easily fixed by upgrading these maps to ConcurrentHashMaps.  Performance seems dramatically better.  

The fix includes exposure of a tuning parameter that allows the user to set the expected concurrency level.  This is the number of concurrent requests you expect the server to be handling.  In the typical davex remoting scenario, this means you should tune this server side value to match the total max connections of all clients pointed at the server.  See JCR-3026. 

USAGE:  Set the 'concurrency-level' init param for the JcrRemotingServlet, via the web.xml of the jackabbit-webapp component.  Default value is 50.  Or you can intervene in a lower level api if appropriate."
1," inconsistent session and persistent state after ReferentialIntegrityExceptionWhen a ReferentialIntegrityException occurs in a session it seems that subsequent actions on that session may result in a inconsistent session state AND even inconsistent persistent state. The latter will even make jackrabbit fail to bootstrap an index from that persistent state.

Typical rootcause:

Caused by: org.apache.jackrabbit.core.state.NoSuchItemStateException: ddb9d3ea-59c1-4eb4-a83e-332f646d4f40
        at org.apache.jackrabbit.core.state.SharedItemStateManager.getItemState(SharedItemStateManager.java:270)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createIndex(MultiIndex.java:1082)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createIndex(MultiIndex.java:1088)
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createInitialIndex(MultiIndex.java:395)

Bootstrap failure:

java.io.IOException: Error indexing workspace
        at org.apache.jackrabbit.core.query.lucene.MultiIndex.createInitialIndex(MultiIndex.java:402)
        at org.apache.jackrabbit.core.query.lucene.SearchIndex.doInit(SearchIndex.java:465)
        at org.apache.jackrabbit.core.query.AbstractQueryHandler.init(AbstractQueryHandler.java:59)
        at org.apache.jackrabbit.core.SearchManager.initializeQueryHandler(SearchManager.java:553)

"
1,"Highligter fails to include non-token at end of string to be highlightedThe following code extract show the problem


		TermQuery query= new TermQuery( new Term( ""data"", ""help"" )); 
		Highlighter hg = new Highlighter(new SimpleHTMLFormatter(), new QueryScorer( query ));
		hg.setTextFragmenter( new NullFragmenter() );
		
		String match = null;
		try {
			match = hg.getBestFragment( new StandardAnalyzer(), ""data"", ""help me [54-65]"" );
		} catch (IOException e) {
			e.printStackTrace();
		}
		System.out.println( match );


The sytsem outputs 

<B>help</B> me [54-65


would expect 

<B>help</B> me [54-65]



"
0,Introduce SessionInfo parameter for AbstractRepositoryService.createRootNodeDefinition()  SPI implementations might require access to the state of the current session in order to fulfill the contract of AbstractRepositoryService.createRootNodeDefinition(). I therefore suggest to add a SessionInfo parameter to this method. 
0,"JCR Test for Adding Node Type Tests That Abstract Nodes Can Be Added as Children, contrary to JCR 2.0 specificationWhen the TCK test method testLegalAndResidualType in the CanAddChildNodeCallWithNodeTypeTest class picks a node with a residual type, it does not filter out abstract nodes.  For example, in my local test, nt:hierarchyNode is selected for the local variable 'type'.

Since abstract node types ""cannot be directly assigned to a node,""[1] canAddChildNode(anyPropertyName, ""nt:hierarchyNode"") must return false.  However, since the test assumes that a non-abstract node type was chosen, it expects canAddChildNode(String, String) to return true.

This could be fixed if NodeTypeUtil.locateChildNodeDef(...) were extended to add an extra argument allowing or disallowing abstract types and that extra argument was used to filter the type used in testLegalAndResidualType (or if locateChildNodeDef(...) automatically excluded abstract types in the same manner that it automatically excludes protected types).

[1] - Section 3.7.1.3 of the JCR2 specification"
0,"Index File Format - Example for frequency file .frq is wrongReported by Johan Stuyts - http://www.nabble.com/Possible-documentation-error--p7012445.html - 

Frequency file example says: 

     For example, the TermFreqs for a term which occurs once in document seven and three times in document eleven would be the following sequence of VInts: 
         15, 22, 3 

It should be: 

     For example, the TermFreqs for a term which occurs once in document seven and three times in document eleven would be the following sequence of VInts: 
         15, 8, 3 


"
1,"System search manager uses a SessionItemStateManagerAs noted in JCR-2000, the system search manager (responsible for indexing the /jcr:system subtree) uses the SessionItemStateManager instance of the system session instead of the SharedItemStateManager of the underlying default workspace.

This can cause a deadlock (see the thread dumps in JCR-2000) when one thread is accessing the LockManager (that also uses the system session) while another thread is persisting versioning changes.

See the search-on-sism.patch attachment in JCR-2000 for a fix to this issue."
0,"Improved reusability of the JCA packageThe jackrabbit-jca package currently has hardcoded references to jackrabbit-core, which makes it difficult to reuse the packaging and related code with other JCR implementations. With the RepositoryFactory interface from JCR 2.0 we can avoid this hard dependency."
1,"FilterIndexReader doesn't work correctly with post-flex SegmentMergerIndexWriter.addIndexes(IndexReader...) internally uses SegmentMerger to add data from input index readers. However, SegmentMerger uses the new post-flex API to do this, which bypasses the pre-flex TermEnum/TermPositions API that FilterIndexReader implements. As a result, filtering is not applied."
0,"Add support for Map of referenced beansOCM should support the mapping of maps of referenced beans.

@Collection(collectionConverter= BeanReferenceCollectionConverterImpl.class)
private java.util.Map<String, ReferencedBean> aMap;

BeanReferenceCollectionConverterImpl (mainly the method doGetCollection) needs to be updated to support the interface ManageableMap interface.
"
0,"Decouple Filter from BitSet{code}
package org.apache.lucene.search;

public abstract class Filter implements java.io.Serializable 
{
  public abstract AbstractBitSet bits(IndexReader reader) throws IOException;
}

public interface AbstractBitSet 
{
  public boolean get(int index);
}

{code}

It would be useful if the method =Filter.bits()= returned an abstract interface, instead of =java.util.BitSet=.

Use case: there is a very large index, and, depending on the user's privileges, only a small portion of the index is actually visible.
Sparsely populated =java.util.BitSet=s are not efficient and waste lots of memory. It would be desirable to have an alternative BitSet implementation with smaller memory footprint.

Though it _is_ possibly to derive classes from =java.util.BitSet=, it was obviously not designed for that purpose.
That's why I propose to use an interface instead. The default implementation could still delegate to =java.util.BitSet=.

"
0,"Tests need to clean up after themselvesI havent run 'ant clean' for a while.

The randomly generated temporarily file names just piled up from running the tests many times... so ant clean is still running after quite a long time.

We should take the logic in the base solr test cases, and push it into LuceneTestCase, so a test cleans up all its temporary stuff.
"
1,"QNodeTypeDefinitionImpl.getSerializablePropertyDefs() might return non serializable property definitions QNodeTypeDefinitionImpl.getSerializablePropertyDefs() returns a set-version of the passed in parameter, irrespective of whether the property defs are serializable or not.

See http://markmail.org/thread/65ngqvyxnu4nn3su"
0,"Move UnInvertedField into Lucene coreSolr's UnInvertedField lets you quickly lookup all terms ords for a
given doc/field.

Like, FieldCache, it inverts the index to produce this, and creates a
RAM-resident data structure holding the bits; but, unlike FieldCache,
it can handle multiple values per doc, and, it does not hold the term
bytes in RAM.  Rather, it holds only term ords, and then uses
TermsEnum to resolve ord -> term.

This is great eg for faceting, where you want to use int ords for all
of your counting, and then only at the end you need to resolve the
""top N"" ords to their text.

I think this is a useful core functionality, and we should move most
of it into Lucene's core.  It's a good complement to FieldCache.  For
this first baby step, I just move it into core and refactor Solr's
usage of it.

After this, as separate issues, I think there are some things we could
explore/improve:

  * The first-pass that allocates lots of tiny byte[] looks like it
    could be inefficient.  Maybe we could use the byte slices from the
    indexer for this...

  * We can improve the RAM efficiency of the TermIndex: if the codec
    supports ords, and we are operating on one segment, we should just
    use it.  If not, we can use a more RAM-efficient data structure,
    eg an FST mapping to the ord.

  * We may be able to improve on the main byte[] representation by
    using packed ints instead of delta-vInt?

  * Eventually we should fold this ability into docvalues, ie we'd
    write the byte[] image at indexing time, and then loading would be
    fast, instead of uninverting
"
0,"Change default Directory impl on 64bit linux to MMapConsistently in my NRT testing on Fedora 13 Linux, 64 bit JVM (Oracle 1.6.0_21) I see MMapDir getting better search and merge performance when compared to NIOFSDir.

I think we should fix the default."
0,"spi2davex: InvalidItemStateException not properly extracted from ambiguous response errorNodeTest#testSaveInvalidStateException
SessionTest#testSaveInvalidStateException

fail with PathNotFoundException instead of InvalidItemStateException.

i remember that i already addressed that issue in spi2dav a long time ago. with the batched writing in
spi2davex it is back: the server isn't aware of the distinction and just isn't able to retrieve that removed
item... either the client side finds a way to distinguish between path-not-found and externally modified
or we have to leave this as known issue...

in spi2dav i added add quick hack: if the operation was some write operation the path-not-found is
simply converted into invaliditemstateexception."
0,"maxDoc should be explicitly stored in the index, not derived from file lengthThis is a spinoff of LUCENE-140

In general we should rely on ""as little as possible"" from the file system.  Right now, maxDoc is derived by checking the file length of the FieldsReader index file (.fdx) which makes me nervous.  I think we should explicitly store it instead.

Note that there are no known cases where this is actually causing a problem. There was some speculation in the discussion of LUCENE-140 that it could be one of the possible, but in digging / discussion there were no specifically relevant JVM bugs found (yet!).  So this would be a defensive fix at this point."
1,"QueryUtils should check that equals properly handles nullIts part of the equals contract, but many classes currently violate"
1,"jcr2spi: versionmanager#checkout(NodeState) should not forward to checkout(NodeState, NodeId)VersionManager#checkout(NodeState nodeState) is called if activity is not supported and thus should call the
corresponding SPI method instead of checkout(NodeState, NodeId activityId)"
0,"Document Package level javadocs need improvingThe document package package level javadocs could use some improving, such as:
1. Info on what a Document is, as well as Field and Fieldable
2. Examples of FieldSelectors and how to implement
3. Samples of using DateTools and NumberTools"
0,"Some contribs depend on core tests to be compiled and fail when ant clean was done beforeIf you do ""ant clean"" on the root level of Lucene and then go to e.g. contrib/queryparser (3.x only) or contrib/misc (3.x and trunk) and call ""ant test"", the build of tests fails:
- contrib/queryparser's ExtendedableQPTests extend a core TestQueryParser (3.x only, in module this works, of course)
- contrib/misc/TestIndexSplitter uses a core class to build its index

To find the root cause: We should first remove the core tests from contrib classpath, so the issue gets visible even without ""ant clean"" before. Then we can fix this."
0,"TestNLS fails with ja localeset ANT_ARGS=""-Dargs=-Duser.language=ja -Duser.country=JP""
ant test-core -Dtestcase=TestNLS

The test has 2 sets of message, one fallback, and one ja.
The tests assume if it asks for a non-ja locale, that it will get the fallback message,
but this is not how ResourceBundle.getBundle works:
{noformat}
Otherwise, the following sequence is generated from the attribute values of the specified locale 
(language1, country1, and variant1) and of the default locale (language2, country2, and variant2):

baseName + ""_"" + language1 + ""_"" + country1 + ""_"" + variant1
baseName + ""_"" + language1 + ""_"" + country1
baseName + ""_"" + language1
baseName + ""_"" + language2 + ""_"" + country2 + ""_"" + variant2
baseName + ""_"" + language2 + ""_"" + country2
baseName + ""_"" + language2
baseName
{noformat}

So in the case of ja default locale, you get a japanese message instead from the baseName + ""_"" + language2 match"
1,"Writers blocked forever when waiting on update operations  Thread 1 calls Session.save() and has a write lock.

Thread 2 is in XA prepare() and is waiting on thread 1 in FineGrainedISMLocking.acquireWriteLock().

Thread 1's save calls SharedItemStateManager.Update#end() and performs a write-lock downgrade to a read-lock, then (at the end of Update#end()) it calls readLock.release(). FineGrainedISMLocking.ReadLockImpl#release thinks activeWriterId is of the current transation and does not notify any writers (activeWriterId is not being reset on downgrade in what seems to be a related to JCR-2753).
Thread 1 waits forever."
0,"Avoid item state reads during Session.logout()This is a follow up issue for JCR-2231. There's a second CachingHierarchyManager attached to the LocalItemStateManager, which it unregistered too late and causes reads on the SharedItemStateManager on Session logout. The hierarchy manager should be unregistered as listener before the state manager is disposed."
1,"ItemStates in the ChangeLog can not be retrieved in the sequence they were created/modified/deletedThe itemstates are ordered by the hash code.
It's an issue with PersistenceManagers that check referencial integrity (e.g. rdbms)."
1,"ConnectionRecoveryManager is created twice in DBDataStore init methodIt seems that after introducing pool, old initizialization of ConnectionRecoveryManager has not been removed.

Index: DbDataStore.java
===================================================================
--- DbDataStore.java	(revision 605626)
+++ DbDataStore.java	(working copy)
@@ -479,8 +479,6 @@
             initDatabaseType();
             connectionPool = new Pool(this, maxConnections);
             ConnectionRecoveryManager conn = getConnection();
-            conn = new ConnectionRecoveryManager(false, driver, url, user, password);
-            conn.setAutoReconnect(true);
             DatabaseMetaData meta = conn.getConnection().getMetaData();
             log.info(""Using JDBC driver "" + meta.getDriverName() + "" "" + meta.getDriverVersion());
             meta.getDriverVersion();

Duplicated initialization should be removed , but i've never run this code yet."
1,"WebDav MKCOL on a directory that already exists generates a IllegalStateExceptionwhen performing a MKCOL on a resource that already exists, following is thrown.

31.03.2010 16:14:10.760 *ERROR* [127.0.0.1 [1270012450463] MKCOL /org.apache.sling.launchpad.testing-6-SNAPSHOT/apps HTTP/1.1] org.apache.sling.engine.impl.SlingMainServlet service: Uncaught Throwable java.lang.IllegalStateException: Response has already been committed
       at org.apache.sling.engine.impl.SlingHttpServletResponseImpl.checkCommitted(SlingHttpServletResponseImpl.java:398)
       at org.apache.sling.engine.impl.SlingHttpServletResponseImpl.setStatus(SlingHttpServletResponseImpl.java:265)
       at org.apache.jackrabbit.webdav.WebdavResponseImpl.setStatus(WebdavResponseImpl.java:276)
       at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doMkCol(AbstractWebdavServlet.java:548)
       at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(AbstractWebdavServlet.java:256)
       at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(AbstractWebdavServlet.java:196)
       at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
       at org.apache.sling.engine.impl.request.RequestData.service(RequestData.java:523)
....


I think a return after the sendError is required ?
in AbstractWebdavServlet.doMkCol(...) 


   protected void doMkCol(WebdavRequest request, WebdavResponse response,
                          DavResource resource) throws IOException, DavException {

       DavResource parentResource = resource.getCollection();
       if (parentResource == null || !parentResource.exists() || !parentResource.isCollection()) {
           // parent does not exist or is not a collection
           response.sendError(DavServletResponse.SC_CONFLICT);
           return;
       }
       // shortcut: mkcol is only allowed on deleted/non-existing resources
       if (resource.exists()) {
           response.sendError(DavServletResponse.SC_METHOD_NOT_ALLOWED);
+          return;
       }

       if (request.getContentLength() > 0 || request.getHeader(""Transfer-Encoding"") != null) {
           parentResource.addMember(resource, getInputContext(request, request.getInputStream()));
       } else {
           parentResource.addMember(resource, getInputContext(request, null));
       }
       response.setStatus(DavServletResponse.SC_CREATED);
   }



"
0,"bad java practices which affect performance (result of code inspection)IntelliJ IDEA found the following issues in the Lucense source code and tests:

1) explicit for loops where calls to System.arraycopy() should have been
2) calls to Boolean constructor (in stead of the appropriate static method/field)
3) instantiation of unnecessary Integer instances for toString, instead of calling the static one
4) String concatenation using + inside a call to StringBuffer.append(), in stead of chaining the append calls

all minor issues. patch is forthcoming.
"
0,Use bundle persistence in default configurationThe default repository configuration files in jackrabbit-core and -webapp still use the old simple database persistence. They should be updated to use bundle persistence in the 1.4 release.
1,"nt:versionedChild problemProblem occurs when both parent and child beans are versionable.  Jackrabbit creates an nt:versionedChild node that is referenced by the parent node, referencing the childs versionedHistory node of the child.  The current OCM code does not handle this correctly and produces an error:  ""Node type  'nt:versionedChild' does not match descriptor node type 'nt:unstructured'""

Below is a example code of the problem and a patch that appears to correctly resolve the problem.


 Within ObjectConverterImpl created the below method.

        public Node getActualNode(Session session,Node node) throws
 RepositoryException
        {
                NodeType type = node.getPrimaryNodeType();
                if (type.getName().equals(""nt:versionedChild""))
                {

                        String uuid =
 node.getProperty(""jcr:childVersionHistory"").getValue().getString();
                        Node actualNode = session.getNodeByUUID(uuid);
                        String name = actualNode.getName();
                        actualNode = session.getNodeByUUID(name);

                        return actualNode;
                }

                return node;
        }

 AND modified the following to call the above method


        public Object getObject(Session session, Class clazz, String path)
        {
                try {
                        if (!session.itemExists(path)) {
                                return null;
                        }

                        if (requestObjectCache.isCached(path))
                    {
                        return requestObjectCache.getObject(path);
                    }

                        ClassDescriptor classDescriptor =
 getClassDescriptor(clazz);

                        checkNodeType(session, classDescriptor);

                        Node node = (Node) session.getItem(path);
                        if (!classDescriptor.isInterface()) {
                                {
                                node = getActualNode(session,node);
                                checkCompatiblePrimaryNodeTypes(session,
 node, classDescriptor, true);
                                }
                        }

                        ClassDescriptor alternativeDescriptor = null;
                        if
 (classDescriptor.usesNodeTypePerHierarchyStrategy())
 {
                                if
 (node.hasProperty(ManagerConstant.DISCRIMINATOR_PROPERTY_NAME))
 {
                        String className =
 node.getProperty(ManagerConstant.DISCRIMINATOR_PROPERTY_NAME
 ).getValue().getString();
                        alternativeDescriptor =
 getClassDescriptor(ReflectionUtils.forName(className));
                                }
                        } else {
                                if
 (classDescriptor.usesNodeTypePerConcreteClassStrategy())
 {
                                        String nodeType =
 node.getPrimaryNodeType().getName();
                                        if
 (!nodeType.equals(classDescriptor.getJcrType()))
 {
                                            alternativeDescriptor =
 classDescriptor.getDescendantClassDescriptor(nodeType);

                                            // in case we an alternative
 could not be found by walking
                                            // the class descriptor
 hierarchy, check whether we
 would
                                            // have a descriptor for the
 node type directly (which
                                            // may the case if the class
 descriptor hierarchy is
                                            // incomplete due to missing
 configuration. See JCR-1145
                                            // for details.
                                            if (alternativeDescriptor ==
 null) {
                                                alternativeDescriptor =
 mapper.getClassDescriptorByNodeType(nodeType);
                                            }
                                        }
                                }
                        }

                        // if we have an alternative class descriptor,
 check whether its
                        // extends (or is the same) as the requested class.
                        if (alternativeDescriptor != null) {
                            Class alternativeClazz =
 ReflectionUtils.forName(alternativeDescriptor.getClassName());
                            if (clazz.isAssignableFrom(alternativeClazz)) {
                                clazz = alternativeClazz;
                                classDescriptor = alternativeDescriptor;
                            }
                        }

                        // ensure class is concrete (neither interface nor
 abstract)
                        if (clazz.isInterface() ||
 Modifier.isAbstract(clazz.getModifiers())) {
                            throw new JcrMappingException( ""Cannot
 instantiate non-concrete
 class "" + clazz.getName()
                        + "" for node "" + path + "" of type "" +
 node.getPrimaryNodeType().getName());
                        }

            Object object =
 ReflectionUtils.newInstance(classDescriptor.getClassName());

            if (! requestObjectCache.isCached(path))
            {
                          requestObjectCache.cache(path, object);
            }

            simpleFieldsHelp.retrieveSimpleFields(session,
 classDescriptor, node, object);
                        retrieveBeanFields(session, classDescriptor, node,
 path, object, false);
                        retrieveCollectionFields(session, classDescriptor,
 node, object, false);

                        return object;
                } catch (PathNotFoundException pnfe) {
                        // HINT should never get here
                        throw new
 ObjectContentManagerException(""Impossible to get
 the object
 at "" + path, pnfe);
                } catch (RepositoryException re) {
                        throw new
 org.apache.jackrabbit.ocm.exception.RepositoryException(""Impossible to
 get the object at "" + path, re);
                }
        }




>
>
>
> > I am building a test application against OCM.  I have the following
> > classes that are annotated for OCM.  The problem is that when I update
> and
> > version the root object PressRelease the Bean Author is versioned to
> > nt:versionedChild.  While the OCM is checking for node type
> compatibility
> > it is throwing the following exception.  It looks like the
> versionedChild
> > is not handled correctly.  Any suggestions?
> >
> > I also attempted to retrieve the version based on the version name for
> the
> > rootVersion but also trapped. From a Version object how should I access
> > each of the versioned entries?
> >
> > Thanks
> > Wes
> >
> > @Node (jcrMixinTypes=""mix:versionable"")
> > public class PressRelease
> > {
> >       @Field(path=true) String path;
> >       @Field String title;
> >       @Field Date pubDate;
> >       @Field String content;
> >       @Bean Author author;
> >       @Collection (elementClassName=Comment.class) List<Comment>
> comments = new
> > ArrayList<Comment>();
> >
> >       public String getPath() {
> >               return path;
> >       }
> >       public void setPath(String path) {
> >               this.path = path;
> >       }
> >       public String getContent() {
> >               return content;
> >       }
> >       public void setContent(String content) {
> >               this.content = content;
> >       }
> >       public Date getPubDate() {
> >               return pubDate;
> >       }
> >       public void setPubDate(Date pubDate) {
> >               this.pubDate = pubDate;
> >       }
> >       public String getTitle() {
> >               return title;
> >       }
> >       public void setTitle(String title) {
> >               this.title = title;
> >       }
> >       public Author getAuthor() {
> >               return author;
> >       }
> >       public void setAuthor(Author author) {
> >               this.author = author;
> >       }
> >       public List<Comment> getComments() {
> >               return comments;
> >       }
> >       public void setComments(List<Comment> comments) {
> >               this.comments = comments;
> >       }
> >
> >
> > }
> >
> > @Node (jcrMixinTypes=""mix:versionable"")
> > public class Author {
> >
> >       @Field(path=true) String path;
> >       @Field String name;
> >
> >
> >       public String getName() {
> >               return name;
> >       }
> >       public void setName(String name) {
> >               this.name = name;
> >       }
> >       public String getPath() {
> >               return path;
> >       }
> >       public void setPath(String path) {
> >               this.path = path;
> >       }
> >
> > }
> >
> > MAIN
> >
> >       while (versionIterator.hasNext())
> >       {
> >           Version version = (Version) versionIterator.next();
> >           System.out.println(""version found : ""+ version.getName() + "" -
> "" +
> >                                 version.getPath() + "" - "" +
> > version.getCreated().getTime());
> >
> >
> >           if (!version.getName().equals(""jcr:rootVersion""))
> >           {
> >
> > //      Get the object matching to the first version
> >           pressRelease = (PressRelease)
> > ocm.getObject(""/newtutorial"",version.getName());
> >
> >
> >               System.out.println(""PressRelease title : "" +
> pressRelease.getTitle());
> >               System.out.println(""             author: "" +
> > pressRelease.getAuthor().getName());
> >               System.out.println(""            content: "" +
> pressRelease.getContent());
> >               List comments = pressRelease.getComments();
> >               Iterator iterator = comments.iterator();
> >               while (iterator.hasNext())
> >               {
> >                       comment = (Comment) iterator.next();
> >                       System.out.println(""Comment : <"" + comment.getData()
> + "">"" +
> > comment.getText());
> >               }
> >           }
> >       }
> >
> >
> > CONSOLE
> > version found : jcr:rootVersion -
> >
> /jcr:system/jcr:versionStorage/fc/0b/fd/fc0bfd89-c487-4fbe-930f-d837e5dfed79/jcr:rootVersion
> > - Thu Feb 28 15:54:42 EST 2008
> > version found : 1.0 -
> >
> /jcr:system/jcr:versionStorage/fc/0b/fd/fc0bfd89-c487-4fbe-930f-d837e5dfed79/1.0
> > - Thu Feb 28 15:54:59 EST 2008
> > Exception in thread ""main""
> > org.apache.jackrabbit.ocm.exception.ObjectContentManagerException:
> Cannot
> > map object of type 'com..pc.repository.Author'. Node type
> > 'nt:versionedChild' does not match descriptor node type
> 'nt:unstructured'
> >       at
> >
> org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverterImpl.checkCompatiblePrimaryNodeTypes
> (ObjectConverterImpl.java:552)
> >       at
> >
> org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverterImpl.getObject
> (ObjectConverterImpl.java:361)
> >       at
> >
> org.apache.jackrabbit.ocm.manager.beanconverter.impl.DefaultBeanConverterImpl.getObject
> (DefaultBeanConverterImpl.java:80)
> >       at
> >
> org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverterImpl.retrieveBeanField
> (ObjectConverterImpl.java:666)
> >       at
> >
> org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverterImpl.retrieveBeanFields
> (ObjectConverterImpl.java:621)
> >       at
> >
> org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverterImpl.getObject
> (ObjectConverterImpl.java:309)
> >       at
> >
> org.apache.jackrabbit.ocm.manager.impl.ObjectContentManagerImpl.getObject(
> ObjectContentManagerImpl.java:313)
> >       at com.pc.repository.Main.main(Main.java:345)






"
0,"Make DirectoryTaxonomyWriter's indexWriter member privateDirectoryTaxonomyWriter has a protected indexWriter member. As far as I can tell, for two reasons:

# protected openIndexWriter method which lets you open your own IW (e.g. with a custom IndexWriterConfig).
# protected closeIndexWriter which is a hook for letting you close the IW you opened in the previous one.

The fixes are trivial IMO:
# Modify the method to return IW, and have the calling code set DTW's indexWriter member
# Eliminate closeIW. DTW already has a protected closeResources() which lets you clean other resources you've allocated, so I think that's enough.

I'll post a patch shortly."
0,"Consolidate Lucene's QueryParsers into a moduleLucene has a lot of QueryParsers and we should have them all in a single consistent place.  

The following are QueryParsers I can find that warrant moving to the new module:

- Lucene Core's QueryParser
- AnalyzingQueryParser
- ComplexPhraseQueryParser
- ExtendableQueryParser
- Surround's QueryParser
- PrecedenceQueryParser
- StandardQueryParser
- XML-Query-Parser's CoreParser

All seem to do a good job at their kind of parsing with extensive tests.

One challenge of consolidating these is that many tests use Lucene Core's QueryParser.  One option is to just replicate this class in src/test and call it TestingQueryParser.  Another option is to convert all tests over to programmatically building their queries (seems like alot of work)."
1,"ChangeLog serialization causes cache inconsistenciesThe ordering of actions is taken into account when a ChangeLog is built through session manipulations (see, for instance,  ChangeLog.deleted(ItemState state)). When it is serialized in ClusterNode.write(Record record, ChangeLog changeLog, EventStateCollection esc), however, this implicit ordering might be changed. As a consequence,  the deserialization in ClusterNode.consume(Record record) might produce a different ChangeLog with the effect that the local caches get out-of-sync with the persistent state of the repository.

The issue should be reproducable as follows:
- Setup a clustered environment with two Jackrabbit instances, say A and B.
- On instance A add a property ""P"" with value ""x"" to some node and save the session.
- On instance B read property ""P"" -> it will have value ""x"".
- On instance A delete property P and then add it again with value ""y"" and save the session.
- On instance B read property ""P"" -> it will still have value ""x"" after the cluster sync..."
1,"incorrect definition of built-in node type nt:hierarchyNodethe property jcr:created of nt:hierarchyNode should be non-mandatory according to the specification (jcr 1.0 and jcr 1.0.1).
both the definition of the built-in node type and the related test case should be fixed accordingly."
0,"TCK: DocumentViewImportTest does not call refresh after direct-to-workspace importAfter performing a direct-to-workspace import, the test does not call refresh to ensure the transient layer doesn't contain stale data.

Proposal: call refresh(false) after performing direct-to-workspace imports.

--- DocumentViewImportTest.java (revision 422074)
+++ DocumentViewImportTest.java (working copy)
@@ -106,6 +106,12 @@
             SAXException, NotExecutableException {
  
         importXML(target, createSimpleDocument(), uuidBehaviour, withWorkspace);
+
+        if (withWorkspace)
+        {
+          session.refresh(false);
+        }
+
         performTests();
     }
  
@@ -127,6 +133,12 @@
             SAXException, IOException, NotExecutableException {
  
         importWithHandler(target, createSimpleDocument(), uuidBehaviour, withWorkspace);
+
+        if (withWorkspace)
+        {
+          session.refresh(false);
+        }
+
         performTests();
     }
"
0,"Improved XML export handlingAs mentioned in JCR-1574, the current XML export functionality is generating workarounds like the new PropertyWrapper class. I'd like to refactor and clean up the XML export stuff so that such workarounds wouldn't be needed.

An additional bonus would be to make both core and jcr2spi use the same XML export mechanism. For example the one in core already supports JSR 283 shareable nodes, but the one in jcr2spi does not."
0,"[PATCH] Clear ThreadLocal instances in close()As already found out in LUCENE-436, there seems to be a garbage collection problem with ThreadLocals at certain constellations, resulting in an OutOfMemoryError.
The resolution there was to remove the reference to the ThreadLocal value when calling the close() method of the affected classes (see FieldsReader and TermInfosReader).
For Java < 5.0, this can effectively be done by calling threadLocal.set(null); for Java >= 5.0, we would call threadLocal.remove()

Analogously, this should be done in *any* class which creates ThreadLocal values

Right now, two classes of the core API make use of ThreadLocals, but do not properly remove their references to the ThreadLocal value
1. org.apache.lucene.index.SegmentReader
2. org.apache.lucene.analysis.Analyzer

For SegmentReader, I have attached a simple patch.
For Analyzer, there currently is no patch because Analyzer does not provide a close() method (future to-do?)

"
1,"JCA Concurrent Modification Exception when JCAManagedConnection.cleanup() calledThe JCAManagedConnection.closeHandles() method causes a ConcurrentModificationException if the handles list is not empty.
This is caused by modification of the handles list by removeHandle(), while closeHandles() is iterating over the list.

Under SunOne AppServer 7 this can be caused simply by not closing the Session handle before the transaction commits.

It is probably not even necessary to send connectionClosed events during cleanup().  According to the API for connectionClosed, the event indicates that an application component has closed  the connection handle.  cleanup() is a container initiated action, and so the connectionClosed event is not applicable.


java.util.ConcurrentModificationException
    at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:552)
    at java.util.LinkedList$ListItr.next(LinkedList.java:488)
    at org.apache.jackrabbit.jca.JCAManagedConnection.closeHandles(JCAManagedConnection.java:382)
    at org.apache.jackrabbit.jca.JCAManagedConnection.cleanup(JCAManagedConnection.java:145)
    at com.sun.enterprise.resource.IASPoolObjectImp.cleanup(IASPoolObjectImp.java:243)
    at com.sun.enterprise.resource.IASGenericPoolObjects.transactionCompleted(IASGenericPoolObjects.java:794)
    at com.sun.enterprise.resource.ResourcePoolManagerImpl.transactionCompleted(ResourcePoolManagerImpl.java:347)
    at com.sun.enterprise.resource.ResourcePoolManagerImpl$SynchronizationListener.afterCompletion(ResourcePoolManagerImpl.java:644)
    at com.sun.jts.jta.SynchronizationImpl.after_completion(SynchronizationImpl.java:70)

"
0,"User-defined ProtocolSocketFactory for secure connection through proxyI use a custom socket implementation with HttpClient, and am having problems 
getting secure connections through a proxy working.

HttpClient requires that my SecureProtocolSocketFactory be able to create a 
secure socket layered over an existing insecure socket, but does not specify 
how that insecure socket is created. Currently, for secure proxied 
connections, the insecure connection is always created using a 
DefaultProtocolSocketFactory (HttpConnection.java, line 702). I wish to be 
able to override this behaviour, so that I can create the insecure socket 
using my own custom implementation.

The problem with the default behaviour is that my custom socket implementation 
is written in C++ using JNI, and the SSL implementation is handled at the 
native level. Hence, layering over an existing JDK socket will not work.

My proposed solution is to add an HTTP connection parameter to specify the 
socket factory to use, perhaps http.connection.insecuresocketfactory of type 
Class."
1,"If there is more than 15 seconds between HttpClient.execute() calls using a MultipartEntity, a ProtocolException is thrown complaining about the Content-Length header already being present.I am not sure if this time-related behaviour is intentional or not (I have only been using this library for a few weeks) , but even if a timeout is to be expected, the exception thrown ought to indicate that there is a time component involved. ""org.apache.http.ProtocolException: Content-Length header already present"" is incredibly misleading. 

A simple-ish compileable program to reproduce the bug is as follows:

import java.nio.charset.Charset;
import org.apache.http.HttpResponse;
import org.apache.http.client.methods.HttpPost;
import org.apache.http.client.params.ClientPNames;
import org.apache.http.client.params.CookiePolicy;
import org.apache.http.entity.mime.MultipartEntity;
import org.apache.http.entity.mime.content.StringBody;
import org.apache.http.impl.client.DefaultHttpClient;
public class Simple {
    static public void main(String [] args)
    {
        try
        {
            DefaultHttpClient client = new DefaultHttpClient();
            client.getParams().setParameter(
                        ClientPNames.COOKIE_POLICY, CookiePolicy.BROWSER_COMPATIBILITY);
            MultipartEntity entity;
            StringBody stringBody;
            HttpPost post;
            HttpResponse response;
            entity = new MultipartEntity();
            stringBody = new StringBody(""field contents"",Charset.forName(""ISO-8859-1""));
            entity.addPart(""field"", stringBody);  
            post = new HttpPost(""http://localhost/simple.php"");
            post.setEntity(entity); 
            response = client.execute(post);
            
            //The exception does not occur if the content is not consumed
            response.getEntity().consumeContent();
            System.out.println(""First post done"");
            
            //The exception does not occur if the time interval between the requests is too short
            Thread.sleep(15000);
            
            //The exception naturally doesn't occur if a new HttpClient is created
            //client = new DefaultHttpClient();

            entity = new MultipartEntity();
            stringBody = new StringBody(""field contents"",Charset.forName(""ISO-8859-1""));
            entity.addPart(""field"", stringBody);  

            post = new HttpPost(""http://localhost/simple.php"");
            post.setEntity(entity); 
            response = client.execute(post); //Will throw the following:
            /*
                org.apache.http.ProtocolException: Content-Length header already present
                at org.apache.http.protocol.RequestContent.process(RequestContent.java:70)
                at org.apache.http.protocol.BasicHttpProcessor.process(BasicHttpProcessor.java:290)
                at org.apache.http.protocol.HttpRequestExecutor.preProcess(HttpRequestExecutor.java:160)
                at org.apache.http.impl.client.DefaultClientRequestDirector.execute(DefaultClientRequestDirector.java:356)
                at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:501)
                at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:456)
                at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:422)
                at test.Simple.main(Simple.java:57)
             */ 
            System.out.println(""Second post done"");
        }
        catch(Exception e)
        {
            System.out.println(e);
            e.printStackTrace();
        }
    }    
}"
0,"URI.java readObject()/writeObject() must be privateIn the class org.apache.commons.httpclient.URI, the readObject/writeObject methods are currently protected - they need to be private, or Java will not invoke them."
0,Move Function grouping collectors from Solr to grouping moduleMove the Function*Collectors from Solr (inside Grouping source file) to grouping module.
0,"allow an alg file to specify the default codecI already committed this one by accident so I better open an issue!

I added this:

  default.codec = Pulsing

so that your alg file can specify the codec to be used when writing new segments in an index."
0,"Remove lib directory from SVN trunkbuild.xml expects to find junit.jar in the lib directory for building and running tests.

The jar is not included in SVN, but nor is the jar ignored, so when it is downloaded it shows up as an unversioned file.

The file should be included in or excluded from SVN.

==

Note: In JMeter we use a lib/opt directory.
This is present in SVN - but all contents are ignored.

This can be used for extra jars that cannot be or are not included in SVN.

Could use the same approach for junit.jar..."
0,"FileDataStore Garbage Collector and empty directoriesWhen the org.apache.jackrabbit.core.data.GarbageCollector is called for a FileDataStore the file objects are correctly deleted.
But the (sub)directories aren't removed.
In time this will result in a huge tree of unused empty directories

I've created a small chance in method FileDataStore.deleteOlderRecursive()
It will remove a directory when it hasn't any files entries. Please note that currently the file objects are stored three levels deep, so it
will take three gc calls remove all directories. Which I think is no problem because the currently implementation is lightweighted.

>>>>> CURRENT FileDataStore.java

    private int deleteOlderRecursive(File file, long min) {
        int count = 0;
        if (file.isFile() && file.exists() && file.canWrite()) {
            if (file.lastModified() < min) {
                DataIdentifier id = new DataIdentifier(file.getName());
                if (!inUse.containsKey(id)) {
                    file.delete();
                    count++;
                }
            }
        } else if (file.isDirectory()) {
            File[] list = file.listFiles();
            for (int i = 0; i < list.length; i++) {
                count += deleteOlderRecursive(list[i], min);
            }
        }
        return count;
    }

>>>>>>> NEW

    private int deleteOlderRecursive(File file, long min) {
        int count = 0;
        if (file.isFile() && file.exists() && file.canWrite()) {
            if (file.lastModified() < min) {
                DataIdentifier id = new DataIdentifier(file.getName());
                if (!inUse.containsKey(id)) {
                    file.delete();
                    count++;
                }
            }
        } else if (file.isDirectory()) {
            File[] list = file.listFiles();
            if (list.length==0) {
              file.delete();
            } else {
              for (int i = 0; i < list.length; i++) {
                count += deleteOlderRecursive(list[i], min);
              }
            }
        }
        return count;
    }
"
0,"SpellChecker has no ""close"" methodSpellChecker has no close method ... which means there is no way to force it to close the IndexSearcher it maintains when you are done using the SpellChecker.  (a quick skim of IndexSearcher doesn't even suggest there is a finalizer self closing in the event of GC)

http://www.nabble.com/SpellChecker-locks-folder-to23171980.html#a23171980

A hackish work around for people who want to force SpellChecker to close an IndexSearcher opened against a directory they care about doing something with... 
{code}yourSpellChecker.setSpellIndex(new RamDirecotry()){code}"
1,"Inconsistent scoring with SpanTermQuery in BooleanQueryWhen a SpanTermQuery is added to a BooleanQuery, incorrect results are 
returned.

I am running Lucene 1.9 RC1 on Windows XP.  I have a test case which has 
several tests.  It has an index with 4 identical documents in it.

When two TermQuerys are used in a BooleanQuery, the score looks like this:
  4 hits for search: two term queries
    ID:1 (score:0.54932046)
    ID:2 (score:0.54932046)
    ID:3 (score:0.54932046)
    ID:4 (score:0.54932046)

Notice how it is correctly setting the score to be the same for each document.

When two SpanQuerys are used in a BooleanQuery, the score looks like this:
  2 hits for search: two span queries
    ID:1 (score:0.3884282)
    ID:4 (score:0.1942141)

Notice how it only returned two documents instead of four.  And the two it did 
return have differing scores.

I believe that there is an error in the scoring algorithm that is making the 
other two documents not show up."
0,"contrib/benchmark tests fail find data dirsThis was exposed by LUCENE-940 - a test was added that uses the Reuters collection. Then tests succeed when ran from contrib/benchmark (e.g. by IDE) but fail when running as part of ""ant test-contrib"" because the test expects to find the Reuters data under trunk/work. 
"
1,"Several Codecs use the same files - PerFieldCodecWrapper can not hold two codec using the same filesCurrently we have a rather simple file naming scheme which prevents us from using more than one codec in a segment that relies on the same file.  For instance pulsing and standard codec can not be used together since they both need the .frq .tii .tis etc. To make this work we either need to write distinct per codec files or set a per field / codec file ID. While the first solution seems to be quiet verbose the second one seems to be more flexible too.

One possibility to do that would be to assign a unique id to each SegmentsWriteState when opening the FieldsConsumer and write the IDs into the segments file to eventually load it once the segment is opened. Otherwise our PerFieldCodec feature will not be really flexible nor useful though.  "
0,"Extend the IndexingConfiguration to allow configuration of reuseable analyzersTo the indexing_configuration.xml a xml block of analyzers should be configurable. In each <index-rule> to a property an analyzer can be assigned. This means, that property will be analyzed with that specific analyzer. In the first place, it enables multilingual indexing. 

Documentation needs to be added explaining the difference in searching in the node scope [jcr:contains(.,'foo')] and in some property [jcr:contains(@myprop,'foo')]. The node scope will always be searched and indexed with the default analyzer, which can be configured in the workspace.xml in  the  <SearchIndex> element.

Below a possible indexing_configuration.xml snippet is shown. Also node the possible enhancement (not sure wether this implementation will have it, because it requires a lot of filter Factories and is probably out of scope). Adding custom filters which do not need a factory might be easier.

<analyzers>
	<analyzer name=""fr"" class=""org.apache.lucene.analysis.fr.FrenchAnalyzer""/>
	<analyzer name=""de"" class=""org.apache.lucene.analysis.de.GermanAnalyzer""/>
        <analyzer name=""compound"" class=""org.apache.lucene.analysis.SimpleAnalyzer"">
             <filter class=""jr.StopFilterFactory"" words=""stopwords.txt""/>
             <filter class=""jr.EdgeNGramTokenizerFactory"" side=""front"" minGram=""1"" maxGram=""2""/>
        </analyzer>
</analyzers>

<index-rule nodeType=""nt:unstructured"">
       <property analyzer=""fr"">bode_fr</property>
       <property analyzer=""de"">bode_de</property>
</index-rule>"
0,"Support for MaxDB / SapSB DatabasesI admit that MaxDB / SapSB are a bit exotic but support is easy to achieve when providing the correct ddls. 
"
0,"add a tokenfilter for icu transformsI pulled the ICUTransformFilter out of LUCENE-1488 and create an issue for it here.

This is a tokenfilter that applies an ICU Transliterator, which is a context-sensitive way
to transform text. 

These are typically rule-based and you can use ones included with ICU (such as Traditional-Simplified)
or you can make your own from your own set of rules.

User's Guide: http://userguide.icu-project.org/transforms/general
Rule Tutorial: http://userguide.icu-project.org/transforms/general/rules
"
1,"Constants.LUCENE_MAIN_VERSION is inlined in code compiled against Lucene JAR, so version detection is incorrectWhen you compile your own code against the Lucene 2.9 version of the JARs and use the LUCENE_MAIN_VERSION constant and then run the code against the 3.0 JAR, the constant still contains 2.9, because javac inlines primitives and Strings into the class files if they are public static final and are generated by a constant (not method).

The attached fix will fix this by using a ident(String) functions that return the String itsself to prevent this inlining.

Will apply to 2.9, trunk and 2.9 BW branch. No I can also reenable one test I removed because of this."
0,"CMS should default its maxThreadCount to 1 (not 3)From rough experience, I think the current default of 3 is too large.  I think we get the most bang for the buck going from 0 to 1.

I think this will especially impact optimize on an index with many segments -- in this case the MergePolicy happily exposes concurrency (multiple pending merges), and CMS will happily launch 3 threads to carry that out."
1,"Deadlock for some Query objects in the equals method (f.ex. PhraseQuery) in a concurrent environmentSome Query objects in lucene 2.3.2 (and previous versions) have internal variables using Vector.   These variables are used during the call to the equals method.   In a concurrent environment a deadlock might occur.    The attached code example shows this happening in lucene 2.3.2, but the patch in LUCENE-1346 fixes this issue (though that doesn't seem to be the intention of that patch according to the description :-)"
0,"New Token filter for adding payloads ""in-stream""This TokenFilter is able to split a token based on a delimiter and use one part as the token and the other part as a payload.  This allows someone to include payloads inline with tokens (presumably setup by a pipeline ahead of time).  An example is apropos.  Given a | delimiter, we could have a stream that looks like:
{quote}The quick|JJ red|JJ fox|NN jumped|VB over the lazy|JJ brown|JJ dogs|NN{quote}

In this case, this would produce tokens and payloads (assuming whitespace tokenization):
Token: the
Payload: null

Token: quick
Payload: JJ

Token: red
Pay: JJ.

and so on.

This patch will also support pluggable encoders for the payloads, so it can convert from the character array to byte arrays as appropriate."
0,"Jcr-Server: Improve implementation of DavResource#getProperty(DavPropertyName)this issue has already been described in JCR-397

problem: even if only a subset of properties has been requested by the client dav resource initializes the complete set of properties."
1,"User-Agent string violates RFCOur User-Agent says ""Jakarta Commons-HttpClient/3.1-rc1"". But space is a reserved character to separate individual *products* and comments according to RFC 2616, section 14.43. Jakarta is not a product. At the same time we may want to drop the Jakarta name altogether.

We should change this to something more standard like: 

""Apache-HttpClient/3.1-rc1 (""+ System.getProperty(""os.name"") +"";""+ System.getProperty(""os.arch"") +"") ""+
""Java/""+ System.getProperty(""java.vm.version"") +"" (""+ System.getProperty(""java.vm.vendor"") +"")""

which renders:

""Apache-HttpClient/3.1-rc1 (Windows XP 5.1;x86) Java/1.5.0_08 (Sun Microsystems Inc.)""

Sun's internal Http client uses something like ""Java/1.5.0_08"".

I am completely ignoring the fact that real-world user agents use almost arbitrary strings.
Some fine examples of misbehaviour from my private logs:

""Jakmpqes dihurxf wfyiupsc"" -- apparently somebody has to hide something...
""Missigua Locator 1.9""
""Poodle predictor 1.0""
""shelob v1.0""
""ISC Systems iRc Search 2.1""
""ping.blogug.ch aggregator 1.0""
""http://www.uni-koblenz.de/~flocke/robot-info.txt""  -- ...sigh

I am very tempted to write a User-Agent string validator that prevents misuse of this field in HttpClient."
0,"Initialize hierarchy cache on startupIn some cases it may be desirable to initialize the hierarchy cache in the search index on startup. Currently this initialization is done in the background. For larger workspaces, this puts considerable load on the repository and may slow down access and queries. There should be a configuration parameter that forces the repository to initialize the hierarchy cache on startup and only return the repository instance when the initialization is completed.  The default value would be the current behaviour (using background thread)."
1,"initVersions crashes with NPEAfter delete some old versions. I get serious problems accessing the version history.
This is the stacktrace:
java.lang.NullPointerException
	at org.apache.jackrabbit.core.version.VersionIteratorImpl.initVersions(VersionIteratorImpl.java:169)
	at org.apache.jackrabbit.core.version.VersionIteratorImpl.<init>(VersionIteratorImpl.java:87)
	at org.apache.jackrabbit.core.version.VersionIteratorImpl.<init>(VersionIteratorImpl.java:72)
	at org.apache.jackrabbit.core.version.VersionHistoryImpl.getAllVersions(VersionHistoryImpl.java:92)

I stepped threw the code and see that the Method 
    currentVersion.getSuccessors() 
returns an empty Array.

After all the VersionHistory seems to be corrupt!!"
0,Annotation based implementation of jackrabbit ocmwe have created an annotation based implementation of jackrabbit-ocm that can be used instead of the digester one
0,"simple webdav server does not support lock timeoutsThe ""simple"" WebDAV server still does not support lock timeouts (HTTP trace shows MS word requests 3600s timeout but gets infinity)."
0,"Add NumericField, make plain text numeric parsers public in FieldCache, move trie parsers to FieldCacheIn discussions about LUCENE-1673, Mike & me wanted to add a new NumericField to o.a.l.document specific for easy indexing. An alternative would be to add a NumericUtils.newXxxField() factory, that creates a preconfigured Field instance with norms and tf off, optionally a stored text (LUCENE-1699) and the TokenStream already initialized. On the other hand NumericUtils.newXxxSortField could be moved to NumericSortField.

I and Yonik tend to use the factory for both, Mike tends to create the new classes.

Also the parsers for string-formatted numerics are not public in FieldCache. As the new SortField API (LUCENE-1478) makes it possible to support a parser in SortField instantiation, it would be good to have the static parsers in FieldCache public available. SortField would init its member variable to them (instead of NULL), so making code a lot easier (FieldComparator has this ugly null checks when retrieving values from the cache).

Moving the Trie parsers also as static instances into FieldCache would make the code cleaner and we would be able to hide the ""hack"" StopFillCacheException by making it private to FieldCache (currently its public because NumericUtils is in o.a.l.util)."
0,"Timeout for Session and/or LockI think there needs to be a mechanism where we can set the timeout for a particular jcr Session.  Or at the most, there should be a provision to set a timeout for a lock on a node.

Hope this is implemented soon.

Thanks."
0,"Cookie class cannot handle IPv6 literalsWhen performing requests using IPv6 literals, Cookie.setDomain() will attempt to trim the port number by cutting off the domain string at the first colon. This leads to MalformedCookieExceptions being thrown by CookieSpecBase later on."
1,"Highlight fragment does not extend to maxDocCharsToAnalyzeThe current highlighter code checks whether the total length of the text to highlight is strictly smaller than maxDocCharsToAnalyze before adding any text remaining after the last token to the fragment. This means that if maxDocCharsToAnalyse is set to exactly the length of the text and the last token of the text is the term to highlight and is followed by non-token text, this non-token text will not be highlighted.

For example, consider the phrase ""this is a text with searchterm in it"". ""In"" and ""it"" are not tokenized because they're stopwords. Setting maxDocCharsToAnalyze to 36 (the length of the sentence) and searching for ""searchterm"" gives a fragment ending in ""searchterm"". The expected behaviour is to have ""in it"" at the end of the fragment, since maxDocCharsToAnalyse explicitely states that the whole phrase should be considered."
1,"DisjunctionMaxScorer.skipTo has bug that keeps it from skippingas reported on the mailing list, DisjunctionMaxScorer.skipTo is broken if called before next in some situations...

http://www.nabble.com/Potential-issue-with-DisjunctionMaxScorer-tf3846366.html#a10894987"
0,"MultiFieldQueryParser field boost multiplierAllows specific boosting per field, e.g. +(name:foo^1 description:foo^0.1).

Went from String[] field to MultiFieldQueryParser.FieldSetting[] field in constructor. "
1,"MsExcelTextFilter throws Exception. Repository is not startableIf i try to add a Excel File (see attachment) i get this Exception

Caused by: java.lang.NumberFormatException: You cannot get a string value from a numeric cell
	at org.apache.poi.hssf.usermodel.HSSFCell.getStringCellValue(HSSFCell.java:800)
	at org.apache.jackrabbit.core.query.MsExcelTextFilter$1.initializeReader(MsExcelTextFilter.java:97)
	at org.apache.jackrabbit.core.query.LazyReader.read(LazyReader.java:79)

The bad news is that if you add this file the repository is not startabel anymore because the file is in the redo.log and you
get a blocker !

The stack from the restart after NumberFormatException

19.09.2006 08:47:23 *ERROR* RepositoryImpl: Unable to start repository, forcing shutdown... 
19.09.2006 08:47:23 *INFO * RepositoryImpl: Shutting down repository... 
19.09.2006 08:47:23 *INFO * RepositoryImpl: shutting down workspace 'default'... 
19.09.2006 08:47:23 *INFO * ObservationManagerFactory: Notification of EventListeners stopped. 
19.09.2006 08:47:23 *INFO * RepositoryImpl: workspace 'default' has been shutdown 

I think its very important that you can not block a whole repository if the indexer throws a exception.
thanks
claus"
1,"HostConfiguration socketFactory is ignoredHostConfiguration doesn't use its host.protocol to execute an HttpMethod with an absolute URL.  It should, if the Protocol's scheme is the same as the method's URL scheme.

This bug makes it difficult to integrate a specialized SSL connection algorithm (in a SecureProtocolSocketFactory) with a module implemented on top of HttpClient.  The latter module must not execute methods with absolute URLs.  Of course, this is difficult when one doesn't control that module.  For example, I recently tried to integrate SSL certificate-based client authentication with XFire.  XFire provides a reasonable API for replacing its HttpClient, but one must hack its source code to prevent it from executing methods with absolute URLs.

Protocol.registerProtocol is a possible answer, but it can't support two or more SSL connection algorithms for one HTTPS host and port."
0,JCR taglibJCR Taglib. Maven generated site at http://cablemodem.fibertel.com.ar/edgarpoce/index.html
0,MultiFields not thread safeMultiFields looks like it has thread safety issues
0,Make PKIndexSplitter and MultiPassIndexSplitter work per segmentSpinoff from LUCENE-3624: DocValuesw merger throws exception on IW.addIndexes(SlowMultiReaderWrapper) as string-index like docvalues cannot provide asSortedSource.
0,"cache module should strip 'Content-Encoding: identity' from responsesPer the RFC, the ""identity"" content coding SHOULD NOT be used in the Content-Encoding header:

http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.5

The current implementation will pass 'Content-Encoding: identity' through unchanged, although it would be simple enough to filter this out.
"
0,"shouldn't throw exception on bad cookiesCurrently, HttpClient throws Exception on bad cookie. This is not expected. The 
user will expect HttpClient to ignore such cookies, but not getting an 
exception. Once exception is throw, user has no way to know if he can continue."
1,NodeTypeDefinitionFactory does not set PropertyDefinition#isQueryOrderable
1,"Incorrect check for replace when importing item with colliding idWhen fixing JCR-1128 bug was introduced due to incorrect check for UUID behavior. Current code is:
201 : 	 if (!(existing.getId().equals(id)
202 : 	&& (uuidBehavior == ImportUUIDBehavior.IMPORT_UUID_COLLISION_REMOVE_EXISTING
203 :	|| uuidBehavior == ImportUUIDBehavior.IMPORT_UUID_COLLISION_REMOVE_EXISTING))) {
204 :	throw new ItemExistsException(existing.safeGetJCRPath());
205 :	}

While it should check for ImportUUIDBehavior.IMPORT_UUID_COLLISION_REPLACE_EXISTING in one of the cases (line 202 or 203).
Also it is possible that id of imported item is not known and therefore value of ""id"" variable is null and check will always fail. Would be nice if this case can be handled as well.
"
1,"ERROR 40XD0: Container has been closed exception with Derby DBThis seems very similar to JCR-1039, only I am getting it on 1.4 using the regular DatabasePersistenceManager.
Was the fix for JCR-1039 in 1.3.3 merged to 1.4.x?

Here is the relevant part of the exception:

INFO | jvm 1 | 2008/04/10 14:00:37 | Caused by: javax.jcr.RepositoryException: failed to retrieve item state of item fb648866-a236-42aa-8039-df68f26dd2ad/{http://www.jcp.org/jcr/1.0}data: failed to read property state: fb648866-a236-42aa-8039-df68f26dd2ad/{http://www.jcp.org/jcr/1.0}data: failed to read property state: fb648866-a236-42aa-8039-df68f26dd2ad/{http://www.jcp.org/jcr/1.0}data
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:570)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:395)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.NodeImpl.getProperty(NodeImpl.java:2553)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.artifactory.jcr.JcrFile.getStream(JcrFile.java:133)
INFO | jvm 1 | 2008/04/10 14:00:37 | ... 55 more
INFO | jvm 1 | 2008/04/10 14:00:37 | Caused by: org.apache.jackrabbit.core.state.ItemStateException: failed to read property state: fb648866-a236-42aa-8039-df68f26dd2ad/{http://www.jcp.org/jcr/1.0}data
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.load(DatabasePersistenceManager.java:406)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.state.SharedItemStateManager.loadItemState(SharedItemStateManager.java:1161)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.state.SharedItemStateManager.getNonVirtualItemState(SharedItemStateManager.java:1086)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.state.SharedItemStateManager.getItemState(SharedItemStateManager.java:248)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.state.LocalItemStateManager.getPropertyState(LocalItemStateManager.java:118)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.state.LocalItemStateManager.getItemState(LocalItemStateManager.java:150)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.state.XAItemStateManager.getItemState(XAItemStateManager.java:226)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:175)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:564)
INFO | jvm 1 | 2008/04/10 14:00:37 | ... 58 more
INFO | jvm 1 | 2008/04/10 14:00:37 | Caused by: javax.jcr.RepositoryException: Error creating temporary file: ERROR 40XD0: Container has been closed.: ERROR 40XD0: Container has been closed.
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.value.BLOBInTempFile.<init>(BLOBInTempFile.java:69)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.value.BLOBInTempFile.getInstance(BLOBInTempFile.java:103)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.value.InternalValue.getBLOBFileValue(InternalValue.java:630)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.value.InternalValue.create(InternalValue.java:265)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.persistence.util.Serializer.deserialize(Serializer.java:296)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.load(DatabasePersistenceManager.java:397)
INFO | jvm 1 | 2008/04/10 14:00:37 | ... 66 more
INFO | jvm 1 | 2008/04/10 14:00:37 | Caused by: java.io.IOException: ERROR 40XD0: Container has been closed.
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.derby.impl.store.raw.data.OverflowInputStream.fillByteHolder(Unknown Source)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.derby.impl.store.raw.data.BufferedByteHolderInputStream.read(Unknown Source)
INFO | jvm 1 | 2008/04/10 14:00:37 | at java.io.DataInputStream.read(DataInputStream.java:132)
INFO | jvm 1 | 2008/04/10 14:00:37 | at java.io.FilterInputStream.read(FilterInputStream.java:116)
INFO | jvm 1 | 2008/04/10 14:00:37 | at java.io.FilterInputStream.read(FilterInputStream.java:116)
INFO | jvm 1 | 2008/04/10 14:00:37 | at java.io.SequenceInputStream.read(SequenceInputStream.java:191)
INFO | jvm 1 | 2008/04/10 14:00:37 | at java.io.InputStream.read(InputStream.java:85)
INFO | jvm 1 | 2008/04/10 14:00:37 | at org.apache.jackrabbit.core.value.BLOBInTempFile.<init>(BLOBInTempFile.java:61)
INFO | jvm 1 | 2008/04/10 14:00:37 | ... 71 more"
1,"RepositoryException when using BindVariables in JCR-SQL2 CONTAINSWhen using a BindVariable in a JCR-SQL2 CONTAINS constraint, the query fails with a RepositoryException.

For example:

String sql = ""SELECT * FROM [nt:unstructured] WHERE ISCHILDNODE([/testroot]) AND CONTAINS(mytext, $searchExpression)"";
Query q = superuser.getWorkspace().getQueryManager().createQuery(sql, Query.JCR_SQL2);
q.bindValue(""searchExpression"", superuser.getValueFactory().createValue(""fox""));
q.execute();

Results in:

javax.jcr.RepositoryException: Unknown static operand type: org.apache.jackrabbit.spi.commons.query.qom.BindVariableValueImpl@591a4d
        at org.apache.jackrabbit.core.query.lucene.LuceneQueryFactoryImpl.create(LuceneQueryFactoryImpl.java:215)
        at org.apache.jackrabbit.core.query.lucene.constraint.FullTextConstraint.<init>(FullTextConstraint.java:42)
        at org.apache.jackrabbit.core.query.lucene.constraint.ConstraintBuilder$Visitor.visit(ConstraintBuilder.java:175)
        at org.apache.jackrabbit.spi.commons.query.qom.FullTextSearchImpl.accept(FullTextSearchImpl.java:117)
        at org.apache.jackrabbit.core.query.lucene.constraint.ConstraintBuilder$Visitor.visit(ConstraintBuilder.java:137)
        at org.apache.jackrabbit.spi.commons.query.qom.AndImpl.accept(AndImpl.java:72)
        at org.apache.jackrabbit.core.query.lucene.constraint.ConstraintBuilder.create(ConstraintBuilder.java:82)
        at org.apache.jackrabbit.core.query.lucene.QueryObjectModelImpl.execute(QueryObjectModelImpl.java:109)
        at org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:127)

I tried to fix this issue but there is no way to access the BindVariables from the ConstraintBuilder from the LuceneQueryFactoryImpl and the ConstraintBuilder just passes the FullTextSearchImpl QOM subtree to the factory (via FullTextConstraint constructor) without any further visiting. If the signature would be ""LuceneQueryFactoryImpl#create(FullTextSearchImpl fts, Value searchExpression)"" we could visit the StaticOperand in the ConstraintBuilder and then modify the FullTextSearchImpl constructor accordingly, but this would imply that LuceneQueryFactory interface would need to be change accordingly and I don't know what that would mean."
1,"Moving a node while index is merged leads to inconsistent indexThe IndexMerger keeps track of nodes that are deleted from the index and applies that change also to the merged index, but if the same node is added again to the index during the merge process the index becomes inconsistent."
1,"XPath query with child axis predicatesExecuting a query using a long child path in a child axis predicate (like //*[a/b/c/d/e/@prop='something']) may return too many or not enough nodes.

I'll attach a zip file containing 2 tests cases showing this issue (I apologize but the test data are in French).
"
1,"DefaultHttpParamsFactory violates applet sandboxThe DefaultHttpParamsFactory in nightly build 20031009 makes two calls to 
System.getProperties().  This is by default verboten in an applet.  I have 
patched the source to catch the security exceptions and set the properties to a 
default value.  My modified code block follows:

        // TODO: To be removed. Provided for backward compatibility
        try {
          String agent = System.getProperties().getProperty
(""httpclient.useragent"");
          if (agent != null) {
            params.setParameter(HttpMethodParams.USER_AGENT, agent);
          }
        }
        catch (SecurityException dontCare) { }

        // TODO: To be removed. Provided for backward compatibility
        try {
          String preemptiveDefault = System.getProperties()
              .getProperty(""httpclient.authentication.preemptive"");
          if (preemptiveDefault != null) {
            preemptiveDefault = preemptiveDefault.trim().toLowerCase();
            if (preemptiveDefault.equals(""true"")) {
              params.setParameter
(HttpClientParams.PREEMPTIVE_AUTHENTICATION, ""on"");
            }
            else if (preemptiveDefault.equals(""false"")) {
              params.setParameter
(HttpClientParams.PREEMPTIVE_AUTHENTICATION, ""off"");
            }
          }
        }
        catch(SecurityException dontCare) { }"
0,"Add configuration options for search managerRight now, if the search manager is active, everything is indexed, even the system branch of a workspace with the versions.

take parameters / conditions into account whether a node should be indexed:
- path
- node type
- property type
- property name


see also http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/3343"
1,JCARepositoryManager does not close InputStreamJCR-3129 opened a already closed issue [JCR-1667]
1,"checkIfNodeLocked()  in jcr mapping layer  does not behave properly when open scoped locks are usedI am planning to use open-scoped lock.  For which , I need to persist the locktoken along with the node  so that it can be used by another session for unlocking.Tested with Jackrabbit RMI client and it works fine.

But I am using jcr-mapping layer to achieve the above in my project.Here I want that  as soon as a node is checked out, it gets locked by the session and the lock is stored in ""lockToken"" property of node ""Document"". For that I need to update the Document node after locking .

*public void checkout(String path)throws CMSException {
       pm = getPersistenceManager();
        try{
           pm.checkout(path);*
*            String lockToken = pm.lock(path,true,false);   **        
    Document doc = this.getDocument(path);**           
  doc.setLockToken(lockToken);     //for persisting lockToken
           doc.update();
        }catch(LockedException le){
          System.out.println(le.getLockedNodePath() + ""is locked by"" + le.getLockOwner());         }catch(Exception e){
            throw new CMSException(e.getMessage(),e.getCause());
        }
   }*


Here doc.update() fails with Locked Exception.  The problem here is PersistenceManagerImpl has a method checkIfNodeLocked(path)  which returns LockException if node is locked. This method is checked before every update/insert. So, I am not able to update a locked node. I need to persist the locktoken in the node . What is the reason of checking  for a lock before saving ? Ideally , it should throw error only if node is locked and session does not hold the lockToken .

If the session who has locked the node tries to save the node without unlocking, it should be allowed .

p.s.
I am able to achieve the above by simple Jackrabbit RMI client.
<code>
               ClientRepositoryFactory factory = new ClientRepositoryFactory();
               Repository repository = factory.getRepository(""rmi://localhost:1101/jackrabbit"");
               Session session = repository.login(new SimpleCredentials(""superuser"", ""superuser"".toCharArray()),""Portal"");                        String user = session.getUserID();
               String name = repository.getDescriptor(Repository.REP_NAME_DESC);
               System.out.println(
                       ""Logged in as "" + user + "" to a "" + name + "" repository."");

               /* Testing the locks functionality */
               Node n = session.getRootNode().getNode(""cms/childfolder1/check.txt"");

              * Lock lck = n.lock(true, false); // deeplock,open-scoped
               n.setProperty(""ps:locktoken"",lck.getLockToken());
               n.setProperty(""ps:language"", ""sanskrit"");
*
               System.out.println(""Lock#isLive="" + lck.isLive());
               System.out.println(""Node#isLocked="" +  session.getRootNode().getNode(""cms/childfolder1/check.txt"").isLocked());
               session.save();
               session.logout();
     <code> "
0,"add ElisionsFilter to ItalianAnalyzerwe set this up for french by default, but we don't for italian.
we should enable it with the standard italian contractions (e.g. definite articles).

the various stemmers for these languages assume this is already being taken care of
and don't do anything about it... in general things like snowball assume really dumb
tokenization, that you will split on the word-internal ', and they add these to stoplists."
0,"Add subset method to BitVectorRecently I needed the ability to efficiently compute subsets of a BitVector. The method is:
  public BitVector subset(int start, int end)
where ""start"" is the starting index, inclusive and ""end"" is the ending index, exclusive.

Attached is a patch including the subset method as well as relevant unit tests."
0,"Add contrib/fast-vector-highlighter to Maven central repoI'm not at all familiar with the Lucene build/deployment process, but it would be very nice if releases of the fast vector highlighter were pushed to the maven central repository, as is done with other contrib modules.

(Issue filed at the request of Grant Ingersoll.)"
0,"Updates to connectionStaleCheckingEnabled docs.Comments from Itai Brickner:

In the Threading section of the UserGuide
(
http://jakarta.apache.org/commons/httpclient/threading.html
)

There is no mentioning of the
'setConnectionStaleCheckingEnabled'
I also felt that it wasn't clear from the APIDOC
(http://jakarta.apache.org/commons/httpclient/apidocs/org/apache/commons/httpclient/MultiThreadedHttpConnectionManager.html)
that staleCheckingEnabled will cause a stale
connection to be reconnected by the
MultiThreadedHttpConnectionManager

thanks,

Itai"
0,"Leftover legacy enum in IndexReaderIn IndexReader we still have some leftover ""handmade"" enum from pre-Java5 times. Unfortunately the Generics/Java5 Policeman did not notice it.

This patch is just code cleanup, no baclkwards breaks, as code using this enum would not see any difference (because only superclass changes).

I will commit this asap."
0,Use configured credentials in RepositoryFactoryImplTest The test currently uses hard coded credentials. It should rather use configured credentials like all other tests do.
0,"wrong assumptions in test cases about lock tokensSeveral test cases assume that Lock.getLockToken has to return null for locks not attached to the current session. However, this is optional. Citing the Javadoc for getLockToken:

     * May return the lock token for this lock. If this lock is open-scoped and
     * the current session either holds the lock token for this lock, or the
     * repository chooses to expose the lock token to the current session, then
     * this method will return that lock token. Otherwise this method will
     * return <code>null</code>."
0,"TokenStream/Tokenizer/TokenFilter/Token javadoc improvementsSome of the javadoc for the new TokenStream/Tokenizer/TokenFilter/Token APIs had javadoc errors.  To the best of my knowledge, I corrected these and refined the copy a bit."
0,"CachingMultiReader has inconsistent nameAll other classes that extend IndexReader are also named that way, except CachingMultiReader. It should be renamed to CachingMultiIndexReader."
0,"Exclude the netcdf dependencyAs discussed on the mailing list, the netcdf dependency we get through Tika since version 0.8 is only used in very rare cases and thus does not justify the added size overhead. We should thus exclude it from default installations."
0,add test case for recovering from broken version history hierarchyThe test should exercise recovery from a missing parent node of a VHR.
0,Add join query to LuceneSolr has (psuedo) join query for a while now. I think this should also be available in Lucene.  
0,"Cleanup use of EncodingUtil and HttpConstantsHttpConstants has become somewhat irrelevant.  Deprecate HttpConstants and move any existing 
functionality to EncodingUtil."
0,"need a way to set request body in PostMethodCurrently, there is no way for user to set the request body in PostMethod 
directly. The only way to do that is by adding parameters to PostMethod. This 
makes sense in most cases. However, there are situations that the user actually 
knows the request body and want to set it directly. adding the following method 
fixes this:

    public void setRequestBody(String requestBody)
    {
        this.requestBody = requestBody;
    }"
0,"Incorporate GeoHash in contrib/spatialBased on comments from Yonik and Ryan in SOLR-773 
GeoHash provides the ability to store latitude / longitude values in a single field consistent hash field.
Which elements the need to maintain 2 field caches for latitude / longitude fields, reducing the size of an index
and the amount of memory needed for a spatial search."
0,"Improve excerpt fragmentsImprove the excerpt fragments:

- If a fragment starts at the very beginning of a text, the first Word is cut off
- If a fragment does not start with the beginning of a sentence a '...' should be prepended
- If matching terms in a fragment are within range of 75 characters the fragment is extended too far and may produce a larger fragment than requested"
1,"LLRect.createBox returned box does not contains all points in (center,distance) discLLRect,createBox computation of a bouding box for a disc given center and distance doest not contains all the point in the distance.

Example : the point north by distance doest not have Lat inferior of Lat of the UpperRight corner of the returned box"
0,"Remove verbosity from tests and make configureableThe parent issue added the functionality to LuceneTestCase(J4), this patch applies it to most tests."
0,"JcrUtils.getRepository(...) for simple repository accessAs discussed on the mailing list, it would be nice to have a trivially simple way (one line of code) to connect to a repository. The RepositoryFactory interface in JCR 2.0 defines a way for clients to get a repository reference without a direct implementation dependency, but a client still needs extra code to handle the Service Provider lookup and the iteration through all the available repository factories.

To simplify client code I'd like to introduce a JcrUtils.getRepository(Map<String, String>) method that takes care of the tasks mentioned above:

    Map<String, String> parameters = ...; // repository settings
    Repository repository = JcrUtils.getRepository(parameters);

As a further simplification, I'd also like to introduce a JcrUtils.getRepository(String) method that builds the parameter map based on a given ""repository URI"".

    Repository repository = JcrUtils.getRepository(""file:///path/to/repository"");

    Repository repository = JcrUtils.getRepository(""http://localhost:8080/server"");

The set of supported URI types is still to be defined."
0,"SerializationTest and AbstractImportXmlTest leak temporary filesBoth test classes leak temporary files when setUp() fails.
"
1,"Impossible to import a string containing _x0020_  with Session.importXmlThe importXml uses the ValueHelper.serialize methods. The option ""decodeBlanks"" does a simple string replace which replaces _x0020_ in spaces (line 695 and 793). This option is always set to true unless the imported data is binary. See: BufferedStringValue and StringValue getValue methods.

The result is that it is now impossible to import a string with _x0020_ in it, because it gets translated in a space. The simple solution would be to just turn off the declodeBlanks option, but I'm not sure why it was added in the first place. Another option would be to use real encoding instead of a replace like the o.a.j.util.ISO9075.

"
1,"Item.isNew() does not work correctly within a transactionjavadoc on Item.isNew() states:

     * Returns <code>true</code> if this is a new item, meaning that it exists only in transient
     * storage on the <code>Session</code> and has not yet been saved. Within a transaction,
     * <code>isNew</code> on an <code>Item</code> may return <code>false</code> (because the item
     * has been saved) even if that <code>Item</code> is not in persistent storage (because the
     * transaction has not yet been committed).

but currently, Item.isNew() returns ""true"" after beeing saved in a transaction."
0,"Some house cleaning in addIndexes*Today, the use of addIndexes and addIndexesNoOptimize is confusing - 
especially on when to invoke each. Also, addIndexes calls optimize() in 
the beginning, but only on the target index. It also includes the 
following jdoc statement, which from how I understand the code, is 
wrong: _After this completes, the index is optimized._ -- optimize() is 
called in the beginning and not in the end. 

On the other hand, addIndexesNoOptimize does not call optimize(), and 
relies on the MergeScheduler and MergePolicy to handle the merges. 

After a short discussion about that on the list (Thanks Mike for the 
clarifications!) I understand that there are really two core differences 
between the two: 
* addIndexes supports IndexReader extensions
* addIndexesNoOptimize performs better

This issue proposes the following:
# Clear up the documentation of each, spelling out the pros/cons of 
  calling them clearly in the javadocs.
# Rename addIndexesNoOptimize to addIndexes
# Remove optimize() call from addIndexes(IndexReader...)
# Document that clearly in both, w/ a recommendation to call optimize() 
  before on any of the Directories/Indexes if it's a concern. 

That way, we maintain all the flexibility in the API - 
addIndexes(IndexReader...) allows for using IR extensions, 
addIndexes(Directory...) is considered more efficient, by allowing the 
merges to happen concurrently (depending on MS) and also factors in the 
MP. So unless you have an IR extension, addDirectories is really the one 
you should be using. And you have the freedom to call optimize() before 
each if you care about it, or don't if you don't care. Either way, 
incurring the cost of optimize() is entirely in the user's hands. 

BTW, addIndexes(IndexReader...) does not use neither the MergeScheduler 
nor MergePolicy, but rather call SegmentMerger directly. This might be 
another place for improvement. I'll look into it, and if it's not too 
complicated, I may cover it by this issue as well. If you have any hints 
that can give me a good head start on that, please don't be shy :). "
0,"Add support for custom ExecutorServices in ParallelMultiSearcherRight now, the ParallelMultiSearcher uses a cachedThreadPool, which is limitless and a poor choice for a web application, given the threaded nature of the requests (say a webapp with tomcat-default 200 threads and 100 indexes could be looking at 2000 searching threads pretty easily).  Support for adding a custom ExecutorService is pretty trivial.  Patch forthcoming."
0,Performance tuning
0,"Resource association not compliant to JTA specAccording to JTA specifcation, section 3.4.4 (Transaction Association), a resource's association may be ended (state T0 in the spec's table) in the suspended state (T2), i.e. without having been resumed (T1) again. The code in XASessionImpl.end(), however, assumes that the resource must be associated in order to end its association. This causes an exception in JBoss 4.0.5.GA:

09:37:15,525 WARN  [TransactionImpl] XAException: tx=TransactionImpl:XidImpl[FormatId=257, GlobalId=kneipix.dev.day.com/14, BranchQual=, localId=14] errorCode=XAER_PROTO
javax.transaction.xa.XAException
        at org.apache.jackrabbit.core.XASessionImpl.end(XASessionImpl.java:279)
        at org.apache.jackrabbit.jca.TransactionBoundXAResource.end(TransactionBoundXAResource.java:46)
        at org.jboss.tm.TransactionImpl$Resource.endResource(TransactionImpl.java:2143)
        at org.jboss.tm.TransactionImpl$Resource.endResource(TransactionImpl.java:2118)
        at org.jboss.tm.TransactionImpl.endResources(TransactionImpl.java:1462)
        at org.jboss.tm.TransactionImpl.beforePrepare(TransactionImpl.java:1116)
        at org.jboss.tm.TransactionImpl.commit(TransactionImpl.java:324)
        at org.jboss.tm.TxManager.commit(TxManager.java:240)
        at org.jboss.aspects.tx.TxPolicy.endTransaction(TxPolicy.java:175)
"
1,"ItemStateException on concurrently committing transactions of versioning operationssee tests in JCR-335


org.apache.jackrabbit.core.state.ItemStateException: Unable to resolve path for item: 69d80165-7ef5-4b6b-8aa9-be9c9be1f994
	at org.apache.jackrabbit.core.observation.EventStateCollection.getPath(EventStateCollection.java:525)
	at org.apache.jackrabbit.core.observation.EventStateCollection.createEventStates(EventStateCollection.java:377)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:547)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:668)
	at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:151)
	at org.apache.jackrabbit.core.version.XAVersionManager.prepare(XAVersionManager.java:431)
	at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:129)
	at org.apache.jackrabbit.core.XASessionImpl.prepare(XASessionImpl.java:309)
	at test.JCRUserTransaction.commit(JCRUserTransaction.java:74)
	at org.apache.jackrabbit.JRTestDeadlock.run(JRTestDeadlock.java:110)
Caused by: javax.jcr.ItemNotFoundException: failed to build path of 69d80165-7ef5-4b6b-8aa9-be9c9be1f994: a0ecd4b0-a442-4b1e-a2f6-51441f40d452 has no child entry for 69d80165-7ef5-4b6b-8aa9-be9c9be1f994
	at org.apache.jackrabbit.core.HierarchyManagerImpl.buildPath(HierarchyManagerImpl.java:308)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:357)
	at org.apache.jackrabbit.core.observation.EventStateCollection.getPath(EventStateCollection.java:520)
	... 9 more"
0,"TestBasicCookieAttribHandlers fails on non-english Locale systemsThe Test checks for written dates in the format for cookies which unfortunately includes a two character abbreviation of the day. This differs by locale, so the dateformat has to be constructed with Locale.US (as in DateUtils)"
1,"trunk:  TestDocumentsWriterDeleteQueue.testStressDeleteQueue seed failurefails 100% of the time for me, trunk r1152089

{code}
    [junit] Testsuite: org.apache.lucene.index.TestDocumentsWriterDeleteQueue
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.585 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestDocumentsWriterDeleteQueue -Dtestmethod=testStressDeleteQueue -Dtests.seed=724635056932528964:-56
53725200660632980
    [junit] NOTE: test params are: codec=RandomCodecProvider: {}, locale=en_US, timezone=Pacific/Port_Moresby
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDocumentsWriterDeleteQueue]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_26 (64-bit)/cpus=8,threads=1,free=86067624,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testStressDeleteQueue(org.apache.lucene.index.TestDocumentsWriterDeleteQueue):    FAILED
{code}"
1,"MaxCount not working correctly in user/group query when restricting to group membersFor user/group queries having a scope *and* a limit clause maxCount does not work correctly.

    builder.setScope(""contributors"", false);
    builder.setLimit(0, 50);

In the above case, the result might contain to few results. 

This is related to JCR-2829"
1,"If index has more than Integer.MAX_VALUE terms, seeking can it AIOOBE due to long/int overflowTom hit a new long/int overflow case: http://markmail.org/thread/toyl2ujcl4suqvf3

This is a regression, in 3.1, introduced with LUCENE-2075.

Worse, our Test2BTerms failed to catch this, so I've fixed that test to show the failure."
0,"no classes with default visibilityThere should be no classes with default (package) visibility. They cause problems when classes using them are extended. All classes should either be public, or nested with protected visibility where they are used. Nesting with private visibility may be acceptable in certain cases, for example in final classes.
"
0,"Reintroduce NamespaceStoragehi jukka

i open this issue as a reminder of our recent discussion in basel:

we decided that you will 
- reintroduce the NamespaceStorage you recently removed from jcr2spi
- reintroduce a namespace cache in jcr2spi (but using a simple map instead of NamespaceCache object)

in addition we agreed that we want to share the NamespaceRegistryImpl between jcr2spi and jackrabbit-core
and you volenteered to provide a patch for that.

thanks in advance
angela"
0,"TCK: RestoreTest.testRestoreLabelAccording to tobi the jackrabbit implementation of 'Node.restoreByLabel' is an interpretation of the
specification regarding the restore behaviour of versionable child nodes. while that interpetration might
be legal unless the specification is violated, i would argue that the TCK should not test the interpretation.

therefore i suggest to modify

org.apache.jackrabbit.test.api.version.RestoreTest.testRestoreLabel

by skipping line 334 - 345 in order to limit the test case to the behaviour that is defined by the specification.

regards
angela

ps: the mentioned test is also executed within the scope of WorkspaceRestoreTest because the latter  extends RestoreTest.... that's misleading."
0,"When adding a large (100MB) binary to the DbDataStore, it fails with an insufficient memory exceptionAttached is a small test case. It fails during save(). I think this is related to what I mentioned in http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/200711.mbox/%3c00fc01c832b9$f1f08730$7309240a@goku%3e

The full stacktrace is the following:

javax.jcr.RepositoryException: /: unable to update item.: failed to write property state: cafebabe-cafe-babe-cafe-babecafebabe/{}bin: failed to write property state: cafebabe-cafe-babe-cafe-babecafebabe/{}bin
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1252)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:858)
	at org.apache.jackrabbit.core.data.BigBinaryTest.testBigBinary(BigBinaryTest.java:16)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:404)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: org.apache.jackrabbit.core.state.ItemStateException: failed to write property state: cafebabe-cafe-babe-cafe-babecafebabe/{}bin
	at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:487)
	at org.apache.jackrabbit.core.persistence.AbstractPersistenceManager.store(AbstractPersistenceManager.java:75)
	at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:282)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:687)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:856)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:324)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:300)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:306)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1244)
	... 21 more
Caused by: org.apache.jackrabbit.core.data.DataStoreException: Can not read identifier a2ada2d96d0b05214288efa03be9005a5bb98c9b: Memoria insuficiente. Utilice cursores del servidor para result sets grandes:Java heap space. Tamao de result set:104.857.723. Tamao memoria total JVM:66.650.112.: Memoria insuficiente. Utilice cursores del servidor para result sets grandes:Java heap space. Tamao de result set:104.857.723. Tamao memoria total JVM:66.650.112.
	at org.apache.jackrabbit.core.data.db.DbDataStore.convert(DbDataStore.java:438)
	at org.apache.jackrabbit.core.data.db.DbDataStore.getInputStream(DbDataStore.java:481)
	at org.apache.jackrabbit.core.data.db.DbDataRecord.getStream(DbDataRecord.java:61)
	at org.apache.jackrabbit.core.value.BLOBInDataStore.getStream(BLOBInDataStore.java:93)
	at org.apache.jackrabbit.core.persistence.util.Serializer.serialize(Serializer.java:198)
	at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:476)
	... 30 more
Caused by: com.microsoft.sqlserver.jdbc.SQLServerException: Memoria insuficiente. Utilice cursores del servidor para result sets grandes:Java heap space. Tamao de result set:104.857.723. Tamao memoria total JVM:66.650.112.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDriverError(Unknown Source)
	at com.microsoft.sqlserver.jdbc.DBComms.receive(Unknown Source)
	at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(Unknown Source)
	at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PreparedStatementExecutionRequest.executeStatement(Unknown Source)
	at com.microsoft.sqlserver.jdbc.CancelableRequest.execute(Unknown Source)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeRequest(Unknown Source)
	at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.execute(Unknown Source)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmtInternal(ConnectionRecoveryManager.java:362)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmtInternal(ConnectionRecoveryManager.java:292)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmt(ConnectionRecoveryManager.java:257)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmt(ConnectionRecoveryManager.java:237)
	at org.apache.jackrabbit.core.data.db.DbDataStore.getInputStream(DbDataStore.java:474)
	... 34 more
org.apache.jackrabbit.core.state.ItemStateException: failed to write property state: cafebabe-cafe-babe-cafe-babecafebabe/{}bin
	at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:487)
	at org.apache.jackrabbit.core.persistence.AbstractPersistenceManager.store(AbstractPersistenceManager.java:75)
	at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:282)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:687)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:856)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:324)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:300)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:306)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1244)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:858)
	at org.apache.jackrabbit.core.data.BigBinaryTest.testBigBinary(BigBinaryTest.java:16)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at junit.framework.TestCase.runBare(TestCase.java:127)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:118)
	at org.apache.jackrabbit.test.AbstractJCRTest.run(AbstractJCRTest.java:404)
	at junit.framework.TestSuite.runTest(TestSuite.java:208)
	at junit.framework.TestSuite.run(TestSuite.java:203)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:128)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: org.apache.jackrabbit.core.data.DataStoreException: Can not read identifier a2ada2d96d0b05214288efa03be9005a5bb98c9b: Memoria insuficiente. Utilice cursores del servidor para result sets grandes:Java heap space. Tamao de result set:104.857.723. Tamao memoria total JVM:66.650.112.: Memoria insuficiente. Utilice cursores del servidor para result sets grandes:Java heap space. Tamao de result set:104.857.723. Tamao memoria total JVM:66.650.112.
	at org.apache.jackrabbit.core.data.db.DbDataStore.convert(DbDataStore.java:438)
	at org.apache.jackrabbit.core.data.db.DbDataStore.getInputStream(DbDataStore.java:481)
	at org.apache.jackrabbit.core.data.db.DbDataRecord.getStream(DbDataRecord.java:61)
	at org.apache.jackrabbit.core.value.BLOBInDataStore.getStream(BLOBInDataStore.java:93)
	at org.apache.jackrabbit.core.persistence.util.Serializer.serialize(Serializer.java:198)
	at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:476)
	... 30 more
Caused by: com.microsoft.sqlserver.jdbc.SQLServerException: Memoria insuficiente. Utilice cursores del servidor para result sets grandes:Java heap space. Tamao de result set:104.857.723. Tamao memoria total JVM:66.650.112.
	at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDriverError(Unknown Source)
	at com.microsoft.sqlserver.jdbc.DBComms.receive(Unknown Source)
	at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePreparedStatement(Unknown Source)
	at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PreparedStatementExecutionRequest.executeStatement(Unknown Source)
	at com.microsoft.sqlserver.jdbc.CancelableRequest.execute(Unknown Source)
	at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeRequest(Unknown Source)
	at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.execute(Unknown Source)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmtInternal(ConnectionRecoveryManager.java:362)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmtInternal(ConnectionRecoveryManager.java:292)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmt(ConnectionRecoveryManager.java:257)
	at org.apache.jackrabbit.core.persistence.bundle.util.ConnectionRecoveryManager.executeStmt(ConnectionRecoveryManager.java:237)
	at org.apache.jackrabbit.core.data.db.DbDataStore.getInputStream(DbDataStore.java:474)
	... 34 more

"
0,"Contrib: Main memory based SynonymMap and SynonymTokenFilter- Contrib: Main memory based SynonymMap and SynonymTokenFilter
- applies to SVN trunk as well as 1.4.3"
0,"Avoidable synchronization bottleneck in MatchAlldocsQuery$MatchAllScorerThe isDeleted() method on IndexReader has been mentioned a number of times as a potential synchronization bottleneck. However, the reason this  bottleneck occurs is actually at a higher level that wasn't focused on (at least in the threads I read).

In every case I saw where a stack trace was provided to show the lock/block, higher in the stack you see the MatchAllScorer.next() method. In Solr paricularly, this scorer is used for ""NOT"" queries. We saw incredibly poor performance (order of magnitude) on our load tests for NOT queries, due to this bottleneck. The problem is that every single document is run through this isDeleted() method, which is synchronized. Having an optimized index exacerbates this issues, as there is only a single SegmentReader to synchronize on, causing a major thread pileup waiting for the lock.

By simply having the MatchAllScorer see if there have been any deletions in the reader, much of this can be avoided. Especially in a read-only environment for production where you have slaves doing all the high load searching.

I modified line 67 in the MatchAllDocsQuery
FROM:
  if (!reader.isDeleted(id)) {
TO:
  if (!reader.hasDeletions() || !reader.isDeleted(id)) {

In our micro load test for NOT queries only, this was a major performance improvement.  We also got the same query results. I don't believe this will improve the situation for indexes that have deletions. 

Please consider making this adjustment for a future bug fix release.




"
0,Remove deprecated SpanQuery.getTerms() and generify Query.extractTerms(Set<Term>)
1,"Combination of BooleanQuery and PhrasePrefixQuery can provoke UnsupportedOperationExceptionA BooleanQuery including a PhrasePrefixQuery can cause an exception to be thrown
from BooleanScorer#skipTo when the search is executed:  

java.lang.UnsupportedOperationException
	at org.apache.lucene.search.BooleanScorer.skipTo(BooleanScorer.java:189)
	at org.apache.lucene.search.ConjunctionScorer.doNext(ConjunctionScorer.java:53)
	at org.apache.lucene.search.ConjunctionScorer.next(ConjunctionScorer.java:48)
	at org.apache.lucene.search.Scorer.score(Scorer.java:37)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:92)
	at org.apache.lucene.search.Hits.getMoreDocs(Hits.java:64)
	at org.apache.lucene.search.Hits.<init>(Hits.java:43)
	at org.apache.lucene.search.Searcher.search(Searcher.java:33)
	at org.apache.lucene.search.Searcher.search(Searcher.java:27)
        ... (non-lucene code)

The problem appears to be that PhrasePrefixQuery optimizes itself into a
BooleanQuery when it contains only one term.  However, it does this in the
createWeight() method of its scorer instead of in the rewrite method of the
query itself.  Thus it bypasses the boolean typecheck when BooleanQuery is
deciding whether to use ConjunctionScorer or BooleanScorer, eventually resulting
in the UOE."
0,"Upgrade to easymock 2.5.2Currently we have a dependency on easymock:easymock:1.1 in the jackrabbit-parent pom's dependency management section. This dependency is not actually used, but I am planning to start using it soon in the core. Upgrading it to 2.5.2 would give us generics support and a lot of improvements and bug fixes.

Note that the upgrade changes the groupId of the dependency."
0,"Fix FuzzyQuery's defaults, so its fast.We worked a lot on FuzzyQuery, but you need to be a rocket scientist to ensure good results.

The main problem is that the default distance is 0.5f, which doesn't take into account the length of the string.
To add insult to injury, the default number of expansions is 1024 (traditionally from BooleanQuery maxClauseCount)

I propose:
* The syntax of FuzzyQuery is enhanced, so that you can specify raw edits too: such as foobar~2 (all terms within 2 levenshtein edits of foobar). Previously if you specified any amount >=1, you got IllegalArgumentException, so this won't break anyone. You can still use foobar~0.5, and it works just as before
* The default for minimumSimilarity then becomes LevenshteinAutomata.MAXIMUM_SUPPORTED_DISTANCE, which is 2. This way if you just do foobar~, its always fast.
* The size of the priority queue is reduced by default from 1024 to a much more reasonable value: 50. This is what FuzzyLikeThis uses.

I think its best to just change the defaults for this query, since it was so aweful before. We can add notes in migrate.txt that if you care about using the old values, then you should provide them explicitly, and you will get the same results!
"
0,"Review and potentially remove unused/unsupported ContribsSome of our contribs appear to be lacking for development/support or are missing tests.  We should review whether they are even pertinent these days and potentially deprecate and remove them.

One of the things we did in Mahout when bringing in Colt code was to mark all code that didn't have tests as @deprecated and then we removed the deprecation once tests were added.  Those that didn't get tests added over about a 6 mos. period of time were removed.

I would suggest taking a hard look at:
ant
db
lucli
swing

(spatial should be gutted to some extent and moved to modules)"
1,"RowIterator view of result for query '//*' only returns jcr:path columnThe RowIterator view of a query result for '//*' only returns the jcr:path column. The spec states that this query is equivalent to:
select * from nt:base. Furthermore a query that selects * properties must return all non-residual properties that are declared for this node type and are not multi-valued. The pseudo properties jcr:path and jcr:score must always be available.

For nt:base this is:
- jcr:primaryType
- jcr:path
- jcr:score"
0,"'ant generate-maven-artifacts' should work for lucene+solr 3.x+The maven build scripts need to be updated so that solr uses the artifacts from lucene.

For consistency, we should be able to have a different 'maven_version' then the 'version'  That is, we want to build: 3.1-SNAPSHOT with a jar file: 3.1-dev"
0,"Lucene benchmark: objective performance test for LuceneWe need an objective way to measure the performance of Lucene, both indexing and querying, on a known corpus. This issue is intended to collect comments and patches implementing a suite of such benchmarking tests.

Regarding the corpus: one of the widely used and freely available corpora is the original Reuters collection, available from http://www-2.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz or http://people.csail.mit.edu/u/j/jrennie/public_html/20Newsgroups/20news-18828.tar.gz. I propose to use this corpus as a base for benchmarks. The benchmarking suite could automatically retrieve it from known locations, and cache it locally."
0,"remove RoutedRequest from APIRemove RoutedRequest from the Client API. It can be moved to impl, or dropped altogether.
HttpClient could accept separate request and target arguments instead of RoutedRequest.
No routes should be passed in the API. "
0,"Add link to ""Benchmarking the HttpClient Caching Module"" article from Comcast Interactive MediaI'd like to add a link into the HttpClient docs detailing some benchmarking we did with the HttpClient Cache module."
1,"[PATCH] Problem with Sort logic on tokenized fieldsWhen you set s SortField to a Text field which gets tokenized
FieldCacheImpl uses the term to do the sort, but then sorting is off 
especially with more then one word in the field. I think it is much 
more logical to sort by field's string value if the sort field is Tokenized and
stored. This way you'll get the CORRECT sort order"
1,"MatchAllDocsQueryNode toString() creates invalid XML-TagMatchAllDocsQueryNode.toString() returns ""<matchAllDocs field='*' term='*'>"", which is inavlid XML should read ""<matchAllDocs field='*' term='*' />.
"
1,"RegexCapabilities is not SerializableThe class RegexQuery is marked Serializable by its super class, but it contains a RegexCapabilities which is not Serializable. Thus attempting to serialize the query results in an exception. 

Making RegexCapabilities serializable should be no problem since its subclasses contain only serializable classes (java.util.regex.Pattern and org.apache.regexp.RE)."
0,"Several final classes have non-overriding protected membersProtected member access in final classes, except where a protected method overrides a superclass's protected method, makes little sense.  The attached patch converts final classes' protected access on fields to private, removes two final classes' unused protected constructors, and converts one final class's protected final method to private."
0,"Generify FST shortestPaths() to take a comparatorNot sure we should do this, it costs 5-10% performance for WFSTSuggester.
But maybe we can optimize something here, or maybe its just no big deal to us.

Because in general, this could be pretty powerful, e.g. if you needed to store 
some custom stuff in the suggester, you could use pairoutputs, or whatever.

And the possibility we might need shortestPaths for other cool things... at the
least I just wanted to have the patch up here.

I haven't tested this on pairoutputs... but i've tested it with e.g. FloatOutputs
and other things and it works fine.

I tried to minimize the generics violations, there is only 1 (cannot create generic array).
"
0,"Minor improvement to JavaDoc for ScoreDocComparatorAbout to attach a very small patch for ScoreDocComparator which broadens the contract of compare(ScoreDoc, ScoreDoc) to follow the same semantics as java.util.Comparator.compare() -- allow any integer to be returned, rather than specifically -1/0/-1.

Note that this behaviour must already be acceptable; the anonymous ScoreDocComparators returned by FieldSortedHitQueue.comparatorStringLocale() already return the result of Collator.compare(), which is not tied to this -1/0/1 restriction."
0,"TestBackwardsCompatibility needs terms with U+E000 to U+FFFFwe changed sort order in 4.0, and have sophisticated backwards compatibility (e.g. surrogates dance),
but we don't test this at all in TestBackwardsCompatibility.

for example, nothing handles this case for term vectors..."
0,"[API Doc] Document exceptions thrown on execute methodsThere should be more detailed documentation on HttpClient::executeMethod and
HttpMethod::execute about exceptions thrown in which cases."
1,"GetMethod.java checks the ""used"" flag which cannot be set at this timeGetMethod.getResponseBodyAsStream calls HttpMethodBase.checkUsed, which asserts
the flag ""used"" is ""true"". But at this time, ""used"" cannot be true, as ""used"" is
set to true in HttpMethodBase.processRequest, two lines after readResponse is
called (which in turn calls readResponseBody / readResponseBodyAsStream).

Maybe ""requestSent"" is the flag which should be checked instead of ""used""?

My stack trace: (fragment)

java.lang.IllegalStateException: Not Used.
        at
org.apache.commons.httpclient.HttpMethodBase.checkUsed(HttpMethodBase.java:1642)
        at
org.apache.commons.httpclient.methods.GetMethod.getResponseBodyAsStream(GetMethod.java:309)
        at
org.apache.commons.httpclient.methods.GetMethod.readResponseBody(GetMethod.java:428)
        at
org.apache.commons.httpclient.HttpMethodBase.readResponse(HttpMethodBase.java:1893)
        at
org.apache.commons.httpclient.HttpMethodBase.processRequest(HttpMethodBase.java:2496)
        at
org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java:1062)
        at
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:599)
        at
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:497)
        at
org.apache.webdav.lib.WebdavResource.getMethodData(WebdavResource.java:2227)
        at
org.apache.webdav.lib.WebdavResource.getMethodData(WebdavResource.java:2206)
[...]"
0,"Use FileLock for locking instead of empty fileThe FSDirectory uses File.createNewFile to effectively lock a directory (in makeLock), yet the Java Spec says explcitly not to use it for this purpose, and instead use FileLock from nio.

The attached patch shows how this is/could be done (change is internal to the makeLock method only, and functionally equivalent, the same tests apply)."
0,"Remove ""System Properties"" page from release specific docsWe no longer use system properties to configure Lucene in version 3.0, the page is obsolete and should be removed before release."
0,make NamespaceContext#getPrefix(java.lang.String) iterative instead of recursiveCurrently the method org.apache.jackrabbit.core.xml.NamespaceContext#getPrefix(java.lang.String) uses recursion. For very large XML files (50 MB Magnolia website exports) this causes a stack overflow. The method can easily be rewritten using iteration.
0,registerNodeType is not implemented in the CLIregisternodetype is listed in the standalone client but not implemented. 
1,"ObjectContentManagerImpl.getObject(Query) throws NoSuchElementException when query does not match an objectWhen a query returns no objects, ObjectContentManagerImpl.getObject(Query) throws the following exception:

java.util.NoSuchElementException
        at java.util.AbstractList$Itr.next(AbstractList.java:427)
        at org.apache.jackrabbit.ocm.manager.impl.ObjectContentManagerImpl.getObject(ObjectContentManagerImpl.java:538)

Javadocs for ObjectContentManager interface suggest that a ObjectContentManagerException should be thrown in this case."
1,"reopen on NRT reader should share readers w/ unchanged segmentsA repoen on an NRT reader doesn't seem to share readers for those segments that are unchanged.
http://search.lucidimagination.com/search/document/9f0335d480d2e637/nrt_and_caching_based_on_indexreader"
1,"Incorrect CND for mix:etagJackrabbit currently defined mix:etag as follows:

[mix:etag]
  mixin
  // currently has a default value because auto-creation not handled see JCR-2116
  - jcr:etag (STRING) = '' protected autocreated

I think this violates the spec, which says:

[mix:etag] mixin
  - jcr:etag (STRING) protected autocreated

This also affects the predefined node type test in jackrabbit-jcr-tests where mix-etag.txt is:

NodeTypeName
  mix:etag
Supertypes
  []
IsMixin
  true
HasOrderableChildNodes
  false
PrimaryItemName
  null
PropertyDefinition
  Name jcr:etag
  RequiredType STRING
  DefaultValues []
  AutoCreated true
  Mandatory false
  OnParentVersion COPY
  Protected true
  Multiple false

but should rather be:

NodeTypeName
  mix:etag
Supertypes
  []
IsMixin
  true
HasOrderableChildNodes
  false
PrimaryItemName
  null
PropertyDefinition
  Name jcr:etag
  RequiredType STRING
  DefaultValues null               <===
  AutoCreated true
  Mandatory false
  OnParentVersion COPY
  Protected true
  Multiple false
"
1,"jcr2spi: NPE with SessionImporter#checkIncludesMixReferenceable if NodeInfo doesn't contain mixin namesissue reported by tobi:

java.lang.NullPointerException
	at java.util.Arrays$ArrayList.<init>(Arrays.java:2355)
	at java.util.Arrays.asList(Arrays.java:2341)
	at org.apache.jackrabbit.jcr2spi.xml.SessionImporter.checkIncludesMixReferenceable(SessionImporter.java:637)
	at org.apache.jackrabbit.jcr2spi.xml.SessionImporter.startNode(SessionImporter.java:209)

including test case:

    public void testEmptyMixins() throws Exception {
        String xml = ""<?xml version=\""1.0\"" encoding=\""UTF-8\""?>\n"" +
                ""<sv:node xmlns:nt=\""http://www.jcp.org/jcr/nt/1.0\""\n"" +
                ""         xmlns:sv=\""http://www.jcp.org/jcr/sv/1.0\""\n"" +
                ""         xmlns:mix=\""http://www.jcp.org/jcr/mix/1.0\""\n"" +
                ""         xmlns:jcr=\""http://www.jcp.org/jcr/1.0\""\n"" +
                ""         sv:name=\""testnode1\"">\n"" +
                ""    <sv:property sv:name=\""jcr:primaryType\""
sv:type=\""Name\"">\n"" +
                ""        <sv:value>nt:unstructured</sv:value>\n"" +
                ""    </sv:property>\n"" +
                ""    <sv:property sv:name=\""jcr:title\"" sv:type=\""String\"">\n"" +
                ""        <sv:value>Test Node</sv:value>\n"" +
                ""    </sv:property>\n"" +
                ""    <sv:property sv:name=\""jcr:uuid\"" sv:type=\""String\"">\n"" +
                ""        <sv:value>1234</sv:value>\n"" +
                ""    </sv:property>\n"" +
                ""</sv:node>"";

        InputStream in = new ByteArrayInputStream(xml.getBytes());
        session.importXML(""/"", in,
ImportUUIDBehavior.IMPORT_UUID_COLLISION_THROW);
        session.save();
    }"
0,"move SmartChineseAnalyzer into the smartcn packagean offshoot of LUCENE-1862, org.apache.lucene.analysis.cn.SmartChineseAnalyzer should become org.apache.lucene.analysis.cn.smartcn.SmartChineseAnalyzer"
1,"Catch SocketTimeoutException not InterruptedIOExceptionThere are a couple of places where you're catching InterruptedIOException 
that should catch SocketTimeoutException instead.  For example, from 
HttpConnection:

    protected boolean isStale() {
        boolean isStale = true;
        if (isOpen) {
            // the connection is open, but now we have to see if we can 
read it
            // assume the connection is not stale.
            isStale = false;
            try {
                if (inputStream.available() == 0) {
                    try {
                        socket.setSoTimeout(1);
                        inputStream.mark(1);
                        int byteRead = inputStream.read();
                        if (byteRead == -1) {
                            // again - if the socket is reporting all data 
read,
                            // probably stale
                            isStale = true;
                        } else {
                            inputStream.reset();
                        }
                    } finally {
                        socket.setSoTimeout(this.params.getSoTimeout());
                    }
                }
            } catch (InterruptedIOException e) {
                // aha - the connection is NOT stale - continue on!

Here the catch of InterruptedIOException is intended to happen when 
inputStream.read() terminates due to the socket.setSoTimeout() time being 
reached.  However, it could also occur because Thread.interrupt() has been 
called, in which case ""continue on"" is not what should happen, instead, the 
request should terminate.

There are legitimate reasons why someone might want to interrupt the 
httpclient code, for example, httpclient does not provide a hard timeout on 
the total length of time a request may take, including connecting, sending 
the request, and receiving the complete response, so to enforce a hard 
timeout it is necessary to run the request in a worker thread and interrupt 
it if it hasn't completed before the timeout expires (the technique used in 
your TimeoutController class).

Note that SocketTimeoutException was added in 1.4.  For compatibility with 
older jdk versions, the code can catch InterruptedIOException and use 
getClass() to see whether it is a SocketTimeoutException.

There are probably other places in the code where InterruptedIOException is 
caught and interpreted as a socket timeout, and where Thread.interrupt() 
will not have the proper effect of causing the request to terminate ASAP, 
but I'm not familiar enough with the code to find them all."
0,Spellchecker should implement java.io.ClosableAs the most of the lucene classes implement Closable (IndexWriter) Spellchecker should do too. 
0,"Initialization error of Junit tests with solr-test-framework with IDEs and MavenI'm currently developping a new component for Solr. And in my Netbeans project, I have created two Test classes for this component: one class for simple unit tests (derived from  SolrTestCaseJ4 class) and a second one for tests with sharding (derived from  BaseDistributedSearchTestCase).
When I launch a test with these two classes, I have an error in the initialization of the second class of tests (no matter the class is, this is always the second executed class which fails). The error comes from an ""assert"" which failed in the begining of the function ""initRandom()"" of LuceneTestCase class :

assert !random.initialized;

But, if I launch each test class separatly, all the tests succeed!

After a discussion with Mr. Muir, the problems seems to be related to the incompatibility of the class LuceneTestCase with the functioning of Maven projects in IDEs.

According to mister Muir:

""
The problem is that via ant, tests work like this (e.g. for 3 test classes):
computeTestMethods
beforeClass
afterClass
computeTestMethods
beforeClass
AfterClass
computeTestMethods
beforeClass
afterClass

but via an IDE, if you run it from a folder like you did, then it does this:
computeTestMethods
computeTestMethods
computeTestMethods
beforeClass
afterClass
beforeClass
afterClass
beforeClass
afterClass 
"""
1,"Adding empty ParallelReader indexes to an IndexWriter may cause ArrayIndexOutOfBoundsException or NoSuchElementExceptionHi,
I recently stumbled upon this:

It is possible (and perfectly legal) to add empty indexes (IndexReaders) to an IndexWriter. However, when using ParallelReaders in this context, in two situations RuntimeExceptions may occur for no good reason.

Condition 1:
The indexes within the ParallelReader are just empty.

When adding them to the IndexWriter, we get a java.util.NoSuchElementException triggered by ParallelTermEnum's constructor. The reason for that is the TreeMap#firstKey() method which was assumed to return null if there is no entry (which is not true, apparently -- it only returns null if the first key in the Map is null).


Condition 2 (Assuming the aforementioned bug is fixed):
The indexes within the ParallelReader originally contained one or more fields with TermVectors, but all documents have been marked as deleted.

When adding the indexes to the IndexWriter, we get a java.lang.ArrayIndexOutOfBoundsException triggered by TermVectorsWriter#addAllDocVectors. The reason here is that TermVectorsWriter assumes that if the index is marked to have TermVectors, at least one field actually exists for that. This unfortunately is not true, either.

Patches and a testcase demonstrating the two bugs are provided.

Cheers,
Christian"
1,"inconsistent session state after Item/Session.save() throwing ReferentialIntegrityExceptionissue reported by Tomasz.Dabrowski@cognifide.com on jackrabbit dev list.

code fragment to reproduce issue:

// setup test case

Node parent = root.addNode(""a"", ""nt:unstructured"");
Node child1 = parent.addNode(""b"", ""nt:unstructured"");
child1.addMixin(""mix:referenceable"");
Node child2 = parent.addNode(""c"", ""nt:unstructured"");
child2.setProperty(""ref"", child1);
root.save();

// perform test

try {
    child1.remove();
    parent.save();
} catch (ReferentialIntegrityException rie) {
    // expected since child1 is still being referenced by property ""ref"" of child2
}

parent.remove();     // ==> should succeed but throws ItemNotFoundException 

"
0,"Move jackrabbit/trunk/contrib to jackrabbit/sandboxAs discussed on the mailing list (see http://www.nabble.com/Moving-contrib-outside-trunk-and-rename-to-sandbox-tf4635301.html), we should do the following:

    svn move https://svn.apache.org/repos/asf/jackrabbit/trunk/contrib https://svn.apache.org/repos/asf/jackrabbit/sandbox

I will do this in a few days unless anyone objects."
0,"Provide a non-pooling connection managerThe current implementations of the connection managers all have a connection
pool. For applications requiring only single requests very rarely this is
overkill. We should provide a very simple connection manager that uses a
connection only one time and then closes it right away."
1,"Unable to delete a non session-scoped locked node in XA EnvironmentYou must first add a valid lockToken to the Session and then try to remove this node in a XA Environment.
This will resulting in a NoSuchItemStateException: State has been marked destroyed.
The  problem is that the unlock Operation will be done after that the node has been marked for destroyed.
"
1,"minimizeHopcroft OOMEs on smallish (2096 states, finite) automatonNot sure what's up w/ this... if you check out the blocktree branch (LUCENE-3030) and comment out the @Ignore in TestTermsEnum2.testFiniteVersusInfinite then this should hit OOME: {[ant test-core -Dtestcase=TestTermsEnum2 -Dtestmethod=testFiniteVersusInfinite -Dtests.seed=-2577608857970454726:-2463580050179334504}}"
1,"offsets issues with multiword synonymsas reported on the list, there are some strange offsets with FSTSynonyms, in the case of multiword synonyms.

as a workaround it was suggested to use the older synonym impl, but it has bugs too (just in a different way).
"
0,"left nav of docs/index.html in dist artifacts links to hudson for javadocsWhen building the zip or tgz release artifacts, the docs/index.html file contained in that release (the starter point for people to read documentation) links ""API Docs"" to 
http://lucene.zones.apache.org:8080/hudson/job/Lucene-Nightly/javadoc/ instead of to ./api/index.html (the local copy of the javadocs)

this relates to the initial migration to hudson for the nightly builds and a plan to copy the javadocs back to lucene.apache.org that wasn't considered urgent since it was just for transient nightly docs, but a side affect is that the release documentation also links to hudson.

even if we don't modify the nightly build process before the 2.2 release, we should update the link in the left nav in the 2.2 release branch before building the final release."
1,"DefaultPrincipalProvider#collectGroupMembership puts wrong principal instance into the cacheDefaultPrincipalProvider#collectGroupMembership adds the passed principal instance to the cache. This may cause
inconsistencies as the cache should only contain principals obtained from by the provider."
0,"Node.setProperty(String, ...) implementation not according to the specificationto illustrate the issue assume the following  property definition:

name: someText
type: String
non-mandatory
non-autocreate

the following call would throw a ConstraintViolationException
if the property doesn't exist yet:

node.setProperty(""someText"", 12345);

the rules used to find an applicable definition in this case are too strict."
1,"Upgrade to commons-compress 1.2Commons Compress bug COMPRESS-127 was fixed in 1.2, so the workaround in benchmark's StreamUtils is no longer required. Compress is also used in solr. Replace with new jar in both benchmark and solr and get rid of that workaround."
0,switch appendingcodec to use appending blocktreecurrently it still uses block terms + fixed gap index
0,Upgrade to Derby 10.2Apache Derby 10.2 was released recently. The release contains a number of improvements (including performance) and requires no special upgrade procedures. I suggest we upgrade to Derby 10.2 along with Lucene 2.0 (JCR-352) and Maven 2 (JCR-332).
0,"SPI implementations currently need to provide implementations of both ValueFactory and QValueFactoryThis should be simplified so that an implementation of QValueFactory is sufficient.
"
0,"web.xml refers to 2.2 dtdthe web.xml present in the jackrabbit-webapp project contains the following doctype:

<!DOCTYPE web-app PUBLIC ""-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN"" ""http://java.sun.com/j2ee/dtds/web-app_2_2.dtd"">

what is probably meant is

<!DOCTYPE web-app PUBLIC ""-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN"" ""http://java.sun.com/dtd/web-app_2_3.dtd"">

since the dependency also points to version 2.3 of the servlet-api

if nobody objects, i will adjust it accordingly."
1,"Issues with compiled permissions of ACL provider- should not use search for infrastructure checks
- event listener never discarded."
1,"XMLPersistenceManager trims string property valuesThe XMLPersistenceManager trims the text of property values read in, so what's returned doesn't match the value set if it included whitespace at the start or end."
0,"Better MimeType HandlingAfter saving a Excel File through WebDAV the mimetype will be changed.
The mimetype for a Win2000 Exel File is application/vnd.ms-excel. This will be changed to application/msexcel.
Also problems makes the new office 07 format (docx,xlsx,pptx). They will also be changed to application/octet-stream (default mimetype).
We have a lot of file types that we store in jackrabbit that are not in the properties file (MSInfoPath-, OutlookMsg-, MsAccess-Files, ...)
I think it will be better to let the mimetype property untouched if a mimetype is present so we must not put all the possible mimetypes in the property file.

BR
claus"
0,"Maven artifacts for Lucene 4 are not stored in the correct pathHello,

I would like to use the maven artifacts for Lucene 4.0 produced by the Hudson build machine. The artifacts are correctly produced (http://hudson.zones.apache.org/hudson/view/Lucene/job/Lucene-trunk/lastSuccessfulBuild/artifact/maven_artifacts/lucene/).
However, the artifacts which should be stored under the path ""org/apache/lucene/"" are currently stored under ""lucene"" which prevents a project using maven to correctly download the Lucene 4.0 artifacts.

Thanks again for your help.  "
1,"Ordering of methods in PostMethod changes behaviourI have just spent the best part of two days trying to work out why
a servlet running in Tomcat was not getting UTF-8 when I had set my
client to send UTF-8. It turns out that if I set my PostMethod request
header after setting the request body the content does not get sent as
UTF-8.

The following gets sent as UTF-8:

      PostMethod post = new PostMethod(destinationUrl.toString());
      post.setStrictMode(false);
      post.setRequestHeader(""Content-Type"",""text/xml; charset=UTF-8"");
      post.setRequestHeader(""user-agent"", ""myAgent"");
      post.setRequestBody(content);
      post.setFollowRedirects(true);

the following doesn't:

      PostMethod post = new PostMethod(destinationUrl.toString());
      post.setStrictMode(false);
      post.setRequestBody(content);
      post.setRequestHeader(""Content-Type"",""text/xml; charset=UTF-8"");
      post.setRequestHeader(""user-agent"", ""myAgent"");
      post.setFollowRedirects(true);

In a live execution I would understand that order makes a big difference, but
when you fill out an object that feels like defining the values of a Java Bean
this likely to be less obvious."
1,"NRTCachingDir has invalid asserts (if same file name is written twice)Normally Lucene is write-once (except for segments.gen file, which NRTCachingDir never caches), but in some tests (TestDoc, TestCrash) we can write the same file more than once.

I don't think NRTCachingDir should have these asserts, and I think on createOutput it should remove any old file if present.

I also found & fixed a possible concurrency issue (if more than one thread syncs at the same time; IndexWriter doesn't ever do this today but it has in the past)."
0,"contrib/benchmark QueryMaker and Task RefactoringsIntroduce an abstract QueryMaker implementation that shares much of the common code between the various QueryMaker implementations.

Add in a new QueryMaker for reading queries from a file that is specified in the properties.

Patch shortly, and if no concerns, will commit tomorrow or Wed."
0,"Track total term freq per termRight now we track docFreq for each term (how many docs have the
term), but the totalTermFreq (total number of occurrences of this
term, ie sum of freq() for each doc that has the term) is also a
useful stat (for flex scoring, PulsingCodec, etc.).
"
1,"When HttpClient-Cache cannot open cache file, should act like missSet up HttpClient-Cache like this:
final String cacheDir = ""cachedir"";
HttpClient cachingHttpClient;
final CacheConfig cacheConfig = new CacheConfig();
cacheConfig.setSharedCache(false);
cacheConfig.setMaxObjectSizeBytes(262144); //256kb

if(! new File(cacheDir, ""httpclient-cache"").exists()){
	if(!new File(cacheDir, ""httpclient-cache"").mkdir()){
		throw new RuntimeException(""failed to create httpclient cache directory: "" + new File(cacheDir, ""httpclient-cache"").getAbsolutePath());
	}
}
final ResourceFactory resourceFactory = new FileResourceFactory(new File(cacheDir, ""httpclient-cache""));

final HttpCacheStorage httpCacheStorage = new ManagedHttpCacheStorage(cacheConfig);

cachingHttpClient = new CachingHttpClient(client, resourceFactory, httpCacheStorage, cacheConfig);

Then make a request:
final HttpGet get = new HttpGet(url);
final HttpResponse response = cachingHttpClient.execute(get);
final StatusLine statusLine = response.getStatusLine();
if (statusLine.getStatusCode() >= 300) {
	if(statusLine.getStatusCode() == 404)
		throw new NoResultException();
    throw new HttpResponseException(statusLine.getStatusCode(),
            statusLine.getReasonPhrase());
}
response.getEntity().getContent();

Everything worked as expected.

Now delete the cache directory (""cachedir/httpclient-cache"" in this example).

And make the same request again.

Actual:
 Caused by: java.lang.IllegalStateException: Content has been consumed
	at org.apache.http.entity.BasicHttpEntity.getContent(BasicHttpEntity.java:84)
	at org.apache.http.conn.BasicManagedEntity.getContent(BasicManagedEntity.java:100)

Expected:
HttpClient shouldn't throw an exception - it should just perform the request again acting like a cache miss."
0,"Add Boosting Function Term Query and Some Payload Query refactoringsSimilar to the BoostingTermQuery, the BoostingFunctionTermQuery is a SpanTermQuery, but the difference is the payload score for a doc is not the average of all the payloads, but applies a function to them instead.  BoostingTermQuery becomes a BoostingFunctionTermQuery with an AveragePayloadFunction applied to it.

Also add marker interface to indicate PayloadQuery types.  Refactor Similarity.scorePayload to also take in the doc id."
0,"NodeStateMerger.merge should abort if the primary type of the 2 states to be compare are not the samethe NodeStateMerger#merge currently aborts if the mixin types of the passed state and its overlayed state are not equal.
as of jsr 283 not the only the mixin types but also the primary type of a node can be modified.

for consistency reasons NodeStateMerger#merge should abort and return false if the primary types are not the same."
1,"FieldCache should not pay attention to deleted docs when creating entriesThe FieldCache uses a key that ignores deleted docs, so it's actually a bug to use deleted docs when creating an entry.  It can lead to incorrect values when the same entry is used with a different reader."
1,"SO_TIMEOUT parameter on the method level has no effectThis bug has been reported on the HttpClient user list by Ilya Kharmatsky <ilyak
-at- mainsoft.com>"
0,"Lock test assumes that changes in one session are immediately visible in different sessionLockTest.testLogout() assumes that a change in one session (logging out, removing a session-scoped lock) is immediately visible in another session.

Proposal: insert a 

 n1.getSession().refresh(true);

call before checking

 assertFalse(""node must not be locked"", n1.isLocked());"
0,"AttributeSource/TokenStream API improvementsThis patch makes the following improvements to AttributeSource and
TokenStream/Filter:

- introduces interfaces for all Attributes. The corresponding
  implementations have the postfix 'Impl', e.g. TermAttribute and
  TermAttributeImpl. AttributeSource now has a factory for creating
  the Attribute instances; the default implementation looks for
  implementing classes with the postfix 'Impl'. Token now implements
  all 6 TokenAttribute interfaces.

- new method added to AttributeSource:
  addAttributeImpl(AttributeImpl). Using reflection it walks up in the
  class hierarchy of the passed in object and finds all interfaces
  that the class or superclasses implement and that extend the
  Attribute interface. It then adds the interface->instance mappings
  to the attribute map for each of the found interfaces.

- removes the set/getUseNewAPI() methods (including the standard
  ones). Instead it is now enough to only implement the new API,
  if one old TokenStream implements still the old API (next()/next(Token)),
  it is wrapped automatically. The delegation path is determined via
  reflection (the patch determines, which of the three methods was
  overridden).

- Token is no longer deprecated, instead it implements all 6 standard
  token interfaces (see above). The wrapper for next() and next(Token)
  uses this, to automatically map all attribute interfaces to one
  TokenWrapper instance (implementing all 6 interfaces), that contains
  a Token instance. next() and next(Token) exchange the inner Token
  instance as needed. For the new incrementToken(), only one
  TokenWrapper instance is visible, delegating to the currect reusable
  Token. This API also preserves custom Token subclasses, that maybe
  created by very special token streams (see example in Backwards-Test).

- AttributeImpl now has a default implementation of toString that uses
  reflection to print out the values of the attributes in a default
  formatting. This makes it a bit easier to implement AttributeImpl,
  because toString() was declared abstract before.

- Cloning is now done much more efficiently in
  captureState. The method figures out which unique AttributeImpl
  instances are contained as values in the attributes map, because
  those are the ones that need to be cloned. It creates a single
  linked list that supports deep cloning (in the inner class
  AttributeSource.State). AttributeSource keeps track of when this
  state changes, i.e. whenever new attributes are added to the
  AttributeSource. Only in that case will captureState recompute the
  state, otherwise it will simply clone the precomputed state and
  return the clone. restoreState(AttributeSource.State) walks the
  linked list and uses the copyTo() method of AttributeImpl to copy
  all values over into the attribute that the source stream
  (e.g. SinkTokenizer) uses. 

- Tee- and SinkTokenizer were deprecated, because they use
Token instances for caching. This is not compatible to the new API
using AttributeSource.State objects. You can still use the old
deprecated ones, but new features provided by new Attribute types
may get lost in the chain. A replacement is a new TeeSinkTokenFilter,
which has a factory to create new Sink instances, that have compatible
attributes. Sink instances created by one Tee can also be added to
another Tee, as long as the attribute implementations are compatible
(it is not possible to add a sink from a tee using one Token instance
to a tee using the six separate attribute impls). In this case UOE is thrown.

The cloning performance can be greatly improved if not multiple
AttributeImpl instances are used in one TokenStream. A user can
e.g. simply add a Token instance to the stream instead of the individual
attributes. Or the user could implement a subclass of AttributeImpl that
implements exactly the Attribute interfaces needed. I think this
should be considered an expert API (addAttributeImpl), as this manual
optimization is only needed if cloning performance is crucial. I ran
some quick performance tests using Tee/Sink tokenizers (which do
cloning) and the performance was roughly 20% faster with the new
API. I'll run some more performance tests and post more numbers then.

Note also that when we add serialization to the Attributes, e.g. for
supporting storing serialized TokenStreams in the index, then the
serialization should benefit even significantly more from the new API
than cloning. 

This issue contains one backwards-compatibility break:
TokenStreams/Filters/Tokenizers should normally be final
(see LUCENE-1753 for the explaination). Some of these core classes are 
not final and so one could override the next() or next(Token) methods.
In this case, the backwards-wrapper would automatically use
incrementToken(), because it is implemented, so the overridden
method is never called. To prevent users from errors not visible
during compilation or testing (the streams just behave wrong),
this patch makes all implementation methods final
(next(), next(Token), incrementToken()), whenever the class
itsself is not final. This is a BW break, but users will clearly see,
that they have done something unsupoorted and should better
create a custom TokenFilter with their additional implementation
(instead of extending a core implementation).

For further changing contrib token streams the following procedere should be used:

    *  rewrite and replace next(Token)/next() implementations by new API
    * if the class is final, no next(Token)/next() methods needed (must be removed!!!)
    * if the class is non-final add the following methods to the class:
{code:java}
      /** @deprecated Will be removed in Lucene 3.0. This method is final, as it should
       * not be overridden. Delegates to the backwards compatibility layer. */
      public final Token next(final Token reusableToken) throws java.io.IOException {
        return super.next(reusableToken);
      }

      /** @deprecated Will be removed in Lucene 3.0. This method is final, as it should
       * not be overridden. Delegates to the backwards compatibility layer. */
      public final Token next() throws java.io.IOException {
        return super.next();
      }
{code}
Also the incrementToken() method must be final in this case
(and the new method end() of LUCENE-1448)
"
1,"StackOverflowError in HttpConnectionWhen the HttpConnection#WrappedOutputStream.flush () encounters IOException 
druign write, it is calling HttpConnection.close which calls 
HttpConnection.closeSocketAndStreams and which eventually calls 
HttpConnection#WrappedOutputStream.flush again.  The circular calls will cause 
StackOverflowError.

I run into this accidentally when I was trying to extend HttpConnection.  But 
looking through the code, I believe any IOException may cause the same 
problem.  The circular calls should be either removed or controlled.  Below is 
part of teh stack trace

java.lang.StackOverflowError
        at java.lang.Exception.<init>(Unknown Source)
        at java.io.IOException.<init>(Unknown Source)
        at java.net.SocketException.<init>(Unknown Source)
        at java.net.SocketOutputStream.socketWrite(Native Method)
        at java.net.SocketOutputStream.write(Unknown Source)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.writ
e(HttpConnection.java:1273)
        at java.io.BufferedOutputStream.flushBuffer(Unknown Source)
        at java.io.BufferedOutputStream.flush(Unknown Source)
        at java.io.FilterOutputStream.close(Unknown Source)
        at org.apache.commons.httpclient.HttpConnection.closeSocketAndStreams(Ht
tpConnection.java:1083)
        at org.apache.commons.httpclient.HttpConnection.close(HttpConnection.jav
a:1024)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.hand
leException(HttpConnection.java:1235)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.writ
e(HttpConnection.java:1275)
        at java.io.BufferedOutputStream.flushBuffer(Unknown Source)
        at java.io.BufferedOutputStream.flush(Unknown Source)
        at java.io.FilterOutputStream.close(Unknown Source)
        at org.apache.commons.httpclient.HttpConnection.closeSocketAndStreams(Ht
tpConnection.java:1083)
        at org.apache.commons.httpclient.HttpConnection.close(HttpConnection.jav
a:1024)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.hand
leException(HttpConnection.java:1235)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.writ
e(HttpConnection.java:1275)
        at java.io.BufferedOutputStream.flushBuffer(Unknown Source)
        at java.io.BufferedOutputStream.flush(Unknown Source)
        at java.io.FilterOutputStream.close(Unknown Source)
        at org.apache.commons.httpclient.HttpConnection.closeSocketAndStreams(Ht
tpConnection.java:1083)
        at org.apache.commons.httpclient.HttpConnection.close(HttpConnection.jav
a:1024)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.hand
leException(HttpConnection.java:1235)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.writ
e(HttpConnection.java:1275)
        at java.io.BufferedOutputStream.flushBuffer(Unknown Source)
        at java.io.BufferedOutputStream.flush(Unknown Source)
        at java.io.FilterOutputStream.close(Unknown Source)
        at org.apache.commons.httpclient.HttpConnection.closeSocketAndStreams(Ht
tpConnection.java:1083)
        at org.apache.commons.httpclient.HttpConnection.close(HttpConnection.jav
a:1024)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.hand
leException(HttpConnection.java:1235)
        at org.apache.commons.httpclient.HttpConnection$WrappedOutputStream.writ
e(HttpConnection.java:1275)"
0,"Allow (or bring back) the ability to setRAMBufferSizeMB on an open IndexWriterIn 3.1 the ability to setRAMBufferSizeMB is deprecated, and removed in trunk. It would be great to be able to control that on a live IndexWriter. Other possible two methods that would be great to bring back are setTermIndexInterval and setReaderTermsIndexDivisor. Most of the other setters can actually be set on the MergePolicy itself, so no need for setters for those (I think)."
0,"o.a.l.messages should be moved to corecontrib/queryParser contains an org.apache.lucene.messages package containing some generallized code that (claims in it's javadocs) is not specific to the queryParser.

If this is truely general purpose code, it should probably be moved out of hte queryParser contrib -- either into it's own contrib, or into the core (it's very small)

*EDIT:* alternate suggestion to rename package to fall under the o.a.l.queryParser namespace retracted due to comments in favor of (eventually) promoting to it's own contrib"
0,XMLIndexFilter should index the attributestext/xml indexer extracts the elements text but not the attribute values.
0,"CLONE -QueryParser is not applicable for the arguments (String, String, Analyzer) error in results.jsp when executing search in the browser (demo from Lucene 2.0)When executing search in the browser (as described in demo3.html Lucene demo) I get error, because the demo uses the method (QueryParser with three arguments) which is deleted (it was deprecated).
I checked the demo from Lucene 1.4-final it with Lucene 1.4-final - it works, because those time the method was there.
But demo from Lucene 2.0 does not work with Lucene 2.0

The error stack is here:
TTP Status 500 -

type Exception report

message

description The server encountered an internal error () that prevented it from fulfilling this request.

exception

org.apache.jasper.JasperException: Unable to compile class for JSP

An error occurred at line: 60 in the jsp file: /results.jsp
Generated servlet error:
The method parse(String) in the type QueryParser is not applicable for the arguments (String, String, Analyzer)


org.apache.jasper.servlet.JspServletWrapper.handleJspException(JspServletWrapper.java:510)
org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:375)
org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:314)
org.apache.jasper.servlet.JspServlet.service(JspServlet.java:264)
javax.servlet.http.HttpServlet.service(HttpServlet.java:802)

root cause

org.apache.jasper.JasperException: Unable to compile class for JSP

An error occurred at line: 60 in the jsp file: /results.jsp
Generated servlet error:
The method parse(String) in the type QueryParser is not applicable for the arguments (String, String, Analyzer)


org.apache.jasper.compiler.DefaultErrorHandler.javacError(DefaultErrorHandler.java:84)
org.apache.jasper.compiler.ErrorDispatcher.javacError(ErrorDispatcher.java:328)
org.apache.jasper.compiler.JDTCompiler.generateClass(JDTCompiler.java:409)
org.apache.jasper.compiler.Compiler.compile(Compiler.java:297)
org.apache.jasper.compiler.Compiler.compile(Compiler.java:276)
org.apache.jasper.compiler.Compiler.compile(Compiler.java:264)
org.apache.jasper.JspCompilationContext.compile(JspCompilationContext.java:563)
org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:303)
org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:314)
org.apache.jasper.servlet.JspServlet.service(JspServlet.java:264)
javax.servlet.http.HttpServlet.service(HttpServlet.java:802)

note The full stack trace of the root cause is available in the Apache Tomcat/5.5.15 logs."
1,"[PATCH] TermInfosReader, SegmentTermEnum Out Of Memory ExceptionWe've been experiencing terrible memory problems on our production search server, running lucene (1.4.3).

Our live app regularly opens new indexes and, in doing so, releases old IndexReaders for garbage collection.

But...there appears to be a memory leak in org.apache.lucene.index.TermInfosReader.java.
Under certain conditions (possibly related to JVM version, although I've personally observed it under both linux JVM 1.4.2_06, and 1.5.0_03, and SUNOS JVM 1.4.1) the ThreadLocal member variable, ""enumerators"" doesn't get garbage-collected when the TermInfosReader object is gc-ed.

Looking at the code in TermInfosReader.java, there's no reason why it _shouldn't_ be gc-ed, so I can only presume (and I've seen this suggested elsewhere) that there could be a bug in the garbage collector of some JVMs.

I've seen this problem briefly discussed; in particular at the following URL:
  http://java2.5341.com/msg/85821.html
The patch that Doug recommended, which is included in lucene-1.4.3 doesn't work in our particular circumstances. Doug's patch only clears the ThreadLocal variable for the thread running the finalizer (my knowledge of java breaks down here - I'm not sure which thread actually runs the finalizer). In our situation, the TermInfosReader is (potentially) used by more than one thread, meaning that Doug's patch _doesn't_ allow the affected JVMs to correctly collect garbage.

So...I've devised a simple patch which, from my observations on linux JVMs 1.4.2_06, and 1.5.0_03, fixes this problem.

Kieran
PS Thanks to daniel naber for pointing me to jira/lucene

@@ -19,6 +19,7 @@
 import java.io.IOException;

 import org.apache.lucene.store.Directory;
+import java.util.Hashtable;

 /** This stores a monotonically increasing set of <Term, TermInfo> pairs in a
  * Directory.  Pairs are accessed either by Term or by ordinal position the
@@ -29,7 +30,7 @@
   private String segment;
   private FieldInfos fieldInfos;

-  private ThreadLocal enumerators = new ThreadLocal();
+  private final Hashtable enumeratorsByThread = new Hashtable();
   private SegmentTermEnum origEnum;
   private long size;

@@ -60,10 +61,10 @@
   }

   private SegmentTermEnum getEnum() {
-    SegmentTermEnum termEnum = (SegmentTermEnum)enumerators.get();
+    SegmentTermEnum termEnum = (SegmentTermEnum)enumeratorsByThread.get(Thread.currentThread());
     if (termEnum == null) {
       termEnum = terms();
-      enumerators.set(termEnum);
+      enumeratorsByThread.put(Thread.currentThread(), termEnum);
     }
     return termEnum;
   }
@@ -195,5 +196,15 @@
   public SegmentTermEnum terms(Term term) throws IOException {
     get(term);
     return (SegmentTermEnum)getEnum().clone();
+  }
+
+  /* some jvms might have trouble gc-ing enumeratorsByThread */
+  protected void finalize() throws Throwable {
+    try {
+        // make sure gc can clear up.
+        enumeratorsByThread.clear();
+    } finally {
+        super.finalize();
+    }
   }
 }



TermInfosReader.java, full source:
======================================
package org.apache.lucene.index;

/**
 * Copyright 2004 The Apache Software Foundation
 *
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.IOException;

import org.apache.lucene.store.Directory;
import java.util.Hashtable;

/** This stores a monotonically increasing set of <Term, TermInfo> pairs in a
 * Directory.  Pairs are accessed either by Term or by ordinal position the
 * set.  */

final class TermInfosReader {
  private Directory directory;
  private String segment;
  private FieldInfos fieldInfos;

  private final Hashtable enumeratorsByThread = new Hashtable();
  private SegmentTermEnum origEnum;
  private long size;

  TermInfosReader(Directory dir, String seg, FieldInfos fis)
       throws IOException {
    directory = dir;
    segment = seg;
    fieldInfos = fis;

    origEnum = new SegmentTermEnum(directory.openFile(segment + "".tis""),
                                   fieldInfos, false);
    size = origEnum.size;
    readIndex();
  }

  public int getSkipInterval() {
    return origEnum.skipInterval;
  }

  final void close() throws IOException {
    if (origEnum != null)
      origEnum.close();
  }

  /** Returns the number of term/value pairs in the set. */
  final long size() {
    return size;
  }

  private SegmentTermEnum getEnum() {
    SegmentTermEnum termEnum = (SegmentTermEnum)enumeratorsByThread.get(Thread.currentThread());
    if (termEnum == null) {
      termEnum = terms();
      enumeratorsByThread.put(Thread.currentThread(), termEnum);
    }
    return termEnum;
  }

  Term[] indexTerms = null;
  TermInfo[] indexInfos;
  long[] indexPointers;

  private final void readIndex() throws IOException {
    SegmentTermEnum indexEnum =
      new SegmentTermEnum(directory.openFile(segment + "".tii""),
			  fieldInfos, true);
    try {
      int indexSize = (int)indexEnum.size;

      indexTerms = new Term[indexSize];
      indexInfos = new TermInfo[indexSize];
      indexPointers = new long[indexSize];

      for (int i = 0; indexEnum.next(); i++) {
	indexTerms[i] = indexEnum.term();
	indexInfos[i] = indexEnum.termInfo();
	indexPointers[i] = indexEnum.indexPointer;
      }
    } finally {
      indexEnum.close();
    }
  }

  /** Returns the offset of the greatest index entry which is less than or equal to term.*/
  private final int getIndexOffset(Term term) throws IOException {
    int lo = 0;					  // binary search indexTerms[]
    int hi = indexTerms.length - 1;

    while (hi >= lo) {
      int mid = (lo + hi) >> 1;
      int delta = term.compareTo(indexTerms[mid]);
      if (delta < 0)
	hi = mid - 1;
      else if (delta > 0)
	lo = mid + 1;
      else
	return mid;
    }
    return hi;
  }

  private final void seekEnum(int indexOffset) throws IOException {
    getEnum().seek(indexPointers[indexOffset],
	      (indexOffset * getEnum().indexInterval) - 1,
	      indexTerms[indexOffset], indexInfos[indexOffset]);
  }

  /** Returns the TermInfo for a Term in the set, or null. */
  TermInfo get(Term term) throws IOException {
    if (size == 0) return null;

    // optimize sequential access: first try scanning cached enum w/o seeking
    SegmentTermEnum enumerator = getEnum();
    if (enumerator.term() != null                 // term is at or past current
	&& ((enumerator.prev != null && term.compareTo(enumerator.prev) > 0)
	    || term.compareTo(enumerator.term()) >= 0)) {
      int enumOffset = (int)(enumerator.position/enumerator.indexInterval)+1;
      if (indexTerms.length == enumOffset	  // but before end of block
	  || term.compareTo(indexTerms[enumOffset]) < 0)
	return scanEnum(term);			  // no need to seek
    }

    // random-access: must seek
    seekEnum(getIndexOffset(term));
    return scanEnum(term);
  }

  /** Scans within block for matching term. */
  private final TermInfo scanEnum(Term term) throws IOException {
    SegmentTermEnum enumerator = getEnum();
    while (term.compareTo(enumerator.term()) > 0 && enumerator.next()) {}
    if (enumerator.term() != null && term.compareTo(enumerator.term()) == 0)
      return enumerator.termInfo();
    else
      return null;
  }

  /** Returns the nth term in the set. */
  final Term get(int position) throws IOException {
    if (size == 0) return null;

    SegmentTermEnum enumerator = getEnum();
    if (enumerator != null && enumerator.term() != null &&
        position >= enumerator.position &&
	position < (enumerator.position + enumerator.indexInterval))
      return scanEnum(position);		  // can avoid seek

    seekEnum(position / enumerator.indexInterval); // must seek
    return scanEnum(position);
  }

  private final Term scanEnum(int position) throws IOException {
    SegmentTermEnum enumerator = getEnum();
    while(enumerator.position < position)
      if (!enumerator.next())
	return null;

    return enumerator.term();
  }

  /** Returns the position of a Term in the set or -1. */
  final long getPosition(Term term) throws IOException {
    if (size == 0) return -1;

    int indexOffset = getIndexOffset(term);
    seekEnum(indexOffset);

    SegmentTermEnum enumerator = getEnum();
    while(term.compareTo(enumerator.term()) > 0 && enumerator.next()) {}

    if (term.compareTo(enumerator.term()) == 0)
      return enumerator.position;
    else
      return -1;
  }

  /** Returns an enumeration of all the Terms and TermInfos in the set. */
  public SegmentTermEnum terms() {
    return (SegmentTermEnum)origEnum.clone();
  }

  /** Returns an enumeration of terms starting at or after the named term. */
  public SegmentTermEnum terms(Term term) throws IOException {
    get(term);
    return (SegmentTermEnum)getEnum().clone();
  }

  /* some jvms might have trouble gc-ing enumeratorsByThread */ 
  protected void finalize() throws Throwable {
    try {
        // make sure gc can clear up.
        enumeratorsByThread.clear();
    } finally {
        super.finalize();
    }
  }
}
"
1,"MultiFieldQueryParser ignores slop parameterMultiFieldQueryParser.getFieldQuery(String, String, int) calls super.getFieldQuery(String, String), thus obliterating any slop parameter present in the query.

It should probably be changed to call super.getFieldQuery(String, String, int), except doing only that will result in a recursive loop which is a side-effect of what may be a deeper problem in MultiFieldQueryParser -- getFieldQuery(String, String, int) is documented as delegating to getFieldQuery(String, String), yet what it actually does is the exact opposite.  This also causes problems for subclasses which need to override getFieldQuery(String, String) to provide different behaviour.
"
0,"add LuceneTestCase.rarely()/LuceneTestCase.atLeast()in LUCENE-3175, the tests were sped up a lot by using reasonable number of iterations normally, but cranking up for NIGHTLY.
we also do crazy things more 'rarely' for normal builds (e.g. simpletext, payloads, crazy merge params, etc)
also, we found some bugs by doing this, because in general our parameters are too fixed.

however, it made the code look messy... I propose some new methods:
instead of some crazy code in your test like:
{code}
int numdocs = (TEST_NIGHTLY ? 1000 : 100) * RANDOM_MULTIPLIER;
{code}

you use:
{code}
int numdocs = atLeast(100);
{code}

this will apply the multiplier, also factor in nightly, and finally add some random fudge... so e.g. in local runs its sometimes 127 docs, sometimes 113 docs, etc.

additionally instead of code like:
{code}
if ((TEST_NIGHTLY && random.nextBoolean()) || (random.nextInt(20) == 17)) {
{code}

you do
{code}
if (rarely()) {
{code}

which applies NIGHTLY and also the multiplier (logarithmic growth).
"
0,"JCR2SPI: add JNDI supportadding jndi support to jcr2spi was one of the improvements that came up during the f2f.
julian volunteered to take a look at it."
0,JSR 283 Observation
1,"Node.orderBefore does not check permissionsIt seems that Node.orderBefore(String, String) does not check if the editing session is allowed to modify the parent, neither immediately nor upon saving the transient changes.

This issue was found by Alexandre Capt. Thanks!"
0,"Regex support and beyond in JavaCC QueryParserSince the early days the standard query parser was limited to the queries living in core, adding other queries or extending the parser in any way always forced people to change the grammar file and regenerate. Even if you change the grammar you have to be extremely careful how you modify the parser so that other parts of the standard parser are affected by customisation changes. Eventually you had to live with all the limitation the current parser has like tokenizing on whitespaces before a tokenizer / analyzer has the chance to look at the tokens. 
I was thinking about how to overcome the limitation and add regex support to the query parser without introducing any dependency to core. I added a new special character that basically prevents the parser from interpreting any of the characters enclosed in the new special characters. I choose the forward slash  '/' as the delimiter so that everything in between two forward slashes is basically escaped and ignored by the parser. All chars embedded within forward slashes are treated as one token even if it contains other special chars like * []?{} or whitespaces. This token is subsequently passed to a pluggable ""parser extension"" with builds a query from the embedded string. I do not interpret the embedded string in any way but leave all the subsequent work to the parser extension. Such an extension could be another full featured query parser itself or simply a ctor call for regex query. The interface remains quiet simple but makes the parser extendible in an easy way compared to modifying the javaCC sources.

The downsides of this patch is clearly that I introduce a new special char into the syntax but I guess that would not be that much of a deal as it is reflected in the escape method though. It would truly be nice to have more than once extension an have this even more flexible so treat this patch as a kickoff though.

Another way of solving the problem with RegexQuery would be to move the JDK version of regex into the core and simply have another method like:
{code}
protected Query newRegexQuery(Term t) {
  ... 
}
{code}

which I would like better as it would be more consistent with the idea of the query parser to be a very strict and defined parser.

I will upload a patch in a second which implements the extension based approach I guess I will add a second patch with regex in core soon too.

"
1,"Event filtering by path not working as specifiedWhen filtering node events by path, the event filter doesn't compare using the ""associated parent path"", see JSR-170, 8.3.3:

""The set of events can be filtered by specifying restrictions based on characteristics of the associated parent node of the event. The associated parent node of an event is the parent node of the item at (or formerly at) the path returned by Event.getPath. The following restrictions are available:

 absPath, isDeep: Only events whose associated parent node is at absPath (or within its subtree, if isDeep is true) will be received. It is permissible to register a listener for a path where no node currently exists.""

(for property events, filtering is correct)

To fix this, the special handling of node events in EventFilter.blocks() simply needs to be removed.
"
0,"Field specified norms in MatchAllDocumentsScorer This patch allows for optionally setting a field to use for norms factoring when scoring a MatchingAllDocumentsQuery.

From the test case:
{code:java}
.
    RAMDirectory dir = new RAMDirectory();
    IndexWriter iw = new IndexWriter(dir, new StandardAnalyzer(), true, IndexWriter.MaxFieldLength.LIMITED);
    iw.setMaxBufferedDocs(2);  // force multi-segment
    addDoc(""one"", iw, 1f);
    addDoc(""two"", iw, 20f);
    addDoc(""three four"", iw, 300f);
    iw.close();

    IndexReader ir = IndexReader.open(dir);
    IndexSearcher is = new IndexSearcher(ir);
    ScoreDoc[] hits;

    // assert with norms scoring turned off

    hits = is.search(new MatchAllDocsQuery(), null, 1000).scoreDocs;
    assertEquals(3, hits.length);
    assertEquals(""one"", ir.document(hits[0].doc).get(""key""));
    assertEquals(""two"", ir.document(hits[1].doc).get(""key""));
    assertEquals(""three four"", ir.document(hits[2].doc).get(""key""));

    // assert with norms scoring turned on

    MatchAllDocsQuery normsQuery = new MatchAllDocsQuery(""key"");
    assertEquals(3, hits.length);
//    is.explain(normsQuery, hits[0].doc);
    hits = is.search(normsQuery, null, 1000).scoreDocs;

    assertEquals(""three four"", ir.document(hits[0].doc).get(""key""));    
    assertEquals(""two"", ir.document(hits[1].doc).get(""key""));
    assertEquals(""one"", ir.document(hits[2].doc).get(""key""));
{code}"
0,Can't specify AttributeSource for TokenizerOne can't currently specify the attribute source for a Tokenizer like one can with any other TokenStream.
0,"org.apache.jackrabbit.server.SessionProvider needs 'releaseSession()'the SessionProvider does not have a 'releaseSession()' method, thus the DavSessionProviderImpl just calls repSession.logout() after is DavSession is released. This should rather be handled over to the given SessionProvider."
1,"Flexible QueryParser fails with local different from en_USI get the following error during the mentioned testcases on my computer, if I use the Locale de_DE (windows 32):

{code}
    [junit] Testsuite: org.apache.lucene.queryParser.standard.TestQPHelper
    [junit] Tests run: 29, Failures: 1, Errors: 0, Time elapsed: 1,156 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] Result: (fieldX:xxxxx fieldy:xxxxxxxx)^2.0
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testLocalDateFormat(org.apache.lucene.queryParser.standard.TestQPHelper): FAILED
    [junit] expected:<1> but was:<0>
    [junit] junit.framework.AssertionFailedError: expected:<1> but was:<0>
    [junit]     at org.apache.lucene.queryParser.standard.TestQPHelper.assertHits(TestQPHelper.java:1148)
    [junit]     at org.apache.lucene.queryParser.standard.TestQPHelper.testLocalDateFormat(TestQPHelper.java:1005)
    [junit]     at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:201)
    [junit]
    [junit]
    [junit] Test org.apache.lucene.queryParser.standard.TestQPHelper FAILED
    [junit] Testsuite: org.apache.lucene.queryParser.standard.TestQueryParserWrapper
    [junit] Tests run: 27, Failures: 1, Errors: 0, Time elapsed: 1,219 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] Result: (fieldX:xxxxx fieldy:xxxxxxxx)^2.0
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testLocalDateFormat(org.apache.lucene.queryParser.standard.TestQueryParserWrapper):       FAILED
    [junit] expected:<1> but was:<0>
    [junit] junit.framework.AssertionFailedError: expected:<1> but was:<0>
    [junit]     at org.apache.lucene.queryParser.standard.TestQueryParserWrapper.assertHits(TestQueryParserWrapper.java:1120)
    [junit]     at org.apache.lucene.queryParser.standard.TestQueryParserWrapper.testLocalDateFormat(TestQueryParserWrapper.java:985)
    [junit]     at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:201)
    [junit]
    [junit]
    [junit] Test org.apache.lucene.queryParser.standard.TestQueryParserWrapper FAILED
{code}

With en_US as locale it works."
1,"LogSource.setLevel incorrectly uses entrySetWhen I call LogSource.setLevel, I get the following exception:

java.lang.ClassCastException: java.util.HashMap$Entry
	at org.apache.commons.httpclient.log.LogSource.setLevel
(LogSource.java:158)

The calling code is :

    LogSource.setLevel (Log.OFF);

The error (I believe) is that you should get the value set from the map, not 
the entry set (in LogSource):

    static public void setLevel(int level) {
        Iterator it = _logs.entrySet().iterator(); <-- should be _logs.values()
        while(it.hasNext()) {
            Log log = (Log)(it.next());
            log.setLevel(level);
        }
    }"
0,"Simplify NRTManagerNRTManager is hairy now, because the applyDeletes is separately passed
to ctor, passed to maybeReopen, passed to getSearcherManager, etc.

I think, instead, you should pass it only to the ctor, and if you have
some cases needing deletes and others not then you can make two
NRTManagers.  This should be no less efficient than we have today,
just simpler.

I think it will also enable NRTManager to subclass ThingyManager
(LUCENE-3761).
"
0,"Provide limit on phrase analysis in FastVectorHighlighterWith larger documents, FVH can spend a lot of time trying to find the best-scoring snippet as it examines every possible phrase formed from matching terms in the document.  If one is willing to accept
less-than-perfect scoring by limiting the number of phrases that are examined, substantial speedups are possible.  This is analogous to the Highlighter limit on the number of characters to analyze.

The patch includes an artifical test case that shows > 1000x speedup.  In a more normal test environment, with English documents and random queries, I am seeing speedups of around 3-10x when setting phraseLimit=1, which has the effect of selecting the first possible snippet in the document.  Most of our sites operate in this way (just show the first snippet), so this would be a big win for us.

With phraseLimit = -1, you get the existing FVH behavior. At larger values of phraseLimit, you may not get substantial speedup in the normal case, but you do get the benefit of protection against blow-up in pathological cases.
"
0,"Response handlersPerhaps plugin handlers should be used to handle various ranges of http
responses.  Could be used to respond, auto-forward, resubmit ... Could solve the
difficulty in handling a 303 response."
0,"Download: improve user experienceThe download section at http://jackrabbit.apache.org/downloads.html
contains many files. The number of files should be reduced.

Some of them contain other files, for example the .rar files, and the .war files. 
The file jackrabbit-jca-1.4.rar  contains an old version of jackrabbit-core:
jackrabbit-core-1.4.jar - however the newest version of jackrabbit-core is 
jackrabbit-core-1.4.5.jar

This often leads to problems."
0,"singletermsenumsingletermsenum for flex (like the existing singletermenum, it is a filteredtermSenum that only matches one term, to preserve multitermquery semantics)"
1,"add checks/asserts if you search across a closed readerif you try to search across a closed reader (and/or searcher too),
there are no checks, not even assertions statements.

this results in crazy scary stacktraces deep inside places like FSTs/various term dictionary implementations etc.

In some situations, depending on codec, you wont even get an error (i'm sure its fun when you try to retrieve the stored fields!)
"
0,"Remove geronimo JTA as a runtime dependencyGeronimo JTA is marked as a dependency for runtime when it should be (at most) a compile time dependency. 
Is it possible to remedy this so when using the war or building your own, you don't get the geronimo jar stowing away?"
0,"Make prefixLength accessible to PrefixTermEnum subclassesPrefixTermEnum#difference() offers a way to influence scoring based on the difference between the prefix Term and a term in the enumeration. To effectively use this facility the length of the prefix should be accessible to subclasses. Currently the prefix term is private to PrefixTermEnum. I added a getter for the prefix length and made PrefixTermEnum#endEnum(), PrefixTermEnum#termCompare() final for consistency with other TermEnum subclasses.

Patch is attached.

Simon"
0,"org.apache.http.impl.client.cache.memcached.MemcachedHttpCacheStorage should allow client to specify custom prefix string for keysorg.apache.http.impl.client.cache.memcached.MemcachedHttpCacheStorage should allow client to specify custom prefix string for keys, so as to ensure collision-avoidance with other memcached keys the client may be using."
1,Creating QValue from stream: stream not closedQValueFactoryImpl.create(InputStream) does not close the input stream as mandated by the contract. 
1,"In modules/analysys/icu, ant gennorm2 does not work
Command to run gennorm2 does not work at present.  Also, icupkg needs to be called to convert the binary file to big-endian.

I will attach a patch."
1,"302 response without location header throws exceptionHi, 

According to HTTP 1.1 Spec : http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.3.3
""The temporary URI SHOULD be given by the Location field in the response. Unless the request method was HEAD, the entity of the response SHOULD contain a short hypertext note with a hyperlink to the new URI(s).""

Now, in ""DefaultRedirectStrategy.getLocationURI()"", there's a ProtocolException thrown if location header is null.  

if (locationHeader == null) {
    // got a redirect response, but no location header
    throw new ProtocolException(
        ""Received redirect response "" + response.getStatusLine()
       + "" but no location header"");
 }

The specs says ""SHOULD"" and not ""MUST"". ProtocolException ""signals that an HTTP protocol violation has occurred"", which is not exactly true."
0,JFlex-based HTMLStripCharFilter replacementA JFlex-based HTMLStripCharFilter replacement would be more performant and easier to understand and maintain.
1,"InternalVersionManagerBase; missing null check after getNode()There are at least two instances where we check for a node with hasNode(), and then call getNode() without checking for null."
1,"DavMethodBase#getResponseException fails if the body is not (valid) XMLI have a set up that uses the JCR Webdav Server from a custom remote client.
I've noticed one thing, anytime I request a node that doesn't exist the error that comes back from the server is as follows:

[Fatal Error] :1:941: The element type ""HR"" must be terminated by the matching end-tag ""</HR>"".
javax.jcr.RepositoryException: The element type ""HR"" must be terminated by the matching end-tag ""</HR>"".: The element type ""HR"" must be terminated by the matching end-tag ""</HR>"".

Doesn't really make sense, but that is OK, I can handle that.

My problem:
I have a partially populated repository that at the root has a few nodes like
/edu/....
/com/ibm/..

So, I want to create a few nodes of type nt:folder under

com/myCompany/folder1

I have no problem creating them, but since ""com"" already exists I end up with
com[2]/myCompany/folder1.

So, I went ahead and used the parentNode.hasNode(""folderName"") method.

This method returns true for the ""com"" portion, but when I test for the ""myCompany"" folder which should return false I get the error response shown above from the server.

The webdav request looks as follows:
PROPFIND /jackrabbit/server/default/jcr%3aroot/com/myCompany

The snippet of code looks as follows:
  private Node createFolders (Session session, Node parentNode, List <String> folders)
	  throws RepositoryException {
    Node folderNode = null;
    for (String folder : folders) {
	if (parentNode.hasNode(folder))
	    folderNode = parentNode.getNode(folder);
	else
	    folderNode = parentNode.addNode(folder, ""nt:folder"");
	parentNode = folderNode;
    }
    session.save();
    return (folderNode);
  }"
0,"Add configurable hook for password validationit's a common use case that applications would like to enforce additional logic associated with 
changing a user password. currently this can only be achieved by using a derived user implementation.
by extending the functionality added with JCR-3118 it was fairly trivial to provide a hook for those
custom password validation checks, writing password expiration date etc.etc. 
"
1,"Missing equals and hashcode preventing the re-use of SharedFieldSortComparatorAs briefly mentioned in the dev email list, improperly implemented (i.e., missing - using the default Object implementation) equals and hashcode in SearchIndex.java prevents the reuse of a SharedFieldSortComparator between different queries when nothing has changed in the repository.  In tests, this appears to have a fairly significant negative performance impact.

Please see the following for the correct code:

http://svn.apache.org/viewvc?view=rev&revision=506908
"
0,"ShingleFilter benchmarkSpawned from LUCENE-2218: a benchmark for ShingleFilter, along with a new task to instantiate (non-default-constructor) ShingleAnalyzerWrapper: NewShingleAnalyzerTask.

The included shingle.alg runs ShingleAnalyzerWrapper, wrapping the default StandardAnalyzer, with 4 different configurations over 10,000 Reuters documents each.  To allow ShingleFilter timings to be isolated from the rest of the pipeline, StandardAnalyzer is also run over the same set of Reuters documents.  This set of 5 runs is then run 5 times.

The patch includes two perl scripts, the first to output JIRA table formatted timing information, with the minimum elapsed time for each of the 4 ShingleAnalyzerWrapper runs and the StandardAnalyzer run, and the second to compare two runs' JIRA output, producing another JIRA table showing % improvement."
0,"Make it easier to run Test2BTermsCurrently, Test2BTerms has an @Ignore annotation which means that the only way to run it as a test is to edit the file.

There are a couple of options to fix this:
# Add a main() so it can be invoked via the command line outside of the test framework
# Add some new annotations that mark it as slow or weekly or something like that and have the test target ignore @slow (or whatever) by default, but can also turn it on."
1,"TestIndexFileDeleter checkIndex failfound on 3.x

{noformat}
ant test-tag -Dtestcase=TestIndexFileDeleter -Dtestmethod=testDeleteLeftoverFiles -Dtests.seed=7631088157098800527:4270221915205524915
{noformat}"
0,"[PATCH] Some Field methods use Classcast check instead of instanceof which is slowI am not sure if this is because Lucene historically needed to work with older
JVM's but with modern JVM's, instanceof is much quicker. 

The Field.stringValue(), .readerValue(), and .binaryValue() methods all use
ClassCastException checking.

Using the following test-bed class, you will see that instanceof is miles quicker:

package com.aconex.index;

public class ClassCastExceptionTest {

    private static final long ITERATIONS = 100000;

    /**
     * @param args
     */
    public static void main(String[] args) {

        runClassCastTest(1); // once for warm up
        runClassCastTest(2);
        
        runInstanceOfCheck(1);
        runInstanceOfCheck(2);

    }
    private static void runInstanceOfCheck(int run) {
        long start = System.currentTimeMillis();

        Object foo = new Foo();
        for (int i = 0; i < ITERATIONS; i++) {
            String test;
            if(foo instanceof String) {
                System.out.println(""Is a string""); // should never print
            }
        }
        long end = System.currentTimeMillis();
        long diff = end - start;
        System.out.println(""InstanceOf checking run #"" + run + "": "" + diff + ""ms"");
        
    }

    private static void runClassCastTest(int run) {
        long start = System.currentTimeMillis();

        Object foo = new Foo();
        for (int i = 0; i < ITERATIONS; i++) {
            String test;
            try {
                test = (String)foo;
            } catch (ClassCastException c) {
                // ignore
            }
        }
        long end = System.currentTimeMillis();
        long diff = end - start;
        System.out.println(""ClassCast checking run #"" + run + "": "" + diff + ""ms"");
    }

    private static final class Foo {
    }

}


Results
=======

Run #1

ClassCast checking run #1: 1660ms
ClassCast checking run #2: 1374ms
InstanceOf checking run #1: 8ms
InstanceOf checking run #2: 4ms


Run #2
ClassCast checking run #1: 1280ms
ClassCast checking run #2: 1344ms
InstanceOf checking run #1: 7ms
InstanceOf checking run #2: 2ms


Run #3
ClassCast checking run #1: 1347ms
ClassCast checking run #2: 1250ms
InstanceOf checking run #1: 7ms
InstanceOf checking run #2: 2ms

This could explain why Documents with more Fields scales worse, as in, for lots
of Documents with lots of Fields, the effect is exacerbated."
0,"Similarity javadocs look ugly if created with java7's javadocThe captions used to illustrate the formulas are tables here:
in jdk 5/6 the table is centered nicely.

But with java7's javadocs (I think due to some css styles changes?),
the table is not centered but instead stretched.

I think we just need to center this table with a different technique?

Have a look at http://people.apache.org/~rmuir/java7-style-javadocs/org/apache/lucene/search/Similarity.html to see what I mean.

NOTE: these javadocs are under TFIDFSimilarity.java in trunk."
1,"Missing XPath escape in query.jspAs reported by Canberk Bolat of ADEO Security in a private communication, there search.jsp script in jackrabbit-webapp is missing an escape when it injects the path of a ""related:"" query into the constructed XPath statement. Further analysis showed that this issue has no security implications, so we can treat this as a normal bug report.

search.jsp
...
String q = request.getParameter(""q"");
...
      if (q != null && q.length() > 0) {
           String stmt;
           if (q.startsWith(""related:"")) {
               String path = q.substring(""related:"".length());
               stmt = ""//element(*, nt:file)[rep:similar(jcr:content,
'"" + path + ""/jcr:content')]/rep:excerpt(.) order by @jcr:score
descending"";
               queryTerms = ""similar to <b>"" +
Text.encodeIllegalXMLCharacters(path) + ""</b>"";
           }
...

"
1,"Versioning fixup leaves persistence in a state where the node can't be made versionable againJackrabbit's version recovery mode (org.apache.jackrabbit.version.recovery system property) disconnects all version histories that expose problems that manifest in unexpected exceptions being thrown. ""disconnects"" means removing the properties defined for mix:versionable and removing the mixin type. The actual versioning related nodes remain in place.

The problem: when re-adding mix:versionable, ItemSaveOperation.initVersionHistories tries to create the new version history in the same location (the path being derived from the versionable node's identifier), and consequently fails because of the broken underlying storage.

(attaching a work-in-progress test case that illustrates the problem)"
1,moving locked node removes locked statewhen moving a locked node it looses it locked state.
1,"Provider org.apache.xalan.processor.TransformerFactoryImpl not found""maven jar"" fails with the following error message on a fresh Jackrabbit source tree:

BUILD FAILED
File...... /home/hukka/tmp/jackrabbit/maven.xml
Element... ant:xslt
Line...... 146
Column.... 25
Provider org.apache.xalan.processor.TransformerFactoryImpl not found
Total time: 4 seconds
Finished at: Sun Feb 13 10:09:03 EET 2005
"
1,"Sorting produces duplicatesIf you run the code below the exception will be thrown. I believe that it isn't 
correct behaviour (the duplicities, of course), index id of hits should be 
unique as it is without sort.

Lucene versions:
1.4-final
1.4.1
CVS 1.5-rc1-dev


import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.document.Document;
import org.apache.lucene.document.Field;
import org.apache.lucene.index.IndexReader;
import org.apache.lucene.index.IndexWriter;
import org.apache.lucene.queryParser.ParseException;
import org.apache.lucene.queryParser.QueryParser;
import org.apache.lucene.search.Hits;
import org.apache.lucene.search.IndexSearcher;
import org.apache.lucene.search.Query;
import org.apache.lucene.search.Searcher;
import org.apache.lucene.search.Sort;
import org.apache.lucene.search.SortField;
import org.apache.lucene.store.Directory;
import org.apache.lucene.store.RAMDirectory;

import java.io.IOException;
import java.util.HashSet;
import java.util.LinkedList;
import java.util.ListIterator;
import java.util.Set;

/**
 * Run this test with Lucene 1.4 final or 1.4.1
 */
public class DuplicityTest
{
    public static void main(String[] args) throws IOException, ParseException
    {
        Directory directory = create_index();

        search_index(directory);
    }

    private static void search_index(Directory directory) throws IOException, 
ParseException
    {
        IndexReader reader = IndexReader.open(directory);
        Searcher searcher = new IndexSearcher(reader);

        Sort sort = new Sort(new SortField(""co"", SortField.INT, false));

        Query q = QueryParser.parse(""sword"", ""text"", new StandardAnalyzer());

        find_duplicity(searcher.search(q), ""no sort"");

        find_duplicity(searcher.search(q, sort), ""using sort"");

        searcher.close();
        reader.close();
    }

    private static void find_duplicity(Hits hits, String message) throws 
IOException
    {
        System.out.println(message + "" hits size: "" + hits.length());

        Set set = new HashSet();
        for (int i = 0; i < hits.length(); i++) {
//            System.out.println(hits.id(i) + "": "" + hits.doc(i).toString());
            Integer id = new Integer(hits.id(i));
            if (!set.contains(id))
                set.add(id);
            else
                throw new RuntimeException(""duplicity found, index id: "" + id);
        }
        System.out.println(""no duplicity found"");
    }

    private static LinkedList words;

    static {
        words = new LinkedList();

        words.add(""word"");
        words.add(""sword"");
        words.add(""dwarf"");
        words.add(""whale"");
        words.add(""male"");
    }

    private static Directory create_index() throws IOException
    {
        Directory directory = new RAMDirectory();

        ListIterator e_words1 = words.listIterator();
        ListIterator e_words2 = words.listIterator(words.size());

        IndexWriter writer = new IndexWriter(directory, new StandardAnalyzer(), 
true);

        int co = 1;

        for (int i = 0; i < 300; i++) {

            if (!e_words1.hasNext()) {
                e_words1 = words.listIterator();
                e_words1.hasNext();
            }
            String word1 = (String)e_words1.next();
            if (!e_words2.hasPrevious()) {
                e_words2 = words.listIterator(words.size());
                e_words2.hasPrevious();
            }
            String word2 = (String)e_words2.previous();

            Document doc = new Document();

            doc.add(Field.Keyword(""co"", String.valueOf(co)));
            doc.add(Field.Text(""text"", word1 + "" "" + word2));
            writer.addDocument(doc);

            if (i % 20 == 0)
                co++;
        }
        writer.optimize();
        System.err.println(""index size: "" + writer.docCount());
        writer.close();

        return directory;
    }
}"
0,"add ability to not count sub-task doLogic increment to contri/benchmarkSometimes, you want to run a sub-task like CloseIndex, and include the time it takes to run, but not include the count that it returns when reporting rec/s.

We could adopt this approach: if a task is preceded by a ""-"" character, then, do not count the value returned by doLogic.

See discussion leading to this here:

  http://www.gossamer-threads.com/lists/lucene/java-dev/57081"
1,"MultiPhraseQuery throws AIOOBESee thread ""MultiPhraseQuery throws ArrayIndexOutOfBounds Exception"" on dev@ by Jayendra Patil."
1,"FieldInfo omitTerms bugAround line 95 you have:

    if (this.omitTf != omitTf) {
      this.omitTf = true;                // if one require omitTf at least once, it remains off for life
    }

Both references of the omitTf booleans in the if statement refer to the same field. I am guessing its meant to be other.omitTf like the norms code above it."
0,"InternalVersion.getFrozenNode confused about root version?It seems the javadoc for InternalVersion.getFrozenNode  is confused:

     * Returns the frozen node of this version or <code>null</code> if this is
     * the root version.

AFAIU, the frozen node of the root version is always present to capture the node type of the versionable node.

Does anybody recall how this got here? (SVN says it has been there forever)"
0,"javadocs creation has too many warnings/errorsCurrently running 'ant javadocs' creates so many warnings that you have to grep the output to verify that new code did not add more.

While most current errors might be minor, they might hide a few serious ones that we will never know abut until someone complains. 

Best if we fix all of them and keep it always clean..."
0,"more performance improvements for snowballi took a more serious look at snowball after LUCENE-2194.

This gives greatly improved performance, but note it has some minor breaks to snowball internals:
* Among.s becomes a char[] instead of a string
* SnowballProgram.current becomes a char[] instead of a StringBuilder
* SnowballProgram.eq_s(int, String) becomes eq_s(int, CharSequence), so that eq_v(StringBuilder) doesnt need to create an extra string.
* same as the above with eq_s_b and eq_v_b
* replace_s(int, int, String) becomes replace_s(int, int, CharSequence), so that StringBuilder-based slice and insertion methods don't need to create an extra string.

all of these ""breaks"" imho are only theoretical, the problem is just that pretty much everything is public or protected in the snowball internals.

the performance improvement here depends heavily upon the snowball language in use, but its way more significant than LUCENE-2194.
"
0,"NetscapeDraftSpec is too strict about cookie expires date formatThe Netscape Draft specification (http://curl.haxx.se/rfc/cookie_spec.html) specifies clearly that the date format for Set-Cookie expires is ""Wdy, DD-Mon-YYYY HH:MM:SS GMT"". But on the other hand, in the examples section of the same document, the only example header that contains ""Expires"" is the following:

Set-Cookie: CUSTOMER=WILE_E_COYOTE; path=/; expires=Wednesday, 09-Nov-99 23:12:40 GMT

Note that the weekday is fully spelled out and that the year is written as two digits only. I would say that the specification therefore makes the 2 or 4 digit year optional. I think NetscapeDraftSpec should reflect this. An example of a product that uses the 2 digit version is jetty 6 and 7. When using httpclient 4 talking to a jetty server, any Set-Cookie headers for persistent cookies will be interpreted as a 4 digit year in the date and the cookie will immediately be disregarded as expired by some 2,000 years or so. Httpclient 3 on the other hand had no problem understanding the persistent cookies from jetty. I filed a bug report https://bugs.eclipse.org/bugs/show_bug.cgi?id=304698 on jetty to change their date format, but on the other hand I also think httpclient 4 is too strict about the date format when even the original specification uses two alternatives.

Workaround is easy by setting CookieSpecPNames.DATE_PATTERNS, but I really think that projects like jetty and httpclient should be compatible by default. Also, since the date format used by jetty is parsable but misinterpreted and disregarded by httpclient makes it especially hard to detect the first time on encounters the problem."
0,"increase default maxFieldLength?To my understanding, Lucene 2.3 will easily index large documents. So shouldn't we get rid of the 10,000 default limit for the field length? 10,000 isn't that much and as Lucene doesn't have any error logging by default, this is a common problem for users that is difficult to debug if you don't know where to look.

A better new default might be Integer.MAX_VALUE.
"
0,"broken link to the release notesOn http://jakarta.apache.org/httpcomponents/httpclient-3.x/downloads.html
the link to the release notes is broken"
0,"move contrib/benchmark to modules/benchmarkI think we should move lucene/contrib/benchmark to a shared modules/benchmark, so you can easily benchmark anything (lucene, solr, other modules like analysis or whatever).

For example, if you want to do some benchmarking of something in solr (LUCENE-2844) you should be able to do this.
Another example is simply being able to benchmark an analyzer definition from a schema.xml, its more convenient than writing the equivalent java analyzer just for benchmarking.

"
0,"Multiple DIGEST authentication attempts with same credentialsHttpMethodBase's processAuthenticationResponse uses a set of realms to which
attempts to authenticate have already been made. The elements of the set are a
concatenation of the requested path and the value of the Authentication response
header.

For digest authentication this response header contains a nonce value, which is
uniquely generated by the server each time a 401 response is made. This makes it
impossible to recognize that authentication against this realm has been
attempted before and so all 100 attempts are made before returning. The nonce
should probably not be used in the realmsUsed element

Reported by Rob Owen <Rob.Owen@sas.com>"
0,"RepositoryStub implementation in jackrabbit-coreCurrently setting up a Jackrabbit repository for use with the TCK is a relatively complex operation with a large repositoryStubImpl.properties file and lots of specially crafted test content and settings to worry about. This makes it hard to set up new TCK test instances with the various JCR and SPI layers we now have.

To simplify things I'd like to introduce a RepositoryStubImpl class and related configuration files inside src/main/java in jackrabbit-core."
1,"When creating multiple repository instances pointing to the same home, opening a second session will remove the .lock fileThe following test case can be used to reproduce the bug:
Repository repo1 = new TransientRepository(repoConfig);
Session session1_1 = repo1.login(...);
Session session2_2 = repo1.login(...);
Repository repo2 = new TransientRepository(repoConfig); // Will not fail (expected)
Session session2_1 = repo2.login(...); // Will fail with javax.jcr.RepositoryException: The repository home /tmp/_repository appears to be already locked by the current process (expected)
Session session2_2 = repo2.login(...); // Will work!
Repository repo3 = new TransientRepository(repoConfig); // Will not fail either (expected)
Session session3_1 = repo3.login(...); // Will fail with javax.jcr.RepositoryException: The repository home /tmp/_repository appears to be already locked by the current process (expected)
Session session3_2 = repo3.login(...); // Will fail with javax.jcr.RepositoryException: Directory was previously created with a different LockFactory instance

Open the first session in repo2 will fails but will also remove the .lock file, thus the second
session will succeed and may corrupt the repository because there are multiple session
opened from multiple repository.
The same behaviour occurs for repo3, the .lock file is removed but it is a slightly different case
as a new exception will be thrown while creating the Lucene index.

This is a clearly a twisted case as repositories pointing to the same home must not be created
simultaneously but i think that it must be more robust to prevent data corruption.

I reproduce the bug on JR 1.4.7 and 1.5.3 but i think it affects at least all versions of JR < 1.5.3."
0,Avoid String.intern() when indexingLucene 3.0 now allows to create Fields with a String name that is already interned. We should use the new constructor in NodeIndexer to avoid unnecessary String.intern() calls. The field names Jackrabbit uses are available in FieldNames and already interned.
0,"Need setURI() methods in HttpMethod interfaceI'd like to have the methods setURI( URI ) and setURI( String ) methods. Also a 
method like getRequestURI() because the uri I get now with the method getURI() 
changes if I execute a request which will be automatically forwarded.

The methods setURI can throw an exception if it has already been executed."
1,"Bad check for sv:name attribute presence in system view importsax content handler checks the wrong variable to see if it's null in SysViewImportHandler

name vs. svName

patch fixes this"
0,"Fix charset problems in XML loading in HyphenationCompoundWordTokenFilter (also Solr's loader from schema)As said in LUCENE-2731, the handling of XML in HyphenationCompoundWordTokenFilter is broken and breaks XML 1.0 (5th edition) spec totally. You should never supply a Reader to any XML api, unless you have internal character data (e.g. created programmatically). Also you should supply a system id, as resolving external entities does not work. The loader from files is much more broken, it always open the file as a Reader and then passes it to InputSource. Instead it should point filename directly to InputSource.

This issue will fix it in trunk and use InputSource in Solr, but will still supply the Reader possibility in previous versions (deprecated)."
0,"Support for embedded index aggregatesIndex aggregates could contain other index aggregates. JR should be able to handle a complete hierarchy of aggregates. 

I'm working on a patch."
0,Use out-of-process text extractionThe upcoming Tika 0.9 release will contain a highly useful out-of-process text extraction feature (TIKA-416) that we should use also in Jackrabbit.
0,"msoffice text extractor for office 2007 filesi created a patch that provides a mstextextractor for jackrabbit. this patch will entirely replace all existing ms extractors.
this patch can be applied as soon as poi-3.5 is available. the ms text extractor supports: doc, docx, ppt, pptx,
xls, xlsx. the patch is not fully tested and uses poi code which is not yet available on the maven repo (needs to be 
build locally)"
0,"Make DocumentsWriter more robust on hitting OOMI've been stress testing DocumentsWriter by indexing wikipedia, but not
giving enough memory to the JVM, in varying heap sizes to tickle the
different interesting cases.  Sometimes DocumentsWriter can deadlock;
other times it will hit a subsequent NPE or AIOOBE or assertion
failure.

I've fixed all the cases I've found, and added some more asserts.  Now
it just produces plain OOM exceptions.  All changes are contained to
DocumentsWriter.java.

All tests pass.  I plan to commit in a day or two!"
1,"Request with two forward slashes for path failsThe following code demonstrates the problem:
        DefaultHttpClient client = new DefaultHttpClient();
        client.execute(new HttpGet(""http://www.google.com//""));

When a request is made, the DefaultRequestDirector invokes rewriteRequestURI(). I don't fully understand why this method does what it does. For a non-proxied request, it attempts to render the URI to a relative URI. In doing so, it tries to create a relative URI whose content is ""//"". Per RFC 2396 section 5 (Relative URI References), a relative URI that begins with ""//"" is a network-path reference, and the ""//"" must be immediately followed by an authority. Therefore, while ""http://www.google.com//"" is a valid absolute URI, ""//"" is not a valid relative one. The resulting exception:

[...]
Caused by: org.apache.http.ProtocolException: Invalid URI: http://www.google.com//
	at org.apache.http.impl.client.DefaultRequestDirector.rewriteRequestURI(DefaultRequestDirector.java:339)
	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:434)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:641)
	... 31 more
Caused by: java.net.URISyntaxException: Expected authority at index 2: //
	at java.net.URI$Parser.fail(URI.java:2809)
	at java.net.URI$Parser.failExpecting(URI.java:2815)
	at java.net.URI$Parser.parseHierarchical(URI.java:3063)
	at java.net.URI$Parser.parse(URI.java:3024)
	at java.net.URI.<init>(URI.java:578)
	at org.apache.http.client.utils.URIUtils.createURI(URIUtils.java:106)
	at org.apache.http.client.utils.URIUtils.rewriteURI(URIUtils.java:141)
	at org.apache.http.client.utils.URIUtils.rewriteURI(URIUtils.java:159)
	at org.apache.http.impl.client.DefaultRequestDirector.rewriteRequestURI(DefaultRequestDirector.java:333)
	... 33 more
"
0,"Add reflection API to AttributeSource/AttributeImplAttributeSource/TokenStream inspection in Solr needs to have some insight into the contents of AttributeImpls. As LUCENE-2302 has some problems with toString() [which is not structured and conflicts with CharSequence's definition for CharTermAttribute], I propose an simple API that get a default implementation in AttributeImpl (just like toString() current):

- Iterator<Map.Entry<String,?>> AttributeImpl.contentsIterator() returns an iterator (for most attributes its a singleton) of a key-value pair, e.g. ""term""->""foobar"",""startOffset""->Integer.valueOf(0),...
- AttributeSource gets the same method, it just concat the iterators of each getAttributeImplsIterator() AttributeImpl

No backwards problems occur, as the default toString() method will work like before (it just gets iterator and lists), but we simply remove the documentation for the format. (Char)TermAttribute gets a special impl fo toString() according to CharSequence and a corresponding iterator.

I also want to remove the abstract hashCode() and equals() methods from AttributeImpl, as they are not needed and just create work for the implementor."
1,"Sloppy Phrase Scorer matches the doc ""A B C D E"" for query = ""B C B""~2This is an extension of https://issues.apache.org/jira/browse/LUCENE-697

In addition to abnormalities Yonik pointed out in 697, there seem to be other issues with slopy phrase search and scoring.

1) A phrase with a repeated word would be detected in a document although it is not there.
I.e. document = A B D C E , query = ""B C B"" would not find this document (as expected), but query ""B C B""~2 would find it. 
I think that no matter how large the slop is, this document should not be a match.

2) A document containing both orders of a query, symmetrically, would score differently for the queru and for its reveresed form.
I.e. document = A B C B A would score differently for queries ""B C""~2 and ""C B""~2, although it is symmetric to both.

I will attach test cases that show both these problems and the one reported by Yonik in 697. "
0,"Omit default BatchReadConfig in Spi2davexRepositoryServiceFactoryi'd like to remove the default batchread configuration created in Spi2davexRepositoryServiceFactory (ll 79) and instead pass 
null if the service configuration doesn't define the batch-read-config.

for test execution e.g. the given default isn't really optimal as sessions only have a short life time and only read
a very limited amount of items (in general)... always reading with depth 4 doesn't add any benefit in this case.
running the level1 jcr tests in jcr2dav (that as far as i saw doesn't define an extra batchread-config took 1.5, 2.5 and 13 minutes
from null-config -> depth2 -> depth4.

if there is a strong reason for keeping that default in the factory we should at least change that for the tests.
michael, what do you think?"
1,"I/O exception in DocsWriter add or updateDocument may not delete unreferenced filesIf an I/O exception is thrown in DocumentsWriter#addDocument or #updateDocument, the stored fields files may not be cleaned up."
1,"Explicit VirtualHosts Can Cause Issues On RedirectsIf you set an explicit virtual host then a getmethod may get stuck in a redirect
loop (up to maxRedirects).

e.g. execute a get on www.google.com (with a www.google.com virtualhost).  That
redirects to www.google.co.nz (at least if you come from an NZ IP).  The current
httpclient behavior is to then connect to www.google.co.nz but pass through,
with the request, ""Host: www.google.com"".  Google will then reply with another
www.google.co.nz redirect and the loop continues.

There are probably a few ways to work around this.  It seems reasonable to drop
an explicity set virtual host in the event a redirect redirects to a different
uri authority.  The following patch works for me:


diff -Naur
../../t2/commons-httpclient/src/java/org/apache/commons/httpclient/HttpMethodDirector.java
src/java/org/apache/commons/httpclient/HttpMethodDirector.java
---
../../t2/commons-httpclient/src/java/org/apache/commons/httpclient/HttpMethodDirector.java
2005-12-22 01:06:55.000000000 +1300
+++ src/java/org/apache/commons/httpclient/HttpMethodDirector.java	2005-12-22
19:09:51.000000000 +1300
@@ -605,6 +605,27 @@
 					redirectUri = new URI(currentUri, redirectUri);
 				}
 			}
+            do {
+                // scenario we're trying to avoid:
+                // virtual host is set (e.g. google.com); a request to that
server responds
+                // with a redirect to google.co.nz; we issue a request to
google.co.nz with 
+                // a virtual host request of google.com
+                // 
+                // This code will remove any set virtual host if the redirect
is to a different 
+                // domain
+
+                if(redirectUri.isRelativeURI()) {
+                    break;
+                }
+
+                String vhost = hostConfiguration.getParams().getVirtualHost();
+                if(vhost==null)
+                    break;
+                if(redirectUri.getAuthority()!=currentUri.getAuthority()) {
+                    hostConfiguration.getParams().setVirtualHost(null);
+                }
+            } while(false);
+"
0,Implement Query.getBindVariableNames()
0,"In J2SDK 1.5.0 (Tiger) enum is a keywordHi!

Tiger adds extensions to the Java Programming Language (JSR201). One is
""Enumerations"", which required to add the new keyword enum.

I just made a grep (grep -lrw) over some sources and found some Apache projects
using enum as a word.

To be compliant with the new specification, please check that enum is not used
as a variable, field or method name.

Regards,
Robert"
1,"OCM: translate-project goal not foundThe jackrabbit-ocm POM doesn't specify the required version of the Retrotranslator plugin it uses. In some cases this causes the build to use an older version of the plugin that doesn't come with the translate-plugin goal.

The goal is included in the latest version (1.0-alpha-4) of the plugin."
1,"on disk full during close, FSIndexOutput fails to close descriptorThe close method just does this:

{code}
      if (isOpen) {
        super.close();
        file.close();
        isOpen = false;
      }
{code}

But super.close = BufferedIndexOutput.close, which tries to flush its buffer.  If disk is full (or something else is wrong) then we hit an exception and don't actually close the descriptor.

I will put a try/finally in so we always close, taking care to preserve the original exception. I'll commit shortly & backport to 2.3.2"
0,"DelimitedPayloadTokenFilter copies the bufer over itsself. Instead it should only set the length. Also optimize logic.This is a small improvement I found when looking around. It is also a bad idea to copy a array over itsself.

All tests pass, will commit later!"
1,"HttpMultipart doesn't generate Content-Type part header in mode BROWSER_COMPATIBLEBrowsers (tested with Firefox 3.6 and IE6) send a Content-Type header for file parts, what org.apache.http.entity.mime.HttpMultipart doesn't do in BROWSER_COMPATIBLE mode.


Example:

-----------------------------142889018617181602061216500409

Content-Disposition: form-data; name=""myFileFieldName2""; filename=""webtest.png""

Content-Type: image/png


In HtmlUnit we wil subclass HttpEntity and MultipartEntity to fix this problem."
1,"Highlighter has problems when you use StandardAnalyzer with LUCENE_29 or simplier StopFilter with stopWordsPosIncr mode switched onThis is a followup on LUCENE-1987:

If you set in HighligterTest the constant static final Version TEST_VERSION = Version.LUCENE_24 to LUCENE_29 or LUCENE_CURRENT, the test testSimpleQueryScorerPhraseHighlighting fails. Please note, that currently (before LUCENE-2002 is fixed), you must also set the QueryParser to respect posIncr."
0,"JCR2SPI: Avoid individual Item reloading upon Session/Item.refresh(true)with CacheBehaviour.INVALIDATE Item.refresh(true) and Session.refresh(true) results in individual reloading of the existing entries in the hierarchy circumventing all batch read optimization.

Apart from general optimization of the refresh itself, refresh(true) should rather mark existing items and force a reload upon next access (similar to the behaviour implemented with refresh(false)).

"
1,"SegmentInfo.sizeInBytes ignore includeDocStore when cachingI noticed that SegmentInfo's sizeInBytes cache is potentially buggy -- it doesn't take into account 'includeDocStores'. I.e., if you call it once w/ 'false' (sizeInBytes won't include the store files) and then with 'true' (or vice versa), you won't get the right sizeInBytes (it won't re-compute, with the store files).

I'll fix and add a test case demonstrating the bug."
0,Error reporting page for jackrabbit-webappThe Jackrabbit webapp should have an error reporting page that would help users collect all relevant environment and log information for inclusion in bug reports.
1,"Term vectors missing after addIndexes + optimizeI encountered a problem with addIndexes where term vectors disappeared following optimize(). I wrote a simple test case which demonstrates the problem. The bug appears with both addIndexes() versions, but does not appear if addDocument is called twice, committing changes in between.

I think I tracked the problem down to IndexWriter.mergeMiddle() -- it sets term vectors before merger.merge() was called. In the addDocs case, merger.fieldInfos is already populated, while in the addIndexes case it is empty, hence fieldInfos.hasVectors returns false.

will post a patch shortly."
1,"Mixin removal exceptionWhen trying to remove a mixin from a non nt:unstructured node (in my case nt:resource), you get the following exception:

Unable to alter mixin types: javax.jcr.nodetype.ConstraintViolationException: no matching property definition found for {http://www.day.com/jcr/cq/1.0}lastRolledout

lastRolledout property is defined by the mixin cq:LiveRelationship that I am trying to remove."
1,"PROPFIND response to a request for a property that does not exist reports an empty DAV:prop element  A PROPFIND response to a request for a property that does not exist reports an empty DAV:prop element:

Request:

<propfind xmlns=""DAV:""><prop><doesnotexist/></prop></propfind>

Response:

<D:multistatus xmlns:xml='http://www.w3.org/XML/1998/namespace' xmlns:D='DAV:'>
  <D:response>
    <D:href>...</D:href>
    <D:propstat>
      <D:prop/>
      <D:status>HTTP/1.1 200 OK</D:status>
    </D:propstat>
    <D:propstat>
      <D:prop><D:doesnotexist/></D:prop>
      <D:status>HTTP/1.1 404 Not Found</D:status>
    </D:propstat>
  </D:response>
</D:multistatus>

  "
0,"spi2dav: temp files are not clean up after batch submitwhen a batch includes larger binary properties, their backing temp files are not clean up after submit.
suggest to dispose those in any case."
0,Set jcr and servlet-api dependency scope to providedThe jcr and servlet API libraries should typically be provided by the deployment environment and not included as compile/runtime dependencies of Jackrabbit artifacts. The scope of those dependencies should thus be set to provided in the <dependencyManagement/> section of the Jackrabbit parent POM.
0,Provide base classes to simplify read only RepositoryService implementationsThere should be base classes that simplify the implementation of a RepositoryService for read only access.
0,"Index merging should run in a separate threadIndexes are merged using the configuration parameters mergeFactor and minMergeDocs. With the default value of 10 for mergeFactor and 100 for minMergeDocs, as soon as 10 index directories exist with less or equal than 100 nodes they are merged into a single one. This process is then repeated by multiplying the minMergeDocs with the mergeFactor. Therefore the second round will merge 10 index directories with less or equal than 1000 nodes.

Because the above process is part of the regular workspace store operation an index merge with more than let's say 10'000 nodes can block the store operation for a couple of seconds. With the current synchronization scheme, all other threads are blocked from writing. This is not acceptable.

Index merging should run in a separate thread in the background.

The process needs to take care of the following:
- While merging indexes, deletes on those indexes must not get lost
- Switching between the indexes that are merged and the new index must be atomic
- Recovery if merging is interrupted, e.g. jackrabbit is shutdown"
0,"SnowballAnalyzer lacks a constructor that takes a Set of Stop WordsAs discussed on the java-user list, the SnowballAnalyzer has been updated to use a Set of stop words. However, there is no constructor which accepts a Set, there's only the original String[] one

This is an issue, because most of the common sources of stop words (eg StopAnalyzer) have deprecated their String[] stop word lists, and moved over to Sets (eg StopAnalyzer.ENGLISH_STOP_WORDS_SET). So, for now, you either have to use a deprecated field on StopAnalyzer, or manually turn the Set into an array so you can pass it to the SnowballAnalyzer

I would suggest that a constructor is added to SnowballAnalyzer which accepts a Set. Not sure if the old String[] one should be deprecated or not.

A sample patch against 2.9.1 to add the constructor is:


--- SnowballAnalyzer.java.orig  2009-12-15 11:14:08.000000000 +0000
+++ SnowballAnalyzer.java       2009-12-14 12:58:37.000000000 +0000
@@ -67,6 +67,12 @@
     stopSet = StopFilter.makeStopSet(stopWords);
   }
 
+  /** Builds the named analyzer with the given stop words. */
+  public SnowballAnalyzer(Version matchVersion, String name, Set stopWordsSet) {
+    this(matchVersion, name);
+    stopSet = stopWordsSet;
+  }
+
"
0,"Add LocalLuceneLocal Lucene (Geo-search) has been donated to the Lucene project, per https://issues.apache.org/jira/browse/INCUBATOR-77.  This issue is to handle the Lucene portion of integration.

See http://lucene.markmail.org/message/orzro22sqdj3wows?q=LocalLucene
"
0,"HTTPClient 4.1 auto slash removalI've put the same comment as in the following issue.

https://issues.apache.org/jira/browse/HTTPCLIENT-929?focusedCommentId=13001748#comment-13001748

I am using httpclient 4.1. I had a problem with this fix. In DefaultRequestDirector.rewriteRequestURI method, for non-proxied URI and when it is a absolute URI, it will call the URIUtils.rewriteURI, which then take the ""RawPath"" from an uri and normalize it. So when I pass an uri, for example, http://www.whatever.com/1//3, it will automatically remove the extra slash and become http://www.whatever.com/1/3. I've got a REStful service to accept the uri (/{param1}/{param2}/{param3}) and it takes when there is an empty value past in. Now because of the auto slash removal, the ""3"" value shift left for a position and match to the {param2}. I wouldn't say the above solution is wrong, but I guess it should not change what value that user pass in."
1,"FilterImpl.getStringValue() does not use custom converter class specified in @Field annotationI have a POJO with the following field:

    @Field(converter = LocaleConverter.class)
    private Locale                locale;

When I attempt to query for objects based on this field, I get a NullPointerException:

java.lang.NullPointerException
        at org.apache.jackrabbit.ocm.query.impl.FilterImpl.getStringValue(FilterImpl.java:281)
        at org.apache.jackrabbit.ocm.query.impl.FilterImpl.addEqualTo(FilterImpl.java:129)

FilterImpl should preferentially use the atomic type converter defined in the @Field annotation to convert the value, then fallback to the global converters.  Converting the Locale to a string for use in the query is a workaround, but the logic for string conversion should only reside in my LocaleConverter class.
"
0,"Add deprecated 'transition' api for Document/FieldI think for 4.0 we should have a deprecated transition api for Field so you can do new Field(..., Field.Store.xxx, Field.Index.yyy) like before.

These combinations would just be some predefined fieldtypes that are used behind the scenes if you use these deprecated ctors

Sure it wouldn't be 'totally' backwards binary compat for Field.java, but why must it be all or nothing? I think this would eliminate a big
hurdle for people that want to check out 4.x"
0,"Provide Readme's for subprojects jcr-mapping and jcr-nodemanagementThere need to be Readme files in each of the subprojects ""jcr-mapping"" and ""jcr-nodemanagement"" to provide some information about the scope of the subproject, building and hints on how to get started."
1,"BundleDBPersistenceManager does not free blobStore resourcesWhen removing binary property from node or removing node containing binary property, resources occupied by binary property are not freed (orphaned records remains in associated ${schemaObjectPrefix}BINVAL table)."
0,"[PATCH] Ant macro for javadocs and javadocs-internalThis removes the duplication introduced in build.xml 
when the javadocs-internal build target was added. 
 
Regards, 
Paul Elschot"
1,"WorkspaceConfig.init() throws NullPointerException if Search configuration is missingWhen the search configuration is missing from the repository.xml configuration, the WorkspaceConfig.sc field is null and consequently the WorkspaceConfig.init() method throws a NullPointerException.

This is by itself a bug, especially since missing search configuration is perfectly ok resulting in Jackrabbit not building the search index (which is - believe it or - what really want).

On that matter, since the configuration file structure seems to be implied by the configuration framework but the DTD is inlined into the configuration file, the configuration framework should act very gracefully to missing or wrong or unexpected configuration elements. Thus, for example, if the file system configuration (WorkspaceConfig.fsc) would be missing, the WorkspaceConfig should probably hint at this point and not throw a NullPointerException without further explanations (of course throwing anything at all is still better than going wild)."
0,"MoreLikeThis - allow to exclude terms that appear in too many documents (patch included)The MoreLikeThis class allows to generate a likeness query based on a given document. So far, it is impossible to suppress words from the likeness query, that appear in almost all documents, making it necessary to use extensive lists of stop words.

Therefore I suggest to allow excluding words for which a certain absolute document count or a certain percentage of documents is exceeded. Depending on the corpus of text, words that appear in more than 50 or even 70% of documents can usually be considered insignificant for classifying a document.      "
1,"IllegalNameException when importing document view with two mixinsAs reported by Manuel Simoni on the dev mailing list:

----

I have nodes with two mixin types, s1NT:comment and s1NT:authored.

I am exporting the document view:

session.exportDocumentView(session.getRootNode().getPath(),
outputStream, false, false);

When I try to import the document again:

session.getWorkspace().importXML(session.getRootNode().getPath(),
inputStream, ImportUUIDBehavior.IMPORT_UUID_COLLISION_THROW);

...I get this exception:

java.lang.Exception: javax.jcr.InvalidSerializedDataException: failed to
parse XML stream: illegal jcr:mixinTypes value: s1NT:comment
s1NT:authored: illegal jcr:mixinTypes value: s1NT:comment s1NT:authored
       at
at.systemone.wiki.webservice.ImportControllerImpl.importDocumentView(ImportControllerImpl.java:30)
       at
at.systemone.wiki.webservice.ImportControllerTest.testImport(ImportControllerTest.java:12)
       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
       at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
       at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
       at java.lang.reflect.Method.invoke(Method.java:585)
       at junit.framework.TestCase.runTest(TestCase.java:154)
       at junit.framework.TestCase.runBare(TestCase.java:127)
       at junit.framework.TestResult$1.protect(TestResult.java:106)
       at junit.framework.TestResult.runProtected(TestResult.java:124)
       at junit.framework.TestResult.run(TestResult.java:109)
       at junit.framework.TestCase.run(TestCase.java:118)
       at junit.framework.TestSuite.runTest(TestSuite.java:208)
       at junit.framework.TestSuite.run(TestSuite.java:203)
       at
org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:478)
       at
org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:344)
       at
org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: javax.jcr.InvalidSerializedDataException: failed to parse XML
stream: illegal jcr:mixinTypes value: s1NT:comment s1NT:authored:
illegal jcr:mixinTypes value: s1NT:comment s1NT:authored
       at
org.apache.jackrabbit.core.WorkspaceImpl.importXML(WorkspaceImpl.java:718)
       at
at.systemone.wiki.webservice.ImportControllerImpl.importDocumentView(ImportControllerImpl.java:27)
       ... 16 more
Caused by: org.apache.jackrabbit.name.IllegalNameException:
's1NT:comment s1NT:authored' is not a valid name
       at
org.apache.jackrabbit.core.xml.DocViewImportHandler.startElement(DocViewImportHandler.java:217)
       at
org.apache.jackrabbit.core.xml.ImportHandler.startElement(ImportHandler.java:234)
       at org.apache.xerces.parsers.AbstractSAXParser.startElement(Unknown Source)
       at
org.apache.xerces.impl.XMLNSDocumentScannerImpl.scanStartElement(Unknown
Source)
       at
org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(Unknown
Source)
       at
org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown
Source)
       at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
       at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
       at org.apache.xerces.parsers.XMLParser.parse(Unknown Source)
       at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)
       at
org.apache.jackrabbit.core.WorkspaceImpl.importXML(WorkspaceImpl.java:709)
       ... 17 more

I think the importer chokes on this node (it is the first node with
these two mixin types in the XML file):

<s1:comment jcr:primaryType=""s1NT:page"" jcr:mixinTypes=""s1NT:comment
s1NT:authored"" jcr:uuid=""3ff1022b-3e21-4e44-9a2e-ae3b67e833e5""
jcr:isCheckedOut=""true""
jcr:versionHistory=""17186f20-dab2-42d8-8f66-0895472debea""
jcr:frozenMixinTypes=""s1NT:comment s1NT:authored""
jcr:frozenUuid=""3ff1022b-3e21-4e44-9a2e-ae3b67e833e5""
s1:author=""8db75ec7-eee8-44d8-aeb2-fbd116ea7e01"" s1:title=""""
jcr:predecessors=""fb9eefb2-e2f8-414c-be94-185111a89be9""
s1:creationDate=""2005-09-07T17:59:12.589Z""
s1:editor=""7b778c51-d8d8-474b-a621-f5759fc24cd0"" s1:orphanedPage=""false""
s1:modificationDate=""2006-03-18T17:55:49.838Z"" s1:lowercaseTitle=""""
jcr:baseVersion=""fb9eefb2-e2f8-414c-be94-185111a89be9""
s1:currentEditor=""8db75ec7-eee8-44d8-aeb2-fbd116ea7e01""
jcr:frozenPrimaryType=""s1NT:page"">

----

This issue is related to JCR-325, but there should be a lot easier fix for this special case."
0,"Disallow setBoost() on StringField, throw exception if boosts are set if norms are omittedOccasionally users are confused why index-time boosts are not applied to their norms-omitted fields.

This is because we silently discard the boost: there is no reason for this!

The most absurd part: in 4.0 you can make a StringField and call setBoost and nothing complains... (more reasons to remove StringField totally in my opinion)"
0,"[patch] better support gcj compilationThere are two methods in IndexReader.java called 'delete'. That is a reserved
keyword in C++ and these methods cause trouble for gcj which implements a clever
workaround in renaming them delete$ but the OS X dynamic linker doesn't pick-up
on it.
The attached patch renames delete(int) to deleteDocument(int) and delete(Term)
to deleteDocuments(Term) and deprecates the delete methods (as requested by Doug
Cutting)."
0,Migrate to Lucene 2.3
0,"all references to incubator need to be replaced with new locationsThe following files under 1.0 branch refer to incubator in one way or another.  Some of them may be benign.

./contrib/bdb-persistence/project.properties
./contrib/bdb-persistence/project.xml
./contrib/bdb-persistence/README.txt
./contrib/classloader/project.properties
./contrib/classloader/project.xml
./contrib/examples/project.xml
./contrib/extension-framework/project.properties
./contrib/extension-framework/project.xml
./contrib/jcr-commands/jmeter-chain/project.properties
./contrib/jcr-commands/project.properties
./contrib/jcr-commands/xdocs/navigation.xml
./contrib/jcr-ext/project.xml
./contrib/jcrtaglib/project.properties
./contrib/orm-persistence/project.properties
./contrib/orm-persistence/project.xml
./jackrabbit/applications/test/cnd-reader-test-input.cnd
./jackrabbit/project.properties
./jackrabbit/project.xml
./jackrabbit/README.txt
./jackrabbit/src/site/fml/faq.fml
./jackrabbit/src/site/xdoc/doc/arch/overview/jcrlevels.xml
./jackrabbit/src/site/xdoc/doc/building.xml
./jackrabbit/src/site/xdoc/doc/config.xml
./jackrabbit/src/site/xdoc/downloads.xml
./jackrabbit/src/site/xdoc/index.xml
./jackrabbit/src/site/xdoc/tasks.xml
./jackrabbit/src/test/java/org/apache/jackrabbit/core/nodetype/compact/CompactNodeTypeDefTest.java
./jca/project.properties
./jca/project.xml
./textfilters/project.properties
./textfilters/project.xml

I'd edit them myself, but I need to sleep... maybe tomorrow if nobody beats me to it.
"
0,"LargeDocHighlighter - another span highlighter optimized for large documentsThe existing Highlighter API is rich and well designed, but the approach taken is not very efficient for large documents.

I believe that this is because the current Highlighter rebuilds the document by running through and scoring every every token in the tokenstream.

With a break in the current API, an alternate approach can be taken: rebuild the document by running through the query terms by using their offsets. The benefit is clear - a large doc will have a large tokenstream, but a query will likely be very small in comparison.

I expect this approach to be quite a bit faster for very large documents, while still supporting Phrase and Span queries.

First rough patch to follow shortly."
0,"Refactor the Mapper & DescriptotReader classesI would like to refactor the mappers and the descriptor readers  in order to : 
* Create an abstract mapper impl because both Mapper classes have a lot of code in common (AnnotedObjectMapper & DigesterMapperImpl). Only the readers are different. The Mappers can make exactly the same process. 
* The Mapper classes should not have the responsibility to create the jcr node types. This can be done outside the mapper and it should be an optional operation. There are certainly some use cases where node type creation is not necessary. Right now, the annotated object mapper creates jcr node types. "
0,"Cannot use Lucene in an unsigned applet due to Java security restrictionsA few of the Lucene source files call System.getProperty and perform a couple of
other operations within static initializers that assume full Java 2 permissions.
This prevents Lucene from being used from an unsigned applet embedded in a
browser page, since the default security permissions for an applet will prohibit
reading of most properties.

I would suggest wrapping the initialization of the properties in try/catch
blocks. This does mean that a couple properties would need to be made non-final,
and in some cases, getter and setter methods might be desirable to allow the
applet programmer to change the property values at runtime (some variables are
public static and could be changed directly without accessors).

This problem occurs with the shipping 1.4.3 version as well as the latest (as of
 07-apr-2005) source code fetched from CVS.

Currently, the files that are affected are org.apache.lucene.index.IndexWriter,
org.apache.lucene.index.SegmentReader, org.apache.lucene.search.BooleanQuery,
and org.apache.lucene.store.FSDirectory.

I have modified versions of these files with some suggested changes, plus a
simple test applet and associated files that demonstrate the situation. The
sample applet can be launched in a browser either by double-clicking the file
locally or by putting it on a web server and launching it from an http URL. As
soon as I can figure out how to attach to a bug report, I'll do that.

P.S. This topic came up in August, 2004 in lucene dev mailing list but as far as
I can tell, has not yet been resolved."
1,"Fix unexpected behavior of Text.getName()Text.getName() and variants does return an empty string, if the given path is already a name. eg:

Text.getName(""foo"") returns """" and not ""foo"" as one would expect for relative paths.
suggest to change this."
0,Support for JNDI configuration of BundleDbPersistenceManagerIt would be nice to have the option to configure BundleDbPersistenceManager database specifying a JNDI name of a DataSource.
0,"activity storage pathJSR-283 states in 15.12.3:

""15.12.3 Activity Storage
Activities are persisted as nodes of type nt:activity under system-generated
node names in activity storage below /jcr:system/jcr:activities.""

As far as I can tell, this is currently not the case.

A related test case, org.apache.jackrabbit.test.api.version.ActivitiesTest#testActivitiesPath, apparently was taken out accidentally.

If Jackrabbit can't implement this JCR requirement, we either need to document it, or raise it as issue in the JCR Expert Group."
0,SimpleTextCodec needs SimpleText DocValues implcurrently SimpleTextCodec uses binary docValues we should move that to a simple text impl.
1,"cache module generates exceptions for non-compliant responses without consuming response bodiesIn the ResponseProtocolCompliance class, the caching module checks the incoming origin response to attempt to make it compliant with RFC2616. However, if there are instances where this is not possible, it currently throws an exception without consuming the origin response body; this causes a connection leak if the general try..catch..finally pattern documented on the HttpClient interface Javadoc is followed.
"
1,"[PATCH] Wirelog correctionsThis patch 

* fixes the problem reported by Geir H. Pettersen <geir at cellus.no>. See
http://marc.theaimsgroup.com/?t=108072355300004&r=1&w=2 for details

* increases the priority of HTTP request/status line & HTTP headers output from
DEBUG to INFO. Quite often request/response content generate excessive amount of
output in the wirelog and produce no valuable debug information of what so ever.
By setting wirelog verbosity to INFO one can turn off the logging of 
request/response content.

I believe the patch should be applied to both CVS HEAD and 2.0 branch. Please
let me know if you agree

Oleg"
0,"TestIndexWriterDelete makes broken segments with payloads onThis could just be a SimpleText problem.... but just in case

Grant added payloads to MockAnalyzer in LUCENE-2692

I wondered what would happen if i turned on his payload filter by default for all tests.

{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriterDelete
    [junit] Testcase: testDeletesOnDiskFull(org.apache.lucene.index.TestIndexWriterDelete):     Caused an ERROR
    [junit] CheckIndex failed
    [junit] java.lang.RuntimeException: CheckIndex failed
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:80)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.doTestOperationsOnDiskFull(TestIndexWriterDelete.java:5
38)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.testDeletesOnDiskFull(TestIndexWriterDelete.java:401)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit]
    [junit]
    [junit] Testcase: testUpdatesOnDiskFull(org.apache.lucene.index.TestIndexWriterDelete):     Caused an ERROR
    [junit] CheckIndex failed
    [junit] java.lang.RuntimeException: CheckIndex failed
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:80)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.doTestOperationsOnDiskFull(TestIndexWriterDelete.java:5
38)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.testUpdatesOnDiskFull(TestIndexWriterDelete.java:405)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit]
    [junit]
    [junit] Tests run: 14, Failures: 0, Errors: 2, Time elapsed: 0.322 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] CheckIndex failed
    [junit] Segments file=segments_2 numSegments=1 version=FORMAT_4_0 [Lucene 4.0]
    [junit]   1 of 1: name=_0 docCount=157
    [junit]     codec=MockFixedIntBlock
    [junit]     compound=true
    [junit]     hasProx=true
    [junit]     numFiles=2
    [junit]     size (MB)=0,017
    [junit]     diagnostics = {os.version=6.0, os=Windows Vista, lucene.version=4.0-SNAPSHOT, source=flush, os.arch=x86,
 java.version=1.6.0_21, java.vendor=Sun Microsystems Inc.}
    [junit]     has deletions [delFileName=_0_1.del]
    [junit]     test: open reader.........OK [13 deleted docs]
    [junit]     test: fields..............OK [2 fields]
    [junit]     test: field norms.........OK [2 fields]
    [junit]     test: terms, freq, prox...ERROR [read past EOF]
    [junit] java.io.IOException: read past EOF
    [junit]     at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:154)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:119)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:94)
    [junit]     at org.apache.lucene.index.codecs.sep.SepPostingsReaderImpl$SepDocsAndPositionsEnum.getPayload(SepPostin
gsReaderImpl.java:689)
    [junit]     at org.apache.lucene.index.CheckIndex.testTermIndex(CheckIndex.java:693)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:489)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:290)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:286)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:76)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.doTestOperationsOnDiskFull(TestIndexWriterDelete.java:5
38)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.testDeletesOnDiskFull(TestIndexWriterDelete.java:401)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)
    [junit]     test: stored fields.......OK [223 total field count; avg 1,549 fields per doc]
    [junit]     test: term vectors........OK [242 total vector count; avg 1,681 term/freq vector fields per doc]
    [junit] FAILED
    [junit]     WARNING: fixIndex() would remove reference to this segment; full exception:
    [junit] java.lang.RuntimeException: Term Index test failed
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:502)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:290)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:286)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:76)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.doTestOperationsOnDiskFull(TestIndexWriterDelete.java:5
38)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.testDeletesOnDiskFull(TestIndexWriterDelete.java:401)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)
    [junit]
    [junit] WARNING: 1 broken segments (containing 144 documents) detected
    [junit]
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterDelete -Dtestmethod=testDeletesOnDiskFull -Dtests.s
eed=-8128829179004133416:-7192468460505114475
    [junit] CheckIndex failed
    [junit] Segments file=segments_1 numSegments=1 version=FORMAT_4_0 [Lucene 4.0]
    [junit]   1 of 1: name=_0 docCount=157
    [junit]     codec=MockFixedIntBlock
    [junit]     compound=false
    [junit]     hasProx=true
    [junit]     numFiles=14
    [junit]     size (MB)=0,017
    [junit]     diagnostics = {os.version=6.0, os=Windows Vista, lucene.version=4.0-SNAPSHOT, source=flush, os.arch=x86,
 java.version=1.6.0_21, java.vendor=Sun Microsystems Inc.}
    [junit]     no deletions
    [junit]     test: open reader.........OK
    [junit]     test: fields..............OK [2 fields]
    [junit]     test: field norms.........OK [2 fields]
    [junit]     test: terms, freq, prox...ERROR [Read past EOF]
    [junit] java.io.IOException: Read past EOF
    [junit]     at org.apache.lucene.store.RAMInputStream.switchCurrentBuffer(RAMInputStream.java:89)
    [junit]     at org.apache.lucene.store.RAMInputStream.readBytes(RAMInputStream.java:73)
    [junit]     at org.apache.lucene.store.MockIndexInputWrapper.readBytes(MockIndexInputWrapper.java:109)
    [junit]     at org.apache.lucene.index.codecs.sep.SepPostingsReaderImpl$SepDocsAndPositionsEnum.getPayload(SepPostin
gsReaderImpl.java:689)
    [junit]     at org.apache.lucene.index.CheckIndex.testTermIndex(CheckIndex.java:693)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:489)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:290)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:286)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:76)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.doTestOperationsOnDiskFull(TestIndexWriterDelete.java:5
38)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.testUpdatesOnDiskFull(TestIndexWriterDelete.java:405)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)
    [junit]     test: stored fields.......OK [237 total field count; avg 1,51 fields per doc]
    [junit]     test: term vectors........OK [254 total vector count; avg 1,618 term/freq vector fields per doc]
    [junit] FAILED
    [junit]     WARNING: fixIndex() would remove reference to this segment; full exception:
    [junit] java.lang.RuntimeException: Term Index test failed
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:502)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:290)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:286)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:76)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.doTestOperationsOnDiskFull(TestIndexWriterDelete.java:5
38)
    [junit]     at org.apache.lucene.index.TestIndexWriterDelete.testUpdatesOnDiskFull(TestIndexWriterDelete.java:405)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:743)
    [junit]
    [junit] WARNING: 1 broken segments (containing 157 documents) detected
    [junit]
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterDelete -Dtestmethod=testUpdatesOnDiskFull -Dtests.s
eed=-8128829179004133416:-5617162412680232634
    [junit] NOTE: test params are: codec=MockFixedIntBlock(blockSize=266), locale=tr_TR, timezone=Africa/Maseru
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.index.TestIndexWriterDelete FAILED
    [junit] Testsuite: org.apache.lucene.index.TestLazyProxSkipping
    [junit] Tests run: 2, Failures: 0, Errors: 0, Time elapsed: 0.034 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestLazyProxSkipping -Dtestmethod=testLazySkipping -Dtests.seed=-7
119541877291237950:-8499388603775233752
    [junit] NOTE: test params are: codec=SimpleText, locale=da_DK, timezone=Pacific/Guam
    [junit] ------------- ---------------- ---------------
{noformat}"
0,"Jcr2Spi: UpdateTest#testUpdateRemovesExtraProperty and #testUpdateAddsMissingSubtree fail occasionallyissue reported by jukka:

Every now and then I see the following jcr2spi test failures on a
clean checkout:

    Tests in error:
      testUpdateRemovesExtraProperty(org.apache.jackrabbit.jcr2spi.UpdateTest)
      testUpdateAddsMissingSubtree(org.apache.jackrabbit.jcr2spi.UpdateTest)

The problem seems to be caused by the test cases automatically
choosing the ""security"" workspace for the update test. The reason why
the tests only fail occasionally is that currently the ordering of the
string array returned by getAccessibleWorkspaceNames() is not
deterministic.

I can work around the issue by making
RepositoryImpl.getWorkspaceNames() explicitly sort the returned array
of names, but a more proper fix would probably be to ensure that the
workspace selected by UpdateTest is useful for the test.


"
1,"""Index already present"" exception when opening a restored repositoryI have created a new repository, added one node, then copied all files while Jackrabbit is running.
Then closed the repository, restored the backup, and tried to open the repository.
Unfortunately, this resulted in the following exception:

javax.jcr.RepositoryException: Index already present: Index already present: Index already present
	at org.apache.jackrabbit.core.SearchManager.initializeQueryHandler(SearchManager.java:575)
	at org.apache.jackrabbit.core.SearchManager.<init>(SearchManager.java:255)
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getSearchManager(RepositoryImpl.java:1613)
	at org.apache.jackrabbit.core.RepositoryImpl.initWorkspace(RepositoryImpl.java:606)

The backup contains the following index file:
workspaces/default/index/redo.log
There are no other files or directories or files in that directory (also no _n directories). The content of redo.log is:

-1 STR
-1 ADD cafebabe-cafe-babe-cafe-babecafebabe
-1 COM
0 STR
0 DEL cafebabe-cafe-babe-cafe-babecafebabe
0 ADD fa87759b-f9fe-4ba8-986c-d1914ffce3de
0 ADD eda04b36-9c21-4712-bedf-206c36f0e3d2
0 ADD ae917cca-a0bb-4ac0-a16d-805aac6c7b10
0 ADD cafebabe-cafe-babe-cafe-babecafebabe
0 COM
1 STR
1 ADD 0121f271-bbe7-4f71-a793-5f4380f3c487
1 COM
"
0,"Default to anonymous access when no Credentials are givenEven though JCR-348 made easier to start a Jackrabbit repository with default configuration, the user still needs to take care of the JAAS configuration. It would be more user-friendly to log a warning and default to superuser access rather than throwing a LoginException when JAAS has not been configured. This behaviour should be limited to only default credential logins (Session.login() with null Credentials) and it should be possible to disable it with a configuration option. We could even have this behaviour disabled by default, but enabled in the configuration file used with the JCR-348 automatic configuration.

This is a case against the ""secure by default"" design principle, but I think that in this case the benefits in easier setup outweight the security drawbacks, especially if coupled with the above restrictions and a clear documentation note about the insecure default.

[Update: As mentioned by Stefan, this is  not a JAAS configuration issue but a problem in handling null Credentials. A more proper alternative for superuser access would be to default to anonymous access when credentials are not given.]"
1,"empty path not handled correctlyWhen requesting for a URL which has an empty path, e.g. http://abcnews.go.com ,
the code sends the following line:

GET  HTTP/1.1

which should be

GET / HTTP/1.1

instead"
0,Missing dependencies in two generated maven pom files There are some missing dependencies in generated maven pom.xml files (benchmark and highlighter)
0,Remove autoCommit from IndexWriterIndexWriter's autoCommit is deprecated; in 3.0 it will be hardwired to false.
0,"Reduce memory usage of DocIdsImplementations of DocIds are used to cache parent child relations of nodes in the index. Usually there are a lot of duplicate objects because a DocId instance is used to identify the parent of a node in the index. That is, sibling nodes will all have DocIds with the same value. Currently a new DocId instance is created for each node. Caching the most recently used DocIds and reuse them might help to reduce the memory usage. Furthermore there are DocIds that could be represented with a short instead of an int when possible."
1,"Range Query works only with lower case termsI am performing a range query that returns results if the terms are lower 
case, but does not return result when the terms are mixed case.

In my collection, I have terms alpha, beta, delta, gamma.  I am using the 
StandardAnalyzer for both indexing and searching.

The query [alpha TO gamma] returns all four terms.  When I perform the query 
[Alpha TO Gamma], no results are returned.

It appears the lowerCaseFilter(), which is a part of the StandardAnalyzer, 
does not work properly on the search terms.  I've used Luke to peek at my 
collection, and the terms are all lower case in the collection.

I'm fairly new to Lucene, so I hope I'm not making a ""common mistake""."
1,"System view XML uses hardcoded sv: prefixJackrabbit enforces that the xml docment that imported into repository through the use of  ContentHandler have attributte ""name"" with specific prefix (""sv""), instead of specific namespace (""com.cisco.topos.jcr.sv"").

Example of wrong behavior:
Calling
marshaller.marshal(entry, session.getImportContentHandler(session.getNodeByUUID(channelId).getPath(), ImportUUIDBehavior.IMPORT_UUID_CREATE_NEW));

where entry is object that represent xml structure with namespace ""com.cisco.topos.jcr.sv"" assigned to prefix other then ""sv"" or as default namespace  will cause  exception
java.lang.RuntimeException: javax.xml.bind.MarshalException

 javax.jcr.InvalidSerializedDataException: missing mandatory sv:name attribute of element sv:node
	at org.apache.jackrabbit.core.xml.SysViewImportHandler.startElement(SysViewImportHandler.java:122)
	at org.apache.jackrabbit.core.xml.ImportHandler.startElement(ImportHandler.java:192)
	at com.sun.xml.bind.v2.runtime.output.SAXOutput.endStartTag(SAXOutput.java:80)
	at com.sun.xml.bind.v2.runtime.XMLSerializer.endAttributes(XMLSerializer.java:273)
	at com.sun.xml.bind.v2.runtime.XMLSerializer.childAsSoleContent(XMLSerializer.java:531)
	at com.sun.xml.bind.v2.runtime.ClassBeanInfoImpl.serializeRoot(ClassBeanInfoImpl.java:283)
	at com.sun.xml.bind.v2.runtime.XMLSerializer.childAsRoot(XMLSerializer.java:461)
	at com.sun.xml.bind.v2.runtime.MarshallerImpl.write(MarshallerImpl.java:292)
	... 24 more


"
1,"workspace.copy does not copy binary properties properlyWorkspace copy works fine for everything else but if you copy a hierarchy which contains binary properties
and remove the source after copying it removes the binary from ""copied"" newly created hierarchy as well.

How to reproduce:

1. create hierarchy with any type of nodes  - /site / en / image [Type Binary]
2. create another hierarchy at different level - /site2
3. copy /site/en under /site2
-- till now everything is fine, its a proper copy and if you export xml out of these 2 hierarchies you will see that ""image"" is actually copied
4. now delete /site/en
5. you will see all other properties as copied before /site2/en... except binary.

are binary types are always referenced? even if you copy via workspace copy
"
0,"Add next() and skipTo() variants to DocIdSetIterator that return the current doc, instead of booleanSee http://www.nabble.com/Another-possible-optimization---now-in-DocIdSetIterator-p23223319.html for the full discussion. The basic idea is to add variants to those two methods that return the current doc they are at, to save successive calls to doc(). If there are no more docs, return -1. A summary of what was discussed so far:
# Deprecate those two methods.
# Add nextDoc() and skipToDoc(int) that return doc, with default impl in DISI (calls next() and skipTo() respectively, and will be changed to abstract in 3.0).
#* I actually would like to propose an alternative to the names: advance() and advance(int) - the first advances by one, the second advances to target.
# Wherever these are used, do something like '(doc = advance()) >= 0' instead of comparing to -1 for improved performance.

I will post a patch shortly"
0,"separate IndexDocValues interface from implementationCurrently the o.a.l.index.values contains both the abstract apis and Lucene40's current implementation.

I think we should move the implementation underneath Lucene40Codec, leaving only the abstract apis.

For example, simpletext might have a different implementation, and we might make a int8 implementation
underneath preflexcodec to support norms."
0,Change NumericRangeQuery to generics (for the numeric type)NumericRangeQuery/Filter can use generics for more type-safety: NumericRangeQuery<T extends Number>
1,"typo in the mimetypes.properties fileThe Powerpoint mime-type (ppt) is wrong in mimetypes.properties file. It was written with a equals(=) caracter in place of a dot (.).
"
0,"Unnecessary parameter in NodeTypeRegistry.persistCustomNodeTypeDefs(NodeTypeDefStore store)The parameter ""store"" is not used within this method. I suggest to delete the parameter to make the code more readable.

"
0,"when a test Assume fails, display informationCurrently if a test uses Assume.assumeTrue, it silently passes.

I think we should output something, at *least* if you have VERBOSE set, maybe always.

Here's an example of what the output might look like:
{noformat}
junit-sequential:
    [junit] Testsuite: org.apache.solr.servlet.SolrRequestParserTest
    [junit] Tests run: 4, Failures: 0, Errors: 0, Time elapsed: 1.582 sec
    [junit]
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: testStreamURL Assume failed (ignored):
    [junit] org.junit.internal.AssumptionViolatedException: got: <java.io.FileNotFoundException: http://www.apdfgdfgache
.org/dist/lucene/solr/>, expected: null
    [junit]     at org.junit.Assume.assumeThat(Assume.java:70)
    [junit]     at org.junit.Assume.assumeNoException(Assume.java:92)
    [junit]     at org.apache.solr.servlet.SolrRequestParserTest.testStreamURL(SolrRequestParserTest.java:123)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    [junit]     at java.lang.reflect.Method.invoke(Method.java:597)
    [junit]     at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    [junit]     at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    [junit]     at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    [junit]     at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
    [junit]     at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:802)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:775)
    [junit]     at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
    [junit]     at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
    [junit]     at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
    [junit]     at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
    [junit]     at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
    [junit]     at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
    [junit]     at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
    [junit]     at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    [junit]     at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:420)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:911)
    [junit]     at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:768)
    [junit] Caused by: java.io.FileNotFoundException: http://www.apdfgdfgache.org/dist/lucene/solr/
    [junit]     at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1311)
    [junit]     at org.apache.solr.servlet.SolrRequestParserTest.testStreamURL(SolrRequestParserTest.java:120)
    [junit]     ... 26 more
    [junit] ------------- ---------------- ---------------
{noformat}"
1,"thread starving in MultiThreadedHttpConnectionManagerHi folks,

I might have found a bug in MTHCM. It has to do with removing HostConnectionPool instances that have no more connections in them. That was a fix for a memory leak we previously had. There are two cases where the pools get deleted. One is in handleLostConnection: (excerpt)
  ...
  if (hostPool.numConnections == 0) mapHosts.remove(config);
  notifyWaitingThread(config);
  ...

Could this delete a pool in which there is still a thread waiting to get a connection? If so, the thread would remain in the global pool. But even if it is interrupted there, it would still use the old HostConnectionPool in which no connection will ever become available again.

I suggest to change the removal check in both cases to:
  if ((hostPool.numConnections < 1) && hostPool.waitingThreads.isEmpty)

What do you think?"
0,"Expose registered node types below /jcr:system/jcr:nodeTypesspec says:

6.8 System Node
[...]
For example, if a repository exposes node type definitions in content, then those node type definitions should be located at /jcr:system/jcr:nodeTypes.
"
1,deadlock on concurrent commit/lockingthere can happen an iterlock between the dispatching part of the shared item state manager and the lockmanager.
0,"bad assumptions on QueryResult.getIterator() semantics in QueryResultNodeIteratorTest.testSkip()testSkip() assumes that calling getIterator() a second time will return a new iterator of the same size. JSR-170 is silent on this. Forcing a server to implement this essantially means that the query result must be cached until there's no reference to QueryResult anymore.

As this is a test of skip(), not getIterator(), the test should really refetch a new QueryResult in order to obtain a new iterator.

(Note: The issue of the semantics of QueryResult.getIterator should be discussed by the JCR EG.)
"
0,"FuzzyLikeThisQuery should set MaxNonCompetitiveBoost for faster speedFuzzyLikeThisQuery uses FuzzyTermsEnum directly, and maintains 
a priority queue for its purposes.

Just like TopTermsRewrite method, it should set the 
MaxNonCompetitiveBoost attribute, so that FuzzyTermsEnum can
run faster. Its already tracking the minScore, just not updating
the attribute.

This would be especially nice as it appears to have nice defaults
already (pq size of 50)
"
0,"Improve lifecycle management of JCA connectorthe shutdown mechanism doesn't work correctly. It shutdowns the repository when the RA (resource adapter) is garbage collected. It causes redeployment to fail because sometimes the new RA is redeployed before the old one is garbage collected.

Implementing the JCA 1.5 interface to manage the lifecycle would be useful."
1,"problem with isIPv4address() for relative uri'sthe following block of code:

try {
	URI uri = new URI(""http://10.0.1.10:8830"");
	System.out.println(""is IP=""+uri. isIPv4address());
	uri = new URI(uri, ""/04-1.html"");
	System.out.println(""is IP=""+uri. isIPv4address());
} catch (URIException e) { ; }

returns the output:

is IP=true
is IP=false

so by being created from a relative uri URI objects don't have the right setting of  isIPv4address()."
0,"ResourceConfig: read additional parameters for IOHandler and PropertyHandler that are covered by public settersthe reason for this is that currently default node types used in DefaultHandler cannot be changed using the resource config, which is
a bit cumbersome and leads to useless copies of DefaultIOManager."
0,"[PATCH] Add StopFilter ignoreCase optionWanted to have the ability to ignore case in the stop filter.  In some cases, I
don't want to have to lower case before passing through the stop filter, b/c I
may need case preserved for other analysis further down the stream, yet I don't
need the stopwords and I don't want to have to apply stopword filters twice."
0,"remove dependencies of XPathQueryBuilder on core Jackrabbit codeThe XPath query parser currently has a single dependency on SearchManager, for the sole purpose of importing two namespace URIs (for XML Schema and XPath 2.0 functions). This makes it harder than it should be to use it stand-alone.

I propose to copy the two namespace URIs into XPathQueryBuilder, getting rid of the dependency.

"
0,"Add missing license headersThere are a few source files and a number of other files with missing or incorrect license headers within the Jackrabbit source tree. See the  discusssion at http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/8698 for details. The missing license headers need to be added. 

There are also W3C licensed files. This needs to be mentioned in the NOTICE file."
0,"Allow PackedInts.ReaderIterator to advance more than one valueThe iterator-like API in LUCENE-2186 makes effective use of PackedInts.ReaderIterator but frequently skips multiple values. ReaderIterator currently requires to loop over ReaderInterator#next() to advance to a certain value. We should allow ReaderIterator to expose a #advance(ord) method to make use-cases like that more efficient. 

This issue is somewhat part of my efforts to make LUCENE-2186 smaller while breaking it up in little issues for parts which can be generally useful."
1,"Node.restore() fails for existing non-versioned OPV=Version child nodesI have a node whose definition has properties and child nodes.  The
definitions of the nodetypes for the node and the child include
mix:versionable.  The properties definitions have onParentVersion=COPY and
the child nodes have onParentVersion=VERSION.  When I create a node with
child nodes and checkin and then restore the node, I get a
""....VersionException: Restore of root node not allowed""  This is
occurring on the restore of the child node.

According to the spec:

Child Node
On checkin of N, the node VN will get a subnode of type nt:versionedChild
with the same name as C. The single property of this node,
jcr:childVersionHistory is a REFERENCE to the version history of C (not to
C or any actual version of C). This also requires that C itself be
versionable (otherwise it would not have a version history).
.
.
.
On restore of VN, if the workspace currently has an already existing node
corresponding to C?s version history and the removeExisting flag of the
restore is set to true, then that instance of C becomes the child of the
restored N. If the workspace currently has an already existing node
corresponding to C?s version history and the removeExisting flag of the
restore is set to false then an ItemExistsException is thrown.


I'm restoring the node using

   node.restore(version, true);

Is this expected behavior?"
0,"Create ChainingCollectorChainingCollector allows chaining a bunch of Collectors, w/o them needing to know or care about each other, and be passed into Lucene's search API, since it is a Collector on its own. It is a convenient, yet useful, class. Will post a patch w/ it shortly."
0,Upgrade contrib/bdb-persistence to work w/Jackrabbit 1.3The bdb-persistence PM in contrib should be upgraded to work w/Jackrabbit 1.3.1
0,"Deprecate/remove unused FileSystem features such as RandomAccessOutputStreamCurrently the FileSystem interface includes a method getRandomAccessOutputStream.
It looks like this method and the class RandomAccessOutputStream is no longer used.
If this is the case, I suggest we remove the method everywhere and deprecate the class."
1,"DateValue.getDate not a copyI noticed that getDate() in org.apache.jackrabbit.value.DateValue is returned 
by reference. According to the specification it should be a copy. (see.  JSR 170 section 6.2.7)

 
 private Calendar date;
 
 public Calendar getDate()
             throws ValueFormatException, IllegalStateException,
             RepositoryException {
         setValueConsumed();
 
         if (date != null) {
             return date; // <-- HERE
         } else {
             throw new ValueFormatException(""empty value"");
         }
     }

short test:

ValueFactory factory = session.getValueFactory();
 Value v = factory.createValue(GregorianCalendar.getInstance());
 Calendar c0 = v.getDate();   
 Calendar c1 = v.getDate();
               
 if(c0 == c1){
                   out.println(""error - references are equal"");
                    out.println(c0);
 }"
1,"Deadlock when concurrently committing and reading versioning statesthere is a rear occation when one thread commits a transaction and another thread reads versioing related information, so that a deadlock can occurr. 

example:

Thread1:
                        ut.begin();
                        session.getWorkspace().clone(""default"", ""/content"", ""/content"", true);
                        ut.commit();

Thread2:
                        VersionHistory vh = folder.getVersionHistory();
                        VersionIterator iter = vh.getAllVersions();
                        while (iter.hasNext()) {
                            Version v = iter.nextVersion();
                        }


to fix this issue we must ensure, that methods below the shareditemstatemgr do not call higher instances (like itemmgr) again."
0,"add Terms.docCountspinoff from LUCENE-3290, where yonik mentioned:

{noformat}
Is there currently a way to get the number of documents that have a value in the field?
Then one could compute the average length of a (sparse) field via sumTotalTermFreq(field)/docsWithField(field)
docsWithField(field) would be useful in other contexts that want to know how sparse a field is (automatically selecting faceting algorithms, etc).
{noformat}

I think this is a useful stat to add, in case you have sparse fields for heuristics or scoring."
0,"MultithreadedConnectionManager and IdleConnectionTimeoutThread improvementsChanges to MultithreadedConnectionManager and IdleConnectionTimeoutThread following the suggestions of Balazs SZCS.


-------- Forwarded Message --------
From: SZCS Balazs <Balazs.Szuecs@wave-solutions.com>
Reply-To: HttpClient User Discussion
<httpclient-user@jakarta.apache.org>
To: 'HttpClient User Discussion' <httpclient-user@jakarta.apache.org>
Subject: RE: MultithreadedConnectionManager pooling strategy
Date: Mon, 15 May 2006 15:26:08 +0200

Hello,

I made two changes to the HttpClient code:

1) in the class ConnectionPool in the method getFreeConnection( ... ) I
changed freeConnections.removeFirst() to freeConnections.removeLast(). Now
the container for free connections behaves as a stack rather than a queue.

2) additionally I changed the IdleConnectionTimeoutThread, in the run()
method I added connectionManager.deleteClosedConnections() so that the pool
size is maintained correctly.

What do you think?
Balazs"
0,"[PATCH] Indexing on Hadoop distributed file systemIn my current project we needed a way to create very large Lucene indexes on Hadoop distributed file system. When we tried to do it directly on DFS using Nutch FsDirectory class - we immediately found that indexing fails because DfsIndexOutput.seek() method throws UnsupportedOperationException. The reason for this behavior is clear - DFS does not support random updates and so seek() method can't be supported (at least not easily).
 
Well, if we can't support random updates - the question is: do we really need them? Search in the Lucene code revealed 2 places which call IndexOutput.seek() method: one is in TermInfosWriter and another one in CompoundFileWriter. As we weren't planning to use CompoundFileWriter - the only place that concerned us was in TermInfosWriter.
 
TermInfosWriter uses IndexOutput.seek() in its close() method to write total number of terms in the file back into the beginning of the file. It was very simple to change file format a little bit and write number of terms into last 8 bytes of the file instead of writing them into beginning of file. The only other place that should be fixed in order for this to work is in SegmentTermEnum constructor - to read this piece of information at position = file length - 8.
 
With this format hack - we were able to use FsDirectory to write index directly to DFS without any problems. Well - we still don't index directly to DFS for performance reasons, but at least we can build small local indexes and merge them into the main index on DFS without copying big main index back and forth. 

"
0,"speed up core testsOur core tests have gotten slower and slower, if you don't have a really fast computer its probably frustrating.

I think we should:
1. still have random parameters, but make the 'obscene' settings like SimpleText rarer... we can always make them happen more on NIGHTLY
2. tests that make a lot of documents can conditionalize on NIGHTLY so that they are still doing a reasonable test on ordinary runs e.g. numdocs = (NIGHTLY ? 10000 : 1000) * multiplier
3. refactor some of the slow huge classes with lots of tests like TestIW/TestIR, at least pull out really slow methods like TestIR.testDiskFull into its own class. this gives better parallelization.
"
0,"Refactor DocumentsWriterI've been working on refactoring DocumentsWriter to make it more
modular, so that adding new indexing functionality (like column-stride
stored fields, LUCENE-1231) is just a matter of adding a plugin into
the indexing chain.

This is an initial step towards flexible indexing (but there is still
alot more to do!).

And it's very much still a work in progress -- there are intemittant
thread safety issues, I need to add tests cases and test/iterate on
performance, many ""nocommits"", etc.  This is a snapshot of my current
state...

The approach introduces ""consumers"" (abstract classes defining the
interface) at different levels during indexing.  EG DocConsumer
consumes the whole document.  DocFieldConsumer consumes separate
fields, one at a time.  InvertedDocConsumer consumes tokens produced
by running each field through the analyzer.  TermsHashConsumer writes
its own bytes into in-memory posting lists stored in byte slices,
indexed by term, etc.

DocumentsWriter*.java is then much simpler: it only interacts with a
DocConsumer and has no idea what that consumer is doing.  Under that
DocConsumer there is a whole ""indexing chain"" that does the real work:

  * NormsWriter holds norms in memory and then flushes them to _X.nrm.

  * FreqProxTermsWriter holds postings data in memory and then flushes
    to _X.frq/prx.

  * StoredFieldsWriter flushes immediately to _X.fdx/fdt

  * TermVectorsTermsWriter flushes immediately to _X.tvx/tvf/tvd

DocumentsWriter still manages things like flushing a segment, closing
doc stores, buffering & applying deletes, freeing memory, aborting
when necesary, etc.

In this first step, everything is package-private, and, the indexing
chain is hardwired (instantiated in DocumentsWriter) to the chain
currently matching Lucene trunk.  Over time we can open this up.

There are no changes to the index file format.

For the most part this is just a [large] refactoring, except for these
two small actual changes:

  * Improved concurrency with mixed large/small docs: previously the
    thread state would be tied up when docs finished indexing
    out-of-order.  Now, it's not: instead I use a separate class to
    hold any pending state to flush to the doc stores, and immediately
    free up the thread state to index other docs.

  * Buffered norms in memory now remain sparse, until flushed to the
    _X.nrm file.  Previously we would ""fill holes"" in norms in memory,
    as we go, which could easily use way too much memory.  Really this
    isn't a solution to the problem of sparse norms (LUCENE-830); it
    just delays that issue from causing memory blowup during indexing;
    memory use will still blowup during searching.

I expect performance (indexing throughput) will be worse with this
change.  I'll profile & iterate to minimize this, but I think we can
accept some loss.  I also plan to measure benefit of manually
re-cycling RawPostingList instances from our own pool, vs letting GC
recycle them.

"
1,"Index segments are only committed on closeThere is a check in AbstractIndex.commit(), which prevents that deleted documents are committed to the index. Up to lucene version 2.0 the index was locked when there were pending changes. Beginning with lucene 2.1 this is not true anymore. See LUCENE-701.

This is a regression of JCR-788, hence it does not occur in a release but only in trunk."
0,"A new Greek Analyzer for LuceneI would like to contribute a greek analyzer for lucene. It is based on the
existing Russian analyzer and features:

- most common greek character sets, such as Unicode, ISO-8859-7 and Windows-1253
- a collection of common greek stop words
- conversion of characters with diacritics (accent, diaeresis) in the lower case
filter, as well as handling of special characters, such as small final sigma

For the character sets I used RFC 1947 (Greek Character Encoding for Electronic
Mail Messages) as a reference. I have incorporated this analyzer in Luke as well
as used it successfully in a recent project of my company (EBS Ltd.).

I hope you will find it a useful addition to the project."
0,"DatabasePersistenceManager: don't log exceptions for each statement when a connection needs to be reestablishedThis is just a ""cosmetic"" fix: when reestablishConnection() is called in DatabasePersistenceManager all the statements are closed but if an error occurs two exceptions are logged for each statement.
Since reestablishConnection() is already called when an exception has been caught and its only purpose is to cleanup an existing connection and to reopen a new one is pretty common that the connection is already not valid and that each statement close will throw an exception.

For example if the connection has been broken due to a network problem DatabasePersistenceManager  will log *40* exceptions (2 for each statement) before trying to establish a connection, and that's pretty annoying (expecially if you use a mail appender for log4j....)
"
1,"processing a synonym in a token stream will remove the following token from the streamIf you do a phrase search on a field derived from a fieldtype with the synonym filter which includes a synonym, the term following the synonym vanishes after synonym expansion.

e.g. http://host:port/solr/corename/select/?q=desc:%22xyzzy%20%20bbb%20pot%20of%20gold%22&version=2.2&start=0&rows=10&indent=on&debugQuery=true   (bbb is in the default synonyms file, desc is a ""text"" fieldtype)

outputs
....
<str name=""rawquerystring"">desc:""xyzzy  bbb pot of gold""</str>
<str name=""querystring"">desc:""xyzzy  bbb pot of gold""</str>
<str name=""parsedquery"">PhraseQuery(desc:""xyzzy bbbb 1 bbbb 2 of gold"")</str>
<str name=""parsedquery_toString"">desc:""xyzzy bbbb 1 bbbb 2 of gold""</str>
....

You can also see this behavior using the admin console analysis.jsp

Solr 3.3 behaves properly.
"
0,"Store DocValues per segment instead of per fieldcurrently we are storing docvalues per field which results in at least one file per field that uses docvalues (or at most two per field per segment depending on the impl.). Yet, we should try to by default pack docvalues into a single file if possible. To enable this we need to hold all docvalues in memory during indexing and write them to disk once we flush a segment. "
1,Some code still compares string equality instead using equalsI found a couple of places where we still use string == otherstring which don't look correct. I will attache a patch soon.
1,"SegmentInfo should explicitly track whether that segment wrote term vectorsToday SegmentInfo doesn't know if it has vectors, which means its files() method must check if the files exist.

This leads to subtle bugs, because Si.files() caches the files but then we fail to invalidate that later when the term vectors files are created.

It also leads to sloppy code, eg TermVectorsReader ""gracefully"" handles being opened when the files do not exist.  I don't like that; it should only be opened if they exist.

This also fixes these intermittent failures we've been seeing:

{noformat}
junit.framework.AssertionFailedError: IndexFileDeleter doesn't know about file _1e.tvx
       at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:979)
       at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:917)
       at org.apache.lucene.index.IndexWriter.filesExist(IndexWriter.java:3633)
       at org.apache.lucene.index.IndexWriter.startCommit(IndexWriter.java:3699)
       at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2407)
       at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2478)
       at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2460)
       at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2444)
       at org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:213)
{noformat}"
1,"SharedItemStateManager not properly synchronizedSome time ago we removed synchronized modifiers from the methods store() hasItemState() and getItemState(). While some care has been taken to ensure the cache integrity, I think the contract for the SharedItemStateManager (SISM) is now broken. The JavaDoc does not clearly document this, but I think all relevant methods of the SISM working on ItemStates should be atomic.

E.g. a call to hasItemState() should not return true for an ItemState that another thread is currently adding in store(). Similarly a getItemState() should not return an ItemState that is currenly added or modified in a store() operation.

Currently I see two options:
- Change the methods to synchronized again. This will actually serialize all calls to the SISM.
- Implement a more sophisticated synchronisation. E.g. multiple store operations can still be allowed, as long as their ChangeLogs do not intersect. Retrieving ItemStates might still be allowed while a ChangeLog is stored, as long as the ItemState to retrieve is not part of the ChangeLog.

Comments and suggestions are very welcome."
0,Change all contrib TokenStreams/Filters to use the new TokenStream APINow that we have the new TokenStream API (LUCENE-1422) we should change all contrib modules to use it.
0,RTFTextExtractor should also support mime type text/rtfThere exist two mime types for RTF documents: application/rtf and text/rtf. The current RTFTextExtractor currently only recognizes the first.
1,"Inconsistency when version with a label is removedWhile executing random operations on the version storage I came across a situation where a version is removed that has a version label.

The current behaviour in jackrabbit is IMO not correct because the version node gets removed, but the version label is still present in the version history. This means there is a referenceable property that points to nowhere.

So I guess either:

- the version label property must be removed as well when the version is removed
or
- the remove version operation should fail because the version is still referenced

I wasn't able to find a relevant section in the spec, though I must admit that I don't know the versioning section that well."
0,"o.a.j.webdav.jcr.DavResourceFactoryImpl#createResource creates VersionControlledResource instances regardless of mix:versionable statusDavResourceFactoryImpl#createResource() first calls createResourceForItem() which threats all nodes as version-controlled. 
it then calls isVersionControlled() which indirectly triggers a call to Node#getVersionHistroy(). 
getVersionHistroy throws a UnsupportedRepositoryException if the node is non-versionable, leading to a DavException further up the call stack.

as a consequence, every request for a non-versionable node leads to unnecessary (and expensive) exception generation which could be avoided by checking the mix:versionable status of a node.


"
0,"Thread safety and visibility ImprovementsAbstractAuthenticationHandler.DEFAULT_SCHEME_PRIORITY is not protected against external changes.

Although the field is private, subclasses can obtain a reference to it and so may be able to change it.

Consider making the list read-only, or returning a copy instead."
0,Update the Highlighter to use the new TokenStream API
0,"Index Writer constructor flags unclear - and annoying in certain casesWouldn't it make more sense if the constructor for the IndexWriter always
created an index if it doesn't exist - and the boolean parameter should be
""clear"" (instead of ""create"")

So instead of this (from javadoc):

IndexWriter

public IndexWriter(Directory d,
                   Analyzer a,
                   boolean create)
            throws IOException

    Constructs an IndexWriter for the index in d. Text will be analyzed with a.
If create is true, then a new, empty index will be created in d, replacing the
index already there, if any.

Parameters:
    d - the index directory
    a - the analyzer to use
    create - true to create the index or overwrite the existing one; false to
append to the existing index 
Throws:
    IOException - if the directory cannot be read/written to, or if it does not
exist, and create is false


We would have this:

IndexWriter

public IndexWriter(Directory d,
                   Analyzer a,
                   boolean clear)
            throws IOException

    Constructs an IndexWriter for the index in d. Text will be analyzed with a.
If clear is true, and a index exists at location d, then it will be erased, and
a new, empty index will be created in d.

Parameters:
    d - the index directory
    a - the analyzer to use
    clear - true to overwrite the existing one; false to append to the existing
index 
Throws:
    IOException - if the directory cannot be read/written to, or if it does not
exist.



Its current behavior is kind of annoying, because I have an app that should
never clear an existing index, it should always append.  So I want create set to
false.  But when I am starting a brand new index, then I have to change the
create flag to keep it from throwing an exception...  I guess for now I will
have to write code to check if a index actually has content yet, and if it
doesn't, change the flag on the fly."
1,"AbstractClientConnAdapter doesn't ensure that only one of ConnectionReleaseTrigger.abortConnection, .releaseConnection has effectIf HttpUriRequest.abort() is called at about the same time that the request completes, it's possible for an aborted connection to be returned to the pool.  The next time the connection is used, HttpClient.execute fails without retrying, throwing this exception:

java.io.IOException: Connection already shutdown
	at org.apache.http.impl.conn.DefaultClientConnection.opening(DefaultClientConnection.java:112)
	at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:120)
	at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:147)
	at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:101)
	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:381)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:641)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:576)

Steps to reproduce:
1) Set a breakpoint in ThreadSafeClientConnManager.releaseConnection just after ""reusable"" is set (and found to be true).
2) Run to the breakpoint in releaseConnection.
3) Call HttpUriRequest.abort.
4) Let releaseConnection complete.

When the connection is next used, the exception will be thrown.

Snippet from ThreadSafeClientConnManager:
    public void releaseConnection(ManagedClientConnection conn, long validDuration, TimeUnit timeUnit) {
		...
            boolean reusable = hca.isMarkedReusable();
            if (log.isDebugEnabled()) {                             // breakpoint here
                if (reusable) {
                    log.debug(""Released connection is reusable."");
                } else {
                    log.debug(""Released connection is not reusable."");
                }
            }
            hca.detach();
            if (entry != null) {
                connectionPool.freeEntry(entry, reusable, validDuration, timeUnit);
            }
        }
    }


I think that AbstractClientConnAdapter should be modified as follows:

1) Add ""released"" flag:

    /** True if the connection has been released. */
    private boolean released;

2) Modify abortConnection:

    public void abortConnection() {
        synchronized(this) {
            if (aborted || released) {
                return;
            }
            aborted = true;
        }
        unmarkReusable(); // this line and all that follow unchanged

3) Modify releaseConnection:

    public void releaseConnection() {
        synchronized(this) {
            if (aborted || released) {
                return;
            }
            released = true;
        }
        if (connManager != null) {
            connManager.releaseConnection(this, duration, TimeUnit.MILLISECONDS);
        }
    }

"
0,"Maven 2 POM includes junit in default ""compile"" scope, rather than ""test"" scopeThe POM at the URL above declares a dependency on JUnit in the default scope, rather than the ""test"" scope."
1,"BytesRef copy short missed the length settingwhen storing a short type integer to BytesRef, BytesRef missed the length setting. then it will cause the storage size is ZERO if no continuous options on this BytesRef"
0,"Add search timeout support to LuceneThis patch is based on Nutch-308. 

This patch adds support for a maximum search time limit. After this time is exceeded, the search thread is stopped, partial results (if any) are returned and the total number of results is estimated.

This patch tries to minimize the overhead related to time-keeping by using a version of safe unsynchronized timer.

This was also discussed in an e-mail thread.
http://www.nabble.com/search-timeout-tf3410206.html#a9501029"
0,QueryHandler.init() should take a context argumentCurrently the QueryHandler.init() method takes a bunch of arguments which are needed by the single jackrabbit implementation for the query handler. To make further extensions easier the arguments should be packaged into a context class which can be extended without effect on the QueryHandler interface.
0,Formatable changes log  (CHANGES.txt is easy to edit but not so friendly to read by Lucene users)Background in http://www.nabble.com/formatable-changes-log-tt15078749.html
1,"HttpUrl does not accept unescaped passwords- Taken from an email from Gustav Munkby posted to the HttpClient dev mailing list -

If I do:

HTTPUrl url = new HTTPUrl(""kurt"", ""nicepass#"", hostname, 80, path);

throws a URIException with message ""port number invalid"".

First of all the message is wrong...

Next attempt was to urlencode the password, which resulted in the above line working, but the 
password was sent url-encoded to the destination, which can hardly be the desired behaviour?"
0,"Plug-in authentication modulesCurrently only basic authentication is supported.  A Authentication interface
should be provided to allow for plug-in support for other authenticaiton
schemes, some of which may be application specific and therefore have no place
in httpclient itself, but would be required by some users."
1,Stackoverflow when calling deprecated CharArraySet.copy()Calling CharArraySet#copy(set) without the version argument (deprecated) with an instance of CharArraySet results in a stack overflow as this method checks if the given set is a CharArraySet and then calls itself again. This was accidentially introduced due to an overloaded alternative method during LUCENE-2169 which was not used in the final patch.
1,"return value of PostMethod#removeParameter<HttpClient2.0-rc1>

About 
public boolean removeParameter(String paramName)
                 throws IllegalArgumentException
method.

-------------------------------------------------
PostMethod method = new PostMethod(uri);
method.addParameter(""name"", ""Matsui Hideki"");
boolean b;
b = method.removeParameter(""name""); // returns ""true"".
b = method.removeParameter(""XXXX""); // returns ""true"". why??? 
---------------------------------------------------

sorry for my poor english."
1,"Dirty Internal State on Transaction-Rollback during Global Transaction (container managed transaction)Running the following code inside an Global Transaction (JTA, container managed transaction) causes problems.

Session session = getRepsoitorySession(); 
      Node rootNode = session.getRootNode(); 

      Node test = rootNode.addNode(""test""); 
      test.addMixin(CTVRepositoryKonstanten.NODE_MIX_TYP_VERSION); 
      session.save(); 
      throw new RuntimeException(""testException"");

Everythink is fine, but if we execute it a second time we get an org.apache.jackrabbit.core.state.NoSuchItemStateException

org.apache.jackrabbit.core.state.NoSuchItemStateException: b36d91bc-8687-428c-a767-2e087b13191a 
at org.apache.jackrabbit.core.state.SharedItemStateManager.getItemState(SharedItemStateManager.java:270) 
at org.apache.jackrabbit.core.state.LocalItemStateManager.getNodeState(LocalItemStateManager.java:107) 
at org.apache.jackrabbit.core.state.LocalItemStateManager.getItemState(LocalItemStateManager.java:172) 
at org.apache.jackrabbit.core.state.XAItemStateManager.getItemState(XAItemStateManager.java:260) 
at org.apache.jackrabbit.core.version.NodeStateEx.store(NodeStateEx.java:519) 
at org.apache.jackrabbit.core.version.NodeStateEx.store(NodeStateEx.java:489) 
at org.apache.jackrabbit.core.version.AbstractVersionManager.getParentNode(AbstractVersionManager.java:414) 
at org.apache.jackrabbit.core.version.AbstractVersionManager.createVersionHistory(AbstractVersionManager.java:357) 
at org.apache.jackrabbit.core.version.XAVersionManager.createVersionHistory(XAVersionManager.java:148) 
at org.apache.jackrabbit.core.version.AbstractVersionManager.getVersionHistory(AbstractVersionManager.java:273) 
at org.apache.jackrabbit.core.ItemImpl.initVersionHistories(ItemImpl.java:738) 
at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1097) 
at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:915) 
at org.apache.jackrabbit.jca.JCASessionHandle.save(JCASessionHandle.java:180) 
at de.continentale.repo.CTVRepository.erstelleDokument(CTVRepository.java:2267)

We think that there is some internal state that is not cleaned up on rollback.
Restarting the runtime (Application Server) ""solved"" this.

May be there are some same causes like in: JCR-2503, JCR-2613

"
1,"UserAccessControlProvider handles users who dont have Jackrabbit managed Principals or User node inconsistently.JR core 2.0.0
In UserAccessControlProvider.compilePermissions(...), if no principal relating to a user node can be found, then a set or read only compiled permissions is provided. That set gives the session read only access to the entire security workspace regardless of path.

If the user node is found, then an instance of UserAccessControlProvider.CompilePermissions is used and in UserAccessControlProvider.CompilePermissions.buildResult(...) there is a check for no user node. If there is no user node, all permissions are denied regardless of path.

Although the first case will never happen for an installation of Jackrabbit where there are no custom PrincipalManagers, I suspect, based on the impl of UserAccessControlProvider.CompilePermissions.buildResult(...) was to deny all access to the security workspace where there was no corresponding user node in a set of principals.

Since this does not effect JR unless there is an external Principal Manager its a bit hard to produce a compact unit test, the issue was found by looking at the code."
0,"NumericRange support for new query parserIt would be good to specify some type of ""schema"" for the query parser in future, to automatically create NumericRangeQuery for different numeric types? It would then be possible to index a numeric value (double,float,long,int) using NumericField and then the query parser knows, which type of field this is and so it correctly creates a NumericRangeQuery for strings like ""[1.567..*]"" or ""(1.787..19.5]"".

There is currently no way to extract if a field is numeric from the index, so the user will have to configure the FieldConfig objects in the ConfigHandler. But if this is done, it will not be that difficult to implement the rest.

The only difference between the current handling of RangeQuery is then the instantiation of the correct Query type and conversion of the entered numeric values (simple Number.valueOf(...) cast of the user entered numbers). Evenerything else is identical, NumericRangeQuery also supports the MTQ rewrite modes (as it is a MTQ).

Another thing is a change in Date semantics. There are some strange flags in the current parser that tells it how to handle dates."
0,"Provide support for non-ASCII charsets in the multipart disposition-content headerBecause of the the following line in getAsciiBytes 
 data.getBytes(""US-ASCII"");

The returned string is modified if has Latin Characters.

Ex : Document non-control -> Document non-control?"
0,"[OCM] Add unit tests with BundleDbPersistenceManagerUntil now, we have not yet check the ocm framework with the BundleDbPersistenceManager"
0,"Support easy pre-authenticated loginSome applications authenticate users themselves and just need to access the repository on behalf of these pre-authenticated users.

Examples of such pre-authentications include SSO solutions or web applications using a web-based authentication protocol not easily implementable in a JAAS LoginModule, for example OpenID or similar.

In such situations a password may not be provided in SimpleCredentials and thus regular login with user name and password is not possible.

Therefore I propose the enhancement of the AbstractLoginModule to allow for setting a specific attribute in the SimpleCredentials attribute map. If this attribute is set, authentication and login succeeds and a session for the user named in the SimpleCredentials is created.

As a starter we might just check for the presence of the attribute."
0,Javadocs clean-upBefore Httpclient can be released Javadocs need to be updated and proof-read
1,"ISO8601 uses default DecimalFormat constructor using locale specific digitsISO8601.java uses the default DecimalFormat constructor which uses locale specific DecimalFormatSymbols. Runnning Jackrabbit in an Indian locale the format() produces a date using DEVANAGARI numeric digits. The saved version (UTF-8) encoded is much longer than usual and is not transportable. On parsing, DecimalFormat works, but TimeZone.getTimeZone(""GMT+09:30"") (with Indian numeric digits) fails and null is returned from ISO8601. Later this traceback occurs.

2010-02-22 15:14:04,059[http-0.0.0.0-8080-16] ERROR org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager - failed to write bundle: ff629488-ebb9-4300-a63b-341553cc1140
java.lang.IllegalArgumentException: argument can not be null
	at org.apache.jackrabbit.util.ISO8601.format(ISO8601.java:217)
	at org.apache.jackrabbit.core.value.InternalValue.toString(InternalValue.java:531)
	at org.apache.jackrabbit.core.persistence.bundle.util.BundleBinding.writeState(BundleBinding.java:689)
	at org.apache.jackrabbit.core.persistence.bundle.util.BundleBinding.writeBundle(BundleBinding.java:273)
	at org.apache.jackrabbit.core.persistence.bundle.BundleFsPersistenceManager.storeBundle(BundleFsPersistenceManager.java:664)
	at org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.putBundle(AbstractBundlePersistenceManager.java:703)
	at org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.store(AbstractBundlePersistenceManager.java:643)


ISO8601 probably meant the chars to be ASCII, and so the constructor with a fixed locale is more appropriate (and this doesn't encounter the TimeZone issue either).

    private static final DecimalFormat XX_FORMAT = new DecimalFormat(""00"", new DecimalFormatSymbols(Locale.US));
    private static final DecimalFormat XXX_FORMAT = new DecimalFormat(""000"", new DecimalFormatSymbols(Locale.US));
    private static final DecimalFormat XXXX_FORMAT = new DecimalFormat(""0000"", new DecimalFormatSymbols(Locale.US));
 "
1,"Save fails after setting a binary property twiceSetting a binary property twice discards the blob value of the first property state but does not remove the change from the changelog, resulting in an error on save:

javax.jcr.RepositoryException: this BLOBFileValue has been disposed
	at org.apache.jackrabbit.core.value.RefCountingBLOBFileValue.copy(RefCountingBLOBFileValue.java:105)

will attach patch that adds the respective test to the jcr2spi tests."
1,"Restore of empty multivalue property always changes property type to StringWhen you do a restore of  empty multivalue property (OPV=COPY), restored property always has the String type (no matter of property type in frozen state). The solution is to set the property type from frozen state instead of retriving it from 'first' value. If mulitvalue does not have any values the type is set to UNDEFINED and finally changed to STRING in restore method.

Attached patch with test case."
0,"Align the code base with checkstyleThe style use in HttpClient is still quite inconsistant.  checkstyle should be
used to align the code base to a similar style.

The checkstyle report can be found at:
http://jakarta.apache.org/commons/httpclient/checkstyle-report.html

And can be generated with:
maven checkstyle:generate-report"
0,"Move repository home directory into target directoryCurrently the spi test client uses the repository home directory from the jackrabbit-core module. Instead it should use its own repository home, preferably in the target directory which can be reset using the maven clean goal."
1,"Some equals methods do not check for null argumentThe equals methods in the following classes do not check for a null argument and thus would incorrectly fail with a null pointer exception if passed null:

- org.apache.lucene.index.SegmentInfo
- org.apache.lucene.search.function.CustomScoreQuery
- org.apache.lucene.search.function.OrdFieldSource
- org.apache.lucene.search.function.ReverseOrdFieldSource
- org.apache.lucene.search.function.ValueSourceQuery

If a null parameter is passed to equals() then false should be returned."
1,"In DatabasePersistenceManager.store(), if the exception is null or its cause is not an SQLException, then the PM keeps looping foreverIn the line
if (ise != null && ise.getCause() instanceof SQLException && --trials > 0) {
if one of the first two checks fails, the shortcircuit doesn't decrement trials."
1,"Memory leak in MultiThreadedHttpClient caused by bad .equals()Note: I have '2.0 release candidate 1'; I'm not sure which version this
translates into.  The bug is definitely present in the current source.

MultiThreadedHttpClient uses the following code:

// Look for a list of connections for the given config
HostConnectionPool listConnections = (HostConnectionPool) 
    mapHosts.get(hostConfiguration);
if (listConnections == null) { 
    // First time for this config
    listConnections = new HostConnectionPool();
    listConnections.hostConfiguration = hostConfiguration;
    mapHosts.put(hostConfiguration, listConnections);
}


The hash map relys on HostConfiguration's .equals() to resolve equality &
determine if there is a mapping for the configuration.

HostConfiguration has the following in it's .equals() method:

if (!protocol.equals(config.getProtocol())) {
    return false;
}

. . . and Protocol has:

if (obj instanceof Protocol) {
            
    Protocol p = (Protocol) obj;
            
    return (
        defaultPort == p.getDefaultPort()
        && scheme.equalsIgnoreCase(p.getScheme())
        && secure == p.isSecure()
        && socketFactory.equals(p.getSocketFactory()));

}

However, there is no .equals() method in any of the ProtocolSocketFactory
objects, and there isn't any note in the interface about the necessity of the
.equals() method."
1,"IndexWriter retains references to Readers used in Fields (memory leak)As described in [1] IndexWriter retains references to Reader used in Fields and that can lead to big memory leaks when using tika's ParsingReaders (as those can take 1MB per ParsingReader). 

[2] shows a screenshot of the reference chain to the Reader from the IndexWriter taken with Eclipse MAT (Memory Analysis Tool) . The chain is the following:

IndexWriter -> DocumentsWriter -> DocumentsWriterThreadState -> DocFieldProcessorPerThread  -> DocFieldProcessorPerField -> Fieldable -> Field (fieldsData) 


-------------
[1] http://markmail.org/thread/ndmcgffg2mnwjo47
[2] http://skitch.com/ecerulm/n7643/eclipse-memory-analyzer

"
0,"Validate the SearchIndex configurationThe validation of the configuration (repository.xml / workspace.xml) was enabled in JCR-1462, unfortunately a problem with the LoginModule and SearchIndex was found (see JCR-1920), so it was later disabled for those two modules.

Validation for SearchIndex is possible but needs some more changes. To avoid problems, those changes should be done in a major version and not in a minor version."
0,"Make TopDocs constructor publicTopDocs constructor is package visible. This prevents instantiating it from outside this package. For example, I wrote a HitColletor that couldn't extend directly from TopDocCollector. I need to create a new TopDocs instance, however since the c'tor is package visible, I can't do that.
For now, I completely duplicated the code, but I hope you'll fix it soon."
0,"Update 3.x legacy homepage with end of life notification and pointer to HTTPComponentsIf you google for ""http client"" or ""httpclient"" or ""apache http client"" the first result is always legacy Commons-HttpClient page.  If you follow the link, there's no indication whatsoever on the legacy page that its not the latest version, or that it's end-of-lifed.  Unless you're already aware of HttpComponents, there's no obvious way to find it from the legacy page.  The only reference to HttpComponents is actually in the 'History' section.  

The source for the legacy page should be updated to indicate Commons-HttpClient is an end-of-lifed project and should provide a pointer to the new HttpComponents page in a more prominent location."
0,"Introduce QValueConstraint and change return type of QPropertyDefinition.getValueConstraints()public interface QValueConstraint {
+
+    /**
+     * Empty array of <code>QValueConstraint</code>.
+     */
+    public static final QValueConstraint[] EMPTY_ARRAY = new QValueConstraint[0];
+
+    /**
+     * Check if the specified value matches this constraint.
+     *
+     * @param value The value to be tested.
+     * @throws ConstraintViolationException If the specified value is
+     * <code>null</code> or does not matches the constraint.
+     * @throws RepositoryException If another error occurs.
+     */
+    void check(QValue value) throws ConstraintViolationException, RepositoryException;
+
+    /**
+     * For constraints that are not namespace prefix mapping sensitive this
+     * method returns the same defined in
+     * <code>{@link PropertyDefinition#getValueConstraints()}</code>.
+     * <p/>
+     * Those that are namespace prefix mapping sensitive (e.g.
+     * <code>NameConstraint</code>, <code>PathConstraint</code> and
+     * <code>ReferenceConstraint</code>) return an expanded string.
+     *
+     * @return the expanded definition String
+     */
+    String getExpandedDefinition();
+
+}


+++ jackrabbit-spi/src/main/java/org/apache/jackrabbit/spi/QPropertyDefinition.java	(working copy)
@@ -45,7 +45,7 @@
      *
      * @return the array of value constraints.
      */
-    public String[] getValueConstraints();
+    public QValueConstraint[] getValueConstraints();
"
1,"cache module should populate Via header to capture upstream and downstream protocolsBecause the cache module is currently implemented as a decorator that behaves like a transparent caching proxy, we need it to correctly populate the Via header so that we can preserve the record of which protocol versions were used upstream and downstream from the caching module.

This is a MUST per the RFC:
http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.45"
1,"Unreleased 2.3 version of IndexWriter.optimize()  consistly throws java.lang.IllegalArgumentException out-of-the-boxSince the upcoming 2.3 version of Lucene has support for the setRAMBufferSizeMB() method in Index Writer,  I thought I would test its performance.   So, using my application that was built upon (and worked with) Lucene 2.2,  I downloaded the nightly build 2007-10-26_03-16-46 and rebuilt my application with new code setting setRAMBufferSizeMB() from a properties file.   My test data resides in a database table of 30 columns holding 1.25 million records.   The good news is that performance is superior to Lucene 2.2.  The indexing completes in roughly 1/3 the time.   The bad news is the Index Writer.optimize() step now throws an java.lang.IllegalArgumentException.
I also run tests against various other tables.  Indexing smaller amounts of data did not throw the exception.  Indexing largers amounts of data did throw the exception.  Note, I also tested nightly builds dating back to 2007-10-05.

...
INFO:  SEIndexThread.commitCheck...
INFO:    ----Commit point reached:  1200000
INFO:  SEIndexThread.commitCheck...
INFO:    ----Commit point reached:  1225000
INFO:  SEIndexThread.commitCheck...
INFO:    ----Commit point reached:  1250000
INFO:  SEIndexThread.closeIndex()...
INFO:    ----commit point reached:  1250659
INFO:    ----optimize index
INFO: SEIndexThread():  java.lang.IllegalArgumentException

java.lang.IllegalArgumentException
        at java.lang.Thread.setPriority(Thread.java(Compiled Code))
        at org.apache.lucene.index.ConcurrentMergeScheduler.merge(ConcurrentMerg
eScheduler.java(Compiled Code))
        at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1750)
        at org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:1686)
        at org.apache.lucene.index.IndexWriter.optimize(IndexWriter.java:1652)
        at LuceneSearchEngine.optimizeIndex(LuceneSearchEngine.java:643)
        at LuceneSearchEngine.optimizeIndex(LuceneSearchEngine.java:636)
        at SEIndexThread.closeIndex(SEIndexThread.java:674)
        at SEIndexThread.processSearchObject(SEIndexThread.java:487)
        at SEIndexThread.prepareIndex(SEIndexThread.java:391)
        at SEIndexThread.run(SEIndexThread.java:41)

"
0,"Add ConstantScore highlighting support to SpanScorerIts actually easy enough to support the family of constantscore queries with the new SpanScorer. This will also remove the requirement that you rewrite queries against the main index before highlighting (in fact, if you do, the constantscore queries will not highlight)."
0,"Add ShingleFilter option to output unigrams if no shingles can be generatedCurrently if ShingleFilter.outputUnigrams==false and the underlying token stream is only one token long, then ShingleFilter.next() won't return any tokens. This patch provides a new option, outputUnigramIfNoNgrams; if this option is set and the underlying stream is only one token long, then ShingleFilter will return that token, regardless of the setting of outputUnigrams.

My use case here is speeding up phrase queries. The technique is as follows:

First, doing index-time analysis using ShingleFilter (using outputUnigrams==true), thereby expanding things as follows:

""please divide this sentence into shingles"" ->
 ""please"", ""please divide""
 ""divide"", ""divide this""
 ""this"", ""this sentence""
 ""sentence"", ""sentence into""
 ""into"", ""into shingles""
 ""shingles""

Second, do query-time analysis using ShingleFilter (using outputUnigrams==false and outputUnigramIfNoNgrams==true). If the user enters a phrase query, it will get tokenized in the following manner:

""please divide this sentence into shingles"" ->
 ""please divide""
 ""divide this""
 ""this sentence""
 ""sentence into""
 ""into shingles""

By doing phrase queries with bigrams like this, I can gain a very considerable speedup. Without the outputUnigramIfNoNgrams option, then a single word query would tokenize like this:

""please"" ->
   [no tokens]

But thanks to outputUnigramIfNoNgrams, single words will now tokenize like this:

""please"" ->
  ""please""

****

The patch also adds a little to the pre-outputUnigramIfNoNgrams option tests.

****

I'm not sure if the patch in this state is useful to anyone else, but I thought I should throw it up here and try to find out.
"
0,"[PATCH] Binary stored fieldsProvides a binary Field type that can be used to store byte arrays in the Lucene
index. Can be used for a variety of applications from compressed text storage,
image storage or as a basis for implementing typed storage (e.g: Integers,
Floats, etc.)

Based on discussion from lucene-dev list started here:
http://marc.theaimsgroup.com/?l=lucene-dev&m=108455161204687&w=2

Directly based on design fleshed out here:
http://marc.theaimsgroup.com/?l=lucene-dev&m=108456898230542&w=2

Patch includes updated code and unit tests not included in the patch sent do the
lucene-dev list."
0,"refactoring of DavSession acquisition in jcr-serveri'm subclassing WebdavServer, and i want to use my own logic for finding credentials in the request, logging into the repository and instantiating a DavSession.

unfortunately, WebdavServlet.getSession and its friend the inner class DavSessionImpl are declared private. i changed WebdavServlet.getSession to be protected so that i could override it, but even so, i have no access to DavSessionImpl, so for now, i've copied and pasted it as an inner class in my subclass. yuck.

here's a proposal for making this more extensible:

1) create the interface DavSessionProvider in org.apache.jackrabbit.server with these methods:

  public void acquireSession(WebdavRequest request) throws DavException;
  public void releaseSession(WebdavRequest request);

2) make JCRWebdavServer implement DavSessionProvider (it already includes the above methods)

3) move WebdavServlet$DavSessionImpl to DavSessionImpl in org.apache.jackrabbit.server.simple

4) create a DavSessionProviderImpl in org.apache.jackrabbit.server.simple implementing DavSessionProvider which returns instances of DavSessionImpl

5) change WebdavServlet to use a DavSessionProvider rather than its own getSession method, and use a DavSessionProviderImpl by default. subclasses can override with setDavSessionProvider().
"
0,"GData html render previewGData output is usually ATOM / RSS e.g plain xml. This feature enables users or admins to preview the server output as html transformed by user defined xsl stylesheet. Stylesheet is configurable per service.

That's just a cool feature for developing and for users wanna use the server for simple blog or feed server.

regards simon"
1,"Move operation may turn AC caches stalethe EntryCollector instance associated with a given workspace is listening to any modifications made to the access control content
(add, modification and removal of access control lists). however, due to the structure of the ac content (the ac node being attached to the affected nodes) 
caches may become stale if a node is moved that contains ac information somewhere in the subtree.

in order to circumvent that problem the EntryCollector should in addition collect any kind of move operations
and make sure that the caches are updated accordingly."
1,"NPE when versioning operations are concurrentInternalVersionManagerBase.getParentNode occasionally throws an NPE:

    protected static NodeStateEx getParentNode(NodeStateEx parent, String uuid, Name interNT)
            throws RepositoryException {
        NodeStateEx n = parent;
        for (int i = 0; i < 3; i++) {
            Name name = getName(uuid.substring(i * 2, i * 2 + 2));
            if (n.hasNode(name)) {
                n = n.getNode(name, 1);
                assert n != null;
            } else if (interNT != null) {
                n.addNode(name, interNT, null, false);
                n.store();
                n = n.getNode(name, 1);
                assert n != null;
            } else {
                return null;
            }
        }
        return n;
    }

Apparently getNode occasionally returns null due to race conditions.

Changing the code to what's below appears to fix it:



    protected static NodeStateEx getParentNode(NodeStateEx parent, String uuid, Name interNT)
            throws RepositoryException {
        NodeStateEx n = parent;
        for (int i = 0; i < 3; i++) {
            Name name = getName(uuid.substring(i * 2, i * 2 + 2));
            NodeStateEx n2 = n.getNode(name, 1);
            if (n2 != null) {
                n = n2;
            } else if (interNT != null) {
                n2 = n.addNode(name, interNT, null, false);
                n.store();
                n = n2;
            } else {
                return null;
            }
        }
        return n;
    }

(but likely moves the race condition somewhere else)"
1,"Protocol interceptors not called when executing CONNECT methodsWhen the DefaultRequestDirector tries to establish a route via a proxy to a https target, registered protocol interceptors aren't being called in the createTunnelToTarget method. "
0,"Deprecate org.apache.jackrabbit.api.JackrabbitNodeTypeManager in 2.0 and 1.6The JackrabbitNodeTypeManager defines 3 methods that can be removed for version 2.0 since they are no longer needed:

    NodeType[] registerNodeTypes(InputSource in)  throws SAXException, RepositoryException;
    NodeType[] registerNodeTypes(InputStream in, String contentType)  throws IOException, RepositoryException;

those deal with directly register nodetypes from a XML or CND source. since we don't want to support XML serialization any longer, and the CND import can be easily done using the spi-commons CompactNodeTypeDefReader with the new JCR2.0 node type registration. if the XML is to be supported, i suggest to detach the reader similar to the CND one.

    boolean hasNodeType(String name) throws RepositoryException;

this is now in the JCR2.0 api
"
1,"The PostMethod did not bring back response headers from proxy serversDescription:

When doing tunnelling through proxy servers, in case of 407 response, the 
wrapper class ConnectMethod failed to pass the response header back to the 
wrapped method (PostMethod in our case).  As result, the response headers are 
not passed back to the application.

Proposed Fix:
Change the ConnectMethod to use the wrapped method instance to get response 
headers.  It will be reinitialized again if ""CONNECT"" is successful.

Also have to modify the addProxyAuthorizationRequestHeader code to use the 
wrapped method for the authenticator to work."
0,"Position Checking Span QueriesI've created a bunch of new SpanQuery classes that allow one to do things like check to see if a SpanQuery falls between two positions (which is a more general form of SpanFirstQuery) and I've also added one that only includes a match if the payload located at the span match also matches a given payload.  With the latter, one can do queries for items w/ specific payloads."
0,"Scan method signatures and add varargs where possibleI changed a lot of signatures, but there may be more. The important ones like MultiReader and MultiSearcher are already done. This applies also to contrib. Varargs are no backwards break, they stay arrays as before."
0,Remaining reallocation should use ArrayUtil.getNextSize()See recent discussion on ArrayUtils.getNextSize().
0,"IndexableBinaryStringTools: convert arbitrary byte sequences into Strings that can be used as index terms, and vice versaProvides support for converting byte sequences to Strings that can be used as index terms, and back again. The resulting Strings preserve the original byte sequences' sort order (assuming the bytes are interpreted as unsigned).

The Strings are constructed using a Base 8000h encoding of the original binary data - each char of an encoded String represents a 15-bit chunk from the byte sequence.  Base 8000h was chosen because it allows for all lower 15 bits of char to be used without restriction; the surrogate range [U+D800-U+DFFF] does not represent valid chars, and would require complicated handling to avoid them and allow use of char's high bit.

This class is intended to serve as a mechanism to allow CollationKeys to serve as index terms."
1,"URI Absolutization does not follow browser behaviorThis was encountered using Heritrix to crawl a prominent website.

The URI resulting from the HttpClient URI constructor (base, relative) does not follow browser behavior:
URI newUrl = new URI(new URI(""http://www.theirwebsite.com/browse/results?type=browse&att=1""), ""?sort=0&offset=11&pageSize=10"")

Results in newUrl:
http://www.theirwebsite.com/browse/?sort=0&offset=11&pageSize=10

The desired behavior based on Firefox and IE should be:
http://www.theirwebsite.com/browse/results?sort=0&offset=11&pageSize=10

These browsers treat the question mark similar to a directory separator and do not require a file to be specified before the query.

HttpClient's current behavior does not correspond to current browser behavior and leads to an inability to crawl certain websites if HttpClient's URI class is used.

"
1,Auth state is not correctly maintained if a successful NTLM authentication results in a redirectHttpClient fails to update the auth state correctly if a successful NTLM authentication results in a redirect response. Reported by Valentin Popov <valentin.po at gmail.com>
1,"Version.merge() corrupts repositoryVersion.merge() corrupts repository. somehow the 'protected' flags of the nt:version nodetype is not properly checked.
this happens due to a wrong test case that calls merge on a Version instead of a normal Node.
"
0,"replace the PostMethod parameters HashMap with a ListPropose to change the parameters datastructure in PostMethod from its
current HashMap to an ArrayList (or Vector) of NameValuePair objects. 
This change would lead to simpler and more robust code in the PostMethod
with more deterministic behaviour for the following reasons:

1) HashMap looses any insertion order.  If the client wants to have the
encoded parameters to show up in a particular order, they are unable.

2) Hash map requres unique keys where there is no reason that multiple
parameters with the same name cannot be POSTed.  The current
implementation replaces the string value with a List if there is a
addParameter with a repeated name key.  Every get from the HashMap then
has to do an instanceOf to see if the value is a String or a List.

3) Hash maps are no faster than a Vector for typical operations.  They
both have O(1) insertions and both do O(n) removal operations to
genPropose to change the parameters datastructure in PostMethod from its
current HashMap to an ArrayList (or Vector) of NameValuePair objects. 
This change would lead to simpler and more robust code in the PostMethod
with more deterministic behaviour for the following reasons:

1) HashMap looses any insertion order.  If the client wants to have the
encoded parameters to show up in a particular order, they are unable.

2) Hash map requres unique keys where there is no reason that multiple
parameters with the same name cannot be POSTed.  The current
implementation replaces the string value with a List if there is a
addParameter with a repeated name key.  Every get from the HashMap then
has to do an instanceOf to see if the value is a String or a List.

3) Hash maps are no faster than a Vector for typical operations.  They
both have O(1) insertions and both do O(n) removal operations to
generate the body.  HashMap is only faster when a search is required,
such as for getParameter(String), setParameter(String, String) and
removeParameter(String) which should rarely be called.

I would also move to depricate setParameter(String, String) as it is
confusing in the API.  (setParameter overwrites any existing parameter
of the same name where addParameter accumulates to the list of
parameters).  The setParameter functionality can also be effected by
calling removeParameter then addParameter already.

erate the body.  HashMap is only faster when a search is required,
such as for getParameter(String), setParameter(String, String) and
removeParameter(String) which should rarely be called.

I would also move to depricate setParameter(String, String) as it is
confusing in the API.  (setParameter overwrites any existing parameter
of the same name where addParameter accumulates to the list of
parameters).  The setParameter functionality can also be effected by
calling removeParameter then addParameter already."
0,"IndexWriter has hard limit on max concurrencyDocumentsWriter has this nasty hardwired constant:

{code}
private final static int MAX_THREAD_STATE = 5;
{code}

which probably I should have attached a //nocommit to the moment I
wrote it ;)

That constant sets the max number of thread states to 5.  This means,
if more than 5 threads enter IndexWriter at once, they will ""share""
only 5 thread states, meaning we gate CPU concurrency to 5 running
threads inside IW (each thread must first wait for the last thread to
finish using the thread state before grabbing it).

This is bad because modern hardware can make use of more than 5
threads.  So I think an immediate fix is to make this settable
(expert), and increase the default (8?).

It's tricky, though, because the more thread states, the less RAM
efficiency you have, meaning the worse indexing throughput.  So you
shouldn't up and set this to 50: you'll be flushing too often.

But... I think a better fix is to re-think how threads write state
into DocumentsWriter.  Today, a single docID stream is assigned across
threads (eg one thread gets docID=0, next one docID=1, etc.), and each
thread writes to a private RAM buffer (living in the thread state),
and then on flush we do a merge sort.  The merge sort is inefficient
(does not currently use a PQ)... and, wasteful because we must
re-decode every posting byte.

I think we could change this, so that threads write to private RAM
buffers, with a private docID stream, but then instead of merging on
flush, we directly flush each thread as its own segment (and, allocate
private docIDs to each thread).  We can then leave merging to CMS
which can already run merges in the BG without blocking ongoing
indexing (unlike the merge we do in flush, today).

This would also allow us to separately flush thread states.  Ie, we
need not flush all thread states at once -- we can flush one when it
gets too big, and then let the others keep running.  This should be a
good concurrency gain since is uses IO & CPU resources ""throughout""
indexing instead of ""big burst of CPU only"" then ""big burst of IO
only"" that we have today (flush today ""stops the world"").

One downside I can think of is... docIDs would now be ""less
monotonic"", meaning if N threads are indexing, you'll roughly get
in-time-order assignment of docIDs.  But with this change, all of one
thread state would get 0..N docIDs, the next thread state'd get
N+1...M docIDs, etc.  However, a single thread would still get
monotonic assignment of docIDs.
"
1,"StringRequestEntity.getContentLength wrong for multibyte charsWhen setting up a PostMethod containing a StringRequestEntity with umlauts and
charset UTF-8 the content-length header is wrong. It should be the number
of bytes, but is the number of chars by now.

(e.g.
Content-Type: text/xml; charset=UTF-8
body='')

Bug-location: org.apache.commons.httpclient.methods.StringRequestEntity"
1,"XML serialization in JDK 1.4 broken (mostly for WebDAV)WebDAV uses XmlRequestEntity for serializing XML, which in turn uses org.apache.jackrabbit.commons.xml.SerializingContentHandler to work around the JDK 1.4 problem (serializing in absence of explicit namespace declarations).

The following test fails under JDK 1.4, but passed with newer JDKs:

    public void testXmlSerialization() throws ParserConfigurationException, IOException, SAXException {
        
        DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();
        dbf.setNamespaceAware(true);
        DocumentBuilder db = dbf.newDocumentBuilder();
        
        Document doc = db.newDocument();
        doc.appendChild(doc.createElementNS(""DAV:"", ""propfind""));
        
        XmlRequestEntity xmlent = new XmlRequestEntity(doc);
        ByteArrayOutputStream bos = new ByteArrayOutputStream();
        xmlent.writeRequest(bos);
        
        Document doc2 = db.parse(new ByteArrayInputStream(bos.toByteArray()));
        Element docelem = doc2.getDocumentElement();
        assertEquals(""DAV:"", docelem.getNamespaceURI());
    }"
0,"Wikipedia Document Generation ChangesThe EnwikiDocMaker currently produces a fair number of documents that are in the download, but are of dubious use in terms of both benchmarking and indexing.  

These issues are:

# Redirect (it currently only handles REDIRECT and redirect, but there are documents as Redirect
# Template files appear to be useless.  These are marked by the term Template: at the beginning of the body.  See for example: http://en.wikipedia.org/wiki/Template:=)
# Image only pages, as in http://en.wikipedia.org/wiki/Image:Sciencefieldnewark.jpg.jpg  These are about as useful as the Redirects and Templates
# Files pending deletion:  This one is a bit trickier to handle, but they are generally marked by ""Wikipedia:Votes for deletion"" or some variation of that depending where along it is in being deleted

I think I can implement this such that it is backward compatible, if there is such a need when it comes to the contrib/benchmark suite.



"
0,"Support http URL inline authenticationIf you try to execute a method with the httpClient using a valid url, conform to the schema http://username:password@host:port/ the authentication will fail, and you will get a 401 error."
0,move nrtcachingdir to core in 4.0in 4.0 with the IOContext changes this implementation is clean and I think we should move it to core and use it in our tests etc.
0,"Allow reuse of Q*DefinitionBuilder in QItemDefinitionsBuilderIt would be nice to reuse the builder implementations in QItemDefinitionsBuilder elsewhere when Q*Definitions need to be built.

I will extract the relevant classes so they can be used independently and make QItemDefinitionsBuilder use them."
1,"TransientRepository does not shutdown if first login failsThe TransientRepository.login() method initializes the underlying repository when it is first called (initially or after the repository has previously been shut down) bug doesn't shut down the initialized repository if the login fails. If the application then decides to exit or otherwise not start another session, then the repository remains in an initialized state with no active sessions.

This issue should be fixed by properly handling login failures in the TransientRepository.login() method."
0,Create enwiki indexable data as line-per-article rather than file-per-articleCreate a line per article rather than a file. Consume with indexLineFile task.
0,"Incorrect license headers in multiple componentsAs noticed by Thomas, we have a number of files with missing or incorrect license headers both in trunk and in the 1.5 branch.

The following lists all troublesome files in the 1.5.1 release candidate:

 !????? ./jackrabbit-api/src/main/java/org/apache/jackrabbit/api/security/user/User.java
 !????? ./jackrabbit-core/src/main/java/org/apache/jackrabbit/core/security/authorization/principalbased/ACLEditor.java
 !????? ./jackrabbit-core/src/main/java/org/apache/jackrabbit/core/security/authorization/principalbased/ACLProvider.java
 !????? ./jackrabbit-core/src/test/java/org/apache/jackrabbit/api/jsr283/retention/AbstractRetentionTest.java
 !????? ./jackrabbit-core/src/test/java/org/apache/jackrabbit/core/security/authorization/combined/TestAll.java
 !????? ./jackrabbit-core/src/test/java/org/apache/jackrabbit/core/security/authorization/principalbased/EvaluationTest.java
 !????? ./jackrabbit-core/src/test/java/org/apache/jackrabbit/core/security/authorization/principalbased/TestAll.java
 !????? ./jackrabbit-core/src/test/java/org/apache/jackrabbit/core/security/principal/TestAll.java
 !????? ./jackrabbit-core/src/test/java/org/apache/jackrabbit/core/security/user/AdministratorTest.java
 !????? ./jackrabbit-ocm/src/main/java/org/apache/jackrabbit/ocm/manager/objectconverter/impl/AbstractLazyLoader.java
 !????? ./jackrabbit-ocm/src/main/java/org/apache/jackrabbit/ocm/manager/objectconverter/impl/OcmProxy.java
 !????? ./jackrabbit-ocm/src/main/java/org/apache/jackrabbit/ocm/manager/objectconverter/impl/OcmProxyUtils.java
 !????? ./jackrabbit-ocm/src/test/java/org/apache/jackrabbit/ocm/testmodel/MultiValueWithObjectCollection.java
 !????? ./jackrabbit-ocm/src/test/java/org/apache/jackrabbit/ocm/testmodel/SimpleAnnotedAbstractClass.java
 !????? ./jackrabbit-ocm/src/test/java/org/apache/jackrabbit/ocm/testmodel/SimpleAnnotedClass.java
 !????? ./jackrabbit-ocm/src/test/java/org/apache/jackrabbit/ocm/testmodel/SimpleInterface.java
 !????? ./jackrabbit-ocm/src/test/java/org/apache/jackrabbit/ocm/testmodel/UnmappedInterface.java
 !????? ./jackrabbit-ocm/src/test/java/org/apache/jackrabbit/ocm/testmodel/version/Author.java
 !????? ./jackrabbit-ocm/src/test/java/org/apache/jackrabbit/ocm/testmodel/version/PressRelease.java
"
0,"masking field of span for cross searching across multiple fields (many-to-one style)This issue is to cover the changes required to do a search across multiple fields with the same name in a fashion similar to a many-to-one database. Below is my post on java-dev on the topic, which details the changes we need:

---

We have an interesting situation where we are effectively indexing two 'entities' in our system, which share a one-to-many relationship (imagine 'User' and 'Delivery Address' for demonstration purposes). At the moment, we index one Lucene Document per 'many' end, duplicating the 'one' end data, like so:

    userid: 1
    userfirstname: fred
    addresscountry: au
    addressphone: 1234

    userid: 1
    userfirstname: fred
    addresscountry: nz
    addressphone: 5678

    userid: 2
    userfirstname: mary
    addresscountry: au
    addressphone: 5678

(note: 2 Documents indexed for user 1). This is somewhat annoying for us, because when we search in Lucene the results we want back (conceptually) are at the 'user' level, so we have to collapse the results by distinct user id, etc. etc (let alone that it blows out the size of our index enormously). So why do we do it? It would make more sense to use multiple fields:
    userid: 1
    userfirstname: fred
    addresscountry: au
    addressphone: 1234
    addresscountry: nz
    addressphone: 5678

    userid: 2
    userfirstname: mary
    addresscountry: au
    addressphone: 5678

But imagine the search ""+addresscountry:au +addressphone:5678"". We'd like this to match ONLY Mary, but of course it matches Fred also because he matches both those terms (just for different addresses).

There are two aspects to the approach we've (more or less) got working but I'd like to run them past the group and see if they're worth trying to get them into Lucene proper (if so, I'll create a JIRA issue for them)

1) Use a modified SpanNearQuery. If we assume that country + phone will always be one token, we can rely on the fact that the positions of 'au' and '5678' in Fred's document will be different.

   SpanQuery q1 = new SpanTermQuery(new Term(""addresscountry"", ""au""));
   SpanQuery q2 = new SpanTermQuery(new Term(""addressphone"", ""5678""));
   SpanQuery snq = new SpanNearQuery(new SpanQuery[]{q1, q2}, 0, false);

the slop of 0 means that we'll only return those where the two terms are in the same position in their respective fields. This works brilliantly, BUT requires a change to SpanNearQuery's constructor (which checks that all the clauses are against the same field). Are people amenable to perhaps adding another constructor to SNQ which doesn't do the check, or subclassing it to do the same (give it a protected non-checking constructor for the subclass to call)?

2) (snipped ... see LUCENE-1626 for second idea)"
1,bad route computed for redirected requestsBasicRouteDirector appears to miscalculate complex routes. Example to follow. 
0,"Switch from log4j to LogbackLogback (http://logback.qos.ch/) is a native SLF4J implementation and is in many ways superior to log4j (see http://logback.qos.ch/reasonsToSwitch.html). Most notably it includes package version information in logged stack traces (http://logback.qos.ch/reasonsToSwitch.html#packagingData), which can be really useful in many cases."
0,"Allow MergePolicy to select non-contiguous mergesI started work on this but with LUCENE-1044 I won't make much progress
on it for a while, so I want to checkpoint my current state/patch.

For backwards compatibility we must leave the default MergePolicy as
selecting contiguous merges.  This is necessary because some
applications rely on ""temporal monotonicity"" of doc IDs, which means
even though merges can re-number documents, the renumbering will
always reflect the order in which the documents were added to the
index.

Still, for those apps that do not rely on this, we should offer a
MergePolicy that is free to select the best merges regardless of
whether they are continuguous.  This requires fixing IndexWriter to
accept such a merge, and, fixing LogMergePolicy to optionally allow
it the freedom to do so.
"
1,"IndexCommit.getFileNames() should not return dupsIf the index was created with autoCommit false, and more than 1
segment was flushed during the IndexWriter session, then the shared
doc-store files are incorrectly duplicated in
IndexCommit.getFileNames().  This is because that method is walking
through each SegmentInfo, appending its files to a list.  Since
multiple SegmentInfo's may share the doc store files, this causes dups.

To fix this, I've added a SegmentInfos.files(...) method, and
refactored all places that were computing their files one SegmentInfo
at a time to use this new method instead.
"
1,"Cluster revision file not closed on repository shutdown.After having shut down a repository that has configured clustering, the cluster revision file is still open."
0,"Simple toString() for BooleanFilterWhile working with BooleanFilter I wanted a basic toString() for debugging.

This is what I came up.  It works ok for me."
0,"Add @Override annotationsDuring removal of deprecated APIs, mostly the problem was, to not only remove the method in the (abstract) base class (e.g. Scorer.explain()), but also remove it in sub classes that override it. You can easily forget that (especially, if the method was not marked deprecated in the subclass). By adding @Override annotations everywhere in Lucene, such removals are simple, because the compiler throws out an error message in all subclasses which then no longer override the method.

Also it helps preventing the well-known traps like overriding hashcode() instead of hashCode().

The patch was generated automatically, and is rather large. Should I apply it, or would it break too many patches (but I think, trunk has changed so much, that this is only a minimum of additional work to merge)?"
1,"TestIndexWriterDelete fails randomly10 out of 9 runs with that see fail on my trunk:

ant test-core -Dtestcase=TestIndexWriterDelete -Dtestmethod=testErrorAfterApplyDeletes -Dtests.seed=4269712067829708991:1888184886355172227 -Dtests.codec=randomPerField


with this result:

{code}

junit-sequential:
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriterDelete
    [junit] Tests run: 1, Failures: 2, Errors: 0, Time elapsed: 1.725 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterDelete -Dtestmethod=testErrorAfterApplyDeletes -Dtests.seed=4269712067829708991:1888184886355172227 -Dtests.codec=randomPerField
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=Standard, contents=SimpleText, city=MockSep}, locale=ar_QA, timezone=VST
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestIndexWriterDelete]
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testErrorAfterApplyDeletes(org.apache.lucene.index.TestIndexWriterDelete):	FAILED
    [junit] 
    [junit] junit.framework.AssertionFailedError: 
    [junit] 	at org.apache.lucene.index.TestIndexWriterDelete.testErrorAfterApplyDeletes(TestIndexWriterDelete.java:736)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1043)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:981)
    [junit] 
    [junit] 
    [junit] Testcase: testErrorAfterApplyDeletes(org.apache.lucene.index.TestIndexWriterDelete):	FAILED
    [junit] ConcurrentMergeScheduler hit unhandled exceptions
    [junit] junit.framework.AssertionFailedError: ConcurrentMergeScheduler hit unhandled exceptions
    [junit] 	at org.apache.lucene.util.LuceneTestCase.tearDown(LuceneTestCase.java:503)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1043)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:981)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.TestIndexWriterDelete FAILED
{code}"
1,"Inconsistent behaviour sorting against field with no related documentsIn StringSortedHitQueue - generateSortIndex seems to mistake 
the TermEnum having values as indicating that the sort field 
has entries in the index.

In the case where the search has matching results an ArrayIndexOutOfBounds
exception is thrown in sortValue (line 177 StringSortedHitQueue)
as generateSortIndex creates a terms array of zero length and fieldOrder
contains 0 for all documents.

It would seem more helpful if:
a) generateSortIndex catches the lack of any documents with the sort field.

or

b) reserve terms[0] as a special value for documents that do not have
matching sort field values. ie Change the current implementation to add 1
to the index and change terms[0] to ensure it sorts ""untagged"" documents to
first or last.

For my application Id much prefer solution (b) as it allows much smaller 
indexes and make searching using sort values less brittle.

Thats the best my communication skills can muster just now. Could change
current code to something like:

private final int[] generateSortIndex()
throws IOException {

	final int[] retArray = new int[reader.maxDoc()];
	final String[] mterms = new String[reader.maxDoc() + 1];  // guess length
	if (retArray.length > 0) {
		TermDocs termDocs = reader.termDocs();
		// change this value to control if documents without sort field come first or last
		mterms[0] = """";  // XXXXXXXXX change
		int t = 1;  // current term number  XXXXXXXXXXXXX change
		try {
	

			do {
				Term term = enumerator.term();
				if (term.field() != field) break;

				// store term text
				// we expect that there is at most one term per document
				if (t >= mterms.length) throw new RuntimeException (""there are more terms
than documents in field \""""+field+""\"""");
				mterms[t] = term.text();

				// store which documents use this term
				termDocs.seek (enumerator);
				while (termDocs.next()) {
					retArray[termDocs.doc()] = t;
				}

				t++;
			} while (enumerator.next());
		} finally {
			termDocs.close();
		}

		// if there are less terms than documents,
		// trim off the dead array space
		if (t < mterms.length) {
			terms = new String[t];
			System.arraycopy (mterms, 0, terms, 0, t);
		} else {
			terms = mterms;
		}
	}
	return retArray;
}

Having very quick look at IntegerSortedHitQueue would seem possible
to do same thing. Maybe creating Integer wrapper objects once.

Hope that made some sort of sense. Im not very familiar with the code
or Lucene terminology.
If the above seems like a useful approach Id be glad to generate patches
for a cleaned up version.

Thanks

Sam"
0,"add icu-based tokenizer for unicode text segmentationI pulled out the last part of LUCENE-1488, the tokenizer itself and cleaned it up some.

The idea is simple:
* First step is to divide text into writing system boundaries (scripts)
* You supply an ICUTokenizerConfig (or just use the default) which lets you tailor segmentation on a per-writing system basis.
* This tailoring can be any BreakIterator, so rule-based or dictionary-based or your own.

The default implementation (if you do not customize) is just to do UAX#29, but with tailorings for stuff with no clear word division:
* Thai (uses dictionary-based word breaking)
* Khmer, Myanmar, Lao (uses custom rules for syllabification)

Additionally as more of an example i have a tailoring for hebrew that treats the punctuation special. (People have asked before
for ways to make standardanalyzer treat dashes differently, etc)
"
0,"Jcr-Server: Usage of Cache-Control headerDeltaV-methods (except for Version-Control and Report) require the Cache-Control header to be present in the response.
In turn, RFC 2518 only requires the Cache-Control header to be present in the request  when dealing with the If header.

Currently the Cache-Control header is always present in the response and not specific for DeltaV methods.
Problems may arise with IE  and a couple of mimetypes such as zip-files"
0,"Commit volatile index to disc after some configurable idle timeThe query handler keeps the most recent part of the index in memory (volatile index). That index is committed to disc when a certain amount of nodes (config param: minMergeDocs) has been added. Most of the times the volatile index will contain some nodes, causing a redo.log that needs to be applied when the jackrabbit process is killed.

In addition to the size threshold (minMergeDocs) of the volatile index, an idle time limit should force a commit to disc. The volatile index would be written to disc after some configurable idle time. This will result in an persistent index (on dic) which is most of the time in sync with the data, but still minimizes the disc IO during heavy modification activity on the workspace."
1,"MultiReader should make a private copy of the subReaders arraySpinoff from here:

    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200806.mbox/%3C88F3F6A4-FBFB-43DF-890D-DB5F0D9A2461@gmail.com%3E

Because MultiReader just holds a reference to the array that was passed in, it's possible to hit scary exceptions (that look like index corruption) if that array is later altered eg by reopening some of the readers.

The fix is trivial: just make a private copy."
0,"caching module should use HttpParams-style configurationThe constructor for CachingHttpClient currently accepts combinations of:
* HttpCache
* HttpClient
* integer for max object size in bytes

As I started looking at being able to configure this for behaving as a non-shared cache, I realized that we actually want to be replacing that last int with an HttpParams argument, and tracking all the various options in that style. I have a patch with this update which I will upload shortly.
"
0,"Add a isDeleted method to IndexCommitI wish to add a IndexCommit.isDeleted() method.

The use-case is that Solr will now support configurable IndexDeletionPolicy (SOLR-617). For the new replication (SOLR-561) to work, we need access to a list of IndexCommit instances which haven't been deleted yet. I can wrap the user specified IndexDeletionPolicy but since the IndexCommit does not have a isDeleted method, I may store a reference to an IndexCommit on which delete() has been called by the deletion policy. I can wrap the IndexCommit objects too just for having a isDeleted() method so a workaround exists. Not a big pain but if it can be managed on the lucene side easily, I'll appreciate it. It would save me from writing some delegate code."
0,"Add static readSnapshotsInfo to PersistentSnapshotDeletionPolicyPSDP persists the snapshots information in a Directory. When you open PSDP, it obtains a write lock on the snapshots dir (by keeping an open IndexWriter), and updates the directory when snapshots are created/released.

This causes problem in the following scenario -- you have two processes, one updates the 'content' index and keeps PSDP open (because it also takes snapshots). Another process wants to read the existing snapshots information and open a read-only IndexReader on the 'content' index. The other process cannot read the existing snapshots information, because p1 keeps a write lock on the snapshots directory.

There are two possible solutions:
# Have PSDP open the IndexWriter over the directory for each snapshot/release. A bit expensive, and unnecessary.
# Introduce a static readSnapshotsInfo on PSDP which accepts a Directory and returns the snapshots information. IMO it's cleaner, and won't have the performance overhead of opening/closing the IW as before.

I'll post a patch (implementing the 2nd approach) shortly. I'd appreciate any comments."
0,Add bundle support for PostgreSQLThe class DbNameIndex does not work with this RDBMS since the RETURN_GENERATED_KEYS JDBC feature is not implemented in current PostgreSQL drivers.
1,"Query does not work after logging into workspace with no indexesWhen I login to workspace that does not have indexes, they are created but my queries do not return results unless I relog. Output from running attached code sample is:

Node [node1240842434531] created in workspace [test1240842434312]
Property [name] set to [someValueOfMyProperty]
Asking query: select * from nt:unstructured where nt:name like 'someValueOfMyProperty'
Found: 1 nodes before deleting index.
Asking query: select * from nt:unstructured where nt:name like 'someValueOfMyProperty'
Found: 0 nodes after deleting index.
done"
1,"Node.restore() throws java.lang.ClassCastExceptionI'm trying to upgrade to 1.5 using existing 1.3.x repository. Restore of versionable node throws ClassCastException.

Caused by: java.lang.ClassCastException: org.apache.jackrabbit.uuid.UUID
	at org.apache.jackrabbit.core.value.InternalValue.getString(InternalValue.java:436)
	at org.apache.jackrabbit.core.version.InternalFrozenNodeImpl.<init>(InternalFrozenNodeImpl.java:113)
	at org.apache.jackrabbit.core.version.AbstractVersionManager.createInternalVersionItem(AbstractVersionManager.java:576)
	at org.apache.jackrabbit.core.version.VersionManagerImpl.getItem(VersionManagerImpl.java:258)
	at org.apache.jackrabbit.core.version.InternalVersionImpl.getFrozenNode(InternalVersionImpl.java:111)
	at org.apache.jackrabbit.core.version.VersionImpl.getFrozenNode(VersionImpl.java:120)
	at org.apache.jackrabbit.core.NodeImpl.internalRestore(NodeImpl.java:4180)
	at org.apache.jackrabbit.core.NodeImpl.internalRestore(NodeImpl.java:4141)
	at org.apache.jackrabbit.core.NodeImpl.restore(NodeImpl.java:3429)

It seems that bug has been introduced already in 1.4 as part of JCR-926 (InternalValue cleanup).

Index: C:/data/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/version/InternalFrozenNodeImpl.java
===================================================================
--- C:/data/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/version/InternalFrozenNodeImpl.java	(revision 549117)
+++ C:/data/jackrabbit/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/version/InternalFrozenNodeImpl.java	(working copy)
@@ -109,10 +109,10 @@
             PropertyState prop = props[i];
             if (prop.getName().equals(QName.JCR_FROZENUUID)) {
                 // special property
-                frozenUUID = UUID.fromString(node.getPropertyValue(QName.JCR_FROZENUUID).internalValue().toString());
+                frozenUUID = UUID.fromString(node.getPropertyValue(QName.JCR_FROZENUUID).getString());

Probably one of the assumptions made was wrong :
- The type of QName.JCR_FROZENUUID is STRING (Object.toString() was used before)."
0,"add FieldInvertState.numUniqueTerms, Terms.sumDocFreqFor scoring systems like lnu.ltc (http://trec.nist.gov/pubs/trec16/papers/ibm-haifa.mq.final.pdf), we need to supply 3 stats:
* average tf within d
* # of unique terms within d
* average number of unique terms across field

If we add FieldInvertState.numUniqueTerms, you can incorporate the first two into your norms/docvalues (once we cut over),
the average tf within d being length / numUniqueTerms.

to compute the average across the field, we can just write the sum of all terms' docfreqs into the terms dictionary header,
and you can then divide this by maxdoc to get the average.
"
0,"IndexSplitter that divides by primary key termIndex splitter that divides by primary key term.  The contrib MultiPassIndexSplitter we have divides by docid, however to guarantee external constraints it's sometimes necessary to split by a primary key term id.  I think this implementation is a fairly trivial change."
1,"Session.checkPermission(""/"", ""add_node"") throws PathNotFoundException instead of AccessControlExceptionWhen invoking Session.checkPermission(""/"", ""add_node""), a PathNotFoundException is thrown:

Exception in thread ""main"" javax.jcr.PathNotFoundException: no such ancestor path of degree 1
	at org.apache.jackrabbit.spi.commons.name.PathFactoryImpl$PathImpl.getAncestor(PathFactoryImpl.java:443)
	at org.apache.jackrabbit.core.SessionImpl.checkPermission(SessionImpl.java:710)
	at Test.main(Test.java:35)

i assume that getAncestor(1) used to return null in an earlier version.



"
0,"MergePolicy.OneMerge.segments should be List<SegmentInfo> not SegmentInfos, Remove Vector<SI> subclassing from SegmentInfos & more refactoringSegmentInfos carries a bunch of fields beyond the list of SI, but for merging purposes these fields are unused.

We should cutover to List<SI> instead.

Also SegmentInfos subclasses Vector<SI>, this should be removed and the collections be hidden inside the class. We can add unmodifiable views on it (asList(), asSet())."
0,"Graduate appendingcodec from contrib/misc* All tests pass with this codec (at least once, maybe we don't test that two-phase commit stuff very well!)
* It doesn't require special client side configuration anymore to work (just set it on indexwriter and go)
* it now works with the compound file format.

I don't think it needs to live in contrib anymore."
0,"Speed up junit testsAs Lucene grows, so does the number of JUnit tests. This is obviously a good thing, but it comes with longer and longer test times. Now that we also run back compat tests in a standard test run, this problem is essentially doubled.

There are some ways this may get better, including running parallel tests. You will need the hardware to fully take advantage, but it should be a nice gain. There is already an issue for this, and Junit 4.6, 4.7 have the beginnings of something we might be able to count on soon. 4.6 was buggy, and 4.7 still doesn't come with nice ant integration. Parallel tests will come though.

Beyond parallel testing, I think we also need to concentrate on keeping our tests lean. We don't want to sacrifice coverage or quality, but I'm sure there is plenty of fat to skim.

I've started making a list of some of the longer tests - I think with some work we can make our tests much faster - and then with parallelization, I think we could see some really great gains."
0,"DatabaseJournal refactoring for subclassing capabilityIn the 1.3 upgrade to JackRabbit, the DatabasePersistenceManager class was refactored to allow easy subclassing.  On my project, the subclassing is required because the DBAs have a specific naming convention for database columns, and the default JackRabbit columns don't fit within the naming convention.

At this point, we're cutting over to a clustered setup in preparation for production.  In my design, I would like to use the database for journaling.  But once again, the DBAs will want to change the column names to their own naming convention.  The existing DatabaseJournal class is not set up for the same type of subclassing that the PersistenceManager (or even the FileSystem) hierarchies.  I'd like the DatabaseJournal class to be updated accordingly.

In specific, here are the changes I'm looking for:

* Extract protected instance variables for selectRevisionsStmtSql, updateGlobalStmtSql, selectGlobalStmtSql, and insertRevisionStmtSql.
* Extract method protected void buildSQLStatements() which sets up the above sqls, and allows subclasses to override.
* Update the existing prepareStatements method to use the above instance variables.
* Update the init method to call the buildSQLStatements method before the call to prepareStatements."
0,"MergeException from CMS threads should record the DirectoryWhen you hit an unhandled exception in ConcurrentMergeScheduler, it
throws a MergePolicy.MergeException, but there's no easy way to figure
out which index caused this (if you have more than one).

I plan to add the Directory to the MergeException.  I also made a few
other small changes to ConcurrentMergeScheduler:

  * Added handleMergeException method, which is called on exception,
    so that you can subclass ConcurrentMergeScheduler to do something
    when an exception occurs.

  * Added getMergeThread() method so you can override how the threads
    are created (eg, if you want to make them in a different thread
    group, use a pool, change priorities, etc.).

  * Added doMerge(...) to actually do this merge, so you can do
    something before starting and after finishing a merge.

  * Changed private -> protected on a few attrs

I plan to commit in a day or two.
"
0,"High memory usage on node with multi-valued string propertiesMulti-valued string properties are tokenized per value, which may consume quite some memory when there are lots of small values in on a property. The memory footprint is 2k per value, because each value is tokenized with a separate tokenizer instance. That tokenizer uses a stream buffer of 2k bytes.

Instead the values should be concatenated (whitespace separated) and then tokenized in one go."
1,"MultipartEntity incorrectly computes unknown lengthIf any Part of a MultipartEntity reports an unknown length (-1), MultipartEntity
reports an erroneous length value. It should report an unknown length (-1) if
any of the parts is of unknown length, that would cause the POST to be chunked.

See
http://mail-archives.apache.org/mod_mbox/jakarta-httpclient-user/200510.mbox/ajax/%3ceb3d689c0510250851t2eb78462tbf701135bbf718c9@mail.gmail.com%3e"
0,"IndexReader.close should forcefully evict entries from FieldCacheSpinoff of java-user thread ""heap memory issues when sorting by a string field"".

We rely on WeakHashMap to hold our FieldCache, keyed by reader.  But this lacks immediacy on releasing the reference, after a reader is closed.

WeakHashMap can't free the key until the reader is no longer referenced by the app. And, apparently, WeakHashMap has a further impl detail that requires invoking one of its methods for it to notice that a key has just become only weakly reachable.

To fix this, I think on IR.close we should evict entries from the FieldCache, as long as the sub-readers are truly closed (refCount dropped to 0)."
0,"TestTermInfosReaderIndex failing (always reproducible)Always fails on branch (use reproduce string below):
git clone --depth 1 -b rr git@github.com:dweiss/lucene_solr.git

{noformat}
[junit4] Running org.apache.lucene.codecs.lucene3x.TestTermInfosReaderIndex
[junit4] FAILURE 0.04s J0 | TestTermInfosReaderIndex.testSeekEnum
[junit4]    > Throwable #1: java.lang.AssertionError: expected:<field9:z91ob3wozm6d> but was:<:>
[junit4]    > 	at __randomizedtesting.SeedInfo.seed([C7597DFBBE0B3D7D:C6D9CEDD0700AAFF]:0)
[junit4]    > 	at org.junit.Assert.fail(Assert.java:93)
[junit4]    > 	at org.junit.Assert.failNotEquals(Assert.java:647)
[junit4]    > 	at org.junit.Assert.assertEquals(Assert.java:128)
[junit4]    > 	at org.junit.Assert.assertEquals(Assert.java:147)
[junit4]    > 	at org.apache.lucene.codecs.lucene3x.TestTermInfosReaderIndex.testSeekEnum(TestTermInfosReaderIndex.java:137)
[junit4]    > 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[junit4]    > 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
[junit4]    > 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
[junit4]    > 	at java.lang.reflect.Method.invoke(Method.java:597)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1766)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner.access$1000(RandomizedRunner.java:141)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:728)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:789)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:803)
[junit4]    > 	at org.apache.lucene.util.LuceneTestCase$SubclassSetupTeardownRule$1.evaluate(LuceneTestCase.java:744)
[junit4]    > 	at org.apache.lucene.util.LuceneTestCase$InternalSetupTeardownRule$1.evaluate(LuceneTestCase.java:636)
[junit4]    > 	at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)
[junit4]    > 	at org.apache.lucene.util.LuceneTestCase$TestResultInterceptorRule$1.evaluate(LuceneTestCase.java:550)
[junit4]    > 	at org.apache.lucene.util.LuceneTestCase$RememberThreadRule$1.evaluate(LuceneTestCase.java:600)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:735)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner.access$600(RandomizedRunner.java:141)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$3$1.run(RandomizedRunner.java:586)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$3.evaluate(RandomizedRunner.java:605)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$4.evaluate(RandomizedRunner.java:641)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:652)
[junit4]    > 	at org.apache.lucene.util.SystemPropertiesInvariantRule$1.evaluate(SystemPropertiesInvariantRule.java:22)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSuite(RandomizedRunner.java:533)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner.access$400(RandomizedRunner.java:141)
[junit4]    > 	at com.carrotsearch.randomizedtesting.RandomizedRunner$2.run(RandomizedRunner.java:479)
[junit4]    > 
[junit4]   2> NOTE: reproduce with: ant test -Dtests.filter=*.TestTermInfosReaderIndex -Dtests.filter.method=testSeekEnum -Drt.seed=C7597DFBBE0B3D7D -Dargs=""-Dfile.encoding=UTF-8""
[junit4]   2>
[junit4]    > (@AfterClass output)
[junit4]   2> NOTE: test params are: codec=Appending, sim=DefaultSimilarity, locale=en, timezone=Atlantic/Stanley
[junit4]   2> NOTE: all tests run in this JVM:
[junit4]   2> [TestLock, TestFileSwitchDirectory, TestWildcardRandom, TestVersionComparator, TestTermdocPerf, TestBitVector, TestParallelTermEnum, TestSimpleSearchEquivalence, TestNumericRangeQuery64, TestSort, TestIsCurrent, TestToken, TestIntBlockCodec, TestDocumentsWriterDeleteQueue, TestPagedBytes, TestThreadedForceMerge, TestOmitTf, TestSegmentTermEnum, TestIndexWriterConfig, TestCheckIndex, TestTermVectorsWriter, TestNumericTokenStream, TestSearchAfter, TestRegexpQuery, InBeforeClass, InAfterClass, InTestMethod, NonStringProperties, TestIndexWriterMergePolicy, TestVirtualMethod, TestFieldCache, TestSurrogates, TestSegmentTermDocs, TestMultiValuedNumericRangeQuery, TestBasicOperations, TestCodecs, TestDateSort, TestPositiveScoresOnlyCollector, TestBooleanQuery, TestIndexInput, TestMinimize, TestNumericRangeQuery32, TestBoolean2, TestSloppyPhraseQuery, TestNoDeletionPolicy, TestFieldCacheTermsFilter, TestRandomStoredFields, TestDocBoost, TestTransactionRollback, TestUnicodeUtil, TestIndexWriterLockRelease, TestUTF32ToUTF8, TestFixedBitSet, TestDoubleBarrelLRUCache, TestTimeLimitingCollector, TestSpanFirstQuery, TestDirectory, TestSpansAdvanced2, TestConcurrentMergeScheduler, TestIndexWriterExceptions, TestDocValues, TestCustomNorms, TestFieldValueFilter, TestTermVectors, TestTermInfosReaderIndex]
[junit4]   2> NOTE: Linux 2.6.32-38-server amd64/Sun Microsystems Inc. 1.6.0_20 (64-bit)/cpus=4,threads=1,free=100102360,total=243859456
[junit4]   2> 
{noformat}"
0,"org.apache.http.impl.client.cache.memcached.MemcachedHttpCacheStorage uses URL as cache key - shouldn't.Spy memcached has 250 defined as max key length:
http://dustin.github.com/java-memcached-client/apidocs/constant-values.html#net.spy.memcached.MemcachedClientIF.MAX_KEY_LENGTH

URLs can be (and often are) much longer than 250 characters.

URLs should be hashed before being used as keys."
1,"BasicClientCookie.toString() contains 'name' instead of 'value' when writing out cookie value{noformat}
buffer.append(""[name: "");
buffer.append(this.value);
{noformat}

should be

{noformat}
buffer.append(""[value: "");
buffer.append(this.value);
{noformat}

Will provide a patch soon."
1,cookies > 20 years invalidated 
0,"HttpParams doesn't document its key valueshttp://hc.apache.org/httpcomponents-core/httpcore/apidocs/org/apache/http/params/HttpParams.html should either list the meaningful parameter names with the meanings of their values, or should link to the other classes like HttpClientParams and AuthParams that make it usable. Probably, each class that uses HttpParams should also describe the meaningful values so that human readers find the descriptions however we look for them."
1,"Lock tokens reains in session after unlockI do the followin steps:

* node.lock()
* session.getLockTokens() -> This show the lock token generated by the
previous lock.
* node.unlock()
* session.getLockTokens() -> Still show me the generated token ??

If I unlock a node, the token should'n be delete from the session?"
1,"Can not set the ""Proxy-connection"" headerWhen using a proxy the HttpClient refuses to set the ""Proxy-connection"" header
to the value ""close"". The value will be converted to ""keep-alive"" when the final
request is sent to network.

The following code snippet can be used to replicate the defect. Method is GET:
...
method.removeRequestHeader(""Proxy-Connection"");
logger.debug(""Proxy-Connection header removed."");
method.addRequestHeader(""Proxy-Connection"", ""close"");
logger.debug(""Proxy-Connection header set to: "" +
method.getRequestHeader(""Proxy-Connection"") );
try {
  	int statusCode = httpclient.executeMethod( method );
...

Now if you look at the wire log, you will notice that the actual value will be
""keep-alive""."
0,"results.jsp in luceneweb.war uses unknown parse-Methodresults.jsp in luceneweb.war demo throws JasperException:

org.apache.jasper.JasperException: Unable to compile class for JSP

An error occurred at line: 60 in the jsp file: /results.jsp
Generated servlet error:
The method parse(String) in the type QueryParser is not applicable for the arguments (String, String, Analyzer)

I think, the code in line 81 of results.jsp should maybe look like the following ?

QueryParser qp = new QueryParser(""contents"", analyzer);
query = qp.parse(queryString);"
0,BoostingNearQuery must implement clone - now it clones to a NearSpanQuery - cloning is required for Span container classes
0,"Add a OnWorkspaceInconsistency with logging onlyIf a Workspace performs a re-index on startup with  a inconsistency in it the process will fail now.
The new OnWorkspaceInconsistency ""log"" will only log the inconsistency but the reindex-process will not fail"
1,"Concurrent Repository.login() throws IllegalStateExceptionSee test case: org.apache.jackrabbit.test.core.ConcurrentLoginTest

java.lang.IllegalStateException: workspace 'default' not initialized
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getItemStateProvider(RepositoryImpl.java:1448)
	at org.apache.jackrabbit.core.RepositoryImpl.getWorkspaceStateManager(RepositoryImpl.java:712)
	at org.apache.jackrabbit.core.SessionImpl.<init>(SessionImpl.java:247)
	at org.apache.jackrabbit.core.SessionImpl.<init>(SessionImpl.java:214)
	at org.apache.jackrabbit.core.XASessionImpl.<init>(XASessionImpl.java:98)
	at org.apache.jackrabbit.core.RepositoryImpl.createSessionInstance(RepositoryImpl.java:1233)
	at org.apache.jackrabbit.core.RepositoryImpl.createSession(RepositoryImpl.java:800)
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1119)
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1146)
	at org.apache.jackrabbit.core.jndi.BindableRepository.login(BindableRepository.java:181)
	at org.apache.jackrabbit.core.ConcurrentLoginTest$1.run(ConcurrentLoginTest.java:56)
	at java.lang.Thread.run(Thread.java:534)"
1,"ContentLengthInputStream does not implement available() properlyContentLengthInputStream should either extend FilterInputStream or should delegate available() to wrappedStream.

Otherwise, available() on the response stream (an instance of AutoCloseInputStream, which is properly extending FilterInputStream, and, therefore, delegating to the ContentLengthInputStream) always returns 0.

This issue is important for the clients that try to improve performance by processing all data that can be read in a non-blocking way before blocking on the network."
0,"Get rid of backwards tagsThis is a followup on: [http://www.lucidimagination.com/search/document/bb6c23b6e87c0b63/back_compat_folders_in_tags_when_i_svn_update#3000a2232c678031]

Currently we use tags for specifying the revision number in the backwards branch that matches the current development branch revision (in common-build.xml). The idea is to just specify the corresponding revision no of the backwards branch in common-build.xml and the backwards-test target automatically handles up/down/co:

- We just give the rev number in common-build in common-build.xml as a property backwards-rev=""XXXX"". This property is used also in building the command line which is also a property backwards-svn-args=""-r $backwards-rev"". By that you can use ""ant -Dbackwards-svn-args=''"" to force test-backwards to checkout/update to head of branch (recommened for developers).

- we should rename target to ""test-backwards"" and keep a ""test-tag"" with dependency to that for compatibility

- The checkout on backwards creates a directory ""backwards/${backwards-branch}"" and uses ""svn co ${backwards-svn-args} 'http://svn.../${backwards-branch}' 'backwards/${backwards-branch}'"". The cool thing, the dir is checked out if non existent, but if the checkout already exists, svn co implicitely does an svn up to the given revision (it will also downgrade and merge if newer). So the test-backwards target always updates the checkout to the correct revision. I had not tried with local changes, but this should simply merge as an svn up.

The workflow for committing fixes to bw would be:

- First use ""svn up"" to upgrade the backwards working copy to HEAD.
- Commit the changes
- Copy and paste the message ""Committed revision XXXX"" to common-build.xml
- Commit the changes in trunk
"
0,"move deletes under codecAfter LUCENE-3631, this should be easier I think.

I haven't looked at it much myself but i'll play around a bit, but at a glance:
* SegmentReader to have Bits liveDocs instead of BitVector
* address the TODO in the IW-using ctors so that SegmentReader doesn't take a parent but just an existing core.
* we need some type of minimal ""MutableBits"" or similar subinterface of bits. BitVector and maybe Fixed/OpenBitSet could implement it
* BitVector becomes an impl detail and moves to codec (maybe we have a shared base class and split the 3.x/4.x up rather than the conditional backwards)
* I think the invertAll should not be used by IndexWriter, instead we define the codec interface to say ""give me a new MutableBits, by default all are set"" ?
* redundant internally-consistent checks in checkLiveCounts should be done in the codec impl instead of in SegmentReader.
* plain text impl in SimpleText."
0,"HttpMime StringBody constructor throws specification unnecessarily declares UnsupportedEncodingExceptionThe string body constructors that take a charset unnecessarily throw UnsupportedEncodingException - if you have Charset, the encoding is by definition supported:

    public StringBody(
            final String text, 
            final String mimeType, 
            Charset charset) throws UnsupportedEncodingException {
        super(mimeType);
        if (text == null) {
            throw new IllegalArgumentException(""Text may not be null"");
        }
        if (charset == null) {
            charset = Charset.defaultCharset();
        }
        this.content = text.getBytes(charset.name());
        this.charset = charset;
    }
    
    public StringBody(final String text, Charset charset) throws UnsupportedEncodingException {
        this(text, ""text/plain"", charset);
    }
    
I suggest to change this to

    public StringBody(
            final String text, 
            final String mimeType, 
            Charset charset)  {
        super(mimeType);
        if (text == null) {
            throw new IllegalArgumentException(""Text may not be null"");
        }
        if (charset == null) {
            charset = Charset.defaultCharset();
        }
        this.content = text.getBytes(charset);
        this.charset = charset;
    }
    
    public StringBody(final String text, Charset charset) {
        this(text, ""text/plain"", charset);
    }

The important change is to change

        this.content = text.getBytes(charset.name());

to 

        this.content = text.getBytes(charset);

which will not throw and hence the throws specifications can be removed.
"
0,"Break the spi2dav dependency to jcr-serverCurrently the spi2dav component has a dependency on the jcr-server component, which is troublesome due to the extra transitive dependencies and which strictly speaking should not be necessary from an architectural point of view.

The dependency exists mostly for sharing a number of JCR-specific WebDAV constants. I'd like to push those constants down to jackrabbit-webdav as they are essentially just shared strings and as jackrabbit-webdav already contains a number of constants used by JCR extensions.

In addition to constant values, code in the following classes is shared between jcr-server and spi2dav: JcrValueType, NamespacesProperty, NodeTypesProperty, SearchResultProperty, SubscriptionImpl, ValuesProperty. The shared code in JcrValueType and SubscriptionImpl is mostly just about mapping constant value mappings and could fairly easily be moved to jackrabbit-webdav. The Property classes are a but trickier, but it looks like it would be possible to split the code to separate server- and client-side classes for jcr-server and spi2dav."
1,repository-2.0.dtd missingWe introduced new configuration elements that need to be reflected in a new DTD version.
0,"Use an enumeration for QOM join typesLike JCR-2094 but for join types. The join type constants in the PFD version of QueryObjectModelConstants are broken, and a type-safe enumeration would in any case be a good alternative to the string constants."
1,"Setting WebDAV property without value causes NPE in DAVResourceImplA WebDAV PROPPATCH of a property without a value <prf:SomeProperty/> causes a NPE in DAVResourceImpl when the value is retrieved and the toString() method called on it. Here is a patch that works around the problem.

Index: C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/webdav/simple/DavResourceImpl.java
===================================================================
--- C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/webdav/simple/DavResourceImpl.java	(revision 388517)
+++ C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/webdav/simple/DavResourceImpl.java	(working copy)
@@ -930,7 +930,10 @@
      */
     private void setJcrProperty(DavProperty property) throws RepositoryException {
         // retrieve value
-        String value = property.getValue().toString();
+        String value = """";
+        if (property.getValue() != null) {
+            value = property.getValue().toString();
+        }
         // set value; since multivalued-properties are not listed in the set
         // of available properties, this extra validation-check is omitted.
         node.setProperty(getJcrName(property.getName()), value);
"
1,"javax.jcr.NamespaceException: : is not a registered namespace uriUsing the first hops with both versions 1.2.3 and 1.3, the repository is created successfully the first time it is run.  Subsequent attempts to login result in a javax.jcr.NamespaceException.


DEBUG - Initializing transient repository
INFO - Starting repository...
INFO - LocalFileSystem initialized at path repository\repository
Exception in thread ""main"" javax.jcr.NamespaceException: : is not a registered namespace uri.
	at org.apache.jackrabbit.core.NamespaceRegistryImpl.getPrefix(NamespaceRegistryImpl.java:538)
	at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.checkNamespace(NodeTypeRegistry.java:1292)
	at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.validateNodeTypeDef(NodeTypeRegistry.java:1415)
	at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.internalRegister(NodeTypeRegistry.java:1221)
	at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.<init>(NodeTypeRegistry.java:671)
	at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.create(NodeTypeRegistry.java:118)
	at org.apache.jackrabbit.core.RepositoryImpl.createNodeTypeRegistry(RepositoryImpl.java:571)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:262)
	at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:584)
	at org.apache.jackrabbit.core.TransientRepository$2.getRepository(TransientRepository.java:245)
	at org.apache.jackrabbit.core.TransientRepository.startRepository(TransientRepository.java:265)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:333)
	at org.apache.jackrabbit.core.TransientRepository.login(TransientRepository.java:388)
	at testing.FirstHops.main(FirstHops.java:24)"
1,"Cloning a tree containing shareable nodes into another workspace throws ItemExistsExceptionThere's a problem when trying to clone a tree in another workspace, when this tree contains shareable nodes.

Let ws1 be one workspace, which contains one node A. This node has two sub-nodes B and C. B and C share a shareable sub-node D :

A 
|   \
B  C
|    |
D  D

Let ws2 be a second workspace. Then calling ws2.clone(""ws1"" , ""/A"" , ""/A"" , false) throws an ItemExistsException ( copyNodeState line 1628 ) . This is done when the copyNodeState is checking if the nodeId is already present in the workspace - which is the case when copying the second instance of the shareable node. I can't find in the specification something about this case - but it would be logical to add a share to the node when coming across this situation - at least in the CLONE ( and probable COPY too ) cases. I don't know what would be expected in the CLONE_REMOVE_EXISTING case - we might not want to remove the node if it's shareable, and also add a share here.

I fixed the issue by handling the case the node is shareable in the COPY and CLONE cases of copyNodeState - you'll find attached the corresponding patch. Do you think this solution is ok ?"
1,CLONE -Aggregate include ignored if no primaryType setIf the include element of an aggregate definition does not have a primaryType attribute then the include is never matched.
0,UserManagement: Limit set of properties exposed by AuthorizableImplAuthorizableImpl currently exposes all properties present on the Node representing the user/group. This should be limited to the properties set through the API (i.e. the non-protected props defined by the rep:Authorizable node type)
1,"IndexReader.lastModified - throws NPEIndexReader.lastModified(String dir) or its variants always return NPE on 2.3, perhaps something to do with SegmentInfo."
0,"Optimizations to TopScoreDocCollector and TopFieldCollectorThis is a spin-off of LUCENE-1575 and proposes to optimize TSDC and TFC code to remove unnecessary checks. The plan is:
# Ensure that IndexSearcher returns segements in increasing doc Id order, instead of numDocs().
# Change TSDC and TFC's code to not use the doc id as a tie breaker. New docs will always have larger ids and therefore cannot compete.
# Pre-populate HitQueue with sentinel values in TSDC (score = Float.NEG_INF) and remove the check if reusableSD == null.
# Also move to use ""changing top"" and then call adjustTop(), in case we update the queue.
# some methods in Sort explicitly add SortField.FIELD_DOC as a ""tie breaker"" for the last SortField. But, doing so should not be necessary (since we already break ties by docID), and is in fact less efficient (once the above optimization is in).
# Investigate PQ - can we deprecate insert() and have only insertWithOverflow()? Add a addDummyObjects method which will populate the queue without ""arranging"" it, just store the objects in the array (this can be used to pre-populate sentinel values)?

I will post a patch as well as some perf measurements as soon as I have them."
1,webapp welcome page shows incorrect port when port is the default portSee summary.
1,"RangeQuery - add equals and hashCode methodsI'm attaching a patch with an equals() and hashCode() implementation for 
RangeQuery.java, and a new unit test for TestRangeQuery.java, as per a recent 
message on lucene-user mailing list (subject ""RangeQuery doesn't override 
equals() or hashCode() - intentional?"")

patches to follow"
0,"IndexWriter.close(false) does not actually stop background merge threadsRight now when you close(false), IndexWriter marks any running merges
as aborted but then does not wait for these merges to finish.  This
can cause problems because those threads still hold files open, so,
someone might think they can call close(false) and then (say) delete
all files from that directory, which would fail on Windows.

Instead, close(false) should notify each running merge that it has
been aborted, and not return until all running merges are done.  Then,
SegmentMerger should periodically check whether it has been aborted
and stop if so.
"
0,"Fix buggy stemmers and Remove duplicate analysis functionalitywould like to remove stemmers in the following packages, and instead in their analyzers use a SnowballStemFilter instead.

* analyzers/fr
* analyzers/nl
* analyzers/ru

below are excerpts from this code where they proudly proclaim they use the snowball algorithm.
I think we should delete all of this custom stemming code in favor of the actual snowball package.


{noformat}
/**
 * A stemmer for French words. 
 * <p>
 * The algorithm is based on the work of
 * Dr Martin Porter on his snowball project<br>
 * refer to http://snowball.sourceforge.net/french/stemmer.html<br>
 * (French stemming algorithm) for details
 * </p>
 */

public class FrenchStemmer {

/**
 * A stemmer for Dutch words. 
 * <p>
 * The algorithm is an implementation of
 * the <a href=""http://snowball.tartarus.org/algorithms/dutch/stemmer.html"">dutch stemming</a>
 * algorithm in Martin Porter's snowball project.
 * </p>
 */
public class DutchStemmer {

/**
 * Russian stemming algorithm implementation (see http://snowball.sourceforge.net for detailed description).
 */
class RussianStemmer
{noformat}

"
0,PackedInts does not support structures above 256MBThe PackedInts Packed32 and Packed64 fails when the internal structure exceeds 256MB. This is due to a missing cast that results in the bit position calculation being limited by Integer.MAX_VALUE (256MB * 8 = 2GB).
0,"[PATCH] Enable application-level management of IndexWriter.ramDirectory sizeIndexWriter currently only supports bounding of in the in-memory index cache using maxBufferedDocs, which limits it to a fixed number of documents.  When document sizes vary substantially, especially when documents cannot be truncated, this leads either to inefficiencies from a too-small value or OutOfMemoryErrors from a too large value.

This simple patch exposes IndexWriter.flushRamSegments(), and provides access to size information about IndexWriter.ramDirectory so that an application can manage this based on total number of bytes consumed by the in-memory cache, thereby allow a larger number of smaller documents or a smaller number of larger documents.  This can lead to much better performance while elimianting the possibility of OutOfMemoryErrors.

The actual job of managing to a size constraint, or any other constraint, is left up the applicatation.

The addition of synchronized to flushRamSegments() is only for safety of an external call.  It has no significant effect on internal calls since they all come from a sychronized caller.
"
0,"Public Suffix ListHi,

I just found this useful list: http://publicsuffix.org/
and thought it would be nice to validate cookie domains against it, basically serving as a black list of domain for which never to set any cookies. What do you think about the attached patch? The download/parsing of the list is of course not part of the implementation.

Ortwin"
0,"Add OSGi bundle metadata to manifestSee this discussion on the Felix mailing list:
http://www.mail-archive.com/felix-dev@incubator.apache.org/msg04041.html

If there is an easy way to generate the information required for an OSGi bundle into the manifest of our distributable JAR, we should do it.
The runtime and API are not affected, it is only a modification of the build process.
I'll follow up on this when I have information about the required tooling.

cheers,
  Roland
"
1,CompactNodeTypeDefWriter uses bad format for residual properties CompactNodeTypeDefWriter writes {}* instead of * for residual properties. 
0,"HTTPClient per default relentlessly spams to stderrHTTPClient relentlessly spams to stderr when including it into a project via maven. This is not a decent default behaviour for a libary. Libaries should, per default, communicate their internal state solely and adequatly via their API and let it be up to the application to react to that state (logging it is one such reaction). From some replies to tickets in the same vain I can see that this is perhaps a sensitive topic as some see logging to be a core concern of HTTPClient. I do agree it's helpful as a debugging tool but as such it needs to be opt-in. As a standard error output, the logging of HTTPClient is absolutely useless because it does not and can not describe what the application is trying to do.

Why this improvement when there is a way to disable HTTClient logging (in fact, there seem to be many ways ... always a bad sign ..)?

Do a google search for: httpclient ""console spam""
204 hit for this harsh phrasing alone. Search this phrase for any other libary you like to use and compare the number of hits. Ask youself, how often have you seen the java standard libary write to stdout or stderr?

Personally, I tried to disable it via JDK14 getLogger(""org.apache"").setLevel(Level.OFF)  which wouldn't work and now am using a solution I found on Stackoverflow which is:

System.setProperty(""org.apache.commons.logging.Log"", ""org.apache.commons.logging.impl.NoOpLog""); }

The problem I have is that I include this lib and suddently my console is useless because httpclient is all over it (writing a system monitor ...). I have to search google to find a solution (http://hc.apache.org/httpcomponents-client-ga/logging.html does not tell you how to turn logging off ...) and the logical one ""turn of the JDK logger"" does not work right away.

It's really a matter of following the principal of least suprise (a libary is not expected to write to the console which is the observable default behaviour of HTTPClient) and the principal of separation of concerns (logging is a concern for applications and not for libaries).

Following at least one of these would substantially increase the joy of working with the HTTPClient libary.

"
0,Factor out SearcherManager from NRTManagerCurrently we have NRTManager and SearcherManager while NRTManager contains a big piece of the code that is already in SearcherManager. Users are kind of forced to use NRTManager if they want to have SearcherManager goodness with NRT. The integration into NRTManager also forces you to maintain two instances even if you know you always want deletes. To me NRTManager tries to do more than necessary and mixes lots of responsibilities ie. handling searchers and handling indexing generations. NRTManager should use a SearcherManager by aggregation rather than duplicate a lot of logic. SearcherManager should have a NRT and Directory based implementation users can simply choose from.
0,"NullPointerException when temporary directory not readableWe have customers reporting errors such as:

Caused by: java.lang.NullPointerException
	at org.apache.lucene.store.FSDirectory.create(FSDirectory.java:200)
	at org.apache.lucene.store.FSDirectory.getDirectory(FSDirectory.java:144)
	at org.apache.lucene.store.FSDirectory.getDirectory(FSDirectory.java:117)
	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:205)
	at com.atlassian.jira.util.LuceneUtils.getIndexWriter(LuceneUtils.java:46)
	at com.atlassian.jira.issue.index.DefaultIndexManager.getIndexWriter(DefaultIndexManager.java:568)
	at com.atlassian.jira.issue.index.DefaultIndexManager.indexIssuesAndComments(DefaultIndexManager.java:287)
	... 59 more


This occurs when the lock directory is unreadable (eg. because Tomcat sets java.io.tmpdir to temp/ and the permissions here are broken). Attached is "
0,"[PATCH] Improved javadoc for maxClauseCountAs discussed on lucene-dev before, queries with lots of terms can use 
up a lot of unused buffer space for their TermDocs, because most terms 
have few documents."
0,"TCK: DerefQueryLevel1Test requires support for optional jcr:deref functionTest fails if jcr:deref is not supported.  Per JSR-170, jcr:deref is optional.

Proposal: introduce new configuration property which indicates whether jcr:deref is supported; if not throw NotExecutableException
"
0,"Allow concurrent index updates and queriesCurrently the query handler either allows one modification or multiple queries at a time. That is, write operations are separated from read operations. With this synchronization scheme multiple queries may run concurrently, but only in the absence of a write operation.

There is one major drawback with this synchronization: a single long running query is able to block the whole workspace from committing changes. Because the query handler is coupled to the Workspace via a synchronous event listener, further processing is blocked until the query handler has finished its event processing (reflecting the changes in the index).

Instead, each index modification should be non-blocking, in the sense that an index modification should not have to wait for any queries to complete."
0,"Test tooling updatesTo leverage advances in test tooling I'd like to upgrade our JUnit dependency to 4.x and switch to using the Maven Failsafe plugin  [1] instead of our current custom POM settings for integration tests. I'll also upgrade the Easymock dependency to 3.0.

[1] http://maven.apache.org/plugins/maven-failsafe-plugin/"
1,"Session returned does not offers transaction supportThe javax.jcr.Session instance returned by the repository is an implementation of org.apache.jackrabbit.jca.JCASessionHandle which doesn't implement the interface org.apache.jackrabbit.api.XASession.
"
0,"Lucene Java Site docsIt would be really nice if the Java site docs where consistent with the rest of the Lucene family (namely, with navigation tabs, etc.) so that one can easily go between Nutch, Hadoop, etc."
0,"allow specifying -Dbootclasspath for javac/javadocsSo that you can compile/javadoc against the actual target JRE libraries
even if you have a newer compiler."
0,"Add shutdown method to SimpleHttpConnectionManagerIt would be useful to be able to close the connection in the
SimpleHttpConnectionManager. This could be achieved by adding a shutdown()
method as per the MultiThreadedConnectionManager.

Ideally this would be added to the HttpConnection interface, but this could
break existing implementations.

To avoid this, perhaps consider introducing a sub-interface with the method in it.

[Could also create an AbstractConnectionManager class - this would make it
easier to add more functions later]"
0,"better error for unknown date formatsThis small patch improves the exception message you get when the date format is unknown.
"
1,"MultiSearcher.rewrite() incorrectly rewrites queriesThis was reported on the userlist, in the context of range queries.

Its also easy to make our existing tests fail with my patch on LUCENE-2751:
{noformat}
ant test-core -Dtestcase=TestBoolean2 -Dtestmethod=testRandomQueries -Dtests.seed=7679849347282878725:-903778383189134045
{noformat}

The fundamental problem is that MultiSearcher first rewrites against individual subs, 
then uses Query.combine() which simply OR's these sub-clauses.

This is incorrect for expanded MUST_NOT queries (e.g. from wildcard), as it violates demorgan's law.
"
1,"Problems with jackrabbit-standaloneI'm having problems with the jackrabbit-standalone component not starting up properly because of two issues:

* The bundle packaging doesn't include the WEB-INF/config.xml file. I assume this is because of the more recent bundle plugin version treating whitespace differently in the inlining settings.

* The RMI binding fails if a local RMI registry is already running with another repository reference. I'm thinking of simply disabling the RMI registry bindings and using the http://localhost:8080/rmi address as the recommended RMI endpoint."
0,"date encoding limitation removingcurrently there is some limitation to date encoding in lucene. I think it's 
because dates should preserve lexicografical ordering, i.e. if one date precedes 
another date then encoded values should keep same ordering.

I know that it can be difficult to integrate it into existing version but there 
is way to remove this limitation.
Date milliseconds can be encoded as unsigned values with prefix that indicates 
positive or negative value.

In more details:
I used hex encoding and prefix &#8216;p&#8217; and &#8216;n&#8217; for positive and negative values. I 
got following results:

Value -10000 is encoded with nffffffffffffd8f0, 
-100	- nffffffffffffff9c
0	- p0000000000000000
100	- p0000000000000064
10000	- p0000000000002710

This preserves ordering between values and theirs encoding.

Also hex encoding can be replaced with Character.MAX_RADIX encoding.

Part of code that do this work:
   final static char[] digits = {
	'0' , '1' , '2' , '3' , '4' , '5' ,
	'6' , '7' , '8' , '9' , 'a' , 'b' ,
	'c' , 'd' , 'e' , 'f' , 'g' , 'h' ,
	'i' , 'j' , 'k' , 'l' , 'm' , 'n' ,
	'o' , 'p' , 'q' , 'r' , 's' , 't' ,
	'u' , 'v' , 'w' , 'x' , 'y' , 'z'
    };


    char prefix;
    if (time >= 0) {
      prefix = 'p';
    } else {
      prefix = 'n';
    }

    char[] chars = new char[DATE_LEN + 1];
    int index = DATE_LEN;
    while (time != 0) {
      int b = (int) (time & 0x0F);
      chars[index--] = digits[b];
      time = time >>> 4;
    }

    while (index >= 0) {
      chars[index--] = '0';
    }
    chars[0] = prefix;

    return new String(chars);"
0,"Authorization credentials should be sent pre-emptivelyWhen a web browser receives a <i>401: Unauthorized</i> response code, the
browser prompts for the user and password credentials for the requested
authentication realm.  An Authorization header is then sent for this request. 
HttpClient models this behaviour quite well.

After the web browser has the authentication credentials for a given host, port
and realm, it then sends the Authorization header for subsequent requests
pre-emptively, whithout need for a 401 response.  HttpClient always reqires a
401 response before it will send out the Authorization header.

As <code>HttpClient.startSession()</code> will take a <code>Credentials</code>
object as a parameter as the default credentials, the default credentials should
be sent as part of every request in that session.  Some mechanisim for
over-riding the default credentials should also be provided to be sent
pre-emptively.

The point of this enhancement request is to minimize the number of unnecessisary
401 responses.

It appears that the simple solution might be to modify the logic of when
<code>Authenticator.authenticate()</code> gets called in
<code>HttpMethodBase.addAuthorizationRequestHeader()</code>"
1,"Removing a mixin that adds a same-name-sibling child node throws an ItemNotFoundExceptionApologies in advance if this has previously been noted in JIRA or on the lists but I couldn't find anything. When removing a mixin that defines a same-name-sibling child an ItemNotFoundException is thrown due to child node indicies not being maintained. A similar method of NodeImpl (onRemove) recongnized this and amended to remove from the tail but the removeMixin also has this issue.

Simple test reproduction:

Types:

[mix:foo] mixin
	+ bar (nt:bar) multiple
	
[nt:bar] > nt:unstructured, mix:referenceable

Code:

public class RemoveMixinTest extends AbstractServerTest {
    private static final String MIXIN = ""mix:foo"";
    private static final String CHILD = ""bar"";
    private static final String PTYPE = ""nt:bar"";
    
    public void testRemoveMixin() throws RepositoryException {
        Session session = getSession();
        Node root = session.getRootNode().addNode(""root"");
        root.addMixin(MIXIN);
        
        root.addNode(CHILD, PTYPE);
        root.addNode(CHILD, PTYPE);
        root.addNode(CHILD, PTYPE);
        
        session.save();
        
        for (NodeIterator it = root.getNodes(); it.hasNext(); ) {
            Node node = it.nextNode();
            System.out.println(node.getPath() + "" : "" + node.getUUID());
        }
        
        try {
            root.removeMixin(MIXIN);
            root.save();
        } catch (RepositoryException ex) {
            ex.printStackTrace();
        }
    }
}

Output:

/root/bar : 0b09e0b4-0727-4194-978a-4eadfbf93fa8
/root/bar[2] : 84d5e556-6f12-43fb-98e3-614bcf1f7bb7
/root/bar[3] : 8db95029-df3b-4e26-affb-438de0206cf5

javax.jcr.ItemNotFoundException: 8db95029-df3b-4e26-affb-438de0206cf5
at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:463)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:319)
	at org.apache.jackrabbit.core.NodeImpl.removeMixin(NodeImpl.java:1212)
	at org.apache.jackrabbit.core.NodeImpl.removeMixin(NodeImpl.java:2624)
	at com.ms.appmw.rcf.server.RemoveMixinTest.testRemoveMixin(RemoveMixinTest.java:48)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at junit.framework.TestCase.runTest(TestCase.java:154)
	at msjava.base.testutils.junit3.MSTestCase.runTest(MSTestCase.java:203)
	at msjava.base.testutils.junit3.TestCaseTearDownEvenIfSetUpFails.runBare(TestCaseTearDownEvenIfSetUpFails.java:92)
	at msjava.base.testutils.junit3.MSTestCase.runBare(MSTestCase.java:170)
	at junit.framework.TestResult$1.protect(TestResult.java:106)

The missing UUID for the last node is the one not found because upon removal of the second the index [2] is still valid. Thanks!"
0,"Cleanup XML QueryParser CodeBefore I move the XML QueryParser to the queryparser module, I want to pass over it and bring it up to module standards."
0,"Field Selection and Lazy Field LoadingThe patch to come shortly implements a Field Selection and Lazy Loading mechanism for Document loading on the IndexReader.

It introduces a FieldSelector interface that defines the accept method:
FieldSelectorResult accept(String fieldName);

(Perhaps we want to expand this to take in other parameters such as the field metadata (term vector, etc.))

Anyone can implement a FieldSelector to define how they want to load fields for a Document.  
The FieldSelectorResult can be one of four values: LOAD, LAZY_LOAD, NO_LOAD, LOAD_AND_BREAK.  
The FieldsReader, as it is looping over the FieldsInfo, applies the FieldSelector to determine what should be done with the current field.

I modeled this after the java.io.FileFilter mechanism.  There are two implementations to date: SetBasedFieldSelector and LoadFirstFieldSelector.  The former takes in two sets of field names, one to load immed. and one to load lazily.  The latter returns LOAD_AND_BREAK on the first field encountered.  See TestFieldsReader for examples.

It should support UTF-8 (I borrowed code from Issue 509, thanks!).  See TestFieldsReader for examples

I added an expert method on IndexInput  named skipChars that takes in the number of characters to skip.  This is a compromise on changing the file format of the fields to better support seeking.  It does some of the work of readChars, but not all of it.  It doesn't require buffer storage and it doesn't do the bitwise operations.  It just reads in the appropriate number of bytes and promptly ignores them.  This is useful for skipping non-binary, non-compressed stored fields.

The biggest change is by far the introduction of the Fieldable interface (as per Doug's suggestion from a mailing list email on Lazy Field loading from a while ago).  Field is now a Fieldable.  All uses of Field have been changed to use Fieldable.  FieldsReader.LazyField also implements Fieldable.

Lazy Field loading is now implemented.  It has a major caveat (that is Documented) in that it clones the underlying IndexInput upon lazy access to the Field value.  IT IS UNDEFINED whether a Lazy Field can be loaded after the IndexInput parent has been closed (although, from what I saw, it does work).  I thought about adding a reattach method, but it seems just as easy to reload the document.  See the TestFieldsReader and DocHelper for examples.

I updated a couple of other tests to reflect the new fields that are on the DocHelper document.

All tests pass."
0,Retrieve row path via hierarchy manager instead of nodeJackrabbit currently loads the node associated with a result row to retrieve the path. Loading the node may be prevented by retrieving the path from a caching hierarchy manager.
0,"UAX29URLEmailTokenizer fails to recognize emails as such when the mailto: scheme is prependedAs [reported by Kai Glzau on solr-user|http://markmail.org/message/n32kji3okqm2c5qn]:

UAX29URLEmailTokenizer seems to split at the wrong place:

{noformat}mailto:test@example.org{noformat} ->
{noformat}mailto:test{noformat}
{noformat}example.org{noformat}

As a workaround I use

{code:xml}
<charFilter class=""solr.PatternReplaceCharFilterFactory"" pattern=""mailto:"" replacement=""mailto: ""/>
{code}"
1,"When fetching node ids in checks for the checker all queries should use the same orderingThe ""bundleSelectAllIdsSQL"" query and the ""bundleSelectAllIdsFromSQL"" should use the same ordering."
1,"NodeTypeRegistry.reregister unregisters dependent typesNodeTypeRegistry.reregister allows modifying a registered node type if the difference to the currently registered node type with the same name is TRIVIAL according to NodeTypeDefDiff.

Before registering the new node type definition the old node type is unregistered. The side effect of that first step is that also all NodeTypes, which depend (extend ?) the node type to be re-registered, are removed from the registry.

After the modified node type is then registered, the previously registered dependent node types will not be registered anymore and will not be known any more.

While it makes sense to me, to temporarily unregister dependent node types, those must be registered again after the re-registered node type has been registered. Otherwise the system may become pretty useless."
0,"Add way to check for release of connections back into poolAs the documentation emphasizes, its important to clean up HttpEntities after use, so they don't tie up the default very small number of connections in the pool.

However, nothing is provided to HttpClient users with the default classes that allows them to unit test their code to help verify that they are in fact properly releasing the connections under all circumstances. One way this could be done is for the API to expose the number of current leased (taken) connections in the pool, which would be connManager.pool.leasedConnections.size() if not for the necessary fields being protected. If this statistic were published through the API, user unit tests could check that it is zero when they finish.

A workaround is for the user to subclass both the connection manager and ConnPoolByRoute and add getter methods. But its kind of a clunky solution, and I think the API should be written to encourage its users to perform this check."
1,"BasicClientConnectionManager.releaseConnection accesses poolEntry using non-standard lockAccording to the annotation, poolEntry is @GuardedBy(""this"").

However, in at least one place, it is accessed without holding a lock on this: 

BasicClientConnectionManager.releaseConnection synchronizes on managedConn, and then accesses poolEntry without synchronising on this.

[Synch. only works if all parties use the same lock.]"
1,"HttpClient does not retry authentication when multiple challenges are present if the primary one failsI'm trying to request a page from IIS (6 and 7.5).  If the IIS is configured with providers for ""negotiate"" and ""ntlm"" then the Negotiate authentication is tried and fails, but it does not then try to use the NTLM authentication which is what I require.  If I removed ""negotiate"" as a provider from IIS and just use NTLM then all works well - but this is not a solution as I don't have control of the web servers. 

Output below...


[DEBUG] SingleClientConnManager - Get connection for route HttpRoute[{}->http://WIN-HNB91NNAB2G]
[DEBUG] DefaultClientConnectionOperator - Connecting to WIN-HNB91NNAB2G/147.183.80.134:80
[DEBUG] RequestAddCookies - CookieSpec selected: best-match
[DEBUG] DefaultHttpClient - Attempt 1 to execute request
[DEBUG] DefaultClientConnection - Sending request: GET / HTTP/1.1
[DEBUG] wire - >> ""GET / HTTP/1.1[\r][\n]""
[DEBUG] wire - >> ""Host: WIN-HNB91NNAB2G[\r][\n]""
[DEBUG] wire - >> ""Connection: Keep-Alive[\r][\n]""
[DEBUG] wire - >> ""User-Agent: Apache-HttpClient/4.1 (java 1.5)[\r][\n]""
[DEBUG] wire - >> ""[\r][\n]""
[DEBUG] headers - >> GET / HTTP/1.1
[DEBUG] headers - >> Host: WIN-HNB91NNAB2G
[DEBUG] headers - >> Connection: Keep-Alive
[DEBUG] headers - >> User-Agent: Apache-HttpClient/4.1 (java 1.5)
[DEBUG] wire - << ""HTTP/1.1 401 Unauthorized[\r][\n]""
[DEBUG] wire - << ""Content-Type: text/html[\r][\n]""
[DEBUG] wire - << ""Server: Microsoft-IIS/7.5[\r][\n]""
[DEBUG] wire - << ""WWW-Authenticate: Negotiate[\r][\n]""
[DEBUG] wire - << ""WWW-Authenticate: NTLM[\r][\n]""
[DEBUG] wire - << ""Date: Fri, 15 Jul 2011 12:15:11 GMT[\r][\n]""
[DEBUG] wire - << ""Content-Length: 58[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] DefaultClientConnection - Receiving response: HTTP/1.1 401 Unauthorized
[DEBUG] headers - << HTTP/1.1 401 Unauthorized
[DEBUG] headers - << Content-Type: text/html
[DEBUG] headers - << Server: Microsoft-IIS/7.5
[DEBUG] headers - << WWW-Authenticate: Negotiate
[DEBUG] headers - << WWW-Authenticate: NTLM
[DEBUG] headers - << Date: Fri, 15 Jul 2011 12:15:11 GMT
[DEBUG] headers - << Content-Length: 58
[DEBUG] DefaultHttpClient - Connection can be kept alive indefinitely
[DEBUG] DefaultHttpClient - Target requested authentication
[DEBUG] DefaultTargetAuthenticationHandler - Authentication schemes in the order of preference: [negotiate, NTLM, Digest, Basic]
[DEBUG] DefaultTargetAuthenticationHandler - negotiate authentication scheme selected
[DEBUG] NegotiateScheme - Received challenge '' from the auth server
[DEBUG] DefaultHttpClient - Authorization challenge processed
[DEBUG] DefaultHttpClient - Authentication scope: NEGOTIATE <any realm>@win-hnb91nnab2g:80
[DEBUG] DefaultHttpClient - Found credentials
[DEBUG] wire - << ""You do not have permission to view this directory or page.""
[DEBUG] RequestAddCookies - CookieSpec selected: best-match
[DEBUG] NegotiateScheme - init WIN-HNB91NNAB2G
[ERROR] RequestTargetAuthentication - Authentication error: Invalid name provided (Mechanism level: Could not load configuration file C:\WINDOWS\krb5.ini (The system cannot find the file specified))
[DEBUG] DefaultHttpClient - Attempt 2 to execute request
[DEBUG] DefaultClientConnection - Sending request: GET / HTTP/1.1
[DEBUG] wire - >> ""GET / HTTP/1.1[\r][\n]""
[DEBUG] wire - >> ""Host: WIN-HNB91NNAB2G[\r][\n]""
[DEBUG] wire - >> ""Connection: Keep-Alive[\r][\n]""
[DEBUG] wire - >> ""User-Agent: Apache-HttpClient/4.1 (java 1.5)[\r][\n]""
[DEBUG] wire - >> ""[\r][\n]""
[DEBUG] headers - >> GET / HTTP/1.1
[DEBUG] headers - >> Host: WIN-HNB91NNAB2G
[DEBUG] headers - >> Connection: Keep-Alive
[DEBUG] headers - >> User-Agent: Apache-HttpClient/4.1 (java 1.5)
[DEBUG] wire - << ""HTTP/1.1 401 Unauthorized[\r][\n]""
[DEBUG] wire - << ""Content-Type: text/html[\r][\n]""
[DEBUG] wire - << ""Server: Microsoft-IIS/7.5[\r][\n]""
[DEBUG] wire - << ""WWW-Authenticate: Negotiate[\r][\n]""
[DEBUG] wire - << ""WWW-Authenticate: NTLM[\r][\n]""
[DEBUG] wire - << ""Date: Fri, 15 Jul 2011 12:15:11 GMT[\r][\n]""
[DEBUG] wire - << ""Content-Length: 58[\r][\n]""
[DEBUG] wire - << ""[\r][\n]""
[DEBUG] DefaultClientConnection - Receiving response: HTTP/1.1 401 Unauthorized
[DEBUG] headers - << HTTP/1.1 401 Unauthorized
[DEBUG] headers - << Content-Type: text/html
[DEBUG] headers - << Server: Microsoft-IIS/7.5
[DEBUG] headers - << WWW-Authenticate: Negotiate
[DEBUG] headers - << WWW-Authenticate: NTLM
[DEBUG] headers - << Date: Fri, 15 Jul 2011 12:15:11 GMT
[DEBUG] headers - << Content-Length: 58
[DEBUG] DefaultHttpClient - Connection can be kept alive indefinitely
[DEBUG] DefaultHttpClient - Target requested authentication
[DEBUG] NegotiateScheme - Received challenge '' from the auth server
[DEBUG] NegotiateScheme - Authentication already attempted
[DEBUG] DefaultHttpClient - Authorization challenge processed
[DEBUG] DefaultHttpClient - Authentication scope: NEGOTIATE <any realm>@win-hnb91nnab2g:80
[DEBUG] DefaultHttpClient - Authentication failed
[DEBUG] wire - << ""You do not have permission to view this directory or page.""
content:You do not have permission to view this directory or page.
[DEBUG] SingleClientConnManager - Releasing connection org.apache.http.impl.conn.SingleClientConnManager$ConnAdapter@17fa65e
"
0,Create docvalues based grouped facet collectorCreate docvalues based grouped facet collector. Currently only term based collectors have been implemented (LUCENE-3802).
1,"Restoring a deleted version does not work (throws Exception)java.lang.IllegalStateException: Not in edit mode
	at org.apache.jackrabbit.core.state.LocalItemStateManager.createNew(LocalItemStateManager.java:258)
	at org.apache.jackrabbit.core.version.NodeStateEx.createChildNode(NodeStateEx.java:561)
	at org.apache.jackrabbit.core.version.NodeStateEx.addNode(NodeStateEx.java:525)
	at org.apache.jackrabbit.core.version.NodeStateEx.addNode(NodeStateEx.java:505)
	at org.apache.jackrabbit.core.version.VersionManagerImplRestore.restore(VersionManagerImplRestore.java:201)
	at org.apache.jackrabbit.core.VersionManagerImpl.restore(VersionManagerImpl.java:240)
	at org.apache.jackrabbit.core.NodeImpl.restore(NodeImpl.java:3379)
	at org.apache.jackrabbit.test.api.version.RestoreTest.testRestoreRemoved(RestoreTest.java:812)"
1,"Cookies with names containing blanks or starting with $ should be rejected by RFC2109 spec onlyReported by John Patterson:

> The Cookie class does not like names with spaces in them.  It throws an
> IllegalArgumentException.  Unfortunately the server that my app interacts
> with uses a space in the cookie name.  Both IE and Mozilla don't mind."
1,"AbstractHttpClient.determineTarget(HttpUriRequest)Issue with 4.1 beta1 fails to parse the right host from the URL, eg. http://my.site.com/search/?for=http://other.site.com
This fails request for eg. REST that has a param value with ':' or '?' or '/'.

AbstractHttpClient.determineTarget(HttpUriRequest)
httpcomponents-client-4.0.3:
    private HttpHost determineTarget(HttpUriRequest request) {
        // A null target may be acceptable if there is a default target.
        // Otherwise, the null target is detected in the director.
        HttpHost target = null;

        URI requestURI = request.getURI();
        if (requestURI.isAbsolute()) {
            target = new HttpHost(
                    requestURI.getHost(),
                    requestURI.getPort(),
                    requestURI.getScheme());
        }
        return target;
    }


httpcomponents-client-4.1-beta1:
    private HttpHost determineTarget(HttpUriRequest request) throws ClientProtocolException {
        // A null target may be acceptable if there is a default target.
        // Otherwise, the null target is detected in the director.
        HttpHost target = null;

        URI requestURI = request.getURI();
        if (requestURI.isAbsolute()) {
            String ssp = requestURI.getSchemeSpecificPart();
            ssp = ssp.substring(2, ssp.length()); //remove ""//"" prefix
            int end = ssp.indexOf(':') > 0 ? ssp.indexOf(':') :
                    ssp.indexOf('/') > 0 ? ssp.indexOf('/') :
                    ssp.indexOf('?') > 0 ? ssp.indexOf('?') : ssp.length();
            String host = ssp.substring(0, end);

            int port = requestURI.getPort();
            String scheme = requestURI.getScheme();
            if (host == null || """".equals(host)) {
                throw new ClientProtocolException(
                        ""URI does not specify a valid host name: "" + requestURI);
            }
            target = new HttpHost(host, port, scheme);
        }
        return target;
    }
"
1,"NullPointerException when accessing the SimpleWebdavServlet at the prefix pathWhen accessing the SimpleWebdavServlet with the ""root"" path, that is the same path as set with the resource-path-prefix, a NullPointerException is thrown:

java.lang.NullPointerException
	at org.apache.jackrabbit.name.ParsingPathResolver.getQPath(ParsingPathResolver.java:91)
	at org.apache.jackrabbit.name.CachingPathResolver.getQPath(CachingPathResolver.java:74)
	at org.apache.jackrabbit.core.SessionImpl.getQPath(SessionImpl.java:601)
	at org.apache.jackrabbit.core.SessionImpl.getItem(SessionImpl.java:804)
	at org.apache.sling.jcr.api.internal.PooledSession.getItem(PooledSession.java:157)
	at org.apache.jackrabbit.webdav.simple.ResourceFactoryImpl.getNode(ResourceFactoryImpl.java:140)
	at org.apache.jackrabbit.webdav.simple.ResourceFactoryImpl.createResource(ResourceFactoryImpl.java:89)
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(AbstractWebdavServlet.java:187)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:487)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:362)
	at org.ops4j.pax.web.service.internal.HttpServiceServletHandler.handle(HttpServiceServletHandler.java:51)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:722)
	at org.ops4j.pax.web.service.internal.HttpServiceContext.handle(HttpServiceContext.java:87)
	at org.ops4j.pax.web.service.internal.JettyServerHandlerCollection.handle(JettyServerHandlerCollection.java:63)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:139)
	at org.mortbay.jetty.Server.handle(Server.java:324)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:505)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:828)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:514)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:211)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:380)
	at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)
	at org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:450)


The problem seems to be that the ResourceFactoryImpl.createResource method (or rather the getNode method) is not prepared to a DavResourceLocator instance whose resourcePath is null.

I could imagine, that the ResourceFactoryImpl.getNode() method might want to return the root node in this case ?"
0,"Unnecessary assert in org.apache.lucene.index.DocumentsWriterThreadState.trimFields()In org.apache.lucene.index.DocumentsWriterThreadState.trimFields() is the following code:

      if (fp.lastGen == -1) {
        // This field was not seen since the previous
        // flush, so, free up its resources now

        // Unhash
        final int hashPos = fp.fieldInfo.name.hashCode() & fieldDataHashMask;
        DocumentsWriterFieldData last = null;
        DocumentsWriterFieldData fp0 = fieldDataHash[hashPos];
        while(fp0 != fp) {
          last = fp0;
          fp0 = fp0.next;
        }
        assert fp0 != null;

The assert at the end is not necessary as fp0 cannot be null.  The first line in the above code guarantees that fp is not null by the time the while loop is hit.  The while loop is exited when fp0 and fp are equal.  Since fp is not null then fp0 cannot be null when the while loop is exited, thus the assert is guaranteed to never occur.

This was detected by FindBugs."
0,"Numeric range searching with large value setsI have a set of enhancements that build on the numeric sorting cache introduced
by Tim Jones and that provide integer and floating point range searches over
numeric ranges that are far too large to be implemented via the current term
range rewrite mechanism.  I'm new to Apache and trying to find out how to attach
the source files for the changes for your consideration."
1,"java.lang.ArrayIndexOutOfBoundsException while importXML in Java 6Using:
- Jackrabbit 1.3
- Java:
  java version ""1.6.0_02""
  Java(TM) SE Runtime Environment (build 1.6.0_02-b05)
  Java HotSpot(TM) Client VM (build 1.6.0_02-b05, mixed mode, sharing)

When importing attached XML, I get an exception:
Caused by: java.lang.ArrayIndexOutOfBoundsException
        at java.lang.System.arraycopy(Native Method)
        at org.apache.jackrabbit.core.xml.BufferedStringValue.append(BufferedStringValue.java:201)
        at org.apache.jackrabbit.core.xml.SysViewImportHandler.characters(SysViewImportHandler.java:187)
        at org.apache.jackrabbit.core.xml.ImportHandler.characters(ImportHandler.java:200)
        at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.characters(AbstractSAXParser.java:538)
        at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:461)
        at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:807)
        at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:737)
        at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:107)
        at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1205)
        at com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl$JAXPSAXParser.parse(SAXParserImpl.java:522)
        at javax.xml.parsers.SAXParser.parse(SAXParser.java:395)
        at org.apache.jackrabbit.core.SessionImpl.importXML(SessionImpl.java:1116)
...

If I use Java 1.5, then it works.

java version ""1.5.0_12""
Java(TM) 2 Runtime Environment, Standard Edition (build 1.5.0_12-b04)
Java HotSpot(TM) Client VM (build 1.5.0_12-b04, mixed mode, sharing)
"
1,"problem in jcr-server/project.properties dependencies?There seems to be a mismatch in the version numbers in the dependencies defined in jcr-server/project.properties. It refers to version 1.0, yet the other components are already at 1.1.

The attached patch solves the problem for me, but I don't claim to fully understand how this works...
"
0,"Add workaround for ICU bug in combination with Java7 to LuceneTestCaseThere is a bug in ICU that makes it fail to load it ULocale class in Java7: http://bugs.icu-project.org/trac/ticket/8734

The problem is caused by some new locales in Java 7, that lead to a chicken-and-egg problem in the static initializer of ULocale. It initializes its default locale from the JDK locale in a static ctor. Until the default ULocale instance is created, the default is not set in ULocale. But ULocales ctor itsself needs the default locale to fetch some ressource bundles and throws NPE.

The code in LuceneTestCase that randomizes the default locale should classload ULocale before it tries to set another random locale, using a defined, safe locale (Locale.US). Patch is easy."
0,JCR-RMI depends on commons-loggingThe commons-logging dependency in JCR-RMI should be removed.
1,"HttpURL creates wrong authority String when user info is changedWhen changing the user info on an existing HttpURL which has additional port
information, the new authority String contains a wrong hostname part: instead of
getting ""hostname:portnumber"" the string is ""hostnameportnumber"", i.e. the "":""
is missing.
Methods which needs to be changed are:

setRawPassword(...)
setRawUser(...)
setRawUserinfo(...)

(look for the line
String hostport = (_port == -1) ? hostname : hostname + _port;
)

Andreas Fnger
ESIGN Software GmbH"
0,Update LICENSE and NOTICE files to match the updated dependenciesWe've made quite a few dependency updates since Jackrabbit 1.6 and need to update the license metadata accordingly.
0,"Buffered output to socket--Posted by Slavik Markovich:

Hi all,

This is probably a known issue (but I haven't found the answer for it yet).
I'm using httpclient to post data to a remote server but as far as I can see
(using ethereal) the client is writing every line to the wire without buffering.
After examining the code, I can see that the HttpConnection class is using the
output stream received from the socket directly.
Is there a reason for the direct writing?
This is a problem for me 'cause the remote server sets a very low timeout and
returns a bad request response after receiving the request line (without any
other header line or request body).

Can I easily add a buffered behavior to the http client?

10x"
0,"InstantiatedIndex - faster but memory consuming indexRepresented as a coupled graph of class instances, this all-in-memory index store implementation delivers search results up to a 100 times faster than the file-centric RAMDirectory at the cost of greater RAM consumption.

Performance seems to be a little bit better than log2n (binary search). No real data on that, just my eyes.

Populated with a single document InstantiatedIndex is almost, but not quite, as fast as MemoryIndex.    

At 20,000 document 10-50 characters long InstantiatedIndex outperforms RAMDirectory some 30x,
15x at 100 documents of 2000 charachters length,
and is linear to RAMDirectory at 10,000 documents of 2000 characters length.

Mileage may vary depending on term saturation.


"
1,"Wrong cookie matching port number reported when using a proxyFollowing the example given in https://issues.apache.org/jira/browse/HTTPCLIENT-852 and the route HttpRoute[{}->http://xyz.webfactional.com:7295->http://www.seoconsultants.com]:

one of the new cookies is reported to be added as:

[java] 2009/05/28 19:58:23:398 CEST [DEBUG] RequestAddCookies - Cookie [version: 0][name: ASPSESSIONIDCSARBQBA][value: MAMPAMKCBDJJFKNAAPKPMDAA][domain: www.seoconsultants.com][path: /][expiry: null] match [www.seoconsultants.com:7295/]

whereas it should be:

[java] 2009/05/28 19:57:46:667 CEST [DEBUG] RequestAddCookies - Cookie [version: 0][name: ASPSESSIONIDCSARBQBA][value: AAMPAMKCMBINHNEHPFEBFADA][domain: www.seoconsultants.com][path: /][expiry: null] match [www.seoconsultants.com:80/]

i.e. the same as without using a proxy. 7295 is the port number used to access the proxy. The target domain www.seoconsultants.com is accessed through the regular HTTP port number 80, thus the cookie matching should also refer to port 80 and not the proxy's port."
1,"SegmentReader.doCommit should be sync'd; norms methods need not be sync'dI fixed the failure in TestNRTThreads, but in the process tripped an assert because SegmentReader.doCommit isn't sync'd.

So I sync'd it, but I don't think the norms APIs need to be sync'd -- we populate norms up front and then never change them.  Un-sync'ing them is important so that in the NRT case calling IW.commit doesn't block searches trying to pull norms.

Also some small code refactoring."
0,"IndexReader.cloneBased on discussion http://www.nabble.com/IndexReader.reopen-issue-td18070256.html.  The problem is reopen returns the same reader if there are no changes, so if docs are deleted from the new reader, they are also reflected in the previous reader which is not always desired behavior."
1,"NullPointerException thrown by equals method in SpanOrQueryPart of our code utilizes the equals method in SpanOrQuery and, in certain cases (details to follow, if necessary), a NullPointerException gets thrown as a result of the String ""field"" being null.  After applying the following patch, the problem disappeared:

Index: src/java/org/apache/lucene/search/spans/SpanOrQuery.java
===================================================================
--- src/java/org/apache/lucene/search/spans/SpanOrQuery.java    (revision 465065)
+++ src/java/org/apache/lucene/search/spans/SpanOrQuery.java    (working copy)
@@ -121,7 +121,8 @@
     final SpanOrQuery that = (SpanOrQuery) o;

     if (!clauses.equals(that.clauses)) return false;
-    if (!field.equals(that.field)) return false;
+    if (field != null && !field.equals(that.field)) return false;
+    if (field == null && that.field != null) return false;

     return getBoost() == that.getBoost();
   }

"
1,"XA Transaction RecoveryIf i add a node to the repository i get a XAException because i run into a Timeout ... 
I see the Warn Message: Transaction rolled back because timeout expired.
The default Timeout is set to 5 sec and i dont know how to set it to a higher value
The Problem is if i restart my server websphere has a RecoveryManager and he try to recover this Transaction
and then i get a NullpointerException in JCAManagedConnectionFactory. createManagedConnection beacuse the given 
ConnectionRequestInfo is null.
So i dont know why the RecoveryManager tries to recover the Transaction ? The only solution for me is to delete the Tran-Log Files wich keep Websphere to recvoer
XA Trasnactions.
"
0,"Add some more constants for newer Java versions to Constants.class, remove outdated ones.Preparation for LUCENE-3235:
This adds constants to quickly detect Java6 and Java7 to Constants.java. It also deprecated and removes the outdated historical Java versions."
0,"unit tests should use private directoriesThis only affects our unit tests...

I run ""ant test"" and ""ant test-tag"" concurrently, but some tests have false failures (eg TestPayloads) because they use a fixed test directory in the filesystem for testing.

I've added a simple method to _TestUtil to get a temp dir, and switched over those tests that I've hit false failures on."
0,"Open IndexWriter API to allow custom MergeScheduler implementationIndexWriter's getNextMerge() and merge(OneMerge) are package-private, which makes it impossible for someone to implement his own MergeScheduler. We should open up these API, as well as any other that can be useful for custom MS implementations."
0,"TimeoutHandler visitor should be extracted into a dedicated classThis is a minor problem that I've stumbled upon when looking at a memory leak.
It seems that the TimeoutHandler thread runs each second and it uses a custom ElementVisitor to do its business. By creating a new instance of ElementVisitor each second this creates some garbage that could be avoided by using a predefined class.

Short story: during a unit test it creates 3x instances each second that have 16 bytes each. Having a dedicated visitor class creates just 3 instances for the lifespan of the repository."
0,Use creation tick instead of weak references in DocNumberCacheThis avoids the need to keep two maps in CachingMultiIndexReader. And I guess the fewer weak references the better...
0,NodeIndexer creates unnecessary string representation of Name valueNodeIndexer.addNameValue() calls Name.toString() but never uses it.
0,"Tika-based type detection in jcr-serverAs discussed on dev@, I'd like to make the jackrabbit-jcr-server component use Apache Tika for automatic media type detection."
1,The move method doesn't remove the source nodeHere is a small unit test that demonstrate that the method move doesn't remove the source node. 
0,"add one setter for start and end offset to OffsetAttributeadd OffsetAttribute. setOffset(startOffset, endOffset);

trivial change, no JUnit needed

Changed CharTokenizer to use it"
0,"Make EasySimilarityProvider a full-fledged class The {{EasySimilarityProvider}} in {{TestEasySimilarity}} would be a good candidate for a full-fledged class. Both {{DefaultSimilarity}} and {{BM25Similarity}} have their own providers, which are effectively the same,so I don't see why we couldn't add one generic provider for convenience."
0,"Allow to pass an instance of RateLimiter to FSDirectory allowing to rate limit merge IO across several directories / instancesThis can come in handy when running several Lucene indices in the same VM, and wishing to rate limit merge across all of them."
0,"TCK: PredefinedNodeTypeTest does not allow additions to the predefined node types hierarchyAs explained in section 6.7.22.2 (page 147) of the JSR 170 specification, an implementation is allowed to customize a predefined noe type definition with additional supertypes. The tests in PredefinedNodeTypeTest do not account for that and expect an exact match with the predefined node types."
1,"TestIndexWriter.testThreadInterruptDeadlock failed (can't reproduce)trunk: r1134163 

ran it a few times with tests.iter=200 and couldn't reproduce, but i believe you like an issue anyway.

{code}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter
    [junit] Testcase: testThreadInterruptDeadlock(org.apache.lucene.index.TestIndexWriter):     FAILED
    [junit]
    [junit] junit.framework.AssertionFailedError:
    [junit]     at org.apache.lucene.index.TestIndexWriter.testThreadInterruptDeadlock(TestIndexWriter.java:1203)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1403)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1321)
    [junit]
    [junit]
    [junit] Tests run: 40, Failures: 1, Errors: 0, Time elapsed: 23.79 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] CheckIndex failed
    [junit] ERROR: could not read any segments file in directory
    [junit] java.io.FileNotFoundException: segments_2w
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:407)
    [junit]     at org.apache.lucene.index.codecs.DefaultSegmentInfosReader.openInput(DefaultSegmentInfosReader.java:112)
    [junit]     at org.apache.lucene.index.codecs.DefaultSegmentInfosReader.read(DefaultSegmentInfosReader.java:45)
    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:257)
    [junit]     at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:287)
    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:698)
    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:533)
    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:283)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:311)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:154)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:144)
    [junit]     at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:1154)
    [junit]
    [junit] CheckIndex FAILED: unexpected exception
    [junit] java.lang.RuntimeException: CheckIndex failed
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:158)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:144)
    [junit]     at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:1154)
    [junit] IndexReader.open FAILED: unexpected exception
    [junit] java.io.FileNotFoundException: segments_2w
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:407)
    [junit]     at org.apache.lucene.index.codecs.DefaultSegmentInfosReader.openInput(DefaultSegmentInfosReader.java:112)
    [junit]     at org.apache.lucene.index.codecs.DefaultSegmentInfosReader.read(DefaultSegmentInfosReader.java:45)
    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:257)
    [junit]     at org.apache.lucene.index.DirectoryReader$1.doBody(DirectoryReader.java:88)
    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:698)
    [junit]     at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:84)
    [junit]     at org.apache.lucene.index.IndexReader.open(IndexReader.java:500)
    [junit]     at org.apache.lucene.index.IndexReader.open(IndexReader.java:293)
    [junit]     at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:1161)

    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testThreadInterruptDeadlock -Dtests.seed=6733070832417768606:3130345095020099096
    [junit] NOTE: test params are: codec=RandomCodecProvider: {=MockRandom, f6=SimpleText, f7=MockRandom, f8=MockSep, f9=Standard, f1=SimpleText, f0=Standard, f3=Standard, f2=MockSep, f5=Pulsing(freqCutoff=12),
 f4=MockFixedIntBlock(blockSize=552), c=MockVariableIntBlock(baseBlockSize=43), d9=MockVariableIntBlock(baseBlockSize=43), d8=MockRandom, d5=MockSep, d4=Pulsing(freqCutoff=12), d7=MockFixedIntBlock(blockSize=55
2), d6=MockVariableIntBlock(baseBlockSize=43), d25=MockSep, d0=MockVariableIntBlock(baseBlockSize=43), c29=MockFixedIntBlock(blockSize=552), d24=Pulsing(freqCutoff=12), d1=MockFixedIntBlock(blockSize=552), c28=
Standard, d23=SimpleText, d2=SimpleText, c27=MockSep, d22=Standard, d3=MockRandom, d21=MockRandom, d20=SimpleText, c22=MockSep, c21=Pulsing(freqCutoff=12), c20=SimpleText, d29=SimpleText, c26=MockVariableIntBlo
ck(baseBlockSize=43), d28=Standard, c25=MockRandom, d27=MockVariableIntBlock(baseBlockSize=43), c24=Pulsing(freqCutoff=12), d26=MockRandom, c23=MockFixedIntBlock(blockSize=552), e9=Pulsing(freqCutoff=12), e8=St
andard, e7=MockSep, e6=MockRandom, e5=SimpleText, c17=MockFixedIntBlock(blockSize=552), e3=MockFixedIntBlock(blockSize=552), d12=Pulsing(freqCutoff=12), c16=MockVariableIntBlock(baseBlockSize=43), e4=Pulsing(fr
eqCutoff=12), d11=MockFixedIntBlock(blockSize=552), c19=MockRandom, e1=MockSep, d14=MockVariableIntBlock(baseBlockSize=43), c18=SimpleText, e2=Standard, d13=MockRandom, e0=SimpleText, d10=MockSep, d19=MockVaria
bleIntBlock(baseBlockSize=43), c11=MockVariableIntBlock(baseBlockSize=43), c10=MockRandom, d16=Standard, c13=MockRandom, c12=SimpleText, d15=MockSep, d18=Pulsing(freqCutoff=12), c15=Standard, d17=MockFixedIntBl
ock(blockSize=552), c14=MockSep, b3=Standard, b2=MockSep, b5=Pulsing(freqCutoff=12), b4=MockFixedIntBlock(blockSize=552), b7=MockFixedIntBlock(blockSize=552), b6=MockVariableIntBlock(baseBlockSize=43), d50=Mock
Random, b9=MockRandom, b8=SimpleText, d43=SimpleText, d42=Standard, d41=MockVariableIntBlock(baseBlockSize=43), d40=MockRandom, d47=Pulsing(freqCutoff=12), d46=MockFixedIntBlock(blockSize=552), b0=MockRandom, d
45=Standard, b1=MockVariableIntBlock(baseBlockSize=43), d44=MockSep, d49=MockRandom, d48=SimpleText, c6=SimpleText, c5=Standard, c4=MockVariableIntBlock(baseBlockSize=43), c3=MockRandom, c9=MockFixedIntBlock(bl
ockSize=552), c8=Standard, c7=MockSep, d30=Standard, d32=Pulsing(freqCutoff=12), d31=MockFixedIntBlock(blockSize=552), c1=Pulsing(freqCutoff=12), d34=MockFixedIntBlock(blockSize=552), c2=MockSep, d33=MockVariab
leIntBlock(baseBlockSize=43), d36=MockRandom, c0=SimpleText, d35=SimpleText, d38=MockSep, d37=Pulsing(freqCutoff=12), d39=MockVariableIntBlock(baseBlockSize=43), e92=MockFixedIntBlock(blockSize=552), e93=Pulsin
g(freqCutoff=12), e90=MockSep, e91=Standard, e89=Standard, e88=MockVariableIntBlock(baseBlockSize=43), e87=MockRandom, e86=MockFixedIntBlock(blockSize=552), e85=MockVariableIntBlock(baseBlockSize=43), e84=MockS
ep, e83=Pulsing(freqCutoff=12), e80=MockFixedIntBlock(blockSize=552), e81=SimpleText, e82=MockRandom, e77=Standard, e76=MockSep, e79=Pulsing(freqCutoff=12), e78=MockFixedIntBlock(blockSize=552), e73=MockVariabl
eIntBlock(baseBlockSize=43), e72=MockRandom, e75=SimpleText, e74=Standard, binary=MockSep, f98=MockRandom, f97=SimpleText, f99=MockSep, f94=Pulsing(freqCutoff=12), f93=MockFixedIntBlock(blockSize=552), f96=Mock
VariableIntBlock(baseBlockSize=43), f95=MockRandom, e95=MockRandom, e94=SimpleText, e97=Standard, e96=MockSep, e99=MockSep, e98=Pulsing(freqCutoff=12), id=Standard, f34=SimpleText, f33=Standard, f32=MockVariabl
eIntBlock(baseBlockSize=43), f31=MockRandom, f30=MockFixedIntBlock(blockSize=552), f39=SimpleText, f38=MockVariableIntBlock(baseBlockSize=43), f37=MockRandom, f36=Pulsing(freqCutoff=12), f35=MockFixedIntBlock(b
lockSize=552), f43=MockSep, f42=Pulsing(freqCutoff=12), f45=MockFixedIntBlock(blockSize=552), f44=MockVariableIntBlock(baseBlockSize=43), f41=Standard, f40=MockSep, f47=SimpleText, f46=Standard, f49=MockSep, f4
8=Pulsing(freqCutoff=12), content=Standard, e19=Standard, e18=MockSep, e17=SimpleText, f12=MockRandom, e16=Standard, f11=SimpleText, f10=MockFixedIntBlock(blockSize=552), e15=MockVariableIntBlock(baseBlockSize=
43), e14=MockRandom, f16=MockFixedIntBlock(blockSize=552), e13=MockSep, e12=Pulsing(freqCutoff=12), f15=MockVariableIntBlock(baseBlockSize=43), e11=SimpleText, f14=MockSep, e10=Standard, f13=Pulsing(freqCutoff=
12), f19=Standard, f18=MockVariableIntBlock(baseBlockSize=43), f17=MockRandom, e29=MockRandom, e26=MockSep, f21=Standard, e25=Pulsing(freqCutoff=12), f20=MockSep, e28=MockFixedIntBlock(blockSize=552), f23=Pulsi
ng(freqCutoff=12), e27=MockVariableIntBlock(baseBlockSize=43), f22=MockFixedIntBlock(blockSize=552), f25=MockRandom, e22=MockFixedIntBlock(blockSize=552), f24=SimpleText, e21=MockVariableIntBlock(baseBlockSize=
43), f27=Standard, e24=MockRandom, f26=MockSep, e23=SimpleText, f29=MockSep, f28=Pulsing(freqCutoff=12), e20=Pulsing(freqCutoff=12), field=MockSep, string=MockVariableIntBlock(baseBlockSize=43), e30=MockFixedIn
tBlock(blockSize=552), e31=Pulsing(freqCutoff=12), a98=MockSep, e34=SimpleText, a99=Standard, e35=MockRandom, f79=MockSep, e32=MockVariableIntBlock(baseBlockSize=43), e33=MockFixedIntBlock(blockSize=552), b97=M
ockRandom, f77=MockRandom, e38=MockVariableIntBlock(baseBlockSize=43), b98=MockVariableIntBlock(baseBlockSize=43), f78=MockVariableIntBlock(baseBlockSize=43), e39=MockFixedIntBlock(blockSize=552), b99=Standard,
 f75=MockFixedIntBlock(blockSize=552), e36=Pulsing(freqCutoff=12), f76=Pulsing(freqCutoff=12), e37=MockSep, f73=Pulsing(freqCutoff=12), f74=MockSep, f71=Standard, f72=SimpleText, f81=Standard, f80=MockSep, e40=
MockVariableIntBlock(baseBlockSize=43), e41=Standard, e42=SimpleText, e43=MockSep, e44=Standard, e45=MockFixedIntBlock(blockSize=552), e46=Pulsing(freqCutoff=12), f86=Standard, e47=SimpleText, f87=SimpleText, e
48=MockRandom, f88=Pulsing(freqCutoff=12), e49=MockSep, f89=MockSep, f82=MockVariableIntBlock(baseBlockSize=43), f83=MockFixedIntBlock(blockSize=552), f84=SimpleText, f85=MockRandom, f90=Pulsing(freqCutoff=12),
 f92=MockVariableIntBlock(baseBlockSize=43), f91=MockRandom, str=MockRandom, a76=Standard, e56=Standard, f59=Pulsing(freqCutoff=12), a77=SimpleText, e57=SimpleText, a78=Pulsing(freqCutoff=12), e54=MockRandom, f
57=Standard, a79=MockSep, e55=MockVariableIntBlock(baseBlockSize=43), f58=SimpleText, e52=MockVariableIntBlock(baseBlockSize=43), e53=MockFixedIntBlock(blockSize=552), e50=Pulsing(freqCutoff=12), e51=MockSep, f
51=MockSep, f52=Standard, f50=MockRandom, f55=MockVariableIntBlock(baseBlockSize=43), f56=MockFixedIntBlock(blockSize=552), f53=Pulsing(freqCutoff=12), e58=MockFixedIntBlock(blockSize=552), f54=MockSep, e59=Pul
sing(freqCutoff=12), a80=Pulsing(freqCutoff=12), e60=Pulsing(freqCutoff=12), a82=MockVariableIntBlock(baseBlockSize=43), a81=MockRandom, a84=MockRandom, a83=SimpleText, a86=Standard, a85=MockSep, a89=SimpleText
, f68=MockVariableIntBlock(baseBlockSize=43), e65=Pulsing(freqCutoff=12), f69=MockFixedIntBlock(blockSize=552), e66=MockSep, a87=MockVariableIntBlock(baseBlockSize=43), e67=MockVariableIntBlock(baseBlockSize=43
), a88=MockFixedIntBlock(blockSize=552), e68=MockFixedIntBlock(blockSize=552), e61=SimpleText, e62=MockRandom, e63=MockSep, e64=Standard, f60=MockFixedIntBlock(blockSize=552), f61=Pulsing(freq

Cutoff=12), f62=MockRandom, f63=MockVariableIntBlock(baseBlockSize=43), e69=Standard, f64=SimpleText, f65=MockRandom, f66=MockSep, f67=Standard, f70=MockFixedIntBlock(blockSize=552), a93=MockSep, a92=Pulsing(freqCutoff=12), a91=SimpleText, e71=SimpleText, a90=Standard, e70=Standard, a97=MockVariableIntBlock(baseBlockSize=43), a96=MockRandom, a95=Pulsing(freqCutoff=12), a94=MockFixedIntBlock(blockSize=552), c58=MockRandom, a63=MockFixedIntBlock(blockSize=552), a64=Pulsing(freqCutoff=12), c59=MockVariableIntBlock(baseBlockSize=43), c56=MockFixedIntBlock(blockSize=552), d59=MockRandom, a61=MockSep, c57=Pulsing(freqCutoff=12), a62=Standard, c54=Pulsing(freqCutoff=12), c55=MockSep, a60=SimpleText, c52=Standard, c53=SimpleText, d53=SimpleText, d54=MockRandom, d51=MockVariableIntBlock(baseBlockSize=43), d52=MockFixedIntBlock(blockSize=552), d57=Pulsing(freqCutoff=12), b62=Standard, d58=MockSep, b63=SimpleText, d55=Standard, b60=MockRandom, d56=SimpleText, b61=MockVariableIntBlock(baseBlockSize=43), b56=Standard, b55=MockSep, b54=MockRandom, b53=SimpleText, d61=MockVariableIntBlock(baseBlockSize=43), b59=MockVariableIntBlock(baseBlockSize=43), d60=MockRandom, b58=MockSep, b57=Pulsing(freqCutoff=12), c62=Standard, c61=MockSep, a59=MockVariableIntBlock(baseBlockSize=43), c60=MockRandom, a58=MockRandom, a57=MockFixedIntBlock(blockSize=552), a56=MockVariableIntBlock(baseBlockSize=43), a55=MockSep, a54=Pulsing(freqCutoff=12), a72=MockRandom, c67=Standard, a73=MockVariableIntBlock(baseBlockSize=43), c68=SimpleText, a74=Standard, c69=Pulsing(freqCutoff=12), a75=SimpleText, c63=MockVariableIntBlock(baseBlockSize=43), c64=MockFixedIntBlock(blockSize=552), a70=MockVariableIntBlock(baseBlockSize=43), c65=SimpleText, a71=MockFixedIntBlock(blockSize=552), c66=MockRandom, d62=MockSep, d63=Standard, d64=MockFixedIntBlock(blockSize=552), b70=Standard, d65=Pulsing(freqCutoff=12), b71=Pulsing(freqCutoff=12), d66=MockVariableIntBlock(baseBlockSize=43), b72=MockSep, d67=MockFixedIntBlock(blockSize=552), b73=MockVariableIntBlock(baseBlockSize=43), d68=SimpleText, b74=MockFixedIntBlock(blockSize=552), d69=MockRandom, b65=Pulsing(freqCutoff=12), b64=MockFixedIntBlock(blockSize=552), b67=MockVariableIntBlock(baseBlockSize=43), b66=MockRandom, d70=SimpleText, b69=MockRandom, b68=SimpleText, d72=MockSep, d71=Pulsing(freqCutoff=12), c71=Pulsing(freqCutoff=12), c70=MockFixedIntBlock(blockSize=552), a69=Pulsing(freqCutoff=12), c73=MockVariableIntBlock(baseBlockSize=43), c72=MockRandom, a66=MockRandom, a65=SimpleText, a68=Standard, a67=MockSep, c32=MockSep, c33=Standard, c30=SimpleText, c31=MockRandom, c36=MockVariableIntBlock(baseBlockSize=43), a41=Pulsing(freqCutoff=12), c37=MockFixedIntBlock(blockSize=552), a42=MockSep, a0=MockRandom, c34=Pulsing(freqCutoff=12), c35=MockSep, a40=SimpleText, b84=MockSep, d79=MockFixedIntBlock(blockSize=552), b85=Standard, b82=SimpleText, d77=MockSep, c38=Standard, b83=MockRandom, d78=Standard, c39=SimpleText, b80=MockRandom, d75=Standard, b81=MockVariableIntBlock(baseBlockSize=43), d76=SimpleText, d73=MockRandom, d74=MockVariableIntBlock(baseBlockSize=43), d83=MockRandom, a9=MockFixedIntBlock(blockSize=552), d82=SimpleText, d81=MockFixedIntBlock(blockSize=552), d80=MockVariableIntBlock(baseBlockSize=43), b79=MockFixedIntBlock(blockSize=552), b78=MockSep, b77=Pulsing(freqCutoff=12), b76=SimpleText, b75=Standard, a1=Pulsing(freqCutoff=12), a35=Pulsing(freqCutoff=12), a2=MockSep, a34=MockFixedIntBlock(blockSize=552), a3=MockVariableIntBlock(baseBlockSize=43), a33=Standard, a4=MockFixedIntBlock(blockSize=552), a32=MockSep, a5=MockRandom, a39=MockRandom, c40=SimpleText, a6=MockVariableIntBlock(baseBlockSize=43), a38=SimpleText, a7=Standard, a37=MockFixedIntBlock(blockSize=552), a8=SimpleText, a36=MockVariableIntBlock(baseBlockSize=43), c41=MockFixedIntBlock(blockSize=552), c42=Pulsing(freqCutoff=12), c43=MockRandom, c44=MockVariableIntBlock(baseBlockSize=43), c45=SimpleText, a50=MockVariableIntBlock(baseBlockSize=43), c46=MockRandom, a51=MockFixedIntBlock(blockSize=552), c47=MockSep, a52=SimpleText, c48=Standard, a53=MockRandom, b93=MockFixedIntBlock(blockSize=552), d88=MockRandom, c49=MockVariableIntBlock(baseBlockSize=43), b94=Pulsing(freqCutoff=12), d89=MockVariableIntBlock(baseBlockSize=43), b95=MockRandom, b96=MockVariableIntBlock(baseBlockSize=43), d84=Pulsing(freqCutoff=12), b90=SimpleText, d85=MockSep, b91=Pulsing(freqCutoff=12), d86=MockVariableIntBlock(baseBlockSize=43), b92=MockSep, d87=MockFixedIntBlock(blockSize=552), d92=Standard, d91=MockSep, d94=Pulsing(freqCutoff=12), d93=MockFixedIntBlock(blockSize=552), b87=MockFixedIntBlock(blockSize=552), b86=MockVariableIntBlock(baseBlockSize=43), d90=SimpleText, b89=MockRandom, b88=SimpleText, a44=MockVariableIntBlock(baseBlockSize=43), a43=MockRandom, a46=SimpleText, a45=Standard, a48=Standard, a47=MockSep, c51=MockFixedIntBlock(blockSize=552), a49=MockFixedIntBlock(blockSize=552), c50=MockVariableIntBlock(baseBlockSize=43), d98=MockFixedIntBlock(blockSize=552), d97=MockVariableIntBlock(baseBlockSize=43), d96=MockSep, d95=Pulsing(freqCutoff=12), d99=MockRandom, a20=MockVariableIntBlock(baseBlockSize=43), c99=SimpleText, c98=Standard, c97=MockVariableIntBlock(baseBlockSize=43), c96=MockRandom, b19=MockVariableIntBlock(baseBlockSize=43), a16=Pulsing(freqCutoff=12), a17=MockSep, b17=Pulsing(freqCutoff=12), a14=Standard, b18=MockSep, a15=SimpleText, a12=SimpleText, a13=MockRandom, a10=MockVariableIntBlock(baseBlockSize=43), a11=MockFixedIntBlock(blockSize=552), b11=MockFixedIntBlock(blockSize=552), b12=Pulsing(freqCutoff=12), b10=Standard, b15=SimpleText, b16=MockRandom, a18=MockRandom, b13=MockVariableIntBlock(baseBlockSize=43), a19=MockVariableIntBlock(baseBlockSize=43), b14=MockFixedIntBlock(blockSize=552), b30=MockRandom, a31=MockSep, a30=Pulsing(freqCutoff=12), b28=SimpleText, a25=MockVariableIntBlock(baseBlockSize=43), b29=MockRandom, a26=MockFixedIntBlock(blockSize=552), a27=SimpleText, a28=MockRandom, a21=MockSep, a22=Standard, a23=MockFixedIntBlock(blockSize=552), a24=Pulsing(freqCutoff=12), b20=MockRandom, b21=MockVariableIntBlock(baseBlockSize=43), b22=Standard, b23=SimpleText, a29=Pulsing(freqCutoff=12), b24=MockSep, b25=Standard, b26=MockFixedIntBlock(blockSize=552), b27=Pulsing(freqCutoff=12), b41=Pulsing(freqCutoff=12), b40=MockFixedIntBlock(blockSize=552), c77=MockRandom, c76=SimpleText, c75=MockFixedIntBlock(blockSize=552), c74=MockVariableIntBlock(baseBlockSize=43), c79=SimpleText, c78=Standard, c80=MockSep, c83=MockRandom, c84=MockVariableIntBlock(baseBlockSize=43), c81=MockFixedIntBlock(blockSize=552), b39=MockFixedIntBlock(blockSize=552), c82=Pulsing(freqCutoff=12), b37=Standard, b38=SimpleText, b35=MockRandom, b36=MockVariableIntBlock(baseBlockSize=43), b33=MockVariableIntBlock(baseBlockSize=43), b34=MockFixedIntBlock(blockSize=552), b31=Pulsing(freqCutoff=12), b32=MockSep, str2=MockSep, b50=MockVariableIntBlock(baseBlockSize=43), b52=SimpleText, str3=MockVariableIntBlock(baseBlockSize=43), b51=Standard, c86=Standard, tvtest=MockSep, c85=MockSep, c88=Pulsing(freqCutoff=12), c87=MockFixedIntBlock(blockSize=552), c89=MockVariableIntBlock(baseBlockSize=43), c90=SimpleText, c91=MockRandom, c92=Standard, c93=SimpleText, c94=Pulsing(freqCutoff=12), c95=MockSep, content1=MockRandom, b46=Pulsing(freqCutoff=12), b47=MockSep, content3=MockFixedIntBlock(blockSize=552), b48=MockVariableIntBlock(baseBlockSize=43), content4=MockVariableIntBlock(baseBlockSize=43), b49=MockFixedIntBlock(blockSize=552), content5=Pulsing(freqCutoff=12), b42=SimpleText, b43=MockRandom, b44=MockSep, b45=Standard}, locale=sk, timezone=America/Rainy_River
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDateTools, TestDeletionPolicy, TestDocsAndPositions, TestFlex, TestIndexReaderCloneNorms, TestIndexWriter]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=86065896,total=127401984
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.index.TestIndexWriter FAILED
{code}"
1,"IndexReader.isCurrent incorrectly returns false after writer.prepareCommit has been calledSpinoff from thread ""2 phase commit with external data"" on java-user.

The IndexReader should not see the index as changed, after a prepareCommit has been called but before commit is called."
0,"Utility to output total term frequency and df from a lucene indexThis is a pair of command line utilities that provide information on the total number of occurrences of a term in a Lucene index.  The first  takes a field name, term, and index directory and outputs the document frequency for the term and the total number of occurrences of the term in the index (i.e. the sum of the tf of the term for each document).   The second reads the index to determine the top N most frequent terms (by document frequency) and then outputs a list of those terms along with  the document frequency and the total number of occurrences of the term. Both utilities are useful for estimating the size of the term's entry in the *prx files and consequent Disk I/O demands. "
0,"Work around ThreadLocal's ""leak""Java's ThreadLocal is dangerous to use because it is able to take a
surprisingly very long time to release references to the values you
store in it.  Even when a ThreadLocal instance itself is GC'd, hard
references to the values you had stored in it are easily kept for
quite some time later.

While this is not technically a ""memory leak"", because eventually
(when the underlying Map that stores the values cleans up its ""stale""
references) the hard reference will be cleared, and GC can proceed,
its end behavior is not different from a memory leak in that under the
right situation you can easily tie up far more memory than you'd
expect, and then hit unexpected OOM error despite allocating an
extremely large heap to your JVM.

Lucene users have hit this many times.  Here's the most recent thread:

  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200809.mbox/%3C6e3ae6310809091157j7a9fe46bxcc31f6e63305fcdc%40mail.gmail.com%3E

And here's another:

  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200807.mbox/%3CF5FC94B2-E5C7-40C0-8B73-E12245B91CEE%40mikemccandless.com%3E

And then there's LUCENE-436 and LUCENE-529 at least.

A google search for ""ThreadLocal leak"" yields many compelling hits.

Sun does this for performance reasons, but I think it's a terrible
trap and we should work around it with Lucene."
1,"Missing Content-Length header causes a SocketExceptionEssentially, we have an invalid HTTP server (Stellent CMS actually and we will file a bug with them), 
which is returning headers like:

HTTP/1.1 401 Unauthorized
WWW-Authenticate: Basic ""Secure Realm""
Connection: keep-alive

Which is clearly missing the Content-Length header.  Now, previously HttpClient handled this 
perfectly by reading until the end of the connection (ie: treating it like it was a Connection: close), 
however for some reason a socket exception is being thrown and the invalid connection is added 
back into the connection pool and then every connection to the server after that thows an 
exception.

See the thread ""SocketException with invalid server"" for the full discussion of the issue.

I'll attach a patch that fixes the problem.  The biggest thing to consider is the changes to the 
duplicate Connection header test cases which resolves around the question: if Connection: keep-
alive is present but no Content-Length is provided, should the connection be closed?  The patch 
requires the answer to be yes and I really can't see any other way to do it."
0,"[PATCH] reduce duplicate conversions from OffsetCharSequence to (lower/upper) stringscode repetitively converts OffsetCharSequence to strings, and then repetitively converts to lower/upper case, when generating search terms.

Patch fixes this."
0,"Avoid element arrays in PathImplThe path handling code in spi-commons shows quite often in thread dumps and profiling results, as the current implementation does quite a bit of repetitive allocating and copying of path element arrays. We should be able to streamline and simplify the path handling code by only tracking the latest path element and a reference to the parent path. To do this efficiently we may need to adjust some of the Path and PathFactory method declarations (that currently assume element array -based paths) also in the SPI.
"
0,"Realm from authentication challenge unavailableThere is currently no way to extract the authentication realm from HttpClient 
except to extract the authentication challenge header and parse it manually.

Either the realm needs to be available to the client or a method in 
Authenticator should extract the realm from a given authentication header.

The same problems occurs with determining which type of authentication is 
being used and what other options there are (basic, digest, NTLM, others)."
1,"ItemInfoCacheImpl.getNodeInfo() and .getPropertyInfo() might not clear all relevant entriesItemInfoCacheImpl.getNodeInfo() and .getPropertyInfo() remove the retrieved entry from the cache.

since entries might be cached by id AND path, entires identified by path are not removed from the cache if they're retrieved by id."
0,"Caching in QueryHandler does not scale wellCaching in class CachingIndexReader uses too much memory. It uses around 500 bytes per node and does not use any strategy to limit the cache.

This improvement covers two goals:
- lower per-node memory cost for caching
- implement a caching strategy using e.g LRU algorithm"
0,"[PATCH] FilteredTermEnum code cleanupFilteredTermEnum's constructor takes two parameters but doesn't use them. This 
patch changes that and thus makes the code easier readable. Maybe the old 
constructor should be kept (as deprecated)? I'm not sure, this version seems 
cleaner to me."
0,Provide additional test coverage for HTTP and HTTPS over proxyHTTP and HTTPS over proxy test coverage is still insufficient
0,"[PATCH] XPathQueryBuilder reports misleading column numbers for faulty queriesXPathQueryBuilder returns an error string with the column offset where a parsing error occurred. Unfortunately this value is difficult to correlate to the users query string, as XPathQueryBuilder embellishes the query by doing

statement = ""for $v in "" + statement + "" return $v"";

This patch appends the modified statement to the query message so that the user can get the real position of the error."
1,"JVM bug 4949631 causes BufferOverflowException in HttpMethodBase.getResponseBodyAsStringava.nio.BufferOverflowException
        at java.nio.charset.CoderResult.throwException(CoderResult.java:259)
        at java.lang.StringCoding$CharsetSD.decode(StringCoding.java:188)
        at java.lang.StringCoding.decode(StringCoding.java:224)
        at java.lang.String.<init>(String.java:320)
        at
org.apache.commons.httpclient.HttpConstants.getContentString(HttpConstants.java:199)
        at
org.apache.commons.httpclient.HttpConstants.getContentString(HttpConstants.java:233)
        at
org.apache.commons.httpclient.HttpMethodBase.getResponseBodyAsString(HttpMethodBase.java:735)


This seems to be caused by a known JVM bug:
http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4949631

Strings over 16Mb can cause the problem.   Some workarounds are listed, the
essence being to split the string and call getBytes on each piece and reassemble
with a ByteBuffer."
0,"HttpState#PREEMPTIVE_PROPERTY removed.Our code no longer compiles as HttpState#PREEMPTIVE_PROPERTY has been removed.
Our code compiles with 2.0.1.

See: 
http://jakarta.apache.org/commons/httpclient/apidocs/org/apache/commons/httpclient/HttpState.html#PREEMPTIVE_PROPERTY"
0,"Setup nightly builds for JackrabbitOnce the Jackrabbit zone is created (see INFRA-1008), setup nightly Jackrabbit builds as discussed in http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/9190/.
"
0,"Contrib Analyzer Setters should be deprecated and replace with ctor argumentsSome analyzers in contrib provide setters for stopword / stem exclusion sets / hashtables etc. Those setters should be deprecated as they yield unexpected behaviour. The way they work is they set the reusable token stream instance to null in a thread local cache which only affects the tokenstream in the current thread. Analyzers itself should be immutable except of the threadlocal. 

will attach a patch soon."
1,"norms reading fails with FileNotFound in exceptional caseIf we can't get to the bottom of this, we can always add the fileExists check back...

{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriterExceptions
    [junit] Testcase: testRandomExceptionsThreads(org.apache.lucene.index.TestIndexWriterExceptions):	Caused an ERROR
    [junit] No sub-file with id _nrm.cfs found (fileName=_19_nrm.cfs files: [.fdt, .fnm, .per, .fdx])
    [junit] java.io.FileNotFoundException: No sub-file with id _nrm.cfs found (fileName=_19_nrm.cfs files: [.fdt, .fnm, .per, .fdx])
    [junit] 	at org.apache.lucene.store.CompoundFileDirectory.createSlicer(CompoundFileDirectory.java:313)
    [junit] 	at org.apache.lucene.store.CompoundFileDirectory.<init>(CompoundFileDirectory.java:65)
    [junit] 	at org.apache.lucene.codecs.lucene40.Lucene40DocValuesProducer.<init>(Lucene40DocValuesProducer.java:48)
    [junit] 	at org.apache.lucene.codecs.lucene40.Lucene40NormsFormat$Lucene40NormsDocValuesProducer.<init>(Lucene40NormsFormat.java:70)
    [junit] 	at org.apache.lucene.codecs.lucene40.Lucene40NormsFormat.docsProducer(Lucene40NormsFormat.java:49)
    [junit] 	at org.apache.lucene.codecs.lucene40.Lucene40NormsFormat.docsProducer(Lucene40NormsFormat.java:62)
    [junit] 	at org.apache.lucene.index.SegmentCoreReaders.<init>(SegmentCoreReaders.java:122)
    [junit] 	at org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:54)
    [junit] 	at org.apache.lucene.index.DirectoryReader$1.doBody(DirectoryReader.java:65)
    [junit] 	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:660)
    [junit] 	at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:55)
    [junit] 	at org.apache.lucene.index.IndexReader.open(IndexReader.java:242)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:304)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:530)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit] 
    [junit] 
    [junit] Tests run: 22, Failures: 0, Errors: 1, Time elapsed: 3.439 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testRandomExceptionsThreads -Dtests.seed=-4ea45cb40d17460b:-459bfb455a2351b9:1abd8f0f3a0611b9 -Dargs=""-Dfile.encoding=UTF-8""
    [junit] NOTE: test params are: codec=Lucene40: {field=MockVariableIntBlock(baseBlockSize=31), id=PostingsFormat(name=NestedPulsing), content=Pulsing40(freqCutoff=2 minBlockSize=58 maxBlockSize=186), contents=MockVariableIntBlock(baseBlockSize=31), content1=MockVariableIntBlock(baseBlockSize=31), content2=PostingsFormat(name=MockSep), content4=Pulsing40(freqCutoff=2 minBlockSize=58 maxBlockSize=186), content5=MockFixedIntBlock(blockSize=964), content6=PostingsFormat(name=Memory), content7=PostingsFormat(name=MockRandom), crash=PostingsFormat(name=NestedPulsing), subid=PostingsFormat(name=NestedPulsing)}, sim=RandomSimilarityProvider(queryNorm=false,coord=true): {other=DFR GB3(800.0), contents=IB SPL-L3(800.0), content=DFR GL3(800.0), id=DFR I(F)L1, field=IB LL-DZ(0.3), content1=DFR I(ne)BZ(0.3), content2=DFR I(n)3(800.0), content3=DFR GZ(0.3), content4=DFR I(ne)B2, content5=IB LL-L3(800.0), content6=IB SPL-D2, crash=DFR I(F)3(800.0), content7=DFR I(F)B3(800.0), subid=IB LL-L1}, locale=de_CH, timezone=Canada/Saskatchewan
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestAssertions, TestNumericTokenStream, TestSimpleAttributeImpl, TestImpersonation, TestPulsingReuse, TestDocument, TestAddIndexes, TestAtomicUpdate, TestByteSlices, TestCheckIndex, TestConcurrentMergeScheduler, TestConsistentFieldNumbers, TestCrashCausesCorruptIndex, TestDocCount, TestDocumentWriter, TestFlex, TestForceMergeForever, TestIndexInput, TestIndexReader, TestIndexWriterConfig, TestIndexWriterExceptions]
    [junit] NOTE: Linux 3.0.0-14-generic amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=186661872,total=245104640
{noformat}
"
0,"[PATCH] new method: Document.remove()Here's a patch that adds a remove() method to the Document class (+test case). This 
is very useful if you have converter classes that return a Lucene Document object but 
you need to make changes to that object. 
 
In my case, I wanted to index PDF files that were saved as BLOBs in a database. The 
files need to be saved to a temporary file and that file name is given to the PDF 
converter class. The PDF converter then saves the name of the temporary file name 
as the file name, which doesn't make sense. So my code needs to remove the 
'filename' field and re-add it, this time with the columns primary ID. This is only possible 
with the attached patch."
0,JSR 283: Shareable nodes support in query
0,"Document number integrity merge policyThis patch allows for document numbers stays the same even after merge of segments with deletions.

Consumer needs to do this:
indexWriter.setSkipMergingDeletedDocuments(false);

The effect will be that deleted documents are replaced by a new Document() in the merged segment, but not marked as deleted. This should probably be some policy thingy that allows for different solutions such as keeping the old document, et c.

Also see http://www.nabble.com/optimization-behaviour-tf3723327.html#a10418880
"
0,"Implement search facility for users and groupsImplement a search facility for users and groups supporting:
- search for users and/or groups with a certain property (value) either directly on the user/group node or on any of its sub nodes
- full text search on user and/or group nodes and its sub nodes
- inclusion/exclusion based on group membership: i.e. restricting search to members of a group or to groups with a certain member
- ordering 
- paging"
0,"Upgrade to Tika 0.8Apache Tika version 0.8 is now available, and we should upgrade to benefit from the various fixes and improvements included in that version."
1,"org.apache.lucene.search.BooleanQuery$TooManyClauses when using '>' operatorwhen using a query with a '>' operator, the query engine does not scale with number of matching properties and a org.apache.lucene.search.BooleanQuery$TooManyClauses exception is thrown"
1,SQL2 ISDESCENDANTNODE can throw BooleanQuery#TooManyClauses if there are too many matching child nodesRunning a query that has a ISDESCENDANTNODE clause can easily go over the max clause limit from lucene's BooleanQuery when there's a bigger hierarchy involved.
0,"Add support for query result highlightingHighlighting matches in a query result list is regularly needed for an application. The query languages should support a pseudo property or function that allows one to retrieve text fragments with highlighted matches from the content of the matching node.

To support this feature the following enhancements are required:
- define a pseudo property or function that returns the text excerpt and can be used in the select clause
- the index needs to *store* the original text it used when the node was indexed. this also includes extracted text from binary properties.
- text fragments must be created based on the original text, the query and index information"
1,"Bundle Persistence Manager error - failing to read bundle the first timeCode:
NodeIterator entiter = null;
Node root = null, contNode = null, entsNode = null;

try
{
    root = session.getRootNode();
    contNode = root.getNode(""sr:cont"");
    entsNode = contNode.getNode(""sr:ents"");
    entiter = entsNode.getNodes();
}
catch (Exception e)
{
    logger.error(""Getting ents nodes"", e);
}

Output:
12359 [http-8080-Processor24] ERROR org.apache.jackrabbit.core.persistence.bundle.BundleDbPersistenceManager - failed to read bundle: c3a09c19-cc6b-45bd-a42e-c4c925b67d02: java.io.IOException: ERROR 40XD0: Container has been closed
12375 [http-8080-Processor24] ERROR com.taxila.editor.sm.RepoOperations - Getting ents nodes
javax.jcr.PathNotFoundException: sr:ents
    at org.apache.jackrabbit.core.NodeImpl.getNode(NodeImpl.java:2435)
    at com.taxila.editor.sm.RepoOperations.getEntityNodes (RepoOperations.java:4583)
    at com.taxila.editor.sm.RepoOperations.displayEntities(RepoOperations.java:4159)
    at com.taxila.editor.sm.RepoOperations.displayEntities(RepoOperations.java:4114)
    at com.taxila.editor.em.um.MainEntityForm.reset (MainEntityForm.java:215)
    at org.apache.struts.taglib.html.FormTag.doStartTag(FormTag.java:640)
    at org.apache.jsp.pages.jsp.entity.MainEntity_jsp._jspService(MainEntity_jsp.java:414)
    at org.apache.jasper.runtime.HttpJspBase.service (HttpJspBase.java:97)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
    at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:332)
    at org.apache.jasper.servlet.JspServlet.serviceJspFile (JspServlet.java:314)
    at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:264)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter (ApplicationFilterChain.java:252)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:173)
    at org.apache.catalina.core.ApplicationDispatcher.invoke(ApplicationDispatcher.java :672)
    at org.apache.catalina.core.ApplicationDispatcher.processRequest(ApplicationDispatcher.java:463)
    at org.apache.catalina.core.ApplicationDispatcher.doForward(ApplicationDispatcher.java:398)
    at org.apache.catalina.core.ApplicationDispatcher.forward (ApplicationDispatcher.java:301)
    at org.apache.struts.action.RequestProcessor.doForward(RequestProcessor.java:1014)
    at org.apache.struts.action.RequestProcessor.processForwardConfig(RequestProcessor.java:417)
    at org.apache.struts.action.RequestProcessor.processActionForward(RequestProcessor.java:390)
    at org.apache.struts.action.RequestProcessor.process(RequestProcessor.java:271)
    at org.apache.struts.action.ActionServlet.process (ActionServlet.java:1292)
    at org.apache.struts.action.ActionServlet.doPost(ActionServlet.java:510)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:709)
    at javax.servlet.http.HttpServlet.service (HttpServlet.java:802)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:252)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java :173)
    at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:213)
    at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:178)
    at org.apache.catalina.core.StandardHostValve.invoke (StandardHostValve.java:126)
    at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:105)
    at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:107)
    at org.apache.catalina.connector.CoyoteAdapter.service (CoyoteAdapter.java:148)
    at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:869)
    at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.processConnection(Http11BaseProtocol.java :664)
    at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpoint.java:527)
    at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFollowerWorkerThread.java:80)
    at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run (ThreadPool.java:684)
    at java.lang.Thread.run(Unknown Source)

On the other hand if I do this:
Code:
try
{
    root = session.getRootNode ();
    contNode = root.getNode(""sr:cont"");
    entsNode = contNode.getNode(""sr:ents"");
    entiter = entsNode.getNodes();
}
catch (Exception e)
{
    logger.error(""Getting ents nodes"", e);
    try
    {
        entsNode = contNode.getNode(""sr:ents"");
        entiter = entsNode.getNodes();
    }
    catch (Exception e1)
    {
        e1.printStackTrace();
    }
}

Output:
The first error as in the previous case comes, but the second execution of the entsNode = contNode.getNode(""sr:ents""); statement returns the right node, and hence the iterator."
1,"Not configuring the adminId, anonymousId, or defaultuserId causes login module to ignore credentialsUsing the DefaultLoginModule, DefaultAccessManager, and DefaultSecurityManager and calling Repository.login(Credentials) causes the following stack trace to be thrown.  

javax.jcr.LoginException: LoginModule ignored Credentials: LoginModule ignored Credentials: LoginModule ignored Credentials
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1353)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:53)
	at com.cerner.system.configuration.repository.jcr.JackrabbitTest.testLoginWithCredentials(JackrabbitTest.java:23)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:59)
	at org.junit.internal.runners.MethodRoadie.runTestMethod(MethodRoadie.java:98)
	at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:79)
	at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:87)
	at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77)
	at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42)
	at org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88)
	at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51)
	at org.junit.internal.runners.JUnit4ClassRunner$1.run(JUnit4ClassRunner.java:44)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37)
	at org.junit.internal.runners.JUnit4ClassRunner.run(JUnit4ClassRunner.java:42)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:45)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: javax.security.auth.login.FailedLoginException: LoginModule ignored Credentials
	at org.apache.jackrabbit.core.security.authentication.LocalAuthContext.login(LocalAuthContext.java:73)
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1346)
	... 24 more
javax.security.auth.login.FailedLoginException: LoginModule ignored Credentials
	at org.apache.jackrabbit.core.security.authentication.LocalAuthContext.login(LocalAuthContext.java:73)
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1346)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:53)
	at com.cerner.system.configuration.repository.jcr.JackrabbitTest.testLoginWithCredentials(JackrabbitTest.java:23)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:59)
	at org.junit.internal.runners.MethodRoadie.runTestMethod(MethodRoadie.java:98)
	at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:79)
	at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:87)
	at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77)
	at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42)
	at org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88)
	at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51)
	at org.junit.internal.runners.JUnit4ClassRunner$1.run(JUnit4ClassRunner.java:44)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37)
	at org.junit.internal.runners.JUnit4ClassRunner.run(JUnit4ClassRunner.java:42)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:45)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)

A testcase and repository.xml file will be attached shortly."
0,"Directory createOutput and openInput should take an IOContextToday for merging we pass down a larger readBufferSize than for searching because we get better performance.

I think we should generalize this to a class (IOContext), which would hold the buffer size, but then could hold other flags like DIRECT (bypass OS's buffer cache), SEQUENTIAL, etc.

Then, we can make the DirectIOLinuxDirectory fully usable because we would only use DIRECT/SEQUENTIAL during merging.

This will require fixing how IW pools readers, so that a reader opened for merging is not then used for searching, and vice/versa.  Really, it's only all the open file handles that need to be different -- we could in theory share del docs, norms, etc, if that were somehow possible."
0,"Transfer-Encoding: identity not supported + possible patchIn HttpMethodBase.readResponseBody only chunked transfer encoding is 
supported.  Some proxy servers like Privoxy, etc send a Transfer-Encoding: 
identity header and HttpClient fails quietly and returns a null result input 
stream.  At line 2037 in HttpMethodBase.java revision 1.160 I inserted the 
following two lines and it appeared to work fine:

} else if (""identity"".equalsIgnoreCase(transferEncodingHeader.getValue())) {
   result = is;

I think it should at least throw an exception or do something when it 
encounters an unsupported Transfer-Encoding instead of returning a null input 
stream."
0,"CheckPermissionTest-testCheckPermission() doesn't allow config of node type to be createdCheckPermissionTest-testCheckPermission() doesn't allow configuration of node type to be created. Proposal to re-use the testNodeType config property.
"
0,"TCK: observation tests are too restrictiveThe basic sequence in all observation tests is:

1) add listener
2) modify workspace
3) remove listener
4) wait for events on listener

This sequence forces an implementation to maintain a logical order for listener registrations and content changes. In the light of the asynchronous nature of observation events this seems too restrictive for certain implementations.

The sequence should be changed to:

1) add listener
2) modify workspace
3) wait for events on listener
4) remove listener

Which is also more intuitive from a user perspective."
1,"errors in text filters can cause indexing to fail without warning the clienti've opened this issue to track the discussion at <http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/5086>. briefly, exceptions thrown by text filters are logged and swallowed by jackrabbit; there's no way for a text filter to signal to the jcr client that indexing failed.

some solutions have been proposed, including throwing an unchecked exception, which doesn't allow jackrabbit to maintain transactional integrity, and giving filters veto power over the observed repository operation. depending on the difficulty of the solution that is eventually determined to be correct, it may be sufficient for 1.0 to document the issue and perhaps improve the warning/error logging.
"
0,"ShingleFilter improvementsShingleFilter should allow configuration of minimum shingle size (in addition to maximum shingle size), so that it's possible to (e.g.) output only trigrams instead of bigrams mixed with trigrams.  The token separator used in composing shingles should be configurable too."
0,workspace-wide default for lock timeoutThere should be a way to define a workspace-wide default for JCR lock timeouts (in case the code creating the lock did not specify one).
0,"Reduce memory usage of ParentAxisScorerThe ParentAxisScorer keeps a map of non-default scores while it calculates the parent matches of the context scorer. In most cases the scores are not equal to the default score, but still may be all the same.

The scorer should therefore use the first score value as the default instead of the currently used 1.0f."
0,"add checks to MockTokenizer to enforce proper consumptionwe can enforce things like consumer properly iterates through tokenstream lifeycle
via MockTokenizer. this could catch bugs in consumers that don't call reset(), etc."
1,"Jcr-Server: BasicCredentialsProviderTest throws NPE if defaultAuthHeader init param misses the passwordissue reported by dominique jaeggi:

a missing-auth-header init param that has the form ""uid"" instead of ""uid:pw"" or ""uid:"" results in NPE upon SimpleCredentials creation.



"
1,"Query Builder and jcr:deref problem. Can't add predicate after jcr:derefCannot add a predicate (like [@property = 'value'] after a jcr:deref function.
The query builder throws an ""InvalidQueryException: Unsupported location for jcr:deref()"".

So for example, the query :

//element(*,nt:category)[@member]/jcr:deref(@member, '*')[@property='value'] 

is invalid and it should be valid.

"
1,"Connection with the proxy is not reopened if an proxy auth failure occurs while SSL tunnel is being establishedConnection with the proxy is not reopened if an proxy auth failure occurs while
SSL tunnel is being established.

This problem has been reported by on the httpclient-user by Gebhard Gaukler
<gebhard.gaukler at db.com>.

My bad.

Oleg"
0,"IndexReader's add/removeCloseListener should not use ConcurrentHashMap, just a synchronized setThe use-case for ConcurrentHashMap is when many threads are reading and less writing to the structure. Here this is just funny: The only reader is close(). Here you can just use a synchronized HashSet. The complexity of CHM is making this just a joke :-)"
0,provide option to automatically dispose idle workspaces
1,NPE in event polling threadThis exception occurs when running the jcr2dav integration tests. This surfaces as a  side effect of JCR-3046. The root cause is refresh(Event) not guarding against null values returned from Event.getItemId().
1,"ConstantScoreRangeQuery - fixes ""too many clauses"" exceptionConstantScoreQuery wraps a filter (representing a set of documents) and returns
a constant score for each document in the set.

ConstantScoreRangeQuery implements a RangeQuery that works for any number of
terms in the range.  It rewrites to a ConstantScoreQuery that wraps a RangeFilter.

Still needed:
  - unit tests (these classes have been tested and work fine in-house, but the
current tests rely on too much application specific code)
  - code review of Weight() implementation (I'm unsure If I got all the score
normalization stuff right)
  - explain() implementation

NOTE: requires Java 1.4 for BitSet.nextSetBit()"
0,"Weight is not serializable for some of the queriesIn order to work with ParallelMultiSearcher, Query weights need to be serializable.  The interface Weight extends java.io.Serializable, but it appears that some of the newer queries unnecessarily store global state in their weights, thus causing serialization errors."
0,"Spellchecker should take IndexWriterConfig... deprecate old methods?When looking at LUCENE-3490, i realized there was no way to specify the codec for the spellchecker to use.

It has the following current methods:
* indexDictionary(Dictionary dict): this causes optimize!
* indexDictionary(Dictionary dict, int mergeFactory, int ramMB): this causes optimize!
* indexDictionary(Dictionary dict, int mergeFactor, int ramMB, boolean optimize)

But no way to specify an IndexwriterConfig. Additionally, I don't like that several of these ctors force an optimize in a tricky way,
even though it was like this all along.

So I think we should add indexDictionary(Dictionary dict, IndexWriterConfig config, boolean optimize).

We should either deprecate all the other ctors in 3.x and nuke in trunk, or at least add warnings to the ones that optimize."
1,"DistanceFilter problem with deleted documentsI know this is the locallucene lib, but wanted to make sure we don't get this bug when it gets into lucene contrib.

I suspect that the issue is that deleted documents are trying to be evaluated by the filter.  I did some debugging and I confirmed that it is bombing on a document that is marked as deleted (using Luke).


Thanks!

Using the locallucene library 1.51, I get a NullPointerException at line 123 of DistanceFilter
The method is 	public BitSet bits(IndexReader reader) 
The line is double x = NumberUtils.SortableStr2double(sx);

The stack trace is:
java.lang.NullPointerException
	at org.apache.solr.util.NumberUtils.SortableStr2long(NumberUtils.java:149)
	at org.apache.solr.util.NumberUtils.SortableStr2double(NumberUtils.java:104)
	at com.pjaol.search.geo.utils.DistanceFilter.bits(DistanceFilter.java:123)
	at org.apache.lucene.search.Filter.getDocIdSet(Filter.java:49)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:140)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:112)
	at org.apache.lucene.search.Hits.getMoreDocs(Hits.java:113)
	at org.apache.lucene.search.Hits.<init>(Hits.java:90)
	at org.apache.lucene.search.Searcher.search(Searcher.java:72)"
0,"A Property and a Node Can Have the Same Name according to paragraph ""3.3.4"" of the of the JSR 283 specification (Public Review Draft), a property and a node can have the same name. "
0,"Change default value for respectDocumentOrderThe current default value for the search index configuration parameter respectDocumentOrder is true. Almost all applications are not interested in document order, while this default adds significant overhead to query execution because document order information is present in the index but has to be calculated over the complete result set.

I propose to change the default value to false and document this change in the 1.4 release notes. If an application relies on document order one can still explicitly set the parameter in the configuration to true."
1,"Directory#copy leaks file handlesDirectory#copy doesn't close the target directories output stream if sourceDir.openInput(srcFile) throws an Exception. Before LUCENE-3218 Directory#copy wasn't used extensively so this wasn't likely to happen during tests. Today we had a failure on the 3.x branch that is likely caused by this bug:

{noformat}
[junit] Testsuite: org.apache.lucene.index.TestAddIndexes
    [junit] Testcase: testAddIndexesWithRollback(org.apache.lucene.index.TestAddIndexes):	Caused an ERROR
    [junit] MockDirectoryWrapper: cannot close: there are still open files: {_co.cfs=1}
    [junit] java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_co.cfs=1}
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:483)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads.closeDir(TestAddIndexes.java:693)
    [junit] 	at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithRollback(TestAddIndexes.java:924)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1277)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1195)
    [junit] Caused by: java.lang.RuntimeException: unclosed IndexOutput: _co.cfs
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.addFileHandle(MockDirectoryWrapper.java:410)
    [junit] 	at org.apache.lucene.store.MockCompoundFileDirectoryWrapper.<init>(MockCompoundFileDirectoryWrapper.java:39)
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.createCompoundOutput(MockDirectoryWrapper.java:439)
    [junit] 	at org.apache.lucene.index.SegmentMerger.createCompoundFile(SegmentMerger.java:128)
    [junit] 	at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:3101)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes3.doBody(TestAddIndexes.java:839)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:667)
    [junit] 
    [junit] 
    [junit] Tests run: 18, Failures: 0, Errors: 1, Time elapsed: 9.034 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] java.lang.IllegalStateException: CFS has pending open files
    [junit] 	at org.apache.lucene.store.CompoundFileWriter.close(CompoundFileWriter.java:143)
    [junit] 	at org.apache.lucene.store.CompoundFileDirectory.close(CompoundFileDirectory.java:181)
    [junit] 	at org.apache.lucene.store.DefaultCompoundFileDirectory.close(DefaultCompoundFileDirectory.java:58)
    [junit] 	at org.apache.lucene.store.MockCompoundFileDirectoryWrapper.close(MockCompoundFileDirectoryWrapper.java:55)
    [junit] 	at org.apache.lucene.index.SegmentMerger.createCompoundFile(SegmentMerger.java:139)
    [junit] 	at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:3101)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes3.doBody(TestAddIndexes.java:839)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:667)
{noformat}"
0,"wrong exception from NativeFSLockFactory (LIA2 test case)As part of integrating Lucene In Action 2 test cases (LUCENE-2661), I found one of the test cases fail

the test is pretty simple, and passes on 3.0. The exception you get instead (LockReleaseFailedException) is 
pretty confusing and I think we should fix it.
"
0,"Query Syntax page does not make it clear that wildcard searches are not allowed in Phrase QueriesThe queryparsersyntax page which is where I expect most novices (such as myself) start with lucene seems to indicate that wildcards can be used in phrase terms

Quoting:
'Terms: A query is broken up into terms and operators. There are two types of terms: Single Terms and Phrases.
A Single Term is a single word such as ""test"" or ""hello"".
A Phrase is a group of words surrounded by double quotes such as ""hello dolly"".

....

Wildcard Searches
Lucene supports single and multiple character wildcard searches.
To perform a multiple character wildcard search use the ""*"" symbol.
Multiple character wildcard searches looks for 0 or more characters. For example, to search for test, tests or tester, you can use the search:

test*
You can also use the wildcard searches in the middle of a term.

'
there is nothing to indicate in the section on Wildcard Searches that it can be performed only on Single word terms not Phrase terms.

Chris  argues 'that there is nothing in the description of a Phrase to indicate that it can be anything other then what it says ""a group of words surrounded by double quotes"" .. at no point does it
suggest that other types of queries or syntax can be used inside the quotes.  likewise the discussion of Wildcards makes no mention of phrases to suggest that wildcard characters can be used in a phrase.'
but I don't accept this because there is nothing in the description of a Single Term either to indicate it can use wildcards either. Wildcards are only mentioned in the Wildcard section and there it says thay can be used in a term, it does not restrict the type of term


I Propose a simple solution modify:

Lucene supports single and multiple character wildcard searches.

to 

Lucene supports single and multiple character wildcard searches within single terms.

(Chris asked for a patch, but Im not sure how to do this, but the change is simple enough)



"
0,"TermAttribute.termLength() optimization
   public int termLength() {
     initTermBuffer(); // This patch removes this method call 
     return termLength;
   }

I see no reason to initTermBuffer() in termLength()... all tests pass, but I could be wrong?

"
0,Create default repository in targetOne of the JCR API tests gets a default repository from the RepositoryFactory. This default repository should use a home directory under target.
1,"While you could use a custom Sort Comparator source with remote searchable before, you can no longer do so with FieldComparatorSourceFieldComparatorSource is not serializable, but can live on a SortField"
0,"Pass resultFetchSize/limit hint to SortedLuceneQueryHitsThe SortedLuceneQueryHits currently uses a default value of 100 (taken from lucene) for initially retrieved and sorted results. For larger result sets this is not optimal because it will cause re-execution of the underlying query with values 200, 400, 800, 1600, 3200, 6400, etc. Instead the query hits should get the limit that is set on the query or the resultFetchSize configured for the SearchIndex."
1,"MultiReader.numDocs incorrect after undeleteAllCalling MultiReader.undeleteAll does not clear cached numDocs value. So the subsequent numDocs() call returns a wrong value if there were deleted documents in the index. Following patch fixes the bug and adds a test showing the issue.


Index: src/test/org/apache/lucene/index/TestMultiReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestMultiReader.java       (revision 354923)
+++ src/test/org/apache/lucene/index/TestMultiReader.java       (working copy)
@@ -69,6 +69,18 @@
     assertTrue(vector != null);
     TestSegmentReader.checkNorms(reader);
   }
+
+  public void testUndeleteAll() throws IOException {
+    sis.read(dir);
+    MultiReader reader = new MultiReader(dir, sis, false, readers);
+    assertTrue(reader != null);
+    assertEquals( 2, reader.numDocs() );
+    reader.delete(0);
+    assertEquals( 1, reader.numDocs() );
+    reader.undeleteAll();
+    assertEquals( 2, reader.numDocs() );
+  }
+

   public void testTermVectors() {
     MultiReader reader = new MultiReader(dir, sis, false, readers);
Index: src/java/org/apache/lucene/index/MultiReader.java
===================================================================
--- src/java/org/apache/lucene/index/MultiReader.java   (revision 354923)
+++ src/java/org/apache/lucene/index/MultiReader.java   (working copy)
@@ -122,6 +122,7 @@
     for (int i = 0; i < subReaders.length; i++)
       subReaders[i].undeleteAll();
     hasDeletions = false;
+    numDocs = -1;      // invalidate cache
   }

   private int readerIndex(int n) {    // find reader for doc n:"
0,InstantiatedIndex supports non-optimized IndexReadersInstantiatedIndex does not currently support non-optimized IndexReaders.  
1,"PayloadTermQuery's explain is broken when span score is not includedWhen setting includeSpanScore to false with PayloadTermQuery, the explain is broken."
1,"recoverable exceptions when reading are not retriedIf a recoverable exception occurs after a request is written then the method is
not retried."
1,"Embedded Derby fails under JBoss because of JMX-related conflictsJBoss fails to start due to a bug in Derby-10.4.2.0. The dependency should be agains derby-10.4.2.1 which seems to has this bug fixed. More info at https://issues.apache.org/jira/browse/DERBY-3887

Please, include this fix in the upcoming 1.6.3"
0,jcr-tests: make property value(s) and property type(s) configurabletest-cases using Node.setProperty or Property.setValue mostly hardcode the value... there should be the possibility to specify type and value in the config.
1,"creating empty field + empty term leads to invalid indexSpinoff from LUCENE-3526.

* if you create new Field("""", """"), you get IllegalArgumentException from Field's ctor: ""name and value cannot both be empty""
* But there are tons of other ways to index an empty term for the empty field (for example initially make it ""garbage"" then .setValue(""""), or via tokenstream).
* If you do this, and you have assertions enabled, you will trip an assert (the assert is fixed in trunk, in LUCENE-3526)
* But If you don't have assertions enabled, you will create a corrupt index: test: terms, freq, prox...ERROR [term : docFreq=1 != num docs seen 0 + num docs deleted 0]
"
0,"Spellchecker ""Suggest Mode"" SupportThis is a spin-off from SOLR-2585.

Currently o.a.l.s.s.SpellChecker and o.a.l.s.s.DirectSpellChecker support two ""Suggest Modes"":
1. Suggest for terms that are not in the index.
2. Suggest ""more popular"" terms.

This issue is to add a third Suggest Mode:
3. Suggest always.

This will assist users in developing context-sensitive spell suggestions and/or did-you-mean suggestions.  See SOLR-2585 for a full discussion.

Note that o.a.l.s.s.SpellChecker already can support this functionality, if the user passes in a NULL term & IndexReader.  This, however, is not intutive.  o.a.l.s.s.DirectSpellChecker currently has no support for this third Suggest Mode."
1,"wrong charset indication in HttpConstants.getContentString()Around line 236 in HttpConstants.getConstentString() the charset is wrongly indicated as 
""DEFAULT_CONTENT_CHARSET"" where it should have been indicated as ""charset"" like in the 
getContentBytes function.

            if (LOG.isWarnEnabled()) {
                LOG.warn(""Unsupported encoding: "" 
                    + DEFAULT_CONTENT_CHARSET // <== should be the variable ""charset"" here
                    + "". Default HTTP encoding used"");
            }

Wrong copy/paste I guess :-)

ZC."
0,"move JDK collation to core, ICU collation to ICU contribAs mentioned on the list, I propose we move the JDK-based CollationKeyFilter/CollationKeyAnalyzer, currently located in contrib/collation into core for collation support (language-sensitive sorting)

These are not much code (the heavy duty stuff is already in core, IndexableBinaryString). 

And I would also like to move the ICUCollationKeyFilter/ICUCollationKeyAnalyzer (along with the jar file they depend on) also currently located in contrib/collation into a contrib/icu.

This way, we can start looking at integrating other functionality from ICU into a fully-fleshed out icu contrib.
"
0,"[PATCH] Use filter bits for next() and skipTo() in FilteredQueryThis improves performance of FilteredQuery by not calling score() 
on documents that do not pass the filter. 
This passes the current tests for FilteredQuery, but these tests 
have not been adapted/extended."
0,"ClientConnectionRelease example is incorrecthttp://svn.apache.org/repos/asf/httpcomponents/httpclient/tags/4.0.1/httpclient/src/examples/org/apache/http/examples/client/ClientConnectionRelease.java

is incorrect: 

1. if error happens in BufferedReader constructor (OutOfMemoryError, StackOverflowError), reader.close() is not called and connection is not released

2. if error happens in reader.readLine(), reader.close() is called, but httpget.abort() is not."
0,"Add the Data Store to the Jackrabbit APICurrently, the garbage collection is not part of the Jackrabbit API. However, the data store garbage collection must be used once in a while if the data store is enabled. I propose to add the required interfaces to the Jackrabbit API. This will also allow to call garbage collection using RMI."
0,"MultiStatusResponse should not call resource.getPropertiescurrent constructor MultiStatusResponse() calls resource.getProperties() even if propFindType == PROPFIND_BY_PROPERTY.

This is inconvenient, because some properties are expensive to generate if they are not requested. MultiStatusResponse() constructor with parameter PROPFIND_BY_PROPERTY should do:

===
if (propFindType == PROPFIND_BY_PROPERTY) {
  for (propName : propNameSet) {
    prop = resource.getProperty(propName);
    if (prop != null)
      status200.addContent(prop);
    else
      status404.addContent(propName);
  }
} else {
  ...
}
==="
1,"NTLM Proxy and basic host authorizationUsing a Microsoft proxy with NTLM validation enabled the authorization against a
remote host does not work. This, of course, assuming that the page is correctly
fetched (which currently is not), see the NTLM authentication bug number 24327"
1,"basetokenstreamtestcase should fail if tokenstream starts with posinc=0This is meaningless for a tokenstream to start with posinc=0,

Its also caused problems and hairiness in the indexer (LUCENE-1255, LUCENE-1542),
and it makes senseless tokenstreams. We should add a check and fix any that do this.

Furthermore the same bug can exist in removing-filters if they have enablePositionIncrements=false.
I think this option is useful: but it shouldnt mean 'allow broken tokenstream', it just means we
don't add gaps. 

If you remove tokens with enablePositionIncrements=false it should not cause the TS to start with
positionincrement=0, and it shouldnt 'restructure' the tokenstream (e.g. moving synonyms on top of a different word).
It should just not add any 'holes'.
"
0,"Problem with formerly escaped JCR node names when upgrading to Jackrabbit 2.2.9The following unit test fails:

{code}
import static org.junit.Assert.*;

import org.apache.jackrabbit.util.Text;
import org.junit.Test;

public class TestEscaping
{
   @Test
   public void testEscaping() throws Exception
   {
      // expect this as an escaped string (e.g. formerly escaped with jackrabbit 1.6)
      String escaped = ""nam%27e"";
      String unescaped = Text.unescapeIllegalJcrChars(escaped);
      assertEquals(escaped, Text.escapeIllegalJcrChars(unescaped));
   }
}
{code}

This is a major problem when upgrading from 1.6.x to 2.2.9. The node names that were escaped in jackrabbit 1.6 are not longer escaped and that breaks the backward compatibility. I think the problem comes in with JCR-2198. "
0,"add target jvm in maven properties for compilationactually the compatibility level for sources/binaries is not defined in project.properties, so if you compile jackrabbit with a 1.5 jdk it will not run on older vm.

It would be nice to add the following properties to assure that the generated jar will work on different jvms:
maven.compile.target=1.4
maven.compile.source=1.4

(or 1.3 if you are targetting also java 1.3)"
1,"DocumentWriter closes TokenStreams too earlyThe DocumentWriter closes a TokenStream as soon as it has consumed its tokens. The javadoc of TokenStream.close() says that it releases resources associated with the stream. However, the DocumentWriter keeps references of the resources (i. e. payload byte arrays, term strings) until it writes the postings to the new segment, which means that DocumentWriter should call TokenStream.close() after it has written the postings.

This problem occurs in multithreaded applications where e. g. pooling is used for the resources. My patch adds a new test to TestPayloads which shows this problem. Multiple threads add documents with payloads to an index and use a pool of byte arrays for the payloads. TokenStream.close() puts the byte arrays back into the pool. The test fails with the old version but runs successfully with the patched version. 

All other units tests pass as well.
"
0,Backport FSTs to 3.x
0,"integrate snowball stopword listsThe snowball project creates stopword lists as well as stemmers, example: http://svn.tartarus.org/snowball/trunk/website/algorithms/english/stop.txt?view=markup

This patch includes the following:
* snowball stopword lists for 13 languages in contrib/snowball/resources
* all stoplists are unmodified, only added license header and converted each one from whatever encoding it was in to UTF-8
* added getSnowballWordSet  to WordListLoader, this is because the format of these files is very different, for example it supports multiple words per line and embedded comments.

I did not add any changes to SnowballAnalyzer to actually automatically use these lists yet, i would like us to discuss this in a future issue proposing integrating snowball with contrib/analyzers.
"
1,"Contrib query org.apache.lucene.search.BoostingQuery sets boost on constructor Query, not cloned copyBoostingQuery sets the boost value on the passed context Query

    public BoostingQuery(Query match, Query context, float boost) {
      this.match = match;
      this.context = (Query)context.clone();        // clone before boost
      this.boost = boost;

      context.setBoost(0.0f);                      // ignore context-only matches
    }

This should be 
      this.context.setBoost(0.0f);                      // ignore context-only matches

Also, boost value of 0.0 may have wrong effect - see discussion at

http://www.mail-archive.com/java-user@lucene.apache.org/msg12243.html 

"
1,NRTCachingDirectory.deleteFile always throws exceptionSilly bug.
0,CND support in jackrabbit-jcr-commonsIt would be nice if the CND parsing functionality in spi-commons could be made available in jcr-commons for use by JCR clients that shouldn't have to know anything about the SPI.
1,"Binary field content lost during optimizeScenario:

* create an index with arbitrary content, and close it
* open IndexWriter again, and add a document with binary field (stored but not compressed)
* close IndexWriter _without_ optimizing, so that the new document is in a separate segment.
* open IndexReader. You can read the last document and its binary field just fine.
* open IndexWriter, optimize the index, close IndexWriter
* open IndexReader. Now the field is still present (not null) and is marked as binary, but the data is not there - Field.getBinaryLength() returns 0.
"
1,"fix reverseStringFilter for unicode 4.0ReverseStringFilter is not aware of supplementary characters: when it reverses it will create unpaired surrogates, which will be replaced by U+FFFD by the indexer (but not at query time).
The wrong words will conflate to each other, and the right words won't match, basically the whole thing falls apart.

This patch implements in-place reverse with the algorithm from apache harmony AbstractStringBuilder.reverse0()
"
1,"charset in Content-Type header shouldn't be in quotesThe charset value in the Content-Type header returned from IOUtil.buildContentType is enclosed in quotes. This value should be a token which does not include double quotes.

Index: C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/server/io/IOUtil.java
===================================================================
--- C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/server/io/IOUtil.java	(revision 397215)
+++ C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/server/io/IOUtil.java	(working copy)
@@ -112,7 +112,7 @@
     public static String buildContentType(String mimeType, String encoding) {
         String contentType = mimeType;
         if (contentType != null && encoding != null) {
-            contentType += ""; charset=\"""" + encoding + ""\"""";
+            contentType += ""; charset="" + encoding;
         }
         return contentType;
     }
"
1,"HttpClient:- Connections not released when SSL Tunneling fails.Trying to use HTTPS, and SSL tunneling fails as expected because the host is not accepted by the squid proxy, so squid proxy return 403. 

The problem I am seeing is that, when ever this happens the connections are not released to the pool. I traced the code and it appears that in 
HttpMethidDirector.java:  executeWithRetry()
when executeConnect() return false and there is no retry, the connections are not released.

Is this expected? Or am I doing something wrong."
0,"SPI: Javadoc Issue with QNodeTypeDefinition#getPropertyDefs and #getChildNodeDefsJavadoc of the mentioned methods currently states:

@return an array containing the property definitions or
     *         <code>null</code> if not set.

while the default implementation returns an empty array, which i find much nicer.

if nobody objects, i would fix the javadoc accordingly."
0,"EdgeNGram* documentation improvementTo clarify what ""edge"" means, I added some description. That edge means the beggining edge of a term or ending edge of a term."
1,"Error downloading text file with gzip content encodingHello I am getting an exception when I try to download certain files.

I don't have control over the host server, only the client.  Here's my client code:

		HttpParams params = new BasicHttpParams();
		params.setParameter(CoreConnectionPNames.CONNECTION_TIMEOUT, 300000L);
		params.setParameter(ClientPNames.HANDLE_REDIRECTS, true);

		// This client indicates to servers that it will support 'gzip'
		// and 'deflate' compressed responses.
		ContentEncodingHttpClient.setDefaultHttpParams(params);
		ContentEncodingHttpClient client = new ContentEncodingHttpClient();

		if (user != null && password != null) {
			String hostname = url.getHost();
			HttpHost hostHttp = new HttpHost(hostname, 80, ""http"");
			HttpHost hostHttps = new HttpHost(hostname, 443, ""https"");
			client.getCredentialsProvider().setCredentials(
			        new AuthScope(hostname, 80), 
			        new UsernamePasswordCredentials(user, password));
	
			client.getCredentialsProvider().setCredentials(
			        new AuthScope(hostname, 443), 
			        new UsernamePasswordCredentials(user, password));
	
			// Create AuthCache instance
			AuthCache authCache = new BasicAuthCache();
			// Generate BASIC scheme object and add it to the local auth cache
			BasicScheme basicAuth = new BasicScheme();
			authCache.put(hostHttp, basicAuth);
			authCache.put(hostHttps, basicAuth);
	
			// Add AuthCache to the execution context
			BasicHttpContext localcontext = new BasicHttpContext();
			localcontext.setAttribute(ClientContext.AUTH_CACHE, authCache);
		}
		HttpGet httpget = new HttpGet(url.toString());
		httpget.setHeader(""If-Modified-Since"", lastModified);


		HttpResponse response = client.execute(httpget);
		responseCode = response.getStatusLine().getStatusCode();
		HttpEntity entity = response.getEntity();
		if (responseCode == HttpStatus.SC_NOT_MODIFIED) {
			
		} else if (responseCode == HttpStatus.SC_OK && entity != null) {
			outStream = new BufferedOutputStream(new FileOutputStream(outFilename));
			entity.writeTo(outStream);
		}

Here's the log output:

DEBUG [2011-08-02 01:23:01,031] [org.apache.http.impl.conn.SingleClientConnManager:212] Get connection for route HttpRoute[{}->http://<host>]
DEBUG [2011-08-02 01:23:01,036] [org.apache.http.impl.conn.DefaultClientConnectionOperator:145] Connecting to <host>/<IP>:80
DEBUG [2011-08-02 01:23:01,057] [org.apache.http.client.protocol.RequestAddCookies:132] CookieSpec selected: best-match
DEBUG [2011-08-02 01:23:01,057] [org.apache.http.client.protocol.RequestAuthCache:75]   Auth cache not set in the context
DEBUG [2011-08-02 01:23:01,058] [org.apache.http.impl.client.DefaultRequestDirector:631]        Attempt 1 to execute request
DEBUG [2011-08-02 01:23:01,058] [org.apache.http.impl.conn.DefaultClientConnection:264] Sending request: GET <file> HTTP/1.1
DEBUG [2011-08-02 01:23:01,058] [org.apache.http.impl.conn.Wire:63]     >> ""GET <file> HTTP/1.1[\r][\n]""
DEBUG [2011-08-02 01:23:01,058] [org.apache.http.impl.conn.Wire:63]     >> ""If-Modified-Since: Mon, 01 Aug 2011 18:26:09 CEST[\r][\n]""
DEBUG [2011-08-02 01:23:01,059] [org.apache.http.impl.conn.Wire:63]     >> ""Host: <host>[\r][\n]""
DEBUG [2011-08-02 01:23:01,059] [org.apache.http.impl.conn.Wire:63]     >> ""Connection: Keep-Alive[\r][\n]""
DEBUG [2011-08-02 01:23:01,059] [org.apache.http.impl.conn.Wire:63]     >> ""User-Agent: Apache-HttpClient/4.1.1 (java 1.5)[\r][\n]""
DEBUG [2011-08-02 01:23:01,059] [org.apache.http.impl.conn.Wire:63]     >> ""Accept-Encoding: gzip,deflate[\r][\n]""
DEBUG [2011-08-02 01:23:01,060] [org.apache.http.impl.conn.Wire:63]     >> ""[\r][\n]""
DEBUG [2011-08-02 01:23:01,060] [org.apache.http.impl.conn.DefaultClientConnection:268] >> GET <file> HTTP/1.1
DEBUG [2011-08-02 01:23:01,060] [org.apache.http.impl.conn.DefaultClientConnection:271] >> If-Modified-Since: Mon, 01 Aug 2011 18:26:09 CEST
DEBUG [2011-08-02 01:23:01,060] [org.apache.http.impl.conn.DefaultClientConnection:271] >> Host: <host>
DEBUG [2011-08-02 01:23:01,061] [org.apache.http.impl.conn.DefaultClientConnection:271] >> Connection: Keep-Alive
DEBUG [2011-08-02 01:23:01,061] [org.apache.http.impl.conn.DefaultClientConnection:271] >> User-Agent: Apache-HttpClient/4.1.1 (java 1.5)
DEBUG [2011-08-02 01:23:01,061] [org.apache.http.impl.conn.DefaultClientConnection:271] >> Accept-Encoding: gzip,deflate
DEBUG [2011-08-02 01:23:01,085] [org.apache.http.impl.conn.Wire:63]     << ""HTTP/1.1 200 OK[\r][\n]""
DEBUG [2011-08-02 01:23:01,086] [org.apache.http.impl.conn.Wire:63]     << ""Server: nginx/0.8.54[\r][\n]""
DEBUG [2011-08-02 01:23:01,086] [org.apache.http.impl.conn.Wire:63]     << ""Date: Mon, 01 Aug 2011 23:23:01 GMT[\r][\n]""
DEBUG [2011-08-02 01:23:01,086] [org.apache.http.impl.conn.Wire:63]     << ""Content-Type: text/plain[\r][\n]""
DEBUG [2011-08-02 01:23:01,086] [org.apache.http.impl.conn.Wire:63]     << ""Last-Modified: Wed, 20 Jul 2011 14:39:57 GMT[\r][\n]""
DEBUG [2011-08-02 01:23:01,087] [org.apache.http.impl.conn.Wire:63]     << ""Transfer-Encoding: chunked[\r][\n]""
DEBUG [2011-08-02 01:23:01,087] [org.apache.http.impl.conn.Wire:63]     << ""Connection: keep-alive[\r][\n]""
DEBUG [2011-08-02 01:23:01,087] [org.apache.http.impl.conn.Wire:63]     << ""Vary: Accept-Encoding[\r][\n]""
DEBUG [2011-08-02 01:23:01,087] [org.apache.http.impl.conn.Wire:63]     << ""Expires: Wed, 31 Aug 2011 23:23:01 GMT[\r][\n]""
DEBUG [2011-08-02 01:23:01,088] [org.apache.http.impl.conn.Wire:63]     << ""Cache-Control: max-age=2592000[\r][\n]""
DEBUG [2011-08-02 01:23:01,088] [org.apache.http.impl.conn.Wire:63]     << ""Content-Encoding: gzip[\r][\n]""
DEBUG [2011-08-02 01:23:01,088] [org.apache.http.impl.conn.Wire:63]     << ""[\r][\n]""
DEBUG [2011-08-02 01:23:01,088] [org.apache.http.impl.conn.DefaultClientConnection:249] Receiving response: HTTP/1.1 200 OK
DEBUG [2011-08-02 01:23:01,089] [org.apache.http.impl.conn.DefaultClientConnection:252] << HTTP/1.1 200 OK
DEBUG [2011-08-02 01:23:01,089] [org.apache.http.impl.conn.DefaultClientConnection:255] << Server: nginx/0.8.54
DEBUG [2011-08-02 01:23:01,089] [org.apache.http.impl.conn.DefaultClientConnection:255] << Date: Mon, 01 Aug 2011 23:23:01 GMT
DEBUG [2011-08-02 01:23:01,089] [org.apache.http.impl.conn.DefaultClientConnection:255] << Content-Type: text/plain
DEBUG [2011-08-02 01:23:01,089] [org.apache.http.impl.conn.DefaultClientConnection:255] << Last-Modified: Wed, 20 Jul 2011 14:39:57 GMT
DEBUG [2011-08-02 01:23:01,090] [org.apache.http.impl.conn.DefaultClientConnection:255] << Transfer-Encoding: chunked
DEBUG [2011-08-02 01:23:01,090] [org.apache.http.impl.conn.DefaultClientConnection:255] << Connection: keep-alive
DEBUG [2011-08-02 01:23:01,090] [org.apache.http.impl.conn.DefaultClientConnection:255] << Vary: Accept-Encoding
DEBUG [2011-08-02 01:23:01,090] [org.apache.http.impl.conn.DefaultClientConnection:255] << Expires: Wed, 31 Aug 2011 23:23:01 GMT
DEBUG [2011-08-02 01:23:01,090] [org.apache.http.impl.conn.DefaultClientConnection:255] << Cache-Control: max-age=2592000
DEBUG [2011-08-02 01:23:01,091] [org.apache.http.impl.conn.DefaultClientConnection:255] << Content-Encoding: gzip
DEBUG [2011-08-02 01:23:01,091] [org.apache.http.impl.client.DefaultRequestDirector:477]        Connection can be kept alive indefinitely
DEBUG [2011-08-02 01:23:01,131] [org.apache.http.impl.conn.Wire:63]     << ""600a[\r][\n]""
DEBUG [2011-08-02 01:23:01,132] [org.apache.http.impl.conn.Wire:77]     << ""[0x1f]""
DEBUG [2011-08-02 01:23:03,838] [org.apache.http.impl.conn.Wire:63]     << ""[\r][\n]""

.... (Content)

DEBUG [2011-08-02 01:23:03,839] [org.apache.http.impl.conn.SingleClientConnManager:267] Releasing connection org.apache.http.impl.conn.SingleClientConnManager$ConnAdapter@2aa3873
DEBUG [2011-08-02 01:23:03,839] [org.apache.http.impl.conn.SingleClientConnManager:285] Released connection open but not reusable.
DEBUG [2011-08-02 01:23:03,839] [org.apache.http.impl.conn.DefaultClientConnection:152] Connection shut down
ERROR [2011-08-02 01:23:03,840] [app]        Exception downloading file
java.io.EOFException
        at java.util.zip.GZIPInputStream.readUByte(GZIPInputStream.java:224)
        at java.util.zip.GZIPInputStream.readUShort(GZIPInputStream.java:214)
        at java.util.zip.GZIPInputStream.readHeader(GZIPInputStream.java:153)
        at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:75)
        at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:85)
        at org.apache.http.client.entity.GzipDecompressingEntity.getContent(GzipDecompressingEntity.java:63)
        at org.apache.http.util.EntityUtils.consume(EntityUtils.java:65)
        at org.apache.http.conn.BasicManagedEntity.ensureConsumed(BasicManagedEntity.java:98)
        at org.apache.http.conn.BasicManagedEntity.writeTo(BasicManagedEntity.java:115)
        at util.FileDownload.download(FileDownload.java:188) <--- my app

Does this happen because the server doesn't specify the content length?"
0,"Extend FieldCache architecture to multiple ValuesI would consider this a bug. It appears lots of people are working around this limitation, 
why don't we just change the underlying data structures to natively support multiValued fields in the FieldCache architecture?

Then functions() will work properly, and we can do things like easily geodist() on a multiValued field.

Thoughts?"
0,"Restore mix:referenceable check to SessionImpl.getNodeByUUIDIn revision 504623 we commented out the mix:referenceable check in the SessionImpl.getNodeByUUID() method:

    // since the uuid of a node is only exposed through jcr:uuid declared
    // by mix:referenceable it's rather unlikely that a client can possibly
    // know the internal uuid of a non-referenceable node; omitting the
    // check for mix:referenceable seems therefore to be a reasonable
    // compromise in order to improve performance.
    /*
    if (node.isNodeType(Name.MIX_REFERENCEABLE)) {
        return node;
    } else {
        // there is a node with that uuid but the node does not expose it
        throw new ItemNotFoundException(uuid.toString());
    }
    */

This solved a minor performance issue issue with client code that used the node UUID as a quick way to access a node. The downside was a slight incompatibility with the spec that says that the getNodeByUUID method is only supposed to work with mix:referenceable nodes.

Now with JCR 2.0 clients can (and should) use the Session.getNodeByIdentifier method that does not have the mix:referenceable limitation. Thus we can restore the original and correct functionality of the getNodeByUUID method."
1,"IndexWriter.setMaxMergeDocs gives non-backwards-compatible exception ""out of the box""Yonik hit this (see details in LUCENE-994): because we have switched
to LogByteSizeMergePolicy by default in IndexWriter, which uses MB to
limit max size of merges (setMaxMergeMB), when an existing app calls
setMaxMergeDocs (or getMaxMergeDocs) it will hit an
IllegalArgumentException on dropping in the new JAR.

I think the simplest solution is to fix LogByteSizeMergePolicy to
allow setting of the max by either MB or by doc count, just like how
in LUCENE-1007 allows flushing by either MB or docCount or both."
1,"Base64 bug - last buffer not flushedI found an issue when using the org.apache.jackrabbit.util.Base64.encode(InputStream in, OutputStream out) method. It appears that the issue is that the last buffer is not flushed on the Writer that it creates before returning from the method. I was able to work around this issue by creating a Writer my own program, and call another encode method, and then call the flush() method before using the data. The source in trunk appears the same.
"
0,"HttpMethodBase logger uses wrong class.I just noticed a minor error in HttpMethodBase: it initializes its Log object
with HttpMethod.class instead of HttpMethodBase.class.  No big deal, but it
probably ought to be fixed at some point.  I'll attach the patch."
0,"Move common implementations of SPI interfaces to spi-commons moduleSome of the spi modules use nearly duplicate code, which should be moved to the spi-commons module."
0,Add workspace population toolAdd a simple tool to jackrabbit-webapp to populate the workspace with content.
1,"if you use setNorm, lucene writes a headerless separate norms fileIn this case SR.reWrite just writes the bytes with no header...
we should write it always.

we can detect in these cases (segment written <= 3.1) with a 
sketchy length == maxDoc check.
"
0,"When 3.1 is released, update backwards tests in 3.x branchWhen we have released the official artifacts of Lucene 3.1 (the final ones!!!), we need to do the following:

- svn rm backwards/src/test
- svn cp https://svn.apache.org/repos/asf/lucene/dev/branches/lucene_solr_3_1/lucene/src/test backwards/src/test
- Copy the lucene-core-3.1.0.jar from the last release tarball to lucene/backwards/lib and delete old one.
- Check that everything is correct: The backwards folder should contain a src/ folder that now contains ""test"". The files should be the ones from the branch.
- Run ""ant test-backwards""

Uwe will take care of this!"
1,"MS Proxy with NTLM authentication set up does not workWhen I try to go via a MS Proxy which is set up with NTLM authentication I
always get a ""407"" error, no matter which credentials used."
0,"Payload QueriesNow that payloads have been implemented, it will be good to make them searchable via one or more Query mechanisms.  See http://wiki.apache.org/lucene-java/Payload_Planning for some background information and https://issues.apache.org/jira/browse/LUCENE-755 for the issue that started it all.  "
1,JCR2SPI: incomplete changelog when combining move with removal of new destination parent
1,"Bug in duplicate mapping checkThere is a bug in the MappingDescriptor for checking if a mapping for a node type is already available. The following patch solves this problem:

Index: /Users/cziegeler/Developer/workspaces/default/jackrabbit/jackrabbit-ocm/src/main/java/org/apache/jackrabbit/ocm/mapper/model/MappingDescriptor.java
===================================================================
--- /Users/cziegeler/Developer/workspaces/default/jackrabbit/jackrabbit-ocm/src/main/java/org/apache/jackrabbit/ocm/mapper/model/MappingDescriptor.java	(revision 614136)
+++ /Users/cziegeler/Developer/workspaces/default/jackrabbit/jackrabbit-ocm/src/main/java/org/apache/jackrabbit/ocm/mapper/model/MappingDescriptor.java	(working copy)
@@ -75,7 +75,7 @@
         if (null != classDescriptor.getJcrType() && !  """".equals(classDescriptor.getJcrType()) && 
         		 ! ManagerConstant.NT_UNSTRUCTURED.equals(classDescriptor.getJcrType()))
         {
-        	if ((classDescriptorsByNodeType.get(classDescriptor.getClassName()) != null) &&
+        	if ((classDescriptorsByNodeType.get(classDescriptor.getJcrType()) != null) &&
         		classDescriptor.usesNodeTypePerConcreteClassStrategy()	)
         	{
         	    log.warn(""Duplicate classdescriptor for node type : "" + classDescriptor.getJcrType());	
"
0,"Add N-Gram String Matching for Spell CheckingN-Gram version of edit distance based on paper by Grzegorz Kondrak, ""N-gram similarity and distance"". Proceedings of the Twelfth International Conference on String Processing and Information Retrieval (SPIRE 2005), pp. 115-126,  Buenos Aires, Argentina, November 2005. 
http://www.cs.ualberta.ca/~kondrak/papers/spire05.pdf
"
1,"[SPI] Node.setProperty with null value throws ItemNotFoundExceptionNode.setProperty with a null value should not throw a ItemNotFoundException in the case a property with the given name does not exist. Rather should it return a stale property which throws an InvalidItemStateException when its methods are accessed. 

This behavior is also consistent with jackrabbit-core.
"
1,"highlighting exact phrase with overlapping tokens fails.Fields with overlapping token are not highlighted in search results when searching exact phrases, when using TermVector.WITH_OFFSET.

The document builded in MemoryIndex for highlight does not preserve positions of tokens in this case. Overlapping tokens get ""flattened"" (position increment always set to 1), the spanquery used for searching relevant fragment will fail to identify the correct token sequence because the position shift.

I corrected this by adding a position increment calculation in sub class StoredTokenStream. I added junit test covering this case.

I used the eclipse codestyle from trunk, but style add quite a few format differences between repository and working copy files. I tried to reduce them, but some linewrapping rules still doesn't match.

Correction patch joined"
0,"Remove sanityCheck() from ItemImpl.getSession()The following code causes an InvalidItemStateException to be thrown for no good reason:

    Property property = ...;
    property.setValue((Value) null);
    property.getSession();

There are cases (I'm looking at one right now) where it's good to be able to access the session of an Item even if it has already been invalidated.

The simple fix is to remove the sanityCheck() call from ItemImpl.getSession(). I'll do that unless someone has a good reason why the sanity check should be kept."
0,"FieldCache introspection APIFieldCache should expose an Expert level API for runtime introspection of the FieldCache to provide info about what is in the FieldCache at any given moment.  We should also provide utility methods for sanity checking that the FieldCache doesn't contain anything ""odd""...
   * entries for the same reader/field with different types/parsers
   * entries for the same field/type/parser in a reader and it's subreader(s)
   * etc...


"
0,decorator enhancementsadded some decorating enhancements as we discussed on the mailing list (apparently there is nothing yet in the gmane /marc archives).
0,"CacheManager (Memory Management in Jackrabbit)Jackrabbit can run out of memory because the the combined size of the various caches is not managed. The biggest problem (for me) is the combined size of the o.a.j.core.state.MLRUItemStateCache caches. Each session seems to create a few (?) of those caches, and each one is limited to 4 MB by default.

I have implemented a dynamic (cache-) memory management service that distributes a fixed amount of memory dynamically to all those caches.

Here is the patch"
0,Update Spatial Lucene sort to use FieldComparatorSourceUpdate distance sorting to use FieldComparator sorting as opposed to SortComparator
0,"Observation tests should throw NotExecutableException when repository does not support observationThe observation tests should throw NotExecutableException when repository does not support observation.
"
0,"Link javadocs of HttpClient, HttpCore and HttpMimePresently the javadocs for HttpCore, HttpClient and HttpMime are isolated from each other.  For new users this can create a great deal of confusion and the appearance of limited functionality of HttpClient.  Please set the javadoc creation task to link the javadoc of these three projects together.  "
1,rep:excerpt() may return malformed XMLThe rep:excerpt() function does not encode the prefined XML entities but writes them as is into the excerpt XML. This may produce malformed XML.
0,"Easier way to run benchmarkMove Benchmark.main to a more easily accessible method exec() that can be easily invoked by external programs.
"
1,"StringIndexOutOfBound exception in RFC2109 cookie validate when host name contains no domain information and is short in length than the cookie domain.If the target server is identified by hostname only (no domain) and the domain
of the cookie is greater in length than the target hostname, a
StringIndexOutOfBoundsException occurs.

Offending line(s) of code: 174-176 in o.a.c.h.cookie.RFC2109Spec.java"
0,"Observation logs error when a node is moved in placeAn error message is written to the log when the following sequence of operations is executed:

- create node 'parent'
- create node 'child' as a child of 'parent'
- save
- create node 'tmp'
- move 'child' under 'tmp'
- remove 'parent'
- move 'tmp' to former path of 'parent'

The log will say: EventStateCollection: Unable to calculate old path of moved node

This is because the zombie path of 'child' is equal to the new path after the move. The EventStateCollection detects a new parentId assigned to 'child' and expects a new path that is different from the zombie path. The above case however shows that there is a use case where the paths are equal and events should be generated."
0,"consolidate FieldCache and ExtendedFieldCache instancesIt's confusing and error prone having two instances of FieldCache... FieldCache .DEFAULT and ExtendedFieldCache .EXT_DEFAULT.
Accidentally use the wrong one and you silently double the memory usage for that field.  Since ExtendedFieldCache extends FieldCache, there's no reason not to share the same instance across both."
0,"Making Tokenizer.reset(Reader) publicIn order to implement reusableTokenStream and be able to reset a Tokenizer, Tokenizer defines a reset(Reader) method. The problem is that this method is protected. I need to call this reset(Reader) method without having to know in advance what will be the type of the Tokenizer (I plan to have several).
I noticed that almost all Tokenizer extensions define this method as public, and I wonder if this can be changed for Tokenizer also (I can't simply create my general Tokenizer extension and inherit from it because I want to use StandardTokenizer as well). "
0,"Include OCM in the main Jackrabbit build when using Java 5Currently the OCM component are separate from the rest of Jackrabbit build due to the fact that they need Java 5 to compile. I'd like to add a java5 profile to the main Jackrabbit build that contains the OCM components and is automatically activated when building with Java 5 or higher.

This would simplify build instructions and allow us to remove the extra Jackrabbit-ocm Hudson build that we currently use to build the OCM component."
0,"TimeLimitingCollector starts thread in static {} with no way to stop themSee the comment in LuceneTestCase.

If you even do Class.forName(""TimeLimitingCollector"") it starts up a thread in a static method, and there isn't a way to kill it.

This is broken."
1,"HttpState.clearCookies() should be synchronizedThe HttpState class has a clearCookies method that is not synchronized but
should be considering it modifies an ArrayList (which is unsynchronized). All
other methods which modify or read from the ArrayList are synchronized except
the clearCookies method. 

I stumbled upon this fact because a webapp I am working on that uses HttpClient
threw an IllegalArgumentException indicating that one of the cookies in the
array returned from HttpState.getCookies() was null, which shouldn't be
possible.  Upon further inspection and testing, the only possible option is that
the threadsafety hole left by the unsynchronized clearCookies method caused the
issue."
1,"ConnectionTimeoutException doesn't releaseConnection()When a ConnectionTimeoutException is thrown, HttpConnection doesn't seem to
release the connection. Instead, the connection is properly released if an
InterruptedIOException is thrown.

This is the pattern I use:

Try {
     method.execute(...);
     method.getResponseBodyAsString();
 } catch (ConnectionTimeoutException cte) {
     ...
 } catch (InterruptedIOException ioe) {
     ...
 } finally {
     method.releaseConnection();
     LOG.info(""RELEASED"");   
 }

The following log shows that no actual release is performed, while the message
""RELEASED"" is logged.

10544  DEBUG [MainCheck2] httpclient.HttpConnection - enter
HttpConnection.isResponseAvailable(int)
10930  WARN  [MainCheck1] httpclient.HttpConnection - The host
www.pccomputing.com:80 (or proxy null:-1) did not accept the connection within
timeout of 3000 milliseconds
10931  WARN  [MainCheck1] CheckPerformer - Connection Timeout occurred..
org.apache.commons.httpclient.HttpConnection$ConnectionTimeoutException
at org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:659) 
...
at PersistenceCheck$MainCheck.run(PersistenceCheck.java:306)
10932  INFO  [MainCheck1] CheckPerformer - RELEASED

->Here no call to HttpConnection.releaseConnection() is performed. 

Thanks"
0,"ItemManager issues WARN message on Node.checkIn and Node.checkOutWenn checking in or checking out a node, the ItemManager.cacheItem method issues a WARN message because a cache entry is being replaced.

While this message might be valuable in certain contexts, in the contetx of checking in or out a node, this is not valuable and harms confidence :-)"
1,"Registering NodeType with defaultvalues fails with IndexOutOfBoundsWhen trying to register more than one nodetpye with default values I get the following exception:

Caused by: java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.toNodeTypeDef(NodeTypeManagerImpl.java:790)
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:560)

I assume there is an index missmatch in the implementation

                Value[] values = pdefs[i].getDefaultValues();
                if (values != null) {
                    InternalValue[] qvalues = new InternalValue[values.length];
                    for (int j = 0; j < values.length; j++) {
                        try {
-->                            qvalues[j] = InternalValue.create(values[i], session);
                        } catch (ValueFormatException e) {
                            throw new InvalidNodeTypeDefinitionException(
                                    ""Invalid default value format"", e);
                        }
                    }
                    qpdef.setDefaultValues(qvalues);
                }
"
0,"SSL contrib files do not use standard javax.net.ssl package provided from JDK 1.4.2Hi all,

While trying to use ssl on AIX, i found that some of the files contributed in 
src/contrib/org/apache/commons/httpclient/contrib/ssl were making hard 
references to com.sun.net.ssl package. Since JDK 1.4.2, one shall use the 
javax.net.ssl package instead.

I have then:
1/ fixed the source files appropriately
2/ updated the build.xml to also build a commons-http-client-contrib.jar 

I will attached to this bug report the resulting unified diff to include in svn"
1,"wordnet parsing bugA user reported that wordnet parses the prolog file incorrectly.

Also need to check the wordnet parser in the memory contrib for this problem.

If this is a false alarm, i'm not worried, because the test will be the first unit test wordnet package ever had.

{noformat}
For example, looking up the synsets for the
word ""king"", we get:

java SynLookup wnindex king
baron
magnate
mogul
power
queen
rex
scrofula
struma
tycoon

Here, ""scrofula"" and ""struma"" are extraneous. This happens because, the line
parser code in Syns2Index.java interpretes the two consecutive single quotes
in entry s(114144247,3,'king''s evil',n,1,1) in  wn_s.pl file, as
termination
of the string and separates into ""king"". This entry concerns
synset of words ""scrofula"" and ""struma"", and thus they get inserted in the
synset of ""king"". *There 1382 such entries, in wn_s.pl* and more in other
WordNet
Prolog data-base files, where such use of two consecutive single quotes
appears.

We have resolved this by adding a statement in the line parsing portion of
Syns2Index.java, as follows:

           // parse line
           line = line.substring(2);
          * line = line.replaceAll(""\'\'"", ""`""); // added statement*
           int comma = line.indexOf(',');
           String num = line.substring(0, comma);  ... ... etc.
In short we replace ""''"" by ""`"" (a back-quote). Then on recreating the
index, we get:

java SynLookup zwnindex king
baron
magnate
mogul
power
queen
rex
tycoon
{noformat}"
0,"Remove background initialization of hierarchy cacheThis is a follow up to JCR-1998.

Rethinking the initialization in a background thread again, I now come to the conclusion that it should be initialized either completely on startup or not at all. A background thread puts additional load on the process, possibly fighting for I/O with other startup procedures.
"
1,"TestNRTThreads test failurehit a fail in TestNRTThreads running tests over and over:
"
0,"Additional excerpt provider implementationThe current DefaultHTMLExcerpt implementation is very simple. It basically picks the first three fragments, regardless of how many matches it contains. There should be an alternative implementation that weights the fragments based on the number of matching terms and the whether phrases have matched."
0,"NodeAddMixinTest assumptions on addMixin behaviourNodeAddMixinTest.testAddMixinReferencable() assumes that mix:referenceable can be added to the test node type. In practice, the node type may already inherit mix:referenceable, but it may not be active until the node is saved. Thus, a ConstraintViolationException upon addMixin should be catched, and the mixin should be checked after save() again.
"
0,"Use bulk-byte-copy when merging term vectorsIndexing all of Wikipedia, with term vectors on, under the YourKit
profiler, shows that 26% of the time (!!) was spent merging the
vectors.  This was without offsets & positions, which would make
matters even worse.

Depressingly, merging, even with ConcurrentMergeScheduler, cannot in
fact keep up with the flushing of new segments in this test, and this
is on a strong IO system (Mac Pro with 4 drive RAID 0 array, 4 CPU
cores).

So, just like Robert's idea to merge stored fields with bulk copying
whenever the field name->number mapping is ""congruent"" (LUCENE-1043),
we can do the same with term vectors.

It's a little trickier because the term vectors format doesn't quite
make it easy to bulk-copy because it doesn't directly encode the
offset into the tvf file.

I worked out a patch that changes the tvx format slightly, by storing
the absolute position in the tvf file for the start of each document
into the tvx file, just like it does for tvd now.  This adds an extra
8 bytes (long) in the tvx file, per document.

Then, I removed a vLong (the first ""position"" stored inside the tvd
file), which makes tvd contents fully position independent (so you can
just copy the bytes).

This adds up to 7 bytes per document (less for larger indices) that
have term vectors enabled, but I think this small increase in index
size is acceptable for the gains in indexing performance?

With this change, the time spent merging term vectors dropped from 26%
to 3%.  Of course, this only applies if your documents are ""regular"".
I think in the future we could have Lucene try hard to assign the same
field number for a given field name, if it had been seen before in the
index...

Merging terms now dominates the merge cost (~20% over overall time
building the Wikipedia index).

I also beefed up TestBackwardsCompatibility unit test: test a non-CFS
and a CFS of versions 1.9, 2.0, 2.1, 2.2 index formats, and added some
term vector fields to these indices.
"
0,"Minimize use of fields in lucene indexCurrently every property name creates a field in the lucene index, bloating the size of the index because of the norm files created for each field.

When values are indexed as is (not tokenized for fulltext indexing), then the property name may be part of the term text. That way lucene must only maintain one field for all property names. With this approach the search terms are always a combination of property name and literal value. e.g. instead of using TermQuery(new Term(""prop"", ""foo"")) the query must be TermQuery(new TermQuery(""common-field"", ""prop:foo"")). this works for general comparison / value comparison operators and also for the like function. the contains function uses the fulltext index which uses a different field anyway.

Using the property name as part of the indexed term text, requires a custom SortComparator which is aware of the property name.

This change will not be backward compatible with earlier indexes created by jackrabbit."
0,"Jar manifest should not contain ${user.name} of the person buildingNot sure if it is a big deal, but I don't particularly like that my user id for my build machine is in the manifest of the JAR that I constructed.  It's a stretch, security-wise, I know, but I don't see how it serves any useful purpose.  We have signatures/logs/SVN tags so we know who built the particular item w/o needing to know what their local user account name is.

The fix is:

{code}
Index: common-build.xml
===================================================================
--- common-build.xml    (revision 661027)
+++ common-build.xml    (working copy)
@@ -281,7 +281,7 @@
                <attribute name=""Implementation-Title"" value=""org.apache.lucene""/>
                <!-- impl version can be any string -->
                <attribute name=""Implementation-Version""
-                          value=""${version} ${svnversion} - ${user.name} - ${DSTAMP} ${TSTAMP}""/>
+                          value=""${version} ${svnversion} - ${DSTAMP} ${TSTAMP}""/>
                <attribute name=""Implementation-Vendor""
                           value=""The Apache Software Foundation""/>
                <attribute name=""X-Compile-Source-JDK"" 
{code} "
0,"Real In-memory RepositoryFor unit tests it is desirable to have an in-memory repository which holds its whole data(even PropertyType.BINARY) in memory.
The actual implementation of org.apache.jackrabbit.core.persistence.mem.InMemPersistenceManager uses the FileSystemBLOBStore along with LocalFileSystem.
The binary properties are serialized to the OS-Filesystem.
"
1,"Spatial checks for a string in an int,double map{code}
  private Map<Integer,Double> distances;
{code}

{code}
    if (precise != null) {
      double xLat = getPrecision(lat, precise);
      double xLng = getPrecision(lng, precise);
      
      String k = new Double(xLat).toString() +"",""+ new Double(xLng).toString();
    
      Double d = (distances.get(k));
      if (d != null){
        return d.doubleValue();
      }
    }
{code}

Something is off here eh?"
1,"Cookie with domain .mydomain.com not sent to host mydomain.comA cookie with for example 
  .mydomain.com 
as domain property is not sent to the host
  mydomain.com
(without www. or anything else before ""mydomain.com"")

This concern all CookieSpec as the relevant code is located in CookieSpecBase:

    public boolean domainMatch(final String host, final String domain) {
        return host.endsWith(domain);
    }

It should be changed for instance to something like:

    public boolean domainMatch(final String host, final String domain) {
        // take care of host ""myDomain.com"" and domain "".myDomain.com""
        return host.endsWith(domain)
	|| _host.equals(_domain.substring(1));
    }"
0,"analysis consumers should use reusable tokenstreamsSome analysis consumers (highlighter, more like this, memory index, contrib queryparser, ...) are using Analyzer.tokenStream but should be using Analyzer.reusableTokenStream instead for better performance."
0,"Remove HitsLUCENE-1290 removed all references to Hits from core.

Most work to be done here is to remove all references from the contrib modules and some new ones that crept into core after 1290."
0,"Remove unnecessary NodeImpl references from LuceneQueryFactoryLuceneQueryFactory casts to NodeImpl just to get the node id. 
This info is available via the api as well, so the cast seems unnecessary.
I'll attach a patch for this tiny issue."
1,"cache returns cached responses even if validators not consistent with all conditional headersThis is a MUST-level requirement in the RFC, where if both ETags and Last-Modified dates are used as validators in a conditional request, a cache cannot return a cached response unless it is consistent with all the conditional headers in the request. There is a unit test for this already, but it is incorrect (it uses 'If-Unmodified-Since' instead of 'If-Modified-Since' in the test case).

"
0,"Use ConcurrentHashMap instead of HashMap wherever thread-safe access is neededConsider using ConcurrentHashMap instead of HashMap for any Maps that are used by multiple threads.

For example SchemeRegistry and AuthSchemeRegistry."
0,Add a testing implementation for DocumentsWriterPerThreadPoolcurrently we only have one impl for DocumentsWriterPerThreadPool. We should add some more to make sure the interface is sufficient and to beef up tests. For testing I'm working on a randomized impl. selecting and locking states randomly.
1,"An HTTP ""204 NO CONTENT"" response results in dropped connectionAfter receiving a ""204 NO CONTENT"" response, HttpClient always closes the 
connection.

This did not happen in earlier versions and appears to have been caused by a 
recent fix to bug# 34262."
0,"CustomScoreQuery should support multiple ValueSourceQueriesCustomScoreQuery's constructor currently accepts a subQuery, and a ValueSourceQuery.  I would like it to accept multiple ValueSourceQueries.  The workaround of nested CustomScoreQueries works for simple cases, but it quickly becomes either cumbersome to manage, or impossible to implement the desired function.

This patch implements CustomMultiScoreQuery with my desired functionality, and refactors CustomScoreQuery to implement the special case of a CustomMultiScoreQuery with 0 or 1 ValueSourceQueries.  This keeps the CustomScoreQuery API intact.

This patch includes basic tests, more or less taken from the original implementation, and customized a bit to cover the new cases."
0,temporary files created by some jUnit test are not automatically removed
0,"CloseableThreadLocal is now obsoleteSince Lucene 3 depends on Java 5, we can use ThreadLocal#remove() to take care or resource management."
0,"Data Store: garbage collection should ignore removed itemsThe GCConcurrentTest fails sometimes. The problem is that
the garbage collector stops if a node or property was removed
while scanning. Instead, the garbage collector should ignore the
removed item and continue.

-------------------------------------------------------------------------------
Test set: org.apache.jackrabbit.core.data.TestAll
-------------------------------------------------------------------------------
Tests run: 19, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 21.282 sec <<< FAILURE!
testGC(org.apache.jackrabbit.core.data.GCConcurrentTest)  Time elapsed: 0.578 sec  <<< ERROR!
javax.jcr.InvalidItemStateException: 5fc4130b-aee4-4bef-b51d-21420d78f315/{}data: the item does not exist anymore
	at org.apache.jackrabbit.core.ItemImpl.sanityCheck(ItemImpl.java:144)
	at org.apache.jackrabbit.core.PropertyImpl.getPropertyState(PropertyImpl.java:89)
	at org.apache.jackrabbit.core.PropertyImpl.getType(PropertyImpl.java:773)
	at org.apache.jackrabbit.core.data.GarbageCollector.recurse(GarbageCollector.java:310)
	at org.apache.jackrabbit.core.data.GarbageCollector.recurse(GarbageCollector.java:327)
	at org.apache.jackrabbit.core.data.GarbageCollector.recurse(GarbageCollector.java:327)
	at org.apache.jackrabbit.core.data.GarbageCollector.scanNodes(GarbageCollector.java:193)
	at org.apache.jackrabbit.core.data.GarbageCollector.scan(GarbageCollector.java:177)
	at org.apache.jackrabbit.core.data.GCThread.run(GCThread.java:52)
	at java.lang.Thread.run(Thread.java:619)
"
0,"Refactor DBMS support for JNDI datasourcesOur shop currently uses Oracle for most projects, most commonly in an application server (Tomcat, WebSphere, etc.), and use configured J2EE datasources. Unfortunately, many of the classes that fix quirks on specific DBMS force you to configure a JDBC connection (look at org.apache.jackrabbit.core.fs.db.OracleFileSystem for instance), which is a ""bad idea"" on an application server -- the application server should be managing resources like DB connections, etc.  If you want to use an DbFileSystem based on an Oracle database, you can't use a datasource from a JNDI lookup.  This in effect makes Jackrabbit unusable in clustered enterprise environments.

It would be much better to refactor the current database support to separate the method that an implementation obtains its connection from its functionality."
0,"allow case insensitive searcheswould be nice to be able to search specific properties like a fulltext search, e.g. with an ignore-case flag, so you could find a subset of the results of

  select * from nt:base where contains('bla')

using something like

  select * from nt:base where jcr:bla like '%bla%'

(currently, the value must contain 'bla' exactly as it is to be found by the second query)

i suggest to extend the contains function with an additional argument for the property to search in, e.g.

  select * from nt:base where contains('bla',jcr:bla)

this could then also easily be used in XPath.
"
0,"jcr-server: make auth-header configurable for JCR-ServerIn WEB-INF/web.xml, there is a section (commented-out by default) that reads:
        <init-param>
            <param-name>authenticate-header</param-name>
            <param-value>Basic realm=""Jackrabbit Webdav Server""</param-value>
            <description>
                Defines the value of the 'WWW-Authenticate' header.
            </description>
        </init-param>

This parameter is ignored (not loaded) by the code - the default string is always used instead
Note: this was more of a problem before JCR-286 had been fixed
"
0,"Provide rename method for nodesCurrently renaming a node is a nuisance if the node's parent has orderable child nodes: The parents child nodes must be searched for the successor of the node to be moved, the node must be moved to its new name and then ordered before the successor. Furthermore the case where the to be moved node is the last node must be special cased. 

I thus propose to provide functionality for directly renaming nodes.  "
0,"Provide support for unconnected socketsOverview description:
If Proxy settings are incorrect or host does not reply, the
HttpClient.executeMethod() hangs, and HttpMethod.abort() does not stop it. Thus,
you cannot assert that the entire application will stop immediately on demand.

Expected Results:
During a HttpMethod.executeMethod(), HttpMethod.abort() should cancel
immediately the executeMethod().

Actual Results:
If HttpMethod.executeMethod() freezes because of Proxy bad settings or not
responding hostname (in fact impossible to open the socket), the abort() method
does not do anything.

Platform:
I tested it on Windows XP and Linux Debian with HttpClient 3.0 RC2 (but if you
look further I point the problem and the source code of the nightly build is
identical).

See comments for the dialogue about the problem, and 2 Test cases. The solution
is described at the end, but it may implies a change in the API and works only
since Java 1.4."
1,"NPE in NearSpansUnordered from PayloadNearQueryThe following query causes a NPE in NearSpansUnordered, and is reproducible with the the attached unit test. The failure occurs on the last document scored.
"
1,"PropertyState binary type desirialsation only returns half of contentCreate a PropertyState for a binary Property (e.g jcr:data) set a value larger than the BLOBFileValues#MAX_BUFFER_SIZE  (e.g. 300Kbyte) serialse it.
On deserialisation the resulting PropertyState's InternalValue's size is only half as the origianl (e.g. 150Kbyte)

Most probably this is due to the States InputStream implementation marking bytes twice to be read.
Following fix solves the issue for call to #read(byte[], in, int),
but other Stream methods may fail as well.

Index: jackrabbit/java/org/apache/jackrabbit/core/state/PropertyState.java
===================================================================
--- jackrabbit/java/org/apache/jackrabbit/core/state/PropertyState.java (revision 399293)
+++ jackrabbit/java/org/apache/jackrabbit/core/state/PropertyState.java (working  copy)
@@ -305,7 +305,6 @@
                                 len = (int) (length - consumed);
                             }
                             int read = super.read(b, off, len);
-                            consumed += read;
                             return read;
                         }
"
0,"indexing-rules should allow wildcards for (global) property nameseg:

<indexing-rule nodeType=""*"">
  <property>text</property>
  <property>*Text</property>
</indexing-rule>

defines that all properties named 'text' and all that end with 'Text' should be fulltext indexed.
if the property name includes namespace prefixes, wildcards are only allowed for 'any' namespace. eg:

*:title

but not: j*:title
"
1,"XMLPersistenceManager incorrectly handles propertiesJCR Property instances are written by the XMLPersistenceManager as java.util.Properties files and loaded through the loadPropertyState() method as Properties files. Unfortunately the reload() method tries to re-load the Property states from XML files, which is not possible."
0,"Cookie Strict Mode independent of regular Strict ModeHi,

I'm having a problem where a web site I'm trying to access is using strict 
cookies (on one line) and a 302 redirect that fails in strict mode.  So I 
cannot access this website because in strict mode it fails because of the 302 
redirect and in non-strict mode the website doesn't recognize the Cookies on 
separate lines.

I'd love to see this added for the next release candidate.

Thanks,

Brent"
1,"Importing strings with special characters failsBoth Session.importXML and Workspace.importXML don't work correctly in some cases.

Importing very large foreign language (for example, Chinese) text property values could result in incorrect values on some platforms. The reason is, BufferedStringValue (buffers very large string to a temporary file) uses the platform default encoding to read and write the text.

BufferedStringValue is relatively slow on some systems when importing large texts or binary data because of using FD().sync().

If an exported string value contains a carriage return (\r), this character was truncated on some platforms.

If an exported string value contains a characters with code below 32 excluding newline (\n) and tab (\t) - for example form feed (\f) - the imported string value was base64 encoded.
"
0,"Intermittent failure in TestThreadedOptimizeFailure looks like this:

{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestThreadedOptimize
    [junit] Testcase: testThreadedOptimize(org.apache.lucene.index.TestThreadedOptimize):	FAILED
    [junit] null
    [junit] junit.framework.AssertionFailedError: null
    [junit] 	at org.apache.lucene.index.TestThreadedOptimize.runTest(TestThreadedOptimize.java:125)
    [junit] 	at org.apache.lucene.index.TestThreadedOptimize.testThreadedOptimize(TestThreadedOptimize.java:149)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:253)
{noformat}

I just committed some verbosity so next time it strikes we'll have more details."
1,"XMLPersistanceManager doesn't preserve a property's 'multiValued' attributewhen a multi-valued property is persisted and later read using the XMLPersistenceManager the 'multiValued' attribute is lost, i.e. PropertyState.isMultiValued() returns always false."
0,"Trim whitespace from parameter names in configuration filesWe've had a couple of issues with extra whitespace in parameter names causing those configuration options being lost. Now with the more strict validation of configuration settings such mistakes can even prevent the repository from starting. On one hand that's a good thing, as the user would then explicitly need to fix such broken configurations, but it would be nice if no user intervention was needed.

Since leading and trailing whitespace is never allowed in parameter names, we can just as well trim it automatically."
1,"FieldsReader does not regard offset and position flagsWhen creating a Field the FieldsReader looks at the storeTermVector flag of the FieldInfo. If true Field.TermVector.YES is used as parameter. But it should be checked if storeOffsetWithTermVector and storePositionWithTermVector are set and Field.TermVector.WITH_OFFSETS, ...WITH_POSITIONS, or ...WITH_POSITIONS_OFFSETS should be used as appropriate."
1,"Writers on two machines over NFS can hit FNFE due to stale NFS client cachingIssue spawned from this thread:

  http://www.gossamer-threads.com/lists/lucene/java-user/50680

When IndexFileDeleter lists the directory, looking for segments_X
files to load, if it hits a FNFE on opening such a file it should
catch this and treat it as if the file does not exist.

On NFS (and possibly other file systems), a directory listing is not
guaranteed to be ""current""/coherent.  Specifically, if machine #1 has
just removed file ""segments_n"" and shortly thereafer machine #2 does a
dir listing, it's possible (likely?) that the dir listing will still
show that segments_n exists.

I think the fix is simple: catch the FNFE and just handle it as if the
segments_n does not in fact exist.

"
1,"entity returns the same stream for getContent()BasicHttpEntity and GzipDecompressingEntity will return the same stream
when getContent() is called multiple times. That is not allowed by the
HttpEntity interface. They should rather throw an IllegalStateException.

Some tests and EntityUtils rely on getContent to return the same stream
for multiple calls.

patch follows,
  Roland"
0,"the jcr:frozenUuid property is of type REFERENCE instead of STRINGThe spec says that jcr:frozenUuid is a STRING but jackrabbit 1.0.1 uses a REFERENCE for it.
"
0,PropertyReadMethodsTest should also work on NAME propertySome test cases in PropertyReadMethodsTest require a String property even though a NAME property like jcr:primaryType would be sufficient.
0,"NGramFilter -- construct n-grams from a TokenStreamThis filter constructs n-grams (token combinations up to a fixed size, sometimes
called ""shingles"") from a token stream.

The filter sets start offsets, end offsets and position increments, so
highlighting and phrase queries should work.

Position increments > 1 in the input stream are replaced by filler tokens
(tokens with termText ""_"" and endOffset - startOffset = 0) in the output
n-grams. (Position increments > 1 in the input stream are usually caused by
removing some tokens, eg. stopwords, from a stream.)

The filter uses CircularFifoBuffer and UnboundedFifoBuffer from Apache
Commons-Collections.

Filter, test case and an analyzer are attached."
0,"DateField class should be publicThe class org.apache.jackrabbit.core.query.lucene.DateField should be made public.  It has several public methods which are useful but are currently not accessible because the class itself is not accessible outside of its package.  All of the other Field classes in that package are public and accessible (LongField, DoubleField, etc.)"
0,"Make contrib/collation/(ICU)CollationKeyAnalyzer constructors publicIn contrib/collation, the constructors for CollationKeyAnalyzer and ICUCollationKeyAnalyzer are package private, and so are effectively unusable."
0,"create a simple test that indexes and searches byte[] termsCurrently, the only good test that does this is Test2BTerms (disabled by default)

I think we should test this capability, and also have a simpler example for how to do this.
"
0,"wrong class name in statemgmt.xml""BasicClientCookie"" should read ""BasicCookieStore"", see the patch for details"
0,JSR 283: Access Controlcontainer issue for JSR 283 access control functionality
0,"use isBinary cached variable instead of instanceof in FieldField class can hold three types of values, 
See: AbstractField.java  protected Object fieldsData = null; 

currently, mainly RTTI (instanceof) is used to determine the type of the value stored in particular instance of the Field, but for binary value we have mixed RTTI and cached variable ""boolean isBinary"" 

This patch makes consistent use of cached variable isBinary.

Benefit: consistent usage of method to determine run-time type for binary case  (reduces chance to get out of sync on cached variable). It should be slightly faster as well.

Thinking aloud: 
Would it not make sense to maintain type with some integer/byte""poor man's enum"" (Interface with a couple of constants)
code:java{
public static final interface Type{
public static final byte BOOLEAN = 0;
public static final byte STRING = 1;
public static final byte READER = 2;
....
}
}

and use that instead of isBinary + instanceof? "
0,"CompoundFileWriter should pre-set its file lengthI've read that if you are writing a large file, it's best to pre-set
the size of the file in advance before you write all of its contents.
This in general minimizes fragmentation and improves IO performance
against the file in the future.

I think this makes sense (intuitively) but I haven't done any real
performance testing to verify.

Java has the java.io.File.setLength() method (since 1.2) for this.

We can easily fix CompoundFileWriter to call setLength() on the file
it's writing (and add setLength() method to IndexOutput).  The
CompoundFileWriter knows exactly how large its file will be.

Another good thing is: if you are going run out of disk space, then,
the setLength call should fail up front instead of failing when the
compound file is actually written.  This has two benefits: first, you
find out sooner that you will run out of disk space, and, second, you
don't fill up the disk down to 0 bytes left (always a frustrating
experience!).  Instead you leave what space was available
and throw an IOException.

My one hesitation here is: what if out there there exists a filesystem
that can't handle this call, and it throws an IOException on that
platform?  But this is balanced against possible easy-win improvement
in performance.

Does anyone have any feedback / thoughts / experience relevant to
this?
"
0,"allow automatontermsenum to work on full byte rangeAutomatonTermsEnum is really agnostic to whats in your byte[], only that its in binary order.
so if you wanted to use this on some non-utf8 terms, thats just fine.

the patch just does some code cleanup and removes ""utf8"" references, etc.
additionally i changed the pkg-private, lucene-internal byte-oriented ctor, to public, lucene.experimental.
"
0,"Make ReqExclScorer package private, and use DocIdSetIterator for excluded part."
1,"Unusual Http status lineThe web server at http://alces.med.umn.edu/Candida.html returns the following
status line:

HTTP 200 Document follows

This page loads in the 3 browsers I tried (though Safari actually rendered the
headers).  The current version of HttpClient reads through the whole page
looking for a line that starts with HTTP/.  I don't know how big of a problem
this is, but it's a fairly easy fix.  Patch to follow."
0,"Log path of missing node when re-indexing failsIf one tries to re-index a corrupt workspace, then the UUID of the missing nodes is logged. If possible the log should also contain the path to the missing node."
0,"Re-index fails on corrupt bundleThe re-indexing process should be more resilient, log an error and simply continue with the next node. It doesn't seem useful to refuse repository startup in this case."
0,"Change MergePolicy & MergeScheduler to be abstract base classes instead of an interfacesThis gives us freedom to add methods with default base implementation over time w/o breaking backwards compatibility.

Thanks to Hoss for raising this!"
1,"DefaultHttpMethodRetryHandler does not check whether the failed method has been abortedDefaultHttpMethodRetryHandler does not check whether the failed method has been
aborted."
1,"Buffered deletes under count RAMI found this while working on LUCENE-2548: when we freeze the deletes (create FrozenBufferedDeletes), when we set the bytesUsed we are failing to account for RAM required for the term bytes (and now term field)."
0,"Arabic Analyzer: Stopwords list needs enhancementThe provided Arabic stopwords list needs some enhancements (e.g. it contains a lot of words that not stopwords, and some cleanup) . patch will be provided with this issue."
1,"Request is retried if preemptive authentication failsHello,

I'm using premptive authentification from an Axis client using BASIC Http
authentification. When the user isn't authenticated/authorized by server (in my
case, credentials are expired), httpclient runs a ""Chalenge"" that produces a
second request to server with same credentials.

when using preemptive mode, chalenge should be skipped if authentication scheme
hasn't changed !"
0,Add a ton of missing license headers throughout test/demo/contrib
0,"PostMethod - Chunked requests are not supported at the moment.For Apache Axis, we'd like send a POST request without needing to calculate the
content-length for HTTP 1.1 based servers. Of course if the server-side does not
support 1.1 then a fallback mechanism could calculate the total size under the
covers. 

Also see related request from ""Trevor O'Reilly"" <wtrevor@yahoo.com>:
http://marc.theaimsgroup.com/?l=jakarta-commons-user&m=102719653201792&w=2"
0,"Remove write access from SegmentReader and possibly move to separate class or IndexWriter/BufferedDeletes/...After LUCENE-3606 is finished, there are some TODOs:

SegmentReader still contains (package-private) all delete logic including crazy copyOnWrite for validDocs Bits. It would be good, if SegmentReader itsself could be read-only like all other IndexReaders.

There are two possibilities to do this:
# the simple one: Subclass SegmentReader and make a RWSegmentReader that is only used by IndexWriter/BufferedDeletes/... DirectoryReader will only use the read-only SegmentReader. This would move all TODOs to a separate class. It's reopen/clone method would always create a RO-SegmentReader (for NRT).
# Remove all write and commit stuff from SegmentReader completely and move it to IndexWriter's readerPool (it must be in readerPool as deletions need a not-changing view on an index snapshot).

Unfortunately the code is so complicated and I have no real experience in those internals of IndexWriter so I did not want to do it with LUCENE-3606, I just separated the code in SegmentReader and marked with TODO. Maybe Mike McCandless can help :-)"
1,"DatabaseJournal assigns same revision id to different revisionsRunning a transaction that updates multiple workspaces (e.g. a versioning operation) will fail in DatabaseJournal, because every individual update will ultimately be assigned the same revision id. An indication of this failure when e.g. using Oracle as backend for journaling will look as follows::

java.sql.SQLException: ORA-00001: unique constraint (JOURNAL_IDX) violated
 at oracle.jdbc.dbaccess.DBError.throwSqlException(DBError.java:134)
 at oracle.jdbc.ttc7.TTIoer.processError(TTIoer.java:289)
 at oracle.jdbc.ttc7.Oall7.receive(Oall7.java:590)
 at oracle.jdbc.ttc7.TTC7Protocol.doOall7(TTC7Protocol.java:1973)
 at oracle.jdbc.ttc7.TTC7Protocol.executeFetch(TTC7Protocol.java:977)
 at oracle.jdbc.driver.OracleStatement.executeNonQuery(OracleStatement.java:2205)
 at oracle.jdbc.driver.OracleStatement.doExecuteOther(OracleStatement.java:2064)
 at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:2989)
 at oracle.jdbc.driver.OraclePreparedStatement.executeUpdate(OraclePreparedStatement.java:658)
 at oracle.jdbc.driver.OraclePreparedStatement.execute(OraclePreparedStatement.java:736)
 at org.apache.jackrabbit.core.journal.DatabaseJournal.append(DatabaseJournal.java:293)
 ... 24 more

This bug has been reported by Rafa Kwiecie."
1,"SegmentReader.hasSeparateNorms always returns falseThe loop in that method looks like this: 
 
for(int i = 0; i < 0; i++){ 
 
I guess ""i < 0"" should be replaced by ""i < result.length""?"
1,"Cluster information is not persisted to database when connected to case sensitive MS SQL Server 2005After a call to Session::save, we observed that cluster information was not written to the ${schemaObjectPrefix}JOURNAL and ${schemaObjectPrefix}GLOBAL_REVISION tables. We tested against Oracle 10 database servers and MS Sql Server 2005 servers. The problem was noticed only with MS Sql Server 2005. 

Initially, the problem was masked since the test was written as part of our unit test environment and the exceptions generated by JDBC were not showing up in the logs. A separate test with was carried out as shown by the code below

<pre>
import java.io.FileInputStream;

import javax.jcr.Node;
import javax.jcr.Repository;
import javax.jcr.Session;
import javax.jcr.SimpleCredentials;

import org.apache.jackrabbit.core.TransientRepository;
import org.apache.jackrabbit.core.config.RepositoryConfig;

public class Main
{
    public static void main(String[] args)
        throws Exception
    {
        System.setProperty(""org.apache.jackrabbit.core.cluster.node_id"", ""testid"");
        
        RepositoryConfig config = RepositoryConfig.create(new FileInputStream(""repository.xml""), ""repository"");
        
        Repository repository = new TransientRepository();
        
        Session session = repository.login(new SimpleCredentials(""username"", ""password"".toCharArray()));
        
        Node root = session.getRootNode();
        
        root.addNode(""node1"");
        root.addNode(""node2"");
        root.addNode(""node3"");
        
        session.save();
    }
}
</pre>

The configuration file used to configure the repository is attached.

After debugging this, we obtained the exceptions that were previously not visible. Note that, JackRabbit continues to run (is that because the cluster code is running in a separate thread?) even after this exception. The problem was that the 'revision_id' field did not exist. The mssql.ddl schema file sets up the table names in capitals. However, at least two of the SQL statements in DatabaseJournal use lower case table names. For example:-

<pre>
        updateGlobalStmt = con.prepareStatement(
                ""update "" + schemaObjectPrefix + ""global_revision "" +
                ""set revision_id = revision_id + 1"");
        selectGlobalStmt = con.prepareStatement(
                ""select revision_id "" +
                ""from "" + schemaObjectPrefix + ""global_revision"");
</pre>

An additional error is that the mssql.ddl file is missing the following:

<pre>
# Inserting the one and only revision counter record now helps avoiding race conditions
insert into ${schemaObjectPrefix}GLOBAL_REVISION VALUES(0)
</pre>

Fixing the above two issues, fixed the problem with MS SQL Server 2005."
1,"HttpClient enter 100% for endless timeI was working masively using HttpClient (I was testing it for usage within a 
server) and it got to 100% CPU for an endless time.

I was querying urls of the type 
http://search.barnesandnoble.com/booksearch/results.asp?WRD=<text>&sort=R&SAT=1

To reproduce it, run 100-200 urls with random words instead of <text> and 
you'll probably reproduce the problem."
1,"SegmentReader.setNorm can fail to remove separate norms file, on Windows
While working through LUCENE-710 I hit this bug: on Windows
only, when SegmentReader.setNorm is called, but separate norms
(_X_N.sY) had already been previously saved, then, on closing the
reader, we will write the next gen separate norm file correctly
(_X_N+1.sY) but fail to delete the current one.

It's quite minor because the next writer to touch the index will
remove the stale file.

This is because the Norm class still holds the IndexInput open when
the reader commits."
0,"Add an option so skip the ""checkSchema"" methodsSometimes the ""checkSchema"" methods in the various components (DB filesystem, DB persistence manager, DB journal) fail to detect that the required tables already exist with as result that the startup  fails. (See the mail thread on the dev list: http://jackrabbit.markmail.org/message/jtq2sqis2aceh7ro).
An option to just skip the checkSchema methods on startup would solve this issue."
1,"HttpMethodBase#aborted variable mistakenly declared transient instead of volatileHttpMethodBase#aborted variable mistakenly declared transient instead of volatile. This is quite nasty. 

Do we want to cut an emergency release (3.0.2) because of that or can this wait until 3.1-beta1?

Fix attached.

Oleg"
0,"Reintegrate flex branch into trunkThis issue is for reintegrating the flex branch into current trunk. I will post the patch here for review and commit, when all contributors to flex have reviewed the patch.

Before committing, I will tag both trunk and flex."
1,"ShingleMatrixFilter eaily throws StackOverFlow as the complexity of a matrix growsShingleMatrixFilter#next makes a recursive function invocation when the current permutation iterator is exhausted or if the current state of the permutation iterator already has produced an identical shingle. In a not too complex matrix this will require a gigabyte sized stack per thread.

My solution is to avoid the recursive invocation by refactoring like this:

{code:java}
public Token next(final Token reusableToken) throws IOException {
    assert reusableToken != null;
    if (matrix == null) {
      matrix = new Matrix();
      // fill matrix with maximumShingleSize columns
      while (matrix.columns.size() < maximumShingleSize && readColumn()) {
        // this loop looks ugly
      }
    }

    // this loop exists in order to avoid recursive calls to the next method
    // as the complexity of a large matrix
    // then would require a multi gigabyte sized stack.
    Token token;
    do {
      token = produceNextToken(reusableToken);
    } while (token == request_next_token);
    return token;
  }

  
  private static final Token request_next_token = new Token();

  /**
   * This method exists in order to avoid reursive calls to the method
   * as the complexity of a fairlt small matrix then easily would require
   * a gigabyte sized stack per thread.
   *
   * @param reusableToken
   * @return null if exhausted, instance request_next_token if one more call is required for an answer, or instance parameter resuableToken.
   * @throws IOException
   */
  private Token produceNextToken(final Token reusableToken) throws IOException {

{code}

"
0,"TCK: Transfer of lock token should be tested using open-scoped locksdespite the fact that jsr170 does not limit the usage of Session.removeLockToken(String) and Session.addLockToken(String) to tokens obtained from open-scoped locks, i don't see too much benefit of it. Therefore (and due to the fact that this issue will be addressed within the scope of jsr283), i would  like to suggest to modify those test-cases dealing with transfer of lock tokens and create open-scoped locks.
"
0,ReadOnlyIndexReaders are re-created on every accessAbstractIndex.getReadOnlyIndexReader() creates a new instance on every call. The returned index reader should instead be cached and kept open as long as there are no changes on the underlying index.
0,"Generalize SearcherManagerI'd like to generalize SearcherManager to a class which can manage instances of a certain type of interfaces. The reason is that today SearcherManager knows how to handle IndexSearcher instances. I have a SearcherManager which manages a pair of IndexSearcher and TaxonomyReader pair.

Recently, few concurrency bugs were fixed in SearcherManager, and I realized that I need to apply them to my version as well. Which led me to think why can't we have an SM version which is generic enough so that both my version and Lucene's can benefit from?

The way I see SearcherManager, it can be divided into two parts: (1) the part that manages the logic of acquire/release/maybeReopen (i.e., ensureOpen, protect from concurrency stuff etc.), and (2) the part which handles IndexSearcher, or my SearcherTaxoPair. I'm thinking that if we'll have an interface with incRef/decRef/tryIncRef/maybeRefresh, we can make SearcherManager a generic class which handles this interface.

I will post a patch with the initial idea, and we can continue from there."
1,".toString on empty MultiPhraseQuery hits NPERoss Woolf hit this on java-user thread ""MultiPhraseQuery.toString() throws null pointer exception"".  It's still present on trunk..."
1,"Registering a Nodetype based on an existing NodeType failIf I create a new NodeTypeTemplate using the code show below,

           NodeTypeManagerImpl ntm = (NodeTypeManagerImpl) session.getWorkspace().getNodeTypeManager();
           NodeTypeDefinition nt = (NodeTypeDefinition) ntm.getNodeType(""wr:entity"");
           NodeTypeTemplate ntt = ntm.createNodeTypeTemplate(nt);

the list of declaredSuperType contains the same name of the original nodeType (repeted twice) and not the declaredSuperType of the original nodeType (in this example [nt:base, nt:file])

          ntt.getDeclaredSupertypeNames(); -> [wr:entity, wr:entity]"
1,"After IW.addIndexesNoOptimize, IW.close may hangSpinoff from here:

  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200804.mbox/%3c43128.192.168.1.71.1208561409.webmail@192.168.1.71%3e

The addIndexesNoOptimize method first merges eligible segments
according to the MergePolicy, and then copies over one by one any
remaining ""external"" segments.

That copy can possibly (rather rarely) result in new merges becoming
eligible because its size can change if the index being added was
created with autoCommit=false.

However, we fail to then invoke the MergeScheduler to run these
merges.  As a result, in close, where we wait until all running and
pending merges complete, we will never return.

The fix is simple: invoke the merge scheduler inside
copyExternalSegments() if any segments were copied.  I also added
defensive invocation of the merge scheduler during close, just in case
other code paths could allow for a merge to be added to the pending
queue but not scheduled.

"
0,Allow name to be set in PropertyInfoBuilder and NodeInfoBuilderCurrently the property name for new Properties and Nodes can only be set when their builder is created. I suggest to add methods for setting the names by themselves. 
1,"Constructor of IndexWriter let's runtime exceptions pop up, while keeping the writeLock obtainedConstructor of IndexWriter let's runtime exceptions pop up, while keeping the writeLock obtained.

The init method in IndexWriter catches IOException only (I got NegativeArraySize by reading up a _corrupt_ index), and now, there is no way to recover, since the writeLock will be kept obtained. Moreover, I don't have IndexWriter instance either, to ""grab"" the lock somehow, since the init() method is called from IndexWriter constructor.

Either broaden the catch to all exceptions, or at least provide some circumvention to clear up. In my case, I'd like to ""fallback"", just delete the corrupted index from disk and recreate it, but it is impossible, since the LOCK_HELD NativeFSLockFactory's entry about obtained WriteLock is _never_ cleaned out and is no (at least apparent) way to clean it out forcibly. I can't create new IndexWriter, since it will always fail with LockObtainFailedException."
1,"URIUtils.extractHost(...) throws a NumberFormatException line 310Original Jboss-seam-wicket-booking application in Jboss-4.2.3.GA started, post a login request thanks httpclient, then NumberFormatException.



regarding this page :
http://hc.apache.org/httpcomponents-client-dev/httpclient/clover/org/apache/http/client/utils/URIUtils.html

305 	   	// Extract the port suffix, if present
306 	   	if (host != null) {
307 	  	   int colon = host.indexOf(':');
308 	   	   if (colon >= 0) {
309 	   	      if (colon+1 < host.length()) {
310 	   	          port = Integer.parseInt(host.substring(colon+1));
311 	   	      }
312 	  	   host = host.substring(0,colon);
313 	   	   }
314 	   	}

resolving the port throw a NumberFormatException

java.lang.NumberFormatException: For input string: ""8080;jsessionid=9E9EDA0B6E1CDD499A0A15C4A8F212D8""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
	at java.lang.Integer.parseInt(Integer.java:458)
	at java.lang.Integer.parseInt(Integer.java:499)
	at org.apache.http.client.utils.URIUtils.extractHost(URIUtils.java:310)
	at org.apache.http.impl.client.AbstractHttpClient.determineTarget(AbstractHttpClient.java:764)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:754)
	at org.tagbrowser.api.TagBrowser.request(TagBrowser.java:109)


another case of this problem canbe found hier :
https://gitorious.org/yacy/rc1/commit/8b0920b0b5eb67ae17eec24c1bf3a059543cb6e8/diffs"
0,"Consolidate type safe wrappers for commons-collection classesVarious places define their own type safe wrappers for classes from commons-collections (i.e. FilterIterator, TransformIterator and the like). I would like to consolidate them into one single place. "
0,"Add FieldCache.getTermBytes, to load term data as byte[]With flex, a term is now an opaque byte[] (typically, utf8 encoded unicode string, but not necessarily), so we need to push this up the search stack.

FieldCache now has getStrings and getStringIndex; we need corresponding methods to load terms as native byte[], since in general they may not be representable as String.  This should be quite a bit more RAM efficient too, for US ascii content since each character would then use 1 byte not 2."
0,"Alternative depth-based DOT layout ordering in FST's UtilsUtils.toDot() dumps GraphViz's DOT file, but it can be quite difficult to read. This patch provides an alternative layout that is probably a little bit easier on the eyes (well, as far as larger FSTs can be ;)"
1,"FSDirectory.copy() impl is unsafeThere are a couple of issues with it:

# FileChannel.transferFrom documents that it may not copy the number of bytes requested, however we don't check the return value. So need to fix the code to read in a loop until all bytes were copied..
# When calling addIndexes() w/ very large segments (few hundred MBs in size), I ran into the following exception (Java 1.6 -- Java 1.5's exception was cryptic):
{code}
Exception in thread ""main"" java.io.IOException: Map failed
    at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:770)
    at sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:450)
    at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:523)
    at org.apache.lucene.store.FSDirectory.copy(FSDirectory.java:450)
    at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:3019)
Caused by: java.lang.OutOfMemoryError: Map failed
    at sun.nio.ch.FileChannelImpl.map0(Native Method)
    at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:767)
    ... 7 more
{code}

I changed the impl to something like this:
{code}
long numWritten = 0;
long numToWrite = input.size();
long bufSize = 1 << 26;
while (numWritten < numToWrite) {
  numWritten += output.transferFrom(input, numWritten, bufSize);
}
{code}

And the code successfully adds the indexes. This code uses chunks of 64MB, however that might be too large for some applications, so we definitely need a smaller one. The question is how small so that performance won't be affected, and it'd be great if we can let it be configurable, however since that API is called by other API, such as addIndexes, not sure it's easily controllable.

Also, I read somewhere (can't remember now where) that on Linux the native impl is better and does copy in chunks. So perhaps we should make a Linux specific impl?"
0,Buffered I/O in IndexInfosIndexInfos currently uses plain Input/OutputStreams without buffering.
0,Config: make all elements in the security configuration optionalin order not to introduce new mandatory elements in the security configuration.
1,"Problems with BundleDbPersistenceManager getAllNodeIdsWhen using MySQL:
The problem arises when the method parameter maxcount is less than the total amount of records in the bundle table.

First of all I found out that mysql orders the nodeid objects different than jackrabbit does. The following test describes this idea:

    public void testMySQLOrderByNodeId() throws Exception {
        NodeId nodeId1 = new NodeId(""7ff9e87c-f87f-4d35-9d61-2e298e56ac37"");
        NodeId nodeId2 = new NodeId(""9fd0d452-b5d0-426b-8a0f-bef830ba0495"");

        PreparedStatement stmt = connection.prepareStatement(""SELECT NODE_ID FROM DEFAULT_BUNDLE WHERE NODE_ID = ? OR NODE_ID = ? ORDER BY NODE_ID"");

        Object[] params = new Object[] { nodeId1.getRawBytes(), nodeId2.getRawBytes() };
        stmt.setObject(1, params[0]);
        stmt.setObject(2, params[1]);

        ArrayList<NodeId> nodeIds = new ArrayList<NodeId>();
        ResultSet resultSet = stmt.executeQuery();
        while(resultSet.next()) {
            NodeId nodeId = new NodeId(resultSet.getBytes(1));
            System.out.println(nodeId);
            nodeIds.add(nodeId);
        }
        Collections.sort(nodeIds);
        for (NodeId nodeId : nodeIds) {
            System.out.println(nodeId);
        }
    }

Which results in the following output:

7ff9e87c-f87f-4d35-9d61-2e298e56ac37
9fd0d452-b5d0-426b-8a0f-bef830ba0495
9fd0d452-b5d0-426b-8a0f-bef830ba0495
7ff9e87c-f87f-4d35-9d61-2e298e56ac37


Now the problem with the getAllNodeIds method is that it fetches an extra 10 records on top of maxcount (to avoid a problem where the first key is not the one you that is wanted). Afterwards it skips a number of records again, this time using nodeid.compareto. This compareto statement returns true unexpectedly for mysql because the code doesn't expect the mysql ordering.

I had the situation where I had about 17000 records in the bundle table but consecutively getting the ids a thousand records at a time returned only about 8000 records in all.
"
0,Add plugable mechanism for import/export of webdav-serveradd plugable mechanism to improve flexible configuration of the jcr-server
0,"Change IndexSearcher multisegment searches to search each individual segment using a single HitCollectorThis issue changes how an IndexSearcher searches over multiple segments. The current method of searching multiple segments is to use a MultiSegmentReader and treat all of the segments as one. This causes filters and FieldCaches to be keyed to the MultiReader and makes reopen expensive. If only a few segments change, the FieldCache is still loaded for all of them.

This patch changes things by searching each individual segment one at a time, but sharing the HitCollector used across each segment. This allows FieldCaches and Filters to be keyed on individual SegmentReaders, making reopen much cheaper. FieldCache loading over multiple segments can be much faster as well - with the old method, all unique terms for every segment is enumerated against each segment - because of the likely logarithmic change in terms per segment, this can be very wasteful. Searching individual segments avoids this cost. The term/document statistics from the multireader are used to score results for each segment.

When sorting, its more difficult to use a single HitCollector for each sub searcher. Ordinals are not comparable across segments. To account for this, a new field sort enabled HitCollector is introduced that is able to collect and sort across segments (because of its ability to compare ordinals across segments). This TopFieldCollector class will collect the values/ordinals for a given segment, and upon moving to the next segment, translate any ordinals/values so that they can be compared against the values for the new segment. This is done lazily.

All and all, the switch seems to provide numerous performance benefits, in both sorted and non sorted search. We were seeing a good loss on indices with lots of segments (1000?) and certain queue sizes / queries, but the latest results seem to show thats been mostly taken care of (you shouldnt be using such a large queue on such a segmented index anyway).

* Introduces
** MultiReaderHitCollector - a HitCollector that can collect across multiple IndexReaders. Old HitCollectors are wrapped to support multiple IndexReaders.
** TopFieldCollector - a HitCollector that can compare values/ordinals across IndexReaders and sort on fields.
** FieldValueHitQueue - a Priority queue that is part of the TopFieldCollector implementation.
** FieldComparator - a new Comparator class that works across IndexReaders. Part of the TopFieldCollector implementation.
** FieldComparatorSource - new class to allow for custom Comparators.
* Alters
** IndexSearcher uses a single HitCollector to collect hits against each individual SegmentReader. All the other changes stem from this ;)
* Deprecates
** TopFieldDocCollector
** FieldSortedHitQueue
"
1,"SloppyPhraseScorer sometimes computes Infinite freqreported on user list:
http://www.lucidimagination.com/search/document/400cbc528ed63db9/score_of_infinity_on_dismax_query
"
0,"Path should not be encoded in HttpMethodBaseI suggest to change the protocol or add a new method for this one

protected static String generateRequestLine(HttpConnection connection,
	String name, String reqPath,
	String qString, String protocol);
so that we can choose to use URIUtil.encode(reqPath, URIUtil.pathSafe()) or not

The reason is that after the encoding process, some server cannot recognize this
Actually, I am handling a project of the Method Propfind(for getting mail from 
Hotmail) and I find that the restriction of Hotmail server is quite high, and 
if the address is encoded, it does not work."
1,"Problems with File Copy using WebDAVWhen i make a copy of files from one workspace to other (CTRL-C -> CTRL-V). The file isnt copied, but the original file is deleted."
0,"SQL2 queries are not loggedSQL2 queries are constructed via QueryObjectModel, and ran via QueryObjectModelImpl which does not log the run time.
I'll attach a run time log similar to the old one."
0,Add user manager performance tests We should add some performance tests for validating JCR-2710 and related. That is we should measure performance for creating users and groups and adding/removing users to/from groups. This should be done for both repository configurations: one with the old content model (group membership in property) and one with the new content model introduced with JCR-2710 (group membership in b-tree like node structure). 
1,"NodeTypeDefDiff compares to restrictiveThe NodeTypeDefDiff class is used to compare NodeTypeDef instances. Unfortunately this class reports two NodeTypeDef instances which are not equal but have no structural difference as having trivial changes. The correct result would be to have no modification at all.

I suggest to modify the NodeTypeDefDiff.init() method such, that the initial type is ""NONE"" instead of ""TRIVIAL"" and to first compare the ""hasOrderableChildNodes"" first and raise the level to ""TRIVIAL"" if not equal. Next the rest of the current comparisons would follow."
1,"Use of java.net.URI.resolve() is buggyThe use of java.net.URI.resolve() is buggy (see <http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4708535>). Affected class: org.apache.http.impl.client.DefaultRedirectHandler.

Proposed solution: Create a resolve(URI, String) method in o.a.h.client.utils.URLUtils."
0,Make HighFreqTerms.TermStats class publicIt's not possible to use public methods in contrib/misc/... /HighFreqTerms from outside the package because the return type has package visibility. I propose to move TermStats class to a separate file and make it public.
0,"deprecate Document.fields(), add getFields()A simple API improvement that I'm going to commit if nobody objects."
1,"IndexReader.isCurrent() lies if documents were only removed by latest commitUsecase is as following:

1. Get indexReader via indexWriter.
2. Delete documents by Term via indexWriter. 
3. Commit indexWriter.
4. indexReader.isCurrent() returns true.

Usually there is a check if index reader is current. If not then it is reopened (re-obtained via writer or ect.). But this causes the problem when documents can still be found through the search after deletion.
Testcase is attached."
0,"Test class in the main source treeorg.apache.jackrabbit.core.TestRepository is in the main source folder
(src/main/java) instead of the test folder (src/test/java).

The build of jackrabbit-core is successful even if I move the class
to the test folder, so it looks like it was just a mistake."
1,"DisjunctionSumScorer gives slightly (float iotas) different scores when you .nextDoc vs .advanceSpinoff from LUCENE-1536.

I dug into why we hit a score diff when using luceneutil to benchmark
the patch.

At first I thought it was BS1/BS2 difference, but because of a bug in
the patch it was still using BS2 (but should be BS1) -- Robert's last
patch fixes that.

But it's actually a diff in BS2 itself, whether you next or advance
through the docs.

It's because DisjunctionSumScorer, when summing the float scores for a
given doc that matches multiple sub-scorers, might sum in a different
order, when you had .nextDoc'd to that doc than when you had .advance'd
to it.

This in turn is because the PQ used by that scorer (ScorerDocQueue)
makes no effort to break ties.  So, when the top N scorers are on the
same doc, the PQ doesn't care what order they are in.

Fixing ScorerDocQueue to break ties will likely be a non-trivial perf
hit, though, so I'm not sure whether we should do anything here..."
1,"TermSpans skipTo() doesn't always move forwardsIn TermSpans (or the anonymous Spans class returned by SpansTermQuery, depending on the version), the skipTo() method is improperly implemented if the target doc is less than or equal to the current doc:

  public boolean skipTo(int target) throws IOException {
          // are we already at the correct position?
          if (doc >= target) {
            return true;
          }

          ...


This violates the correct behavior (as described in the Spans interface documentation), that skipTo() should always move forwards, in other words the correct implementation would be:

if (doc >= target) {
  return next();
}

This bug causes particular problems if one wants to use the payloads feature - this is because if one loads a payload, then performs a skipTo() to the same document, then tries to load the ""next"" payload, the spans hasn't changed position and it attempts to load the same payload again (which is an error)."
0,"SweetSpotSimiliarityThis is a new Similarity implimention for the contrib/miscellaneous/ package, it provides a Similiarty designed for people who know the ""sweetspot"" of their data.  three major pieces of functionality are included:

1) a lengthNorm which creates a ""platuea"" of values.
2) a baseline tf that provides a fixed value for tf's up to a minimum, at which point it becomes a sqrt curve (this is used by the tf(int) function.
3) a hyperbolic tf function which is best explained by graphing the equation.  this isn't used by default, but is available for subclasses to call from their own tf functions.

All constants used in all functions are configurable.  In the case of lengthNorm, the constants are configurable per field, as well as allowing for defaults for unspecified fields."
0,SPI: provide batch read functionalityextend RepositoryService interface to allow for BatchRead and modify jcr2spi accordingly.
0,CharacterCache - references deleted CharacterCache is deprecated by Character.valueOf(c) . Hence the latter is chosen over the former. 
0,"Mark pending nodes in IndexingQueue directly in indexThe index currently writes an indexing_queue.log file which contains all nodes that timed out while text was extracted. Instead, the index itself should mark an indexed node as pending. This is more robust because no additional file must be written."
0,"JSR 283: Access Nodes and Properties by Array of ""NameGlob""The proposed final draft contains new variants of Node.getNodes and Node.getProperties:

- Node.getNodes(String[] nameGlobs)
- Node.getProperties(String[] nameGlobs)

see section 5.2.2 Iterating Over Child Items and 5.2.2.2 Name Globs"
0,"TopTermsScoringBooleanQueryRewrite minscorewhen using the TopTermsScoringBooleanQueryRewrite (LUCENE-2123), it would be nice if MultiTermQuery could set an attribute specifying the minimum required score once the Priority Queue is filled. 

This way, FilteredTermsEnums could adjust their behavior accordingly based on the minimal score needed to actually be a useful term (i.e. not just pass thru the pq)

An example is FuzzyTermsEnum: at some point the bottom of the priority queue contains words with edit distance of 1 and enumerating any further terms is simply a waste of time.
This is because terms are compared by score, then termtext. So in this case FuzzyTermsEnum could simply seek to the exact match, then end.

This behavior could be also generalized for all n, for a different impl of fuzzyquery where it is only looking in the term dictionary for words within edit distance of n' which is the lowest scoring term in the pq (they adjust their behavior during enumeration of the terms depending upon this attribute).

Other FilteredTermsEnums could make use of this minimal score in their own way, to drive the most efficient behavior so that they do not waste time enumerating useless terms.
"
0,"FastVectorHighlighter: support for additional queriesI am using fastvectorhighlighter for some strange languages and it is working well! 

One thing i noticed immediately is that many query types are not highlighted (multitermquery, multiphrasequery, etc)
Here is one thing Michael M posted in the original ticket:

{quote}
I think a nice [eventual] model would be if we could simply re-run the
scorer on the single document (using InstantiatedIndex maybe, or
simply some sort of wrapper on the term vectors which are already a
mini-inverted-index for a single doc), but extend the scorer API to
tell us the exact term occurrences that participated in a match (which
I don't think is exposed today).
{quote}

Due to strange requirements I am using something similar to this (but specialized to our case).
I am doing strange things like forcing multitermqueries to rewrite into boolean queries so they will be highlighted,
and flattening multiphrasequeries into boolean or'ed phrasequeries.
I do not think these things would be 'fast', but i had a few ideas that might help:

* looking at contrib/highlighter, you can support FilteredQuery in flatten() by calling getQuery() right?
* maybe as a last resort, try Query.extractTerms() ?
"
0,"Update link for javadocs from 1.0 to 1.3On this page: http://jackrabbit.apache.org/doc/arch/overview/jcrlevels.html

You see this link:
Browse current Jackrabbit API: http://jackrabbit.apache.org/api-1/index.html

This should point to the latest javadoc version."
0,"improve how IndexWriter uses RAM to buffer added documentsI'm working on a new class (MultiDocumentWriter) that writes more than
one document directly into a single Lucene segment, more efficiently
than the current approach.

This only affects the creation of an initial segment from added
documents.  I haven't changed anything after that, eg how segments are
merged.

The basic ideas are:

  * Write stored fields and term vectors directly to disk (don't
    use up RAM for these).

  * Gather posting lists & term infos in RAM, but periodically do
    in-RAM merges.  Once RAM is full, flush buffers to disk (and
    merge them later when it's time to make a real segment).

  * Recycle objects/buffers to reduce time/stress in GC.

  * Other various optimizations.

Some of these changes are similar to how KinoSearch builds a segment.
But, I haven't made any changes to Lucene's file format nor added
requirements for a global fields schema.

So far the only externally visible change is a new method
""setRAMBufferSize"" in IndexWriter (and setMaxBufferedDocs is
deprecated) so that it flushes according to RAM usage and not a fixed
number documents added.
"
1,FastVectorHighlighter: highlighted term is out of alignment in multi-valued NOT_ANALYZED field
1,"Highlighting overlapping tokens outputs doubled wordsIf for the text ""the fox did not jump"" we generate following tokens :
(the, 0, 0-3),({fox},0,0-7),(fox,1,4-7),(did,2,8-11),(not,3,12,15),(jump,4,16,18)

If TermVector for field is stored WITH_OFFSETS and not WITH_POSITIONS_OFFSETS, highlighing would output
""the<em>the fox</em> did not jump""

I join a patch with 2 additive JUnit tests and a fix of TokenSources class where token ordering by offset did'nt manage well overlapping tokens.
"
0,"[PATCH] remove minor unneeded code stutterCode has a repeated method call on isOrderable for no reason as such

{code}
public String getSupportedMethods() {
        String ms = super.getSupportedMethods();
        if (isOrderable()) {
            StringBuffer sb = new StringBuffer(ms);
            // Ordering
            if (isOrderable()) {
                sb.append("", "").append(OrderingResource.METHODS);
            }
            return sb.toString();
        } else {
            return ms;
        }
    }
{code}

patch cleans this up."
0,"internal hashing improvementsInternal power-of-two closed hashtable traversal in DocumentsWriter and CharArraySet could be better.

Here is the current method of resolving collisions:
    if (text2 != null && !equals(text, len, text2)) {
      final int inc = code*1347|1;
      do {
        code += inc;
        pos = code & mask;
        text2 = entries[pos];
      } while (text2 != null && !equals(text, len, text2));

The problem is that two different hashCodes with the same lower bits will keep picking the same slots (the upper bits will be ignored).
This is because multiplication (*1347) only really shifts bits to the left... so given that the two codes already matched on the right, they will both pick the same increment, and this will keep them on the same path through the table (even though it's being added to numbers that differ on the left).  To resolve this, some bits need to be moved to the right when calculating the increment.

"
0,"IndexReader.reopen()This is Robert Engels' implementation of IndexReader.reopen() functionality, as a set of 3 new classes (this was easier for him to implement, but should probably be folded into the core, if this looks good).
"
1,"Data Store: DB2 fails to create the tableDB2 throws an exception(1) when creating the table. The correct SQL sentence to create it is:
createTable=CREATE TABLE ${tablePrefix}${table}(ID VARCHAR(255) PRIMARY KEY NOT NULL, LENGTH BIGINT, LAST_MODIFIED BIGINT, DATA BLOB(1000M))
(1): Sorry but I don't have the exception information since I made this change a few weeks ago."
1,"Locking two same-name siblings and unlocking first apparently unlocks second instead.Executing the following test that unlocks the first of two locked same-name siblings:

public void testLocking() throws RepositoryException {
       Session jcrSession = ((S1SessionImpl) session).getSession();
       Node rootNode = jcrSession.getRootNode();

       Node n1 = rootNode.addNode(""path"");
       n1.addMixin(""mix:lockable"");
       Node n2 = rootNode.addNode(""path"");
       n2.addMixin(""mix:lockable"");

       jcrSession.save();

       n1.lock(true, true);
       n2.lock(true, true);

       System.out.println(""n1.isLocked() = "" + n1.isLocked());
       System.out.println(""n2.isLocked() = "" + n2.isLocked());
       assertTrue(n1.isLocked());
       assertTrue(n2.isLocked());

       n1.save();
       n1.unlock();

       System.out.println(""n1.isLocked() = "" + n1.isLocked());
       System.out.println(""n2.isLocked() = "" + n2.isLocked());
       assertFalse(n1.isLocked());
       assertTrue(n2.isLocked());
   }

Results in:

n1.isLocked() = true
n2.isLocked() = true
n1.isLocked() = true
n2.isLocked() = false

which is wrong."
0,"Allow Directory.copy() to accept a collection of file names to be copiedPar example, I want to copy files pertaining to a certain commit, and not everything there is in a Directory."
0,"Using deprecated class javax.servlet.http.HttpUtils[javac]
test-webapp/src/org/apache/commons/httpclient/RedirectServlet.java:74: warning:
javax.servlet.http.HttpUtils in javax.servlet.http has been deprecated
    [javac]             to =
HttpUtils.getRequestURL(request).append(""?"").append(request.getQueryString()).toString();

The javax.servlet.http.HttpUtils class is deprecated in Tomcat 4.1.18 and should
not be used."
0,"DateTools.java general improvementsApplying the attached patch shows the improvements to DateTools.java that I think should be done. All logic that does anything at all is moved to instance methods of the inner class Resolution. I argue this is more object-oriented.

1. In cases where Resolution is an argument to the method, I can simply invoke the appropriate call on the Resolution object. Formerly there was a big branch if/else.
2. Instead of ""synchronized"" being used seemingly everywhere, synchronized is used to sync on the object that is not threadsafe, be it a DateFormat or Calendar instance.
3. Since different DateFormat and Calendar instances are created per-Resolution, there is now less lock contention since threads using different resolutions will not use the same locks.
4. The old implementation of timeToString rounded the time before formatting it. That's unnecessary since the format only includes the resolution desired.
5. round() now uses a switch statement that benefits from fall-through (no break).

Another debatable improvement that could be made is putting the resolution instances into an array indexed by format length. This would mean I could remove the switch in lookupResolutionByLength() and avoid the length constants there. Maybe that would be a bit too over-engineered when the switch is fine."
1,"MultiSearcher.explain returns incorrect score/explanation relating to docFreqCreating 2 different indexes, searching  each individually and print score details and compare to searching both indexes with MulitSearcher and printing score details.  
 
The ""docFreq"" value printed isn't correct - the values it prints are as if each index was searched individually.

Code is like:
{code}
MultiSearcher multi = new MultiSearcher(searchables);
Hits hits = multi.search(query);
for(int i=0; i<hits.length(); i++)
{
  Explanation expl = multi.explain(query, hits.id(i));
  System.out.println(expl.toString());
}
{code}

I raised this in the Lucene user mailing list and was advised to log a bug, email thread given below.

{noformat} 
-----Original Message-----
From: Chris Hostetter  
Sent: Friday, December 07, 2007 10:30 PM
To: java-user
Subject: Re: does the MultiSearcher class calculate IDF properly?


a quick glance at the code seems to indicate that MultiSearcher has code 
for calcuating the docFreq accross all of the Searchables when searching 
(or when the docFreq method is explicitly called) but that explain method 
just delegates to Searchable that the specific docid came from.

if you compare that Explanation score you got with the score returned by 
a HitCollector (or TopDocs) they probably won't match.

So i would say ""yes MultiSearcher calculates IDF properly, but 
MultiSeracher.explain is broken.  Please file a bug about this, i can't 
think of an easy way to fix it, but it certianly seems broken to me.


: Subject: does the MultiSearcher class calculate IDF properly?
: 
: I tried the following.  Creating 2 different indexes, search each
: individually and print score details and compare to searching both
: indexes with MulitSearcher and printing score details.  
: 
: The ""docFreq"" value printed don't seem right - is this just a problem
: with using Explain together with the MultiSearcher?
: 
: 
: Code is like:
: MultiSearcher multi = new MultiSearcher(searchables);
: Hits hits = multi.search(query);
: for(int i=0; i<hits.length(); i++)
: {
:   Explanation expl = multi.explain(query, hits.id(i));
:   System.out.println(expl.toString());
: }
: 
: 
: Output:
: id = 14 score = 0.071
: 0.07073946 = (MATCH) fieldWeight(contents:climate in 2), product of:
:   1.0 = tf(termFreq(contents:climate)=1)
:   1.8109303 = idf(docFreq=1)
:   0.0390625 = fieldNorm(field=contents, doc=2)
{noformat} "
1,"Garbage data when reading a compressed, text field, lazilylazy compressed text fields is a case that was neglected during lazy field implementation.  TestCase and patch provided.

"
1,"PrecedenceQueryParser misinterprets queries starting with NOT""NOT foo AND baz"" is parsed as ""-(+foo +baz)"" instead of ""-foo +bar"".

(I'm setting parser.setDefaultOperator(PrecedenceQueryParser.AND_OPERATOR) but the issue applies otherwise too.)
"
0,"small speedups to bulk mergingThe bulk merging code, for stored fields & term vectors, was calling isDeleted twice for each deleted doc.

Patch also changes DocumentsWriter to use IndexWriter.message for its infoStream messages."
0,"random localization test failuresSome tests fail randomly (hard to reproduce). It appears to me that this is caused by uninitialized date fields. For example Uwe reported a failure today in this test of TestQueryParser:

{code}
 /** for testing legacy DateField support */
  public void testLegacyDateRange() throws Exception {
    String startDate = getLocalizedDate(2002, 1, 1, false);
    String endDate = getLocalizedDate(2002, 1, 4, false);
{code}

if you look at the helper getLocalizedDate, you can see if the 4th argument is false, it does not initialize all date field functions.
{code}
  private String getLocalizedDate(int year, int month, int day, boolean extendLastDate) {
 Calendar calendar = new GregorianCalendar();
 calendar.set(year, month, day);
 if (extendLastDate) {
      calendar.set(Calendar.HOUR_OF_DAY, 23);
      calendar.set(Calendar.MINUTE, 59);
      calendar.set(Calendar.SECOND, 59);
 ...
}
{code}

I think the solution to this is that in all tests, whereever we create new GregorianCalendar(), it should be followed by a call to Calendar.clear().
This will ensure that we always initialize unused calendar fields to zero, rather than being dependent on the local time."
0,Disable consistency check per defaultThere should be a way to disable the consistency check entirely. Currently a consistency check is performed on startup whenever the redo log is applied. For large workspaces this may take a long time and should only be performed when 'requested'.
1,"While indexing Turkish web pages, ""Parse Aborted: Lexical error...."" occursWhen I try to index Turkish page if there is a Turkish specific character in the HTML specific tag HTML parser gives ""Parse Aborted: Lexical error.on ... line"" error.
For this case ""<IMG SRC=""../images/head.jpg"" WIDTH=570 HEIGHT=47 BORDER=0 ALT="""">"" exception address """" character (which has 351 ascii value) as an error. OR  character in title tag.
<a title=""()"">

Turkish character in the content do not create any problem."
0,"Some concurrency improvements for NRTSome concurrency improvements for NRT

I found & fixed some silly thread bottlenecks that affect NRT:

  * Multi/DirectoryReader.numDocs is synchronized, I think so only 1
    thread computes numDocs if it's -1.  I removed this sync, and made
    numDocs volatile, instead.  Yes, multiple threads may compute the
    numDocs for the first time, but I think that's harmless?

  * Fixed BitVector's ctor to set count to 0 on creating a new BV, and
    clone to copy the count over; this saves CPU computing the count
    unecessarily.

  * Also strengthened assertions done in SR, testing the delete docs
    count.

I also found an annoying thread bottleneck that happens, due to CMS.
Whenever CMS hits the max running merges (default changed from 3 to 1
recently), and the merge policy now wants to launch another merge, it
forces the incoming thread to wait until one of the BG threads
finishes.

This is a basic crude throttling mechanism -- you force the mutators
(whoever is causing new segments to appear) to stop, so that merging
can catch up.

Unfortunately, when stressing NRT, that thread is the one that's
opening a new NRT reader.

So, the first serious problem happens when you call .reopen() on your
NRT reader -- this call simply forwards to IW.getReader if the reader
was an NRT reader.  But, because DirectoryReader.doReopen is
synchronized, this had the horrible effect of holding the monitor lock
on your main IR.  In my test, this blocked all searches (since each
search uses incRef/decRef, still sync'd until LUCENE-2156, at least).
I fixed this by making doReopen only sync'd on this if it's not simply
forwarding to getWriter.  So that's a good step forward.

This prevents searches from being blocked while trying to reopen to a
new NRT.

However... it doesn't fix the problem that when an immense merge is
off and running, opening an NRT reader could hit a tremendous delay
because CMS blocks it.  The BalancedSegmentMergePolicy should help
here... by avoiding such immense merges.

But, I think we should also pursue an improvement to CMS.  EG, if it
has 2 merges running, where one is huge and one is tiny, it ought to
increase thread priority of the tiny one.  I think with such a change
we could increase the max thread count again, to prevent this
starvation.  I'll open a separate issue....
"
0,TestFieldsReader - TestLazyPerformance problems w/ permissions in temp dir in multiuser environmentWas trying to setup some enhancements to the nightly builds and the testLazyPerformance test failed in TestFieldsReader since it couldn't delete the lazyDir directory from someone else's running of the test.  Will change it to append user.name System property to the directory name.
0,Backport JCR-1111: Access to version history results in reading all versions of versionable nodeBackport issue JCR-1111 (Accesss to version history results in reading all versions of versionable node) to 1.3 branch for 1.3.4 (separate issue to avoid re-opening JCR-1111 which was already released with 1.4).
0,"Http Client give sme message when proxy/http endpoint is downWhether Http sever endpoint is down or the proxy server is down we get the same stack trace as:

java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
	at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
	at java.net.Socket.connect(Socket.java:518)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.commons.httpclient.protocol.ReflectionSocketFactory.createSocket(ReflectionSocketFactory.java:139)
	at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:124)
	at org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:706)
	at org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnectionAdapter.open(MultiThreadedHttpConnectionManager.java:1321)
	at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:386)
	at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:170)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:396)
	at com.approuter.module.http.protocol.HttpTransportSender.perform(HttpTransportSender.java:214)
	at 


It will be good if we can get information whether the proxy was down or the Http endpoint.

"
0,"javadoc writing and generation with mvn""mvn -source 1.5 javadoc:javadoc"" does not work because following lines must be added to pom.xml
<plugin>
      <artifactId>maven-javadoc-plugin</artifactId>
      <configuration>
        <source>1.5</source>
      </configuration>
    </plugin> 
Please also write more comprehensive javadocs to allow better source understanding and developer co-operation"
0,"Safe namespace registrationThe namespace registration methods provided by the JCR NamespaceRegistry API are cumbersome to use and vulnerable to race conditions in the event of conflicting prefix mappings. This problem was discussed lately on the mailing list (see http://article.gmane.org/gmane.comp.apache.jackrabbit.devel/6805) and one symptom of the problem is the new code in NodeTypeManagerImpl (see JCR-349):

    //  Registers a namespace...
    try {
        nsReg.getPrefix(uri);
    } catch (NamespaceException e1) {
        String original = prefix;
        for (int i = 2; true; i++) {
            try {
                nsReg.registerNamespace(prefix, uri);
                return;
            } catch (NamespaceException e2) {
                prefix = original + i;
            }
        }
    }

We should add an internal convenience method like NamespaceRegistryImpl.safeRegisterNamespace(String prefixHint, String uri) that, instead of throwing an exception, would automatically generate and use a unique prefix based on the given hint when a prefix conflict is detected."
0,"JSR 283: References and Dereferencing of Property ValuesReferences
--------------------------------------------------------------------------------------------------------------------------
new methods are:

- Node.getReferences(String name) PropertyIterator 
- Node.getWeakReferences() PropertyIterator 
- Node.getWeakReferences(String name) PropertyIterator


Derferencing
--------------------------------------------------------------------------------------------------------------------------
As of JSR 283 the following property types may be dereferenced to a Node:

- REFERENCE
- WEAKREFERENCE
- PATH
- any type that can be converted to either of the types above

The new method
- Property.getProperty() returns the Property pointed to by a PATH value.
- any type that can be converted to PATH




"
1,"PathElement.equals doesn't handle INDEX_UNDEFINEDPathElement (and therefore Path) comparisons fail when INDEX_UNDEFINED is used (it's treated differently from INDEX_DEFAULT).
"
0,"It should be possible to create a non-transient Repository inside the JCARepositoryManagerWith JCR-2555 jukka changed the code to create a Repository with the RepositoryFactory mechanism.
It should be possible to create a non-transient Repository
"
1,"Deadlock in acl.EntryCollector / ItemManagerHere's another three-way deadlock that we've encountered:

* Thread A holds a downgraded SISM write lock and is about to start delivering observation events to synchronous listeners
* Thread B wants to write something and blocks waiting for the SISM write lock (since A holds the lock)
* Thread C wants to read something and blocks waiting for the SISM read lock (since B waits for the lock)

Normally such a scenario is handled without any problems, but there's a problem if the session used by thread C has a synchronous observation listener that attempts to read something from the repository during event delivery. In this case the following can happen:

* Thread C holds the ItemManager synchronization lock higher up in the call chain
* Observation listener code called by thread A attempts to read something from the repository, and blocks trying to acquire the ItemManager synchronization lock (since C holds it)

In principle such a scenario should never happen as an observation listener (much less a synchronous one) should never try to use the session that might already be in use by another thread.

Unfortunately the EntryCollector class in o.a.j.core.security.authorization.acl does not follow this guideline, which leads to the deadlock as shown below:

Thread A:
""127.0.0.1 [1297191119365] POST /bin/wcmcommand HTTP/1.0"" nid=1179 state=BLOCKED
    - waiting on <0x11c329fd> (a org.apache.jackrabbit.core.ItemManager)
    - locked <0x11c329fd> (a org.apache.jackrabbit.core.ItemManager)
     owned by 127.0.0.1 [1297191138443] POST /bin/wcmcommand HTTP/1.0 id=67
    at org.apache.jackrabbit.core.ItemManager.getNode(ItemManager.java:653)
    at org.apache.jackrabbit.core.ItemManager.getNode(ItemManager.java:605)
    at org.apache.jackrabbit.core.SessionImpl.getNode(SessionImpl.java:1406)
    at org.apache.jackrabbit.core.security.authorization.acl.EntryCollector.onEvent(EntryCollector.java:253)
    at org.apache.jackrabbit.core.observation.EventConsumer.consumeEvents(EventConsumer.java:246)
    at org.apache.jackrabbit.core.observation.ObservationDispatcher.dispatchEvents(ObservationDispatcher.java:214)
    at org.apache.jackrabbit.core.observation.EventStateCollection.dispatch(EventStateCollection.java:475)
    at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:786)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:1488)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:349)
    at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:354)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:324)
    at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:328)
    at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1141)
    at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:920)

Thread B:
""Thread-2438"" nid=2582 state=WAITING
    - waiting on <0x166e790e> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
    - locked <0x166e790e> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
    at java.lang.Object.wait(Native Method)
    at java.lang.Object.wait(Object.java:485)
    at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock.acquire(Unknown Source)
    at org.apache.jackrabbit.core.state.DefaultISMLocking$WriteLockImpl.<init>(DefaultISMLocking.java:76)
    at org.apache.jackrabbit.core.state.DefaultISMLocking$WriteLockImpl.<init>(DefaultISMLocking.java:70)
    at org.apache.jackrabbit.core.state.DefaultISMLocking.acquireWriteLock(DefaultISMLocking.java:64)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.acquireWriteLock(SharedItemStateManager.java:1808)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.access$200(SharedItemStateManager.java:112)
    at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:565)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:1458)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:1488)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:349)
    at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:354)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:324)
    at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:328)
    at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1141)
    at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:920)

Thread C:
""127.0.0.1 [1297191138443] POST /bin/wcmcommand HTTP/1.0"" nid=67 state=WAITING
    - waiting on <0xf820edb> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$ReaderLock)
    - locked <0xf820edb> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$ReaderLock)
    at java.lang.Object.wait(Native Method)
    at java.lang.Object.wait(Object.java:485)
    at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$ReaderLock.acquire(Unknown Source)
    at org.apache.jackrabbit.core.state.DefaultISMLocking$ReadLockImpl.<init>(DefaultISMLocking.java:102)
    at org.apache.jackrabbit.core.state.DefaultISMLocking$ReadLockImpl.<init>(DefaultISMLocking.java:96)
    at org.apache.jackrabbit.core.state.DefaultISMLocking.acquireReadLock(DefaultISMLocking.java:53)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.acquireReadLock(SharedItemStateManager.java:1794)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.getItemState(SharedItemStateManager.java:257)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.getNodeState(LocalItemStateManager.java:107)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.getItemState(LocalItemStateManager.java:171)
    at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:200)
    at org.apache.jackrabbit.core.ItemManager.getItemData(ItemManager.java:391)
    at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:337)
    at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:638)
    at org.apache.jackrabbit.core.security.authorization.acl.ACLProvider$AclPermissions.canRead(ACLProvider.java:507)
      - locked java.lang.Object@6ad9b475
    at org.apache.jackrabbit.core.security.DefaultAccessManager.canRead(DefaultAccessManager.java:251)
    at org.apache.jackrabbit.core.query.lucene.QueryResultImpl.isAccessGranted(QueryResultImpl.java:374)
    at org.apache.jackrabbit.core.query.lucene.QueryResultImpl.collectScoreNodes(QueryResultImpl.java:353)
    at org.apache.jackrabbit.core.query.lucene.QueryResultImpl.getResults(QueryResultImpl.java:310)
    at org.apache.jackrabbit.core.query.lucene.SingleColumnQueryResult.<init>(SingleColumnQueryResult.java:70)
    at org.apache.jackrabbit.core.query.lucene.QueryImpl.execute(QueryImpl.java:133)
    at org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:127)
"
0,"Jackrabbit does not allow concurrent reads to the data store if copyWhenReading=falseJackrabbit does not allow concurrent reads to the data store if copyWhenReading=false, even if maxConnections>1.
See JCR-1184 for a test for this problem (run it with copyWhenReading=false).
"
1,"Error reading dataHi,

I have some problems with HttpClient HEAD. It works fine with a build of 
20020720 of HttpClient though.

It seems HttpClient is not reading correctly the returned HTTP response.

I'm attaching the logs.

Here is the output from Cactus build:



     [java]     [junit] Testcase: testLongProcess took 3.645 sec
     [java]     [junit]         Caused an ERROR
     [java]     [junit] Failed to get the test results. This is probably due 
to an error that happen
ed on the server side when trying to execute the tests. Here is what was 
returned by the server : [<
html><head><Long Process></head><body>Some data</body></html>
     [java]     [junit] ]
     [java]     [junit] org.apache.cactus.util.ChainedRuntimeException: Failed 
to get the test resul
ts. This is probably due to an error that happened on the server side when 
trying to execute the tes
ts. Here is what was returned by the server : [<html><head><Long 
Process></head><body>Some data</bod
y></html>
     [java]     [junit] ]
     [java]     [junit]         at 
org.apache.cactus.client.AbstractHttpClient.doTest(Unknown Source
)
     [java]     [junit]         at 
org.apache.cactus.AbstractWebTestCase.runWebTest(Unknown Source)
     [java]     [junit]         at 
org.apache.cactus.AbstractWebTestCase.runGenericTest(Unknown Sour
ce)
     [java]     [junit]         at org.apache.cactus.ServletTestCase.runTest
(Unknown Source)
     [java]     [junit]         at org.apache.cactus.AbstractTestCase.runBare
(Unknown Source)
     [java]     [junit] org.apache.cactus.client.ParsingException: Not a valid 
response. First 100 c
haracters of the reponse: [</webresult>HTTP/1.1 200 OK
     [java]     [junit] Server: Resin/2.1.2
     [java]     [junit] Content-Length: 23
     [java]     [junit] Date: Tue, 13 Aug 2002 08:45:2]
     [java]     [junit]         at 
org.apache.cactus.client.WebTestResultParser.readExceptionClassna
me(Unknown Source)
     [java]     [junit]         at 
org.apache.cactus.client.WebTestResultParser.parse(Unknown Source

Thanks
-Vincent"
1,"Observation tests failPath returned by Event.getPath() is wrong. It always returns the path to the parent node connected to the event. That is, if e.g. a node /foo/bar is created the path /foo is returned instead of /foo/bar.

This issue had been introduced with changed from api version 0.14 to 0.15."
1,"Bundle of events may be dropped due to NP.In [1], if the resolver fails to lookup a node entry, a NP is thrown. This exception will break the loop which forwards the events to the observer in [2].
This will result in an observer not receiving events that he should have.

[1] org.apache.jackrabbit.jcr2spi.hierarchy.HierarchyManagerImpl#lookup(ItemId workspaceItemId)
[2] org.apache.jackrabbit.jcr2spi.WorkspaceManager#onEventReceived(EventBundle[] eventBundles,InternalEventListener[] lstnrs)"
1,"An IOException or RuntimeException leaves the underlying socket in an undetermined stateIf an application level IOException or RuntimeException occurs, the underlying
socket will be in an undetermined state. In many cases, this will lead to zombie
connections in the pool that do not respond properly.

Simple example: uploading a file via POST. If we promise the server 1MB of data.
Shortly after starting the transfer an IOException occurs (e.g. the NFS server
the file was residing on stops responding). The connection is returned to the
pool (see HTTPCLIENT-302) but the the server is still expecting close to 1MB of data
on that socket. The next request on that socket (e.g. a GET) will send the HTTP
header but  the server thinks the header is part of the old stream and doesn't
respond."
0,JSR 283 support
0,"Typo in PropertyDefinitionTemplatesetValueConstarints should read setValueConstraints


"
1,"BlockJoinQuery advance fails on an assert in case of a single parent with child segmentThe BlockJoinQuery will fail on an assert when advance in called on a segment with a single parent with a child. The call to parentBits.prevSetBit(parentTarget - 1) will cause -1 to be returned, and the assert will fail, though its valid. Just removing the assert fixes the problem, since nextDoc will handle it properly.

Also, I don't understand the ""assert parentTarget != 0;"", with a comment of each parent must have one child. There isn't really a reason to add this constraint, as far as I can tell..., just call nextDoc in this case, no?"
1,"Benchmark package uses new TopFieldCollector but also still uses AUTO without resolving it - result is, our sort algorithms won't runAUTO does not work with TopFieldCollector. If you want to use AUTO with TopFieldCollector, we have a convienence method called detectType on SortField, but it is package protected and so cannot be used here as a stop gap or by users if they wanted to mix AUTO with TopFieldCollector. Lucene does still handle this for back compat internally. Solr got bit here when it was switched to use TopFieldCollector - no auto resolution was added (detectType help couldn't have been used due to visibility), and the result was that plugin code that used to be able to use AUTO would now blow up. You shouldn't use AUTO in Solr anyway though.

The Benchmark package got bit as well  when it moved to TopFieldCollector. Sort algorithms allowed auto if you specified it, or if you left off the type. Now our sort algs fail because they didn't specify a type.

I'll change to require the type to be specified to get the algs working again. I was thinking of just putting auto resolution in as a stop gap till 3.0 (when auto is removed), but since detectFieldType is package protected and I don't want to repeat it, disallowing auto seems the best way to go."
1,"Oracle bundle PM fails checking schema if 2 users use the same databaseWhen using the OracleBundlePersistenceManager there is an issue when two users use the same database for persistence. In  that case, the checkSchema() method of the BundleDbPersistenceManager  does not work like it should. More precisely, the call ""metaData.getTables(null,  null, tableName, null);"" will also includes table names of other  schemas/users. Effectively, only the first user of a database is able to create  the schema.

probably same issue as here: JCR-582"
1,"Benchmark alg line -  {[AddDoc(4000)]: 4} : * - causes an infinite loopBackground in http://www.mail-archive.com/java-dev@lucene.apache.org/msg10831.html 
The line  
   {[AddDoc(4000)]: 4} : * 
causes an infinite loop because the parallel sequence would mask the exhaustion from the outer sequential sequence.

To fix this the DocMaker exhaustion check should be modified to rely  on the doc maker instance only, and to be reset when the inputs are being reset. "
0,"Clean up spi-commons pom.xmlThe pom.xml contains lines that were copied from the jackrabbit-core but are not actually needed. A log4j.properties is also missing in test resources.

See attached patch."
0,"FastVectorHighlighter: some classes and members should be publicly accessible to implement FragmentsBuilderI intended to design custom FragmentsBuilder can be written and pluggable, though, when I tried to write it out of the FVH package, it came out that some classes and members should be publicly accessible."
1,"spi2dav: ItemInfoCache causes failure of (Workspace)RestoreTest#testRestoreWithUUIDConflict and variantswhile running the API version tests i found the (Workspace)RestoreTest.testRestoreWithUUIDConfict and variants failing. to be precise the test passes but
transiently removing the versionableNode2 in the teardown fails upon removal of the jcr:uuid property of the moved childnode.

having a closer look at it revealed that the problem is caused in the WorkspaceItemStateFactory where the property entry is retrieved from the
cache and subsequently checking if the path really matches fails. for test purposes i prevented the usage of the cached entry by returning false in WorkspaceItemStateFactory.isUpToDate  => the tests passed. 

as far as i know the same tests pass with spi2jcr.
michael, could it be that this is caused by a flaw in the iteminfo-cache logic? or is there something specific that needs to be adjusted in spi2dav?"
1,"non-contiguous LogMergePolicy should be careful to not select merges already runningNow that LogMP can do non-contiguous merges, the fact that it disregards which segments are already being merged is more problematic since it could result in it returning conflicting merges and thus failing to run multiple merges concurrently.
"
1,"Parsing built-in CND and XML nodetypes does not result in equal nt-definitionsi created a test in order to make sure builtin-nodetypes.xml and builtin-nodetypes.cnd provide the same definitions (actually i only wanted to test my own changes).

it reveals that the existing built-in NodeTypeDefinitions are not equal due to the following reason:

- in the xml-format nt:base is always specified if no other super type extends from nt:base
- in the cnd notation the nt:base is omitted (see below for quote from appendix of jsr 283) even if other super type(s) are
  defined and none of them extends from nt:base.

this affects the following nodetypes (all extending from mix:referenceable only):

nt:versionHistory
nt:version
nt:frozenNode
nt:resource


quote from public-review of jsr 283:

""7.2.2.4 Supertypes [...]
After the node type name comes the optional list of supertypes. If this element is not present and the node type is not a mixin (see 7.2.2.5 Options), then a supertype of nt:base is assumed.""


I'm not totally sure, if according to the quote above the built-in cnd-definitions are valid at all. since it states, that the nt:base is assumed if no other super type is defined. In the case of the node types above, mix:referenceable is defined to be the only super type, which is not totally true... the non-mixin types are always sub types of nt:base.

In either case: From my understanding the node types resulting from parsing the xml and the cnd file should be equal.
If the definitions are valid, we may need to adjust the CompactNodeTypeDefReader.




"
0,"potential memory leak when using ThreadSafeClientConnManagerWhen using ThreadSafeClientConnManager and developing with Jetty using auto-redeploy feature eventually I run into a PermGen out of memory exception.  I investigated with YourKit 8.0.6 and found a class loader circular reference in RefQueueWorker.  Not really sure what I was doing I made the refQueueHandler non-final and nulled it in the shutdown method of RedQueueWorker.  I don't seem to have the problem any longer with circular class loader references.

Here is a diff from 4.0-beta2


--- httpclient/src/main/java/org/apache/http/impl/conn/tsccm/RefQueueWorker.jav(revision 763223)
+++ httpclient/src/main/java/org/apache/http/impl/conn/tsccm/RefQueueWorker.jav(working copy)
@@ -50,7 +50,7 @@
     protected final ReferenceQueue<?> refQueue;
 
     /** The handler for the references found. */
-    protected final RefQueueHandler refHandler;
+    protected RefQueueHandler refHandler;
 
 
     /**
@@ -112,6 +112,8 @@
             this.workerThread = null; // indicate shutdown
             wt.interrupt();
         }
+
+        refHandler = null;
     }
 
 
"
1,"IW.getReader() returns inconsistent reader on RT BranchI extended the testcase TestRollingUpdates#testUpdateSameDoc to pull a NRT reader after each update and asserted that is always sees only one document. Yet, this fails with current branch since there is a problem in how we flush in the getReader() case. What happens here is that we flush all threads and then release the lock (letting other flushes which came in after we entered the flushAllThread context, continue) so that we could concurrently get a new segment that transports global deletes without the corresponding add. They sneak in while we continue to open the NRT reader which in turn sees inconsistent results.

I will upload a patch soon"
1,"incorrect jcr:uuid on frozen subnodeThe following program:

import javax.jcr.Repository;
import javax.jcr.Session;
import javax.jcr.SimpleCredentials;
import javax.jcr.Node;
import org.apache.jackrabbit.core.TransientRepository;

public class debug2 {
    public static void main(String[] args) throws Exception {
        Repository repository = new TransientRepository();
        Session session = repository.login(
                new SimpleCredentials(""username"", ""password"".toCharArray()));
        try {
            Node root = session.getRootNode();

            Node foo = root.addNode(""foo"");
            foo.addMixin(""mix:versionable"");

            Node bar = foo.addNode(""bar"");
            bar.addMixin(""mix:referenceable"");
            System.out.println(""bar:            "" + bar.getUUID());

            session.save();
            foo.checkin();

            Node frozenbar = foo.getBaseVersion().getNode(""jcr:frozenNode"").getNode(""bar"");
            System.out.println(""frozenbar UUID: "" + frozenbar.getUUID());
            System.out.println(""jcr:uuid:       "" + frozenbar.getProperty(""jcr:uuid"").getValue().getString());
            System.out.println(""jcr:frozenUuid: "" + frozenbar.getProperty(""jcr:frozenUuid"").getValue().getString());

        } finally {
            session.logout();
        }
    }
}

Gives as sample output:
bar:            fcf0affb-7476-4a64-a480-3039e8c53d53
frozenbar UUID: ed9fece9-9837-4ecc-9b7e-55bdfb8284e2
jcr:uuid:       fcf0affb-7476-4a64-a480-3039e8c53d53
jcr:frozenUuid: fcf0affb-7476-4a64-a480-3039e8c53d53

The jcr:uuid of the frozen bar is incorrect (althoug getUUID() returns the correct value).
"
1,"NPE Exception Thrown By FileJournal During Commit OperationThe following exception stack traces appearing repeatedly during a performance test of a JCR cluster at a customer site. 

ERROR - Unexpected error while preparing log entry.
java.lang.NullPointerException
	at org.apache.jackrabbit.core.cluster.FileRevision.unlock(FileRevision.java:117)
	at org.apache.jackrabbit.core.cluster.FileRevision.get(FileRevision.java:146)
	at org.apache.jackrabbit.core.cluster.FileJournal.sync(FileJournal.java:296)
	at org.apache.jackrabbit.core.cluster.FileJournal.begin(FileJournal.java:435)
	at org.apache.jackrabbit.core.cluster.ClusterNode.updatePrepared(ClusterNode.java:399)
	at org.apache.jackrabbit.core.cluster.ClusterNode.access$000(ClusterNode.java:40)
	at org.apache.jackrabbit.core.cluster.ClusterNode$WorkspaceUpdateChannel.updatePrepared(ClusterNode.java:559)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:647)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:778)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:808)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:302)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:295)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1204)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:821)
	
ERROR - Unexpected error while committing log entry.
java.lang.NullPointerException
	at org.apache.jackrabbit.core.cluster.FileJournal.commit(FileJournal.java:660)
	at org.apache.jackrabbit.core.cluster.ClusterNode.updateCommitted(ClusterNode.java:425)
	at org.apache.jackrabbit.core.cluster.ClusterNode$WorkspaceUpdateChannel.updateCommitted(ClusterNode.java:566)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:712)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:808)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:302)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:295)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1204)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:821)
	"
0,"jcr:deref and parent axis in xpath predicatesCurrently, the jcr:deref() function is not allowed in a xpath query predicate. Example :
book holds a reference property on its author(s)
authors have a name

We want all books from a specific author :

/jcr:root/element(*, bookType)[jcr:deref(@author, 'authorType')/@name = 'King']

This fails with an InvalidQueryException currently (not supported).

The error is raised in the XPathQueryBuilder class, in function : private QueryNode createFunction(SimpleNode node, QueryNode queryNode), in the block :
else if (NameFormat.format(JCR_DEREF, resolver).equals(fName))

Problem is that with this query, when evaluating the jcr:deref() function, then in this method at this point, queryNode.getType() is 0 and tests raise the exception if queryNode.getType() is neither QueryNode.TYPE_LOCATION nor QueryNode.TYPE_PATH.

I think this is a useful place to put a deref function in a query, as I don't know how we could test the referenced node properties another way.

Frederic Esnault"
0,Enable DataStore in default configurationCurrently the default configuration causes binary properties to be stored in the derby database. This is very inefficient. The standalone server (and the web application it contains) should run with a reasonable default configuration. 
1,"Registering cyclic dependent nodetypes does not workwhen registering the followin 2 nodetypes:

[foo] 
+ mybar (bar)

[bar]
+ myfoo (foo)

NodeTypeRegistry.registerNodeTypes(Collection) throws:

 org.apache.jackrabbit.core.nodetype.InvalidNodeTypeDefException: the following node types could not be registered because of unresolvable dependencies: {}foo {}bar 
"
0,"Simultaneous updates by multiple sessions might not appear in the journalIn a clustering environment, simultaneous updates by multiple sessions in the same cluster node might not appear in the journal, because only record at a time can be handled by the cluster's workspace-specific callback method. When such a situtation arises, the following warnings can be found in the log:

*WARN * ClusterNode: No record created.
*WARN * ClusterNode: No record prepared.
"
0,"Add MergePolicy to IndexWriterConfigNow that IndexWriterConfig is in place, I'd like to move MergePolicy to it as well. The change is not straightforward and so I've kept it for a separate issue. MergePolicy requires in its ctor an IndexWriter, however none can be passed to it before an IndexWriter actually exists. And today IW may create an MP just for it to be overridden by the application one line afterwards. I don't want to make iw member of MP non-final, or settable by extending classes, however it needs to remain protected so they can access it directly. So the proposed changes are:

* Add a SetOnce object (to o.a.l.util), or Immutable, which can only be set once (hence its name). It'll have the signature SetOnce<T> w/ *synchronized set<T>* and *T get()*. T will be declared volatile, so that get() won't be synchronized.
* MP will define a *protected final SetOnce<IndexWriter> writer* instead of the current writer. *NOTE: this is a bw break*. any suggestions are welcomed.
* MP will offer a public default ctor, together with a set(IndexWriter).
* IndexWriter will set itself on MP using set(this). Note that if set will be called more than once, it will throw an exception (AlreadySetException - or does someone have a better suggestion, preferably an already existing Java exception?).

That's the core idea. I'd like to post a patch soon, so I'd appreciate your review and proposals."
0,JSR 283 Node Identifier
0,Remove HitCollectorRemove the rest of HitCollectors
0,"Add IW.add/updateDocuments to support nested documentsI think nested documents (LUCENE-2454) is a very compelling addition
to Lucene.  It's also a popular (many votes) issue.

Beyond supporting nested document querying, which is already an
incredible addition since it preserves the relational model on
indexing normalized content (eg, DB tables, XML docs), LUCENE-2454
should also enable speedups in grouping implementation when you group
by a nested field.

For the same reason, it can also enable very fast post-group facet
counting impl (LUCENE-3097) when you what to
count(distinct(nestedField)), instead of unique documents, as your
""identifier"".  I expect many apps that use faceting need this ability
(to count(distinct(nestedField)) not distinct(docID)).

To support these use cases, I believe the only core change needed is
the ability to atomically add or update multiple documents, which you
cannot do today since in between add/updateDocument calls a flush (eg
due to commit or getReader()) could occur.

This new API (addDocuments(Iterable<Document>), updateDocuments(Term
delTerm, Iterable<Document>) would also further guarantee that the
documents are assigned sequential docIDs in the order the iterator
provided them, and that the docIDs all reside in one segment.

Segment merging never splits segments apart, so this invariant would
hold even as merges/optimizes take place.
"
1,"missing sync in InternalVersionManagerImpl.externalUpdate can cause ConcurrentModificationExceptionIn

        for (Map.Entry<ItemId, InternalVersionItem> entry : versionItems.entrySet()) {
            if (changes.has(entry.getKey())) {
                items.add(entry.getValue());
            }
        }

we need to sync on versionItems, I believe."
0,"Cookie guide lists RFC 2965 as unsupportedHttpClient 3.1 added support for RFC 2965 (port-sensitive cookies), but the Cookie guide on the 3.x website still lists that as unsupported.
http://jakarta.apache.org/httpcomponents/httpclient-3.x/cookies.html

cheers,
  Roland
 "
1,"Create or Append mode determined before obtaining write lockIf an IndexWriter(""writer1"") is opened in CREATE_OR_APPEND mode, it determines whether to CREATE or APPEND before obtaining the write lock.  When another IndexWriter(""writer2"") is in the process of creating the index, this can result in writer1 entering create mode and then waiting to obtain the lock.  When writer2 commits and releases the lock, writer1 is already in create mode and overwrites the index created by write2.

This bug was probably effected by LUCENE-2386 as prior to that Lucene generated an empty commit when a new index was created.  I think the issue could still have occurred prior to that but the two IndexWriters would have needed to be opened nearly simultaneously and the first IndexWriter would need to release the lock before the second timed out."
0,"fix LowerCaseFilter for unicode 4.0lowercase suppl. characters correctly. 

this only fixes the filter, the LowerCaseTokenizer is part of a more complex issue (CharTokenizer)
"
0,Move PersistenceManagerTest from o.a.j.core to o.a.j.core.persistenceThe subject pretty much sums it up. The PMTest should be placed together with the other PM related tests in o.a.j.core.persistence.
0,"FastVectorHighlighter: Make FragmentsBuilder use EncoderMake FragmentsBuilder use Encoder, as Highlighter does."
0,Add multi-part post supportAdd a new method to support multi-part post.
0,"MergePolicy should require an IndexWriter upon constructionMergePolicy does not require an IW upon construction, but requires one to be passed as method arg to various methods. This gives the impression as if a single MP instance can be shared across various IW instances, which is not true for all MPs (if at all). In addition, LogMergePolicy uses the IW instance passed to these methods incosistently, and is currently exposed to potential NPEs.

This issue will change MP to require an IW instance, however for back-compat reasons the following changes will be made:
# A new MP ctor w/ IW as arg will be introduced. Additionally, for back-compat a default ctor will also be declared which will assign null to the member IW.
# Methods that require IW will be deprecated, and new ones will be declared.
#* For back-compat, the new ones will not be made abstract, but will throw UOE, with a comment that they will become abstract in 3.0.
# All current MP impls will move to use the member instance.
# The code which calls MP methods will continue to use the deprecated methods, passing an IW even that it won't be necessary --> this is strictly for back-compat.

In 3.0, we'll remove the deprecated default ctor and methods, and change the code to not call the IW method variants anymore.

I hope that I didn't leave anything out. I'm sure I'll find out when I work on the patch :)."
1,"Version 1.3 reports IOException when re-creating an indexVersion: Lucene 1.3 final 
Error reported when I am (re-)doing an initialization on the index created 
previously:
java.io.IOException: couldn't delete _26a.f1

The problem disappearred after a re-start of the jvm, some files may be locked 
after the index writer action !
Problem does not appear in Version 1.2."
0,"Remove references to older versions of Lucene in ""per-release"" documentationSome of the documentation that is ""per release"" contains references to older versions, which is often confusing.  This is most noticeable in the file formats docs, but there might be other places too."
1,"Webdav: creating resource in case of RepositoryExceptionif accessing item fails for any other reason than PathNotFoundException, creating
the resource should rather fail (throwing 403).

(reported by brian)"
0,"WikipediaTokenizerI have extended StandardTokenizer to recognize Wikipedia syntax and mark tokens with certain attributes.  It isn't necessarily complete, but it does a good enough job for it to be consumed and improved by others.

It sets the Token.type() value depending on the Wikipedia syntax (links, internal links, bold, italics, etc.) based on my pass at http://en.wikipedia.org/wiki/Wikipedia:Tutorial

I have only tested it with the benchmarking EnwikiDocMaker wikipedia stuff and it seems to do a decent job.

Caveats:  I am not sure how to best handle testing, since the content is licensed under GNU Free Doc License, I believe I can't copy and paste a whole document into the unit test.  I have hand coded one doc and have another one that just generally runs over the benchmark Wikipedia download.

One more question is where to put it.  It could go in analysis, but the tests at least will have a dependency on Benchmark.  I am thinking of adding a new contrib/wikipedia where this could grow to have other wikipedia things (perhaps we would move EnwikiDocMaker there????) and reverse the dependency on Benchmark.

I will post a patch over the next few days."
0,"Drop deprecations from trunksubj.
Also, to each remaining deprecation add release version when it first appeared.

Patch incoming."
0,"XMLPersistenceManager fails after creating too many directories on linuxWhen using the  XMLPersistenceManager it creates a bunch of directories in jackrabbit/home/version/data. Eventually I reach 32000 directories in the data directory and subsequent writes fail.

I believe this is caused by XMLPersistenceManager.buildNodeFolderPath() method where it does 
   if (cnt == 4 || cnt == 8) {
      sb.append('/');
   }

This causes the subdirectories to be 4 characters, 0-f i.e. 16^4 which is 65536, if what I'm seeing is correct, on linux ext3, it's limited to 32000 entries. If the XMLPersistence manager used 2 or 3 characters this should fix the problem, or if it were configurable it would also solve this (I think).

an 
   ls jackrabbit/home/version/data | wc -l
returns 
   32001

A stack trace for when this happens is as follows :
Caused by: javax.jcr.RepositoryException: failed to write node state: da2cd5d1-9776-4dbe-a42b-842b0134dbfb: failed to write node state: da2cd5d1-9776-4dbe-a42b-842b0134dbfb
        at org.apache.jackrabbit.core.version.AbstractVersionManager.createVersionHistory(AbstractVersionManager.java:181)
        at org.apache.jackrabbit.core.version.VersionManagerImpl$1.run(VersionManagerImpl.java:194)
        at org.apache.jackrabbit.core.version.VersionManagerImpl$DynamicESCFactory.doSourced(VersionManagerImpl.java:526)
        at org.apache.jackrabbit.core.version.VersionManagerImpl.createVersionHistory(VersionManagerImpl.java:191)
        at org.apache.jackrabbit.core.version.XAVersionManager.createVersionHistory(XAVersionManager.java:140)
        at org.apache.jackrabbit.core.ItemImpl.initVersionHistories(ItemImpl.java:754)
        at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1166)
        at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:805)
        ... 166 more
Caused by: org.apache.jackrabbit.core.state.ItemStateException: failed to write node state: da2cd5d1-9776-4dbe-a42b-842b0134dbfb
        at org.apache.jackrabbit.core.state.xml.XMLPersistenceManager.store(XMLPersistenceManager.java:579)
        at org.apache.jackrabbit.core.state.AbstractPersistenceManager.store(AbstractPersistenceManager.java:66)
        at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:574)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:697)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:315)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:291)
        at org.apache.jackrabbit.core.version.AbstractVersionManager.createVersionHistory(AbstractVersionManager.java:174)
        ... 173 more
Caused by: org.apache.jackrabbit.core.fs.FileSystemException: failed to create folder /home/cms/pepsiaccess/jackrabbit/home/version/data/da2c/d5d1/97764dbea42b842b0134dbfb
        at org.apache.jackrabbit.core.fs.local.LocalFileSystem.createFolder(LocalFileSystem.java:208)
        at org.apache.jackrabbit.core.fs.BasedFileSystem.createFolder(BasedFileSystem.java:99)
        at org.apache.jackrabbit.core.fs.FileSystemResource.makeParentDirs(FileSystemResource.java:100)
        at org.apache.jackrabbit.core.state.xml.XMLPersistenceManager.store(XMLPersistenceManager.java:517)
        ... 179 more"
1,"TestTermsEnum.testIntersectRandom fail{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestTermsEnum
    [junit] Testcase: testIntersectRandom(org.apache.lucene.index.TestTermsEnum):	FAILED
    [junit] (null)
    [junit] junit.framework.AssertionFailedError
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1530)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1432)
    [junit] 	at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$IntersectEnum.getState(BlockTreeTermsReader.java:894)
    [junit] 	at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$IntersectEnum.seekToStartTerm(BlockTreeTermsReader.java:969)
    [junit] 	at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$IntersectEnum.<init>(BlockTreeTermsReader.java:786)
    [junit] 	at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader.intersect(BlockTreeTermsReader.java:483)
    [junit] 	at org.apache.lucene.index.TestTermsEnum.testIntersectRandom(TestTermsEnum.java:293)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1530)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1432)
    [junit] 
    [junit] 
    [junit] Tests run: 6, Failures: 1, Errors: 0, Time elapsed: 14.762 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestTermsEnum -Dtestmethod=testIntersectRandom -Dtests.seed=320d0949741fc6b1:3cabdb9b04d0d243:-4b7c80572775ed92 -Dtests.multiplier=3
{noformat}"
0,Snowball package contains BSD licensed code with ASL headerAll classes in org.tartarus.snowball (but not in org.tartarus.snowball.ext) has for some reason been given an ASL header. These classes are licensed with BSD. Thus the ASL header should be removed. I suppose this a misstake or possible due to the ASL header automation tool.
0,"java.util.UUID.fromString() too slowBenchmarking shows that the java.util.UUID.fromString() method is 10 times slower than the previous version we used from jackrabbit-jcr-commons. This method is quite heavily used in the query section or more generally whenever a NodeId is created from a String.

I'd like to introduce the custom String UUID parsing code again that we had in the jackrabbit-jcr-commons UUID class and use it in the NodeId(String) constructor.

WDYT?"
0,"light/minimal stemming for euro languagesThe snowball stemmers are very aggressive and it would be nice if there were lighter alternatives.

Some applications may want to perform less aggressive stemming, for example:
http://www.lucidimagination.com/search/document/5d16391e21ca6faf/plural_only_stemmer

Good, relevance tested algorithms exist and I think we should provide these alternatives."
0,"bogus javadocs for FieldValueHitQuery.fillFieldsFieldValueHitQuery.fillFields has javadocs that seem to be left over from a completely different method...

{code}
  /**
   * Given a FieldDoc object, stores the values used to sort the given document.
   * These values are not the raw values out of the index, but the internal
   * representation of them. This is so the given search hit can be collated by
   * a MultiSearcher with other search hits.
   * 
   * @param doc
   *          The FieldDoc to store sort values into.
   * @return The same FieldDoc passed in.
   * @see Searchable#search(Weight,Filter,int,Sort)
   */
  FieldDoc fillFields(final Entry entry) {
    final int n = comparators.length;
    final Comparable[] fields = new Comparable[n];
    for (int i = 0; i < n; ++i) {
      fields[i] = comparators[i].value(entry.slot);
    }
    //if (maxscore > 1.0f) doc.score /= maxscore;   // normalize scores
    return new FieldDoc(entry.docID, entry.score, fields);
  }

{code}"
0,"Build failed in the flexscoring branch because of Javadoc warningsAnt build log:
  [javadoc] Standard Doclet version 1.6.0_24
  [javadoc] Building tree for all the packages and classes...
  [javadoc] /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/src/java/org/apache/lucene/search/Similarity.java:93: warning - Tag @link: can't find tf(float) in org.apache.lucene.search.Similarity
  [javadoc] /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/src/java/org/apache/lucene/search/TFIDFSimilarity.java:588: warning - @param argument ""term"" is not a parameter name.
  [javadoc] /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/src/java/org/apache/lucene/search/TFIDFSimilarity.java:588: warning - @param argument ""docFreq"" is not a parameter name.
  [javadoc] /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/src/java/org/apache/lucene/search/TFIDFSimilarity.java:618: warning - @param argument ""terms"" is not a parameter name.
  [javadoc] Generating /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/org/apache/lucene/store/instantiated//package-summary.html...
  [javadoc] Copying file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/classdiagram.png to directory /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/org/apache/lucene/store/instantiated/doc-files...
  [javadoc] Copying file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/HitCollectionBench.jpg to directory /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/org/apache/lucene/store/instantiated/doc-files...
  [javadoc] Copying file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/classdiagram.uxf to directory /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/org/apache/lucene/store/instantiated/doc-files...
  [javadoc] Generating /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/serialized-form.html...
  [javadoc] Copying file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/prettify/stylesheet+prettify.css to file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/stylesheet+prettify.css...
  [javadoc] Building index for all the packages and classes...
  [javadoc] Building index for all classes...
  [javadoc] Generating /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/help-doc.html...
  [javadoc] 4 warnings
"
0,"Unit tests for persistence managersCurrently we only test our persistence managers indirectly via JCR-level test cases. The downside of this approach is that we can only test one persistence manager implementation at a time, and need separate build profiles to switch from one implementation to another. To ensure better coverage and consistent behaviour across all our persistence managers I implemented a simple unit test that works directly against the PersistenceManager interface."
0,"Support for new Resources model in ant 1.7 in Lucene ant task.Ant Task for Lucene should use modern Resource model (not only FileSet child element).
There is a patch with required changes.

Supported by old (ant 1.6) and new (ant 1.7) resources model:
<index ....> <!-- Lucene Ant Task -->
  <fileset ... />
</index> 

Supported only by new (ant 1.7) resources model:
<index ....> <!-- Lucene Ant Task -->
  <filelist ... />
</index> 

<index ....> <!-- Lucene Ant Task -->
  <userdefinied-filesource ... />
</index> "
0,"Setting different MAX_HOST_CONNECTION values per host using a single MultiThreadedHttpConnectionManagerRight now, it's not possible to use the
MultiThreadedHttpConnectionManager.setMaxConnectionsPerHost(int) method in a per
HostConfiguration basis. The value applies to every HostConfiguration the
current connection manager is managing.

I would be quite useful to allow the connection manager to set different values
depending on the HostConfiguration."
0,"Could we get a way to know if the response has been served from the cache or not ?Is there a way to know if the response has been served from the cache or not ?
That's an information which might be useful for monitoring the activity of the cache.

If there's no current way, maybe a flag could be added in the request context whenever the response comes from the cache ... ?

"
0,"Javadoc of TokenStream.end() somehow confusingThe Javadocs of TokenStream.end() are somehow confusing, because they also refer to the old TokenStream API (""after next() returned null""). But one who implements his TokenStream with the old API cannot make use of the end() feature, as he would not use attributes and so cannot update the end offsets (he could, but then he should rewrite the whole TokenStream). To be conform to the old API, there must be an end(Token) method, which we will not add.

I would drop the old API from this docs."
0,Use jackrabbit 1.2.1Use Jackarabit 1.2.1
0,"FST should allow more than one output for the same inputFor the block tree terms dict, it turns out I need this case."
0,Replace xerces for serialization by JAXPThe org.apache.jackrabbit.rmi.xml.ImportContentHandler class currently uses Xerces to implement the SAX DocumentHandler and serialize XML into a byte[]. This dependency should be dropped and JAXP be used instead for this functionality.
0,"RFC4918 feature: PROPFIND/includeRFC4918, Section 14.8 (<http://greenbytes.de/tech/webdav/rfc4918.html#rfc.section.14.8>) defines an extension to PROPFIND that allows clients to retrieve all RFC2518 properties, the dead properties, plus a set of additional live properties. This can help avoiding a second roundtrip to retrieve really all properties.
"
1,"Removal of versions throws javax.jcr.ReferentialIntegrityExceptionFrom the following thread : http://www.mail-archive.com/jackrabbit-dev%40incubator.apache.org/msg03483.html

When trying to remove a version of a Node  the VersionHistory.removeVersion() method throws : ""javax.jcr.ReferentialIntegrityException: Unable to remove version. At least once referenced."".

Secton 8.2.2.10 (Removal of Versions) of the specification indicates that the version graph should be automatically repaired upon removal. Then, VersionHistory.removeVersion() should take care of references. (In fact, a user cannot alter the references (jcr:predecessors and jcr:successors), since they are protected properties.)

Here's the example (*updated) :

Node root1 = session.getRootNode() ;
Node test1 = root1.addNode(""test"") ;
test1.addMixin(""mix:versionable"");
test1.setProperty(""test"", ""1"");
session.save();
test1.checkin();

test1.checkout();
test1.setProperty(""test"", ""2"");
session.save();
test1.checkin();

test1.checkout();
test1.setProperty(""test"", ""3"");
session.save();
test1.checkin();

String baseVersion = test1.getBaseVersion().getName();
System.out.println(""Base version name: "" + baseVersion);

VersionHistory vh = test1.getVersionHistory();
for (VersionIterator vi = vh.getAllVersions(); vi.hasNext(); ) {
    Version currenVersion = vi.nextVersion();
    String versionName = currenVersion.getName();
    if (!versionName.equals(""jcr:rootVersion"") && !versionName.equals(baseVersion)) { 
        String propertyValue = currenVersion.getNode(""jcr:frozenNode"").getProperty(""test"").getString();
        System.out.println(""Removing version : "" + versionName + "" with value: "" + propertyValue);
        vh.removeVersion(versionName);
    }
}

Regards, 

Nicolas"
0,Allow parent path to be set explicitly in NodeInfoBuilderCurrently there is no way to explicitly set the parent of an NodeInfo build by NodeInfoBuilder. I suggest to add a setParentPath() method to NodeInfoBuilder to fix this.
0,"Revert subsequent token-node updates (tentatively introduced)i would like to revert this improvement has been tentatively introduced based on the following
thread on the dev list: http://www.mail-archive.com/dev@jackrabbit.apache.org/msg24437.html
as i am still concerned about undesired effects. in addition i still feel that this somehow
violates the basic contract."
0,"ShingleMatrixFilter, a three dimensional permutating shingle filterBacked by a column focused matrix that creates all permutations of shingle tokens in three dimensions. I.e. it handles multi token synonyms.

Could for instance in some cases be used to replaces 0-slop phrase queries with something speedier.

{code:java}
Token[][][]{
  {{hello}, {greetings, and, salutations}},
  {{world}, {earth}, {tellus}}
}
{code}

passes the following test  with 2-3 grams:

{code:java}
assertNext(ts, ""hello_world"");
assertNext(ts, ""greetings_and"");
assertNext(ts, ""greetings_and_salutations"");
assertNext(ts, ""and_salutations"");
assertNext(ts, ""and_salutations_world"");
assertNext(ts, ""salutations_world"");
assertNext(ts, ""hello_earth"");
assertNext(ts, ""and_salutations_earth"");
assertNext(ts, ""salutations_earth"");
assertNext(ts, ""hello_tellus"");
assertNext(ts, ""and_salutations_tellus"");
assertNext(ts, ""salutations_tellus"");
{code}

Contains more and less complex tests that demonstrate offsets, posincr, payload boosts calculation and construction of a matrix from a token stream.

The matrix attempts to hog as little memory as possible by seeking no more than maximumShingleSize columns forward in the stream and clearing up unused resources (columns and unique token sets). Can still be optimized quite a bit though."
0,Apply the supplied patch. Sets 2 variable in the base class to protectedThe patch attached to the main task contains minimal changes to allow the HttpMethodBase class to be overloaded by base class.
1,"Cookies with null path attribute are rejected in the compatibility modeWeblogic sends cookies with path empty, httpclient emits a warning
and doesn't send back the cookie to server.

Maybe httpclient works in the RFC's ways but this doesn't reproduce
common web browsers behaviours. Our application works well with IE,
Opera and Netscape, httpunit also sends back the cookie to the server.

When receving the response, httpclient emits the followin warning :

[WARN] HttpMethod - -Invalid cookie header: ""JTD=O%
2FdF13CDb1W7H2GNfUTS2YQ3Zt6bCW6ZKZRvVJ9FwaadQLxXVI7rgii%2FwbxeCsqym7dcWKDxSj%
2Bg1ubJRSVRhYGb7wRLjp5c0v2R3QrCIXVhMKDjuwuXDXnjbH3LHSWG7bfzJSmS7nXk9R%
2FqMIRHb5najLQkU7WkuPGgXUnUln%2BF51TajkVmXkrLMYN7MHDT48BEHvFQFNXBlmSRejWqrd%
2Fiiao0flObOrT3HcaWI09B1vekpAcPmgvMD2oZzXQWJwjDZIX6QoVVD6U8CXPSvVQjITyaxf6AqaS%
2BAFJgRsqbZBc0%2BV5G%2FnzE87ggOVIozfPFn99ny0kxiPGBEisJIy%3D%3D; Version=1; 
Path=; Max-Age=604800"". Missing value for path attribute

That's right, maybe the http header is not correct, but I think httpclient
should handle this case without error in order to have the same behaviour
as common browsers. We have no way to give a better value to this path."
0,"Can't build lucene 06/13/2004 CVS under jdk 1.5.0[root@shilo jakarta-lucene]# /usr/local/ant/bin/ant build
Buildfile: build.xml

BUILD FAILED
Target `build' does not exist in this project. 

Total time: 1 second
[root@shilo jakarta-lucene]# /usr/local/ant/bin/ant 
Buildfile: build.xml

init:
    [mkdir] Created dir: /usr/src/jakarta-lucene/build
    [mkdir] Created dir: /usr/src/jakarta-lucene/dist

compile-core:
    [mkdir] Created dir: /usr/src/jakarta-lucene/build/classes/java
    [javac] Compiling 160 source files to /usr/src/jakarta-
lucene/build/classes/java
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/FilterIndexReader.java:42: as of 
release 1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     public void seek(TermEnum enum) throws IOException { in.seek
(enum); }
    [javac]                               ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/FilterIndexReader.java:42: as of 
release 1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     public void seek(TermEnum enum) throws IOException { in.seek
(enum); }
    [javac]                                                                  ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/SegmentTermDocs.java:55: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]   public void seek(TermEnum enum) throws IOException {
    [javac]                             ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/SegmentTermDocs.java:59: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     if (enum instanceof SegmentTermEnum && ((SegmentTermEnum) 
enum).fieldInfos == parent.fieldInfos)          // optimized case
    [javac]         ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/SegmentTermDocs.java:59: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     if (enum instanceof SegmentTermEnum && ((SegmentTermEnum) 
enum).fieldInfos == parent.fieldInfos)          // optimized case
    [javac]                                                               ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/SegmentTermDocs.java:60: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]       ti = ((SegmentTermEnum) enum).termInfo();
    [javac]                               ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/SegmentTermDocs.java:62: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]       ti = parent.tis.get(enum.term());
    [javac]                           ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:63: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     SegmentTermEnum enum = (SegmentTermEnum)enumerators.get();
    [javac]                     ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:64: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     if (enum == null) {
    [javac]         ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:65: enum types 
must not be local
    [javac]       enum = terms();
    [javac]       ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:65: <identifier> 
expected
    [javac]       enum = terms();
    [javac]            ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:65: '{' expected
    [javac]       enum = terms();
    [javac]                     ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:66: <identifier> 
expected
    [javac]       enumerators.set(enum);
    [javac]                      ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:66: ';' expected
    [javac]       enumerators.set(enum);
    [javac]                       ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:66: <identifier> 
expected
    [javac]       enumerators.set(enum);
    [javac]                           ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:66: '{' expected
    [javac]       enumerators.set(enum);
    [javac]                            ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:68: illegal start 
of type
    [javac]     return enum;
    [javac]     ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:68: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     return enum;
    [javac]            ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:75: illegal start 
of expression
    [javac]   private final void readIndex() throws IOException {
    [javac]   ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:199: ';' expected
    [javac] }
    [javac] ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:200: '}' expected
    [javac] ^
    [javac] 21 errors

BUILD FAILED
/usr/src/jakarta-lucene/build.xml:140: Compile failed; see the compiler error 
output for details.

Total time: 4 seconds
[root@shilo jakarta-lucene]#"
0,add support for RFC 3253 to the simple serverhttp://www.ietf.org/rfc/rfc3253.txt
0,"benchmark tests always fail on windows because directory cannot be removedThis seems to be a bug recently introduced. I have no idea what's wrong. Attached is a log file, reproduces everytime.

"
0,"LowerCaseFilter for Turkish languagejava.lang.Character.toLowerCase() converts 'I' to 'i' however in Turkish alphabet lowercase of 'I' is not 'i'. It is LATIN SMALL LETTER DOTLESS I.

"
0,"Tests using Version.LUCENE_CURRENT will produce problems in backwards branch, when development for 3.2 startsA lot of tests for the most-recent functionality in Lucene use Version.LUCENE_CURRENT, which is fine in trunk, as we use the most recent version without hassle and changing this in later versions.

The problem is, if we copy these tests to backwards branch after 3.1 is out and then start to improve analyzers, we then will have the maintenance hell for backwards tests. And we loose backward compatibility testing for older versions. If we would specify a specific version like LUCENE_31 in our tests, after moving to backwards they must work without any changes!

To not always modify all tests after a new version comes out (e.g. after switching to 3.2 dev), I propose to do the following:
- declare a static final Version TEST_VERSION = Version.LUCENE_CURRENT (or better) Version.LUCENE_31 in LuceneTestCase(4J).
- change all tests that use Version.LUCENE_CURRENT using eclipse refactor to use this constant and remove unneeded import statements.

When we then move the tests to backward we must only change one line, depending on how we define this constant:
- If in trunk LuceneTestCase it's Version.LUCENE_CURRENT, we just change the backwards branch to use the version numer of the released thing.
- If trunk already uses the LUCENE_31 constant (I prefer this), we do not need to change backwards, but instead when switching version numbers we just move trunk forward to the next major version (after added to Version enum)."
1,HTML Text Extractor does not extract or index numericsNumerics such as addresses/dates/financial figures are not extracted or indexed by the current HTML Extractor.  These values are handled properly and searchable when done via the PlainTextExtractor
0,"Links in Section ""Example Code"" are brokenSteps to Reproduce: Go to http://hc.apache.org/user-docs.html
Click of one of the Links in the Section ""Example Code"""
0,"IndexWriter#addIndexesNoOptimize has redundent try/catchWith the new transaction code, the try/catch clause at the beginning of IndexWriter#addIndexesNoOptimize is redundant."
0,"TCK: more tests assuming that 'addMixin' immediately taking effectjsr170 allows an implementation to have Node.addMixin only taking affect upon a save-call.

some tests already got adjusted.
attached patch for additional tests, that make usage of addMixin"
0,NodeBasedGroup#isMember(Principal) should have shortcut for the everyone group.
0,"Make observation polling time configurableCurrently the polling time is hard coded to 3 seconds in org.apache.jackrabbit.jcr2spi.WorkspaceImpl. I suggest to make it configurable similar to CacheBehaviour. That is, add a respective setting to org.apache.jackrabbit.jcr2spi.config.RepositoryConfig"
0,"Deprecate SimilarityDelegator and Similarity.lengthNormSimilarityDelegator is a back compat trap (see LUCENE-2828).

Apps should just [statically] subclass Sim or DefaultSim; if they really need ""runtime subclassing"" then they can make their own app-level delegator.

Also, Sim.computeNorm subsumes lengthNorm, so we should deprecate lengthNorm in favor of computeNorm."
0,"Support for NTLM authenticationA late write in for this would be support for NTLM authenticatin as well as
basic and digest.  Obviously non-trivial but it would be a very big feature.

Adrian Sutton, Software Engineer
Ephox Corporation
www.ephox.com <http://www.ephox.com>"
0,"RMIC not working in subprojects when compiling parent using maven2This is because there is a bug such that if you have a child build which uses the ant plugin it inherits the plugin dependencies of the first time the plugin is declared.

The workaround is to put the antrun plugin in the toplevel, and add the java jar to its plugin dependencies.

(reference: http://mail-archives.apache.org/mod_mbox/maven-users/200602.mbox/%3CC2CDEFBECFC9A14892BCCFB4C95F4868044F8230@EX-201.mail.navisite.com%3E)"
0,"BindableRepositoryFactory requires exact resource typeThe org.apache.jackrabbit.jndi.BindableRepositoryFactory class requires the exact class name org.apache.jackrabbit.jndi.BindableRepository to be specified for the JNDI resource that the factory is responsible for. However the current deployment model 2 howto document suggest that the more generic interface name javax.jcr.Repository be used instead. Currently this suggested configuration results in a null JNDI resource .

This issue should be fixed by either fixing the documentation (I can do this) or by relaxing the code in BindableRepositoryFactory. I'll attach a patch that does the latter, please comment on what you think is the best solution.

This issue was detected during the mailing list thread
http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/2303"
0,"WebdavResponseImpl should cache TransformerFactoryJackrabbitResponeImpl.sendXmlResponse creates an instance of TransformerFactory on each invocation. We see, that this TransformerFactory initialization consumes significant amount of time, because of complex logic inside:

{code}
    at java.lang.String.intern(Native Method)
    at java.util.jar.Attributes$Name.<init>(Attributes.java:449)
    at java.util.jar.Attributes.putValue(Attributes.java:151)
    at java.util.jar.Attributes.read(Attributes.java:404)
    at java.util.jar.Manifest.read(Manifest.java:234)
    at sun.security.util.SignatureFileVerifier.processImpl(SignatureFileVerifier.java:188)
    at sun.security.util.SignatureFileVerifier.process(SignatureFileVerifier.java:176)
    at java.util.jar.JarVerifier.processEntry(JarVerifier.java:277)
    at java.util.jar.JarVerifier.update(JarVerifier.java:188)
    at java.util.jar.JarFile.initializeVerifier(JarFile.java:321)
    at java.util.jar.JarFile.getInputStream(JarFile.java:386)
    at sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:144)
    at java.net.URL.openStream(URL.java:1009)
    at java.lang.ClassLoader.getResourceAsStream(ClassLoader.java:1170)
    at javax.xml.transform.SecuritySupport$4.run(SecuritySupport.java:94)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.xml.transform.SecuritySupport.getResourceAsStream(SecuritySupport.java:87)
    at javax.xml.transform.FactoryFinder.findJarServiceProvider(FactoryFinder.java:250)
    at javax.xml.transform.FactoryFinder.find(FactoryFinder.java:223)
    at javax.xml.transform.TransformerFactory.newInstance(TransformerFactory.java:102)
    at org.apache.jackrabbit.webdav.WebdavResponseImpl.sendXmlResponse(WebdavResponseImpl.java:163)
{code}

TransformerFactory can be cached in static field:

private static final TransofmerFactory transformerFactory = TransformerFactory.newInstance()."
0,"Multivalued property sorted by last/random valueSorting on multivalued property may produce incorrect result because sorting is performed only by last value of multivalued property.
Steps to reproduce:
1. Create multivalued field in repository. Example from nodetypes file:
<propertyDefinition name=""MyProperty"" requiredType=""String"" autoCreated=""false"" mandatory=""false""
   onParentVersion=""COPY"" protected=""false"" multiple=""false"">
2. Create few records so that all records except one would contain single value for MyProperty and one record would contain 
first value which is greater then of any other record and the second value is somewhere in the middle. Here is an example:
1st record: ""aaaa""
2nd record: ""cccc""
3rd record: ""dddd"", ""bbbb""
3. Run some query which sorts Example of XPath query:
//*[...here are some criteria...] order by @MyProperty ascending
The query would return documents in such order:
""aaaa""
""dddd"", ""bbbb""
""cccc""
which is not expected order (expected same order as they were entered - as ""aaaa"" < ""cccc"", ""cccc"" < ""dddd"")

After some digging I found out that it happens because method 
org.apache.jackrabbit.core.query.lucene.SharedFieldCache.getValueIndex
(called from org.apache.jackrabbit.core.query.lucene.SharedFieldSortComparator.SimpleScoreDocComparator constructor)
returns only last Comparable of the document. Here is overwrites previous value:
retArray[termDocs.doc()] = getValue(value, type);

I tried to concatenate comparables (just to check if it would work for my case):
	if(retArray[termDocs.doc()] == null) {
		retArray[termDocs.doc()] = getValue(value, type);
	} else {
		retArray[termDocs.doc()] =
				retArray[termDocs.doc()] + "" "" + getValue(value, type);
	}
But it didn't worked well either - TermEnum returns terms not in the same order as JackRabbit returns values of multivalued field
(as an example [""qwer"", ""asdf""] may become [""asdf"", ""qwer""] ). So, simple concatenation doesn't help.
"
0,"Provide factory method to create DefaultHttpClient instances pre-configured based on JSSE and networking system propertiesProvide factory method or a factory class intended to create DefaultHttpClient instances pre-configured based on JSSE [1] and networking [2] system properties.

[1] http://download.oracle.com/javase/1,5.0/docs/guide/security/jsse/JSSERefGuide.html
[2] http://download.oracle.com/javase/1.5.0/docs/guide/net/properties.html"
1,MoreLikeThis ignores custom similarityMoreLikeThis only allows the use of the DefaultSimilarity.  Patch shortly
0,Add method to set uuid in NodeInfoBuilder
0,"Modified QueryImpl to enable external query builders to read and write JCR expressions in the orderBy ClauseThe QueryImpl does not create the JCR expression on the fly. The OrderByExpression does the job. If an external querybuilder class needs to dynamically collect properties against which order by is required, QueryImpl does not support updating the JCR Expression. It can only return the built expression since arrayList is used for collecting the properties. The change enables one to add JCRExpression to the QueryImpl object. A test has been added.

Changed files:
Path
src/main/java/org/apache/jackrabbit/ocm/query/impl/QueryImpl.java
src/test/java/org/apache/jackrabbit/ocm/manager/query/DigesterSimpleQueryTest.java
"
0,"Extend contrib Highlighter to properly support PhraseQuery, SpanQuery,  ConstantScoreRangeQueryThis patch adds a new Scorer class (SpanQueryScorer) to the Highlighter package that scores just like QueryScorer, but scores a 0 for Terms that did not cause the Query hit. This gives 'actual' hit highlighting for the range of SpanQuerys, PhraseQuery, and  ConstantScoreRangeQuery. New Query types are easy to add. There is also a new Fragmenter that attempts to fragment without breaking up Spans.

See http://issues.apache.org/jira/browse/LUCENE-403 for some background.

There is a dependency on MemoryIndex."
0,"IndexWriter.mergeSegments should not hold the commit lock while cleaning up.Same happens in IndexWriter.addIndexes(IndexReader[] readers).

The commit lock should be obtained whenever the Index structure/version is read or written.  It should be kept for as short a period as possible.

The write lock is needed to make sure only one IndexWriter or IndexReader instance can update the index (multiple IndexReaders can of course use the index for searching).

The list of files that can be deleted is stored in the file ""deletable"".  It is only read or written by the IndexWriter instance that holds the write lock, so there's no need to have the commit lock to to update it.

On my production system deleting the obsolete segment files after a mergeSegments() happens can occasionally take several seconds(!) and the commit lock blocks the searcher machines from updating their IndexReader instance.
Even on a standalone machine, the time to update the segments file is about 3ms, the time to delete the obsolete segments about 30ms.
"
0,"use packed ints for the terms dict indexTerms dict index needs to store large RAM resident arrays of ints, but, because their size is bound & variable (depending on the segment/docs), we should used packed ints for them."
0,"Move SimpleWebdavServlet to jcr-server and make it abstractIn line with isse JCR-417, I suggest to partially move the SimpleWebdavServlet from the jcr-webapp project to the jcr-server project. By partially I mean, that the new (moved) servlet will be abstract and the getRepository() method will be abstract. The jcr-webapp project will still contain a SimpleWebdavServlet (for backwards compatibility maintaing the same name) which just extends the new servlet and implements the getRepository() method using the RepositoryAccess servlet.

This allows for the reuse of the jcr-server project including the abstract SimpleWebdavServlet in other environments. My intention is to include this project (along with the webdav) project in Sling.

Will provide a patch for this proposal

(This issue is separated out of JCR-1262 as suggested by Angela)
"
1,"Wrong creation of AuthScope objectClass Name: org.apache.http.client.protocol.RequestAuthCache
Line #: 118-119

Issue: Want to create a new Object of AuthScope by passing host name, port and scheme name but due to incorrect constructor call, Getting a object with realm name as scheme name.
Current Code: Credentials creds = credsProvider.getCredentials(new AuthScope(host.getHostName(), host.getPort(), null, schemeName));"
0,"CMS merge throttling is not aggressive enoughI hit this crab while working on the NRT benchmarker (in luceneutil).

CMS today forcefully idles any incoming threads, when there are too many merges pending.

This is the last line of defense that it has, since it also juggles thread priorities (and forcefully idles the biggest merges) to try to reduce the outstanding merge count.

But when it cannot keep up it has no choice but to stall those threads responsible for making new segments.

However, the logic is in the wrong place now -- the stalling happens after pulling the next merge from IW.  This is poor because it means if you have N indexing threads, you allow max + N outstanding merges.

I have a simple fix, which is to just move the stall logic to before we pull the next merge from IW."
0,"Utility class to tranform JCR-SQL2 to/from JCR-JQOMThe JCR2 doc specify that both contain the same thing and can be translated from one to another
in a straightforward manner. The jackrabbit-jcr-commons module should offer a utility class to transform
from one language to another in a generic way, 

for exemple :
- String toSQL2(QueryObjectModel qom)
- QueryObjectModel toJQOM(QueryObjectModelFactory factory, String query)"
1,"Exception in HttpConnection because of unchecked buffer sizeFrom the httpclient-dev mailing list:

Date: Tue, 8 Mar 2005 19:08:35 +0100
Subject: Error with multiple connections

Hello,

 

I am having some problems while trying multiple connections over a
HttpClient object with a MultiThreadedHttpConnectionManager. I am
launching 10 threads and each thread executes some GetMethods using this
HttpClient object.

 

Some times I got an error like this:

 

java.lang.IllegalArgumentException: Buffer size <= 0

      at java.io.BufferedInputStream.<init>(Unknown Source)

      at
org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:70
3)

      at
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpCon
nectionAdapter.open(MultiThreadedHttpConnectionManager.java:1170)

      at
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:6
28)

      at
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:4
97)

      at Main$Hilo.run(Main.java:58)

 

Does anybody have any idea? 

 

Thanks in advance,

Jorge"
0,"Indonesian AnalyzerThis is an implementation of http://www.illc.uva.nl/Publications/ResearchReports/MoL-2003-02.text.pdf

The only change is that I added an option to disable derivational stemming, 
in case you want to just remove inflectional particles and possessive pronouns.

"
0,"Add ""testpackage"" to common-build.xmlOne can define ""testcase"" to execute just one test class, which is convenient. However, I didn't notice any equivalent for testing a whole package. I find it convenient to be able to test packages rather than test cases because often it is not so clear which test class to run.

Following patch allows one to ""ant test -Dtestpackage=search"" (for example) and run all tests under the \*/search/\* packages in core, contrib and tags, or do ""ant test-core -Dtestpackage=search"" and execute similarly just for core, or do ""ant test-core -Dtestpacakge=lucene/search/function"" and run all the tests under \*/lucene/search/function/\* (just in case there is another o.a.l.something.search.function package out there which we want to exclude."
1,"addIndexes unexpectedly closes indexIt seems that in 1.4rc2, a call to IndexWriter.addIndexes (IndexReader[]) will
close the provided IndexReader; in 1.3-final this does not happen.  So my code
which uses addIndexes to merge new information into an index and then calls
close() on the IndexReader now crashes with an ""already closed"" exception.  I
can attach test code which works in 1.3 but not in 1.4rc2 if that would be helpful.

If this is an intentional change in behavior, it needs to be documented.  Thanks!"
0,"NamespaceRegistryTest uses an invalid URI as namespace URIThe test cases use ""www.apache.org/..."" as a namespace URI, but this is not a URI.

Suggest to fix by using a proper URI, such as by prefixing with ""http://"".

A related question is what our expectation is for JCR implementations. Are they allowed to reject something that doesn't parse as a URI according to RFC3986?
"
1,"DocValues infinite loop caused by - a call to getMinValue | getMaxValue | getAverageValueorg.apache.lucene.search.function.DocValues offers 3 public (optional) methods to access value statistics like min, max and average values of the internal values. A call to one of the methods will result in an infinite loop. The internal counter is not incremented. 
I added a testcase, javadoc and a slightly different implementation to it. I guess this is not breaking any back compat. as a call to those methodes would have caused an infinite loop anyway.
I changed the return value of all of those methods to Float.NaN if the DocValues implementation does not contain any values.

It might be considerable to fix this in 2.4.2 and 2.3.3

"
0,"Add option to ReverseStringFilter to mark reversed tokensThis patch implements additional functionality in the filter to ""mark"" reversed tokens with a special marker character (Unicode 0001). This is useful when indexing both straight and reversed tokens (e.g. to implement efficient leading wildcards search)."
0,"Various improvment to Path and PathImplThere are various issues with Path and PathImpl which the following patch addresses:
- Fixed problem with normalization of some paths in PathImpl. 
- Fixed handling of relative paths in PathImpl. 
- Fixed wrong return value for depth and ancestor count in PathImpl. 
- Added method for determining equivalence of paths in PathImpl.
- Fixed subPath method in PathImpl. 
- Clarified blurry contract for Path.
- Added many new test cases

For many of the fixes credits are due to Angela."
0,"spellchecker: make hard-coded values configurablethe class org.apache.lucene.search.spell.SpellChecker uses the following hard-coded values in its method
indexDictionary:
        writer.setMergeFactor(300);
        writer.setMaxBufferedDocs(150);
this poses problems when the spellcheck index is created on systems with certain limits, i.e. in unix
environments where the ulimit settings are restricted for the user (http://www.gossamer-threads.com/lists/lucene/java-dev/47428#47428).

there are several ways to circumvent this:
1. add another indexDictionary method with additional parameters:
    public void indexDictionary (Dictionary dict, int mergeFactor, int maxBufferedDocs) throws IOException
    
2. add setter methods for mergeFactor and maxBufferedDocs 
    (see code in http://www.gossamer-threads.com/lists/lucene/java-dev/47428#47428 )

3. Make SpellChecker subclassing easier as suggested by Chris Hostetter 
   (see reply  http://www.gossamer-threads.com/lists/lucene/java-dev/47463#47463)

thanx,
karin
"
0,"norms file can become unexpectedly enormous
Spinoff from this user thread:

   http://www.gossamer-threads.com/lists/lucene/java-user/46754

Norms are not stored sparsely, so even if a doc doesn't have field X
we still use up 1 byte in the norms file (and in memory when that
field is searched) for that segment.  I think this is done for
performance at search time?

For indexes that have a large # documents where each document can have
wildly varying fields, each segment will use # documents times # fields
seen in that segment.  When optimize merges all segments, that product
grows multiplicatively so the norms file for the single segment will
require far more storage than the sum of all previous segments' norm
files.

I think it's uncommon to have a huge number of distinct fields (?) so
we would need a solution that doesn't hurt the more common case where
most documents have the same fields.  Maybe something analogous to how
bitvectors are now optionally stored sparsely?

One simple workaround is to disable norms.
"
0,"BUILD.txt instructions wrong for JavaCCThe text in BUILD.txt for javacc says to set the property to the bin directory in the javacc installation. It should actually be set to the javacc installation directory, the directory containing the bin directory. The comments common-build.xml correctly state this."
0,"Allow access to registered cookie policiesIt would be useful for JMeter (and perhaps other applications) to have access to the list of registered Cookie policy names.

[If this is acceptable, let me know if you want me to provide a patch.]"
0,"Add Google Analytics to Jackrabbit web siteI'd like to add Google Analytics to our web site to better track how the site is used and how much traffic we are generating.

Currently the best stats we have are at http://people.apache.org/~vgritsenko/stats/projects/jackrabbit.html, which is nice but not nearly as good as we could have."
1,"Jcr-Server: ValuesProperty missing property type informationJCR specific dav-property ValuesProperty does not reveal the PropertyType of the value, which is therefore lost during (de)serialization. 

Solution: 
- Pass type of the JCR-value as attribute to the xml-element containing the value."
0,"Maybe rename Field.omitTf, and strengthen the javadocsSpinoff from here:

  http://www.nabble.com/search-problem-when-indexed-using-Field.setOmitTf()-td22456141.html

Maybe rename omitTf to something like omitTermPositions, and make it clear what queries will silently fail to work as a result."
0,Add UserManager#getAuthorizableByPath(String) for symmetry with JCR-3037JCR-3037 added Authorizable#getPath. I would suggest to also add UserManager#getAuthorizableByPath that was symmetric to Authorizable#getPath
1,"when many query clases are specified in boolean or dismax query, highlighted terms are always ""yellow"" if multi-colored feature is usedThe problem is the following snippet:

{code}
protected String getPreTag( int num ){
  return preTags.length > num ? preTags[num] : preTags[0];
}
{code}

it should be:

{code}
protected String getPreTag( int num ){
  int n = num % preTags.length;
  return  preTags[n];
}
{code}
"
1,"TCK: OrderByMultiTypeTest doesn't respect nodetype configuration propertyOrderByMultiTypeTest creates test data by calling addNode(String).  This fails if there is no default primary type.

Proposal: call addNode(String, String)

--- OrderByMultiTypeTest.java   (revision 428760)
+++ OrderByMultiTypeTest.java   (working copy)
@@ -43,9 +43,9 @@
      * Tests order by queries with a String property and a long property.
      */
     public void testMultipleOrder() throws Exception {
-        Node n1 = testRootNode.addNode(nodeName1);
-        Node n2 = testRootNode.addNode(nodeName2);
-        Node n3 = testRootNode.addNode(nodeName3);
+        Node n1 = testRootNode.addNode(nodeName1, testNodeType);
+        Node n2 = testRootNode.addNode(nodeName2, testNodeType);
+        Node n3 = testRootNode.addNode(nodeName3, testNodeType);
  
         n1.setProperty(propertyName1, ""aaa"");
         n1.setProperty(propertyName2, 3);
"
0,"jcr ext doesn't compile with jdk 1.4IllegalStateException(String str, Exception e) isn't supported."
0,"Non-standards configuration and trackingA simple strict or setLenient is likely inadequate.  Each particular
non-standard behaviour should be tagged, and settable from the client.  A mask
for particular behavioural features could be provided, with STRICT meaning none
and LENIENT meaning all."
0,"allow unsetting of DEFAULT_PROXY and FORCED_ROUTE parameters in the client stackSince we don't want to delay client alpha3 until HTTPCORE-139 is solved in beta2, we need a parameter specific solution for unsetting these client parameters on the request level.
"
0,"Add SimpleFragListBuilder constructor with margin parameter{{SimpleFragListBuilder}} would benefit from an additional constructor that takes in {{margin}}. Currently, the margin is defined as a constant, so to ""implement"" a {{FragListBuilder}} with a different margin, one has no choice but to copy and paste {{SimpleFragListBuilder}} into a new class that must be placed in the {{org.apache.lucene.search.vectorhighlight}} package due to accesses of package-protected fields in other classes.

If this change were made, the precondition check of the constructor's {{fragCharSize}} should probably be altered to ensure that it's less than {{max(1, margin*3)}} to allow for a margin of 0."
1,"Multipart post is brokenI tried to do HttpPost request with MultipartEntity, this request was encoded to wire with 3 line separators after header and not processed correctly by http server.
MultipartEntry add 1 extra line separator before write itself to wire. I'm not sure about standards, but it is at least not ""browser compatible"".

"
0,"refactoring of docvalues params in Codec.javaWhile working on LUCENE-2621 I am trying to do some cleanup of the Codec APIs, currently Codec.java has a boolean for getDocValuesUseCFS()

I think this is an impl detail that should not be in Codec.java: e.g. i might make a SimpleText impl that uses only 1 file and then the param
is awkward.

So, instead I created Sep impls that dont use CFS (use separate files) and placed them under the sep package, if you don't want to use
CFS you can just use these implementations in your codec."
0,"Introduce 'SecurityConfig' for better extensability.the current repository configuration parser parses the security confguration (inluding appName, AccessManagerConfig and LoginModuleconfig) internally and the passes those 3 values to the repository config. i suggest to add a new 'SecurityConfig' object that encapsulates those 3 values and is parsed in a seperate method, in order to allow for better extensability. this also reduces the size of the alredy bloated repository config constructor."
0, Reduce number of compiler warning by adding @Override and generics where appropriate Same as JCR-2482 for the webdav library and the modules using it.
1,"Jcr-server: DeltaVResource lists MKWORKSPACE in the method constant.RFC 3253 requires REPORT to be supported by all DeltaV compliant resources.
The method constant therefore should list REPORT only."
0,Random access non RAM resident IndexDocValues (CSF)There should be a way to get specific IndexDocValues by going through the Directory rather than loading all of the values into memory.
0,"move contrib/analyzers to modules/analysisThis is a patch to move contrib/analyzers under modules/analyzers

We can then continue consolidating (LUCENE-2413)... in truth this will sorta be 
an ongoing thing anyway, as we try to distance indexing from analysis, etc
"
0,Documentation - jackrabbit features beyond the spec
0,"TestIndexReaderReopen nightly build failureAn interesting failure in last night's build (http://hudson.zones.apache.org/hudson/job/Lucene-trunk/920).

I think the root cause wast he AIOOB exception... all the ""lock obtain timed out"" exceptions look like they cascaded.

{code}
    [junit] Testsuite: org.apache.lucene.index.TestIndexReaderReopen
    [junit] Lock obtain timed out: org.apache.lucene.store.SingleInstanceLock@6ac615: write.lock)
    [junit] Tests run: 15, Failures: 1, Errors: 0, Time elapsed: 31.087 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] java.lang.ArrayIndexOutOfBoundsException: Array index out of range: 148
    [junit] 	at org.apache.lucene.util.BitVector.getAndSet(BitVector.java:74)
    [junit] 	at org.apache.lucene.index.SegmentReader.doDelete(SegmentReader.java:908)
    [junit] 	at org.apache.lucene.index.IndexReader.deleteDocument(IndexReader.java:1122)
    [junit] 	at org.apache.lucene.index.DirectoryReader.doDelete(DirectoryReader.java:521)
    [junit] 	at org.apache.lucene.index.IndexReader.deleteDocument(IndexReader.java:1122)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$8.modifyIndex(TestIndexReaderReopen.java:638)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen.refreshReader(TestIndexReaderReopen.java:840)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen.access$400(TestIndexReaderReopen.java:47)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$9.run(TestIndexReaderReopen.java:681)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$ReaderThread.run(TestIndexReaderReopen.java:822)
    [junit] org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: org.apache.lucene.store.SingleInstanceLock@88d319: write.lock
    [junit] 	at org.apache.lucene.store.Lock.obtain(Lock.java:85)
    [junit] 	at org.apache.lucene.index.DirectoryReader.acquireWriteLock(DirectoryReader.java:666)
    [junit] 	at org.apache.lucene.index.IndexReader.setNorm(IndexReader.java:994)
    [junit] 	at org.apache.lucene.index.IndexReader.setNorm(IndexReader.java:1020)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$8.modifyIndex(TestIndexReaderReopen.java:634)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen.refreshReader(TestIndexReaderReopen.java:840)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen.access$400(TestIndexReaderReopen.java:47)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$9.run(TestIndexReaderReopen.java:681)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$ReaderThread.run(TestIndexReaderReopen.java:822)
    ...
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testThreadSafety(org.apache.lucene.index.TestIndexReaderReopen):	FAILED
    [junit] Error occurred in thread Thread-36:
    [junit] Lock obtain timed out: org.apache.lucene.store.SingleInstanceLock@6ac615: write.lock
    [junit] junit.framework.AssertionFailedError: Error occurred in thread Thread-36:
    [junit] Lock obtain timed out: org.apache.lucene.store.SingleInstanceLock@6ac615: write.lock
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen.testThreadSafety(TestIndexReaderReopen.java:764)
    [junit] 
    [junit] 
{code}"
0,"LocalTestServer and supporting classes should be available as a separate jarLocalTestServer and it's supporting classes are useful to anyone who wants to easily ""mock""/test simple http calls without having to embed a full jetty or something.
It would be awesome if these were available in a separate http-localtestserver.jar that could be used in projects outside of httpclient."
0,"Cut over numeric docvalues to fixed straight bytesCurrently numeric docvalues types are encoded and stored individually which creates massive duplication of writing / indexing code. Yet, almost all of them (except packed ints) are essentially a fixed straight bytes variant. "
0,"Add possibility to disable automatic authentication header processing.In the application I'm working on I need the possibility to manually get the 
WWW-Authenticate header instead of letting the HttpClient process it 
automatically. Instead of rewriting the execute() method in a subclass I added 
a configuration setting, similar to the followRedirects flag. Diffs below for 
anyone interested."
0,JSR 283: Workspace Management
0,"Clone proxStream lazily in SegmentTermPositionsIn SegmentTermPositions the proxStream should be cloned lazily, i. e. at the first time nextPosition() is called. Then the initialization costs of TermPositions are not higher anymore compared to TermDocs and thus there is no reason anymore for Scorers to use TermDocs instead of TermPositions. In fact, all Scorers should use TermPositions, because custom subclasses of existing scorers might want to access payloads, which is only possible via TermPositions. We could further merge SegmentTermDocs and SegmentTermPositions into one class and deprecate the interface TermDocs.

I'm going to attach a patch once the payloads feature (LUCENE-755) is committed."
0,"Port to Java5For my needs I've updated Lucene so that it uses Java 5 constructs. I know Java 5 migration had been planned for 2.1 someday in the past, but don't know when it is planned now. This patch against the trunk includes :

- most obvious generics usage (there are tons of usages of sets, ... Those which are commonly used have been generified)
- PriorityQueue generification
- replacement of indexed for loops with for each constructs
- removal of unnececessary unboxing

The code is to my opinion much more readable with those features (you actually *know* what is stored in collections reading the code, without the need to lookup for field definitions everytime) and it simplifies many algorithms.

Note that this patch also includes an interface for the Query class. This has been done for my company's needs for building custom Query classes which add some behaviour to the base Lucene queries. It prevents multiple unnnecessary casts. I know this introduction is not wanted by the team, but it really makes our developments easier to maintain. If you don't want to use this, replace all /Queriable/ calls with standard /Query/.

"
0,"Persian AnalyzerA simple persian analyzer.

i measured trec scores with the benchmark package below against http://ece.ut.ac.ir/DBRG/Hamshahri/ :

SimpleAnalyzer:
SUMMARY
  Search Seconds:         0.012
  DocName Seconds:        0.020
  Num Points:           981.015
  Num Good Points:       33.738
  Max Good Points:       36.185
  Average Precision:      0.374
  MRR:                    0.667
  Recall:                 0.905
  Precision At 1:         0.585
  Precision At 2:         0.531
  Precision At 3:         0.513
  Precision At 4:         0.496
  Precision At 5:         0.486
  Precision At 6:         0.487
  Precision At 7:         0.479
  Precision At 8:         0.465
  Precision At 9:         0.458
  Precision At 10:        0.460
  Precision At 11:        0.453
  Precision At 12:        0.453
  Precision At 13:        0.445
  Precision At 14:        0.438
  Precision At 15:        0.438
  Precision At 16:        0.438
  Precision At 17:        0.429
  Precision At 18:        0.429
  Precision At 19:        0.419
  Precision At 20:        0.415

PersianAnalyzer:
SUMMARY
  Search Seconds:         0.004
  DocName Seconds:        0.011
  Num Points:           987.692
  Num Good Points:       36.123
  Max Good Points:       36.185
  Average Precision:      0.481
  MRR:                    0.833
  Recall:                 0.998
  Precision At 1:         0.754
  Precision At 2:         0.715
  Precision At 3:         0.646
  Precision At 4:         0.646
  Precision At 5:         0.631
  Precision At 6:         0.621
  Precision At 7:         0.593
  Precision At 8:         0.577
  Precision At 9:         0.573
  Precision At 10:        0.566
  Precision At 11:        0.572
  Precision At 12:        0.562
  Precision At 13:        0.554
  Precision At 14:        0.549
  Precision At 15:        0.542
  Precision At 16:        0.538
  Precision At 17:        0.533
  Precision At 18:        0.527
  Precision At 19:        0.525
  Precision At 20:        0.518

"
0,EventImpl should implement toStringThis would simplify logging and debugging.
1,Mandatory jcr:activities node missing after upgradeThe mandatory node is only created when the repository is initially empty. The node is missing when an existing repository instance is upgraded. 
1,"IndexWriter memory leak when large docs are indexedSpinoff from the java-user thread ""IndexWriter and memory usage""...

IndexWriter has had a long standing memory leak, since LUCENE-843.

When the byte/char/int blocks are recycled to the common pool, the
per-thread DW classes incorrectly still hold a reference to them.

This normally is not a problem, since these buffers will be re-used
again.

But, if you index a massive document, causing IW to allocate more than
the RAM buffer allocated to it, then the leak happens.  So you could
have a 16 MB RAM buffer set, but if a huge doc required allocation of
200 MB worth of arrays, those 200 MB are never freed (well, until you
close the IW and deref it from the app).

It's even worse if you use multiple threads: if each thread has ever
had to index a massive document, then that thread incorrectly holds
onto the extra arrays.
"
0,"Expose DocValues via FieldsDocValues Reader are currently exposed / accessed directly via IndexReader. To integrate the new feature in a more ""native"" way we should expose the DocValues via Fields on a perSegment level and on MultiFields in the multi reader case. DocValues should be side by side with Fields.terms  enabling access to Source, SortedSource and ValuesEnum something like that:

{code}
public abstract class Fields {
...

  public DocValues values();

}

public abstract class DocValues {
  /** on disk enum based API */
  public abstract ValuesEnum getEnum() throws IOException;
  /** in memory Random Access API - with enum support - first call loads values in ram*/
  public abstract Source getSource() throws IOException;
  /** sorted in memory Random Access API - optional operation */
  public SortedSource getSortedSource(Comparator<BytesRef> comparator) throws IOException, UnsupportedOperationException;
  /** unloads previously loaded source only but keeps the doc values open */
  public abstract unload();
  /** closes the doc values */
  public abstract close();
}
{code}

"
0,"Let users set Similarity for MoreLikeThisLet users set Similarity used for MoreLikeThis

For discussion, see:
http://www.nabble.com/MoreLikeThis-API-changes--tf3838535.html"
0,"TCK: TextNodeTest and jcr:xmltext/jcr:xmlcharactersTest creates jcr:xmltext nodes without jcr:xmlcharacters properties.  Some repositories may require jcr:xmltext nodes to have jcr:xmlcharacters properties, causing this test to fail.

Proposal: add a jcr:xmlcharacters property to each jcr:xmltext node.

--- TextNodeTest.java   (revision 422074)
+++ TextNodeTest.java   (working copy)
@@ -62,6 +62,7 @@
      */
     public void testTextNodeTest() throws RepositoryException {
         Node text1 = testRootNode.addNode(jcrXMLText);
+        text1.setProperty(jcrXMLCharacters, ""foo"");
         testRootNode.save();
         String xpath = ""/"" + jcrRoot + testRoot + ""/text()"";
         executeXPathQuery(superuser, xpath, new Node[]{text1});
@@ -73,7 +74,9 @@
      */
     public void testTextNodeTestMultiNodes() throws RepositoryException {
         Node text1 = testRootNode.addNode(jcrXMLText);
+        text1.setProperty(jcrXMLCharacters, ""foo"");
         Node text2 = testRootNode.addNode(nodeName1, testNodeType).addNode(jcrXMLText);
+        text2.setProperty(jcrXMLCharacters, ""foo"");
         testRootNode.save();
         String xpath = ""/"" + jcrRoot + testRoot + ""//text()"";
         executeXPathQuery(superuser, xpath, new Node[]{text1, text2});
@@ -105,11 +108,13 @@
             throw new NotExecutableException(""Repository does not support position index"");
         }
         Node text1 = testRootNode.addNode(jcrXMLText);
+        text1.setProperty(jcrXMLCharacters, ""foo"");
         if (!text1.getDefinition().allowsSameNameSiblings()) {
             throw new NotExecutableException(""Node at path: "" + testRoot + "" does not allow same name siblings with name: "" + jcrXMLText);
         }
         testRootNode.addNode(nodeName1, testNodeType);
         Node text2 = testRootNode.addNode(jcrXMLText);
+        text1.setProperty(jcrXMLCharacters, ""foo"");
         testRootNode.save();
         String xpath = ""/"" + jcrRoot + testRoot + ""/text()[2]"";
         executeXPathQuery(superuser, xpath, new Node[]{text2});
"
0,"TestIndexModifier.testIndexWithThreads is not valid?I recently started playing with the trunk of SVN, and noticed that intermitently, TestIndexModifier.testIndexWithThreads (revision 292010) would fail.

The basic premise of the test seems to be that 3 pairs of IndexThread instances can be started in parallel, each pair using the same instance of IndexModifier to concurrently and randomly add/delete/optimize a single FSDirectory index.  
The test is considered a success if the sum of additions-deletions recorded by each pair of threads equals the final docCount() for the IndexModifier instance used by that pair of threads.

Now I freely admit that I'm not 100% familiar with the code for IndexModifier, but at a glance, the basic premise seems to be: 
   a) If method for IndexWriter is called, open it if needed, close the IndexReader first if needed.
   b) if method for IndexReader is called, open it if needed, close the IndexWriter first if needed.

If I'm understnading that correctly, I see no reason to assume this test will pass.  
It seems like there could be plenty of scenerios in which the number of additions-deletions != docCount(). The most trivial example I can think of is:
   1) the first IndexThread instance which has a chance to run adds a document, and optimizes before any other IndexThreads ever open the Directory.
   2) a subsequent pair of IndexThread instances open their IndexModifier instance before any documents are deleted.
   3) the IndexThread instances from #2 do nothing but add documents
...that pair of IndexThreads is now garunteed to have recorded a differnet number of additions then the docCount returned by their IndexModifier.

Am I missing something, or should this test be removed?

"
0,"[PATCH] documentation typoJust a small patch that fixes a typo and changes the first sentence, as that 
one is used by Javadoc as a kind of summary so it should be something more 
useful than ""The Jakarta Lucene API is divided into several packages."""
0,"Allow parsing custom elements in workspace configIn RepositoryConfigurationParser, most *Config elements can be extended in a derived class, e.g.

    public LoginModuleConfig parseLoginModuleConfig(Element security)

Unfortunately, parseWorkspaceConfig expects an InputSource. One should add a

    protected WorkspaceConfig parseWorkspaceConfig(Element root)

to allow returning a WorkspaceConfig derived class, without having to copy the entire implementation."
0,Stop creating huge arrays to represent the absense of field normsCreating and keeping around huge arrays that hold a constant value is very inefficient both from a heap usage standpoint and from a localility of reference standpoint. It would be much more efficient to use null to represent a missing norms table.
0,"A number of documentation fixes for the search package summaryImproves readability and clarity, corrects some basic English, makes some example text even more clear, and repairs typos."
0,Make CFS appendable  Currently CFS is created once all files are written during a flush / merge. Once on disk the files are copied into the CFS format which is basically a unnecessary for some of the files. We can at any time write at least one file directly into the CFS which can save a reasonable amount of IO. For instance stored fields could be written directly during indexing and during a Codec Flush one of the written files can be appended directly. This optimization is a nice sideeffect for lucene indexing itself but more important for DocValues and LUCENE-3216 we could transparently pack per field files into a single file only for docvalues without changing any code once LUCENE-3216 is resolved.
0,"CartesianTierPlotter fieldPrefix should be configurableCartesianTierPlotter field prefix is currrently hardcoded to ""_localTier"" -- this should be configurable"
0,"Fix JFlex tokenizer compiler warningsWe get lots of distracting fallthrough warnings running ""ant compile""
in modules/analysis, from the tokenizers generated from JFlex.

Digging a bit, they actually do look spooky.

So I managed to edit the JFlex inputs to insert a bunch of break
statements in our rules, but I have no idea if this is
right/dangerous, and it seems a bit weird having to do such insertions
of ""naked"" breaks.

But, this does fix all the warnings, and all tests pass...
"
0,"QueryParser is not applicable for the arguments (String, String, Analyzer) error in results.jsp when executing search in the browser (demo from Lucene 2.0)When executing search in the browser (as described in demo3.html Lucene demo) I get error, because the demo uses the method (QueryParser with three arguments) which is deleted (it was deprecated).
I checked the demo from Lucene 1.4-final it with Lucene 1.4-final - it works, because those time the method was there.
But demo from Lucene 2.0 does not work with Lucene 2.0

The error stack is here:
TTP Status 500 -

type Exception report

message

description The server encountered an internal error () that prevented it from fulfilling this request.

exception

org.apache.jasper.JasperException: Unable to compile class for JSP

An error occurred at line: 60 in the jsp file: /results.jsp
Generated servlet error:
The method parse(String) in the type QueryParser is not applicable for the arguments (String, String, Analyzer)


org.apache.jasper.servlet.JspServletWrapper.handleJspException(JspServletWrapper.java:510)
org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:375)
org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:314)
org.apache.jasper.servlet.JspServlet.service(JspServlet.java:264)
javax.servlet.http.HttpServlet.service(HttpServlet.java:802)

root cause

org.apache.jasper.JasperException: Unable to compile class for JSP

An error occurred at line: 60 in the jsp file: /results.jsp
Generated servlet error:
The method parse(String) in the type QueryParser is not applicable for the arguments (String, String, Analyzer)


org.apache.jasper.compiler.DefaultErrorHandler.javacError(DefaultErrorHandler.java:84)
org.apache.jasper.compiler.ErrorDispatcher.javacError(ErrorDispatcher.java:328)
org.apache.jasper.compiler.JDTCompiler.generateClass(JDTCompiler.java:409)
org.apache.jasper.compiler.Compiler.compile(Compiler.java:297)
org.apache.jasper.compiler.Compiler.compile(Compiler.java:276)
org.apache.jasper.compiler.Compiler.compile(Compiler.java:264)
org.apache.jasper.JspCompilationContext.compile(JspCompilationContext.java:563)
org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:303)
org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:314)
org.apache.jasper.servlet.JspServlet.service(JspServlet.java:264)
javax.servlet.http.HttpServlet.service(HttpServlet.java:802)

note The full stack trace of the root cause is available in the Apache Tomcat/5.5.15 logs."
0,Improve password hashing
0,"[PATCH] don't delete all files in index directory on index creationMany people use Lucene to index a part of their file system. The chance that  
you some day mix up index directory and document directory isn't that bad.  
Currently Lucene will delete *all* files in the index directory when the  
create paramater passed to IndexWriter is true, thus deleting your documents 
if you mixed up the parameters. I'll attach a patch that fixes  
this. Any objections?"
0,"remove IndexSearcher.docFreq/maxDocAs pointed out by Mark on SOLR-1632, these are no longer used by the scoring system.

We've added new stats to Lucene, so having these methods on indexsearcher makes no sense.
Its confusing to people upgrading if they subclassed IndexSearcher to provide distributed stats,
only to find these are not used (LUCENE-3555 has a correct API for them to do this).

So I think we should remove these in 4.0."
1,"Repository Home locked not released despite RepositoryException being thrown.When an exception is thrown when calling RepositoryImpl.create(...) a .lock file is created in the repository home directory and not removed despite there no longer being an active connection.  If the user attempts to create the repository again (e.g recover from the exception because the url of the repository was temporarily unavailable) a RepositoryException is thrown again indicating that the repository home is locked by another process because there is a .lock file.  If a Repository is not successfully created then the repository home should not be locked.

The lock is only released when the repository is shutdown but in this case the Repository object is never created successfully for that method to be called.

"
0,Remove SegmentReader.document synchronizationThis is probably the last synchronization issue in Lucene.  It is the document method in SegmentReader.  It is avoidable by using a threadlocal for FieldsReader.  
1,"NullPointerException thrown when invalid header encounteredIf a server returns a header with no name but with a value (ie: an invalid line in the headers), HttpClient throws a NullPointerException instead of just skipping that header line or perhaps treating it as a continuation of the previous header (need to consult the RFC to confirm this).

Problem reported by Eduardo Francos on the commons-user list.

A good test URL for this problem is:

http://www.pc.ibm.com/us/accessories/monitors/p_allmodelos.html

which should return a 404 error but throws the NullPointerException instead."
0,"Simple Google style queryIn the Sling project there's a need for a simple query language. See SLING-573.

I've created a parser that translates the simple query into an XPath query statement and executes it on a JCR workspace.

I'll commit it to the jackrabbit-jcr-commons module."
0,"Allow configuration of SO_LINGERThere is currently no way to configure the SO_LINGER option on a socket.

Please change the HttpClient class to allow the configuration of the SO_LINGER
option on a socket, similar to the way the SO_TIMEOUT can be configured.

Suggested extension to the interface of the HttpClient class:
- Add method setSoLinger() to set the current setting for SO_LINGER. The method
could accept one argument. A negative value could indicate that the SO_LINGER
should be disabled.
- Add method getSoLinger() that returns the current setting for SO_LINGER. A
negative value would indicate that the SO_LINGER option is disabled.

See:
http://java.sun.com/j2se/1.4.2/docs/api/java/net/Socket.html#setSoLinger(boolean,%20int)"
1,"SQL query with jcr:path LIKE '/foo/%' only selects childrenA query like: 

SELECT * FROM nt:base WHERE jcr:path LIKE '/foo/%'

only selects the children of /foo instead off all descendants of /foo."
0,"use VersionInfo of coreWith core alpha5, a version detection scheme was introduced.
Replace the preliminary version detection of client alpha1 with that in core.
That means new version.properties files, at least one per JAR, maybe one per potential JAR.
"
0,"Benchmark contrib should allow multiple locations in ext.classpathWhen {{ant run-task}} is invoked with the  {{-Dbenchmark.ext.classpath=...}} option, only a single location may be specified.  If a classpath with more than one location is specified, none of the locations is put on the classpath for the invoked JVM."
1,"{XML|Object}PersistenceManager.destroy(*) may failThe destroy methods of the ObjectPersistenceManager class try to delete their files without checking for their existence. This may result in a FileSystemException being thrown because according to the specification of FileSystem.deleteFile() a FileSystemException is thrown ""if this path does not denote a file or if another error occurs.""

While the Jackrabbit LocalFileSystem implementation silently ignores a request to delete a non-existing file, our internal implementation of the interface throws a FileSystemException in this case, which cause destroy to fail.

I suggest all destroy methods should be extended to first check for the existence of the file to prevent from being thrown.

Note: This not only applies to ObjectPersistenceManager but also to XMLPersistenceManager."
0,"Polish AnalyzerAndrzej Bialecki has written a Polish stemmer and provided stemming tables for it under Apache License.

You can read more about it here: http://www.getopt.org/stempel/

In reality, the stemmer is general code and we could use it for more languages too perhaps."
1,"XML import should not access external entitiesWith current Jackrabbit the following XML document can not be imported:

    <!DOCTYPE foo SYSTEM ""http://invalid.address/""><foo/>

Even if the DTD address (or some other external resource referenced in the XML document) is correct, I don't think importXML() should even try resolving those references."
0,"ProtocolException thrown on slightly broken headersHTTPClient throws an exception when parsing headers returned by GET from the
following URL:

 http://butler.cit.nih.gov/hembase/hembase.taf

The headers returned are as follows:

HTTP/1.0 200 OK\r\nServer: WebSTAR/1.0 ID/ACGI\r\nMIME-Version:
1.0\r\nContent-Type: text/html\r\nSet-Cookie:
Tango_UserReference=ADC5871C57FABEDEC63DD47B; path=/\n\r\r\n\r\n<!DOCTYPE HTML
PUBLIC ""-//W3C//DTD HTML 4.01 Transitional//EN"">...

Please note the superfluous \r in the line separating headers from the body.
IMHO this type of error should generate a warning, but then it should cause a
graceful recovery. Currently a ProtocolException is thrown.

Standard java.net.HttpURLConnection handles this just fine, without giving any
warning."
0,"CheckIndex should verify numUniqueTerms == recomputedNumUniqueTermsJust glancing at the code it seems to sorta do this check, but only in the hasOrd==true case maybe (which seems to be testing something else)?

It would be nice to verify this also for terms dicts that dont support ord.

we should add explicit checks per-field in 4.x, and for-all-fields in 3.x and preflex"
0,"Remove Maven 1 filesNow that we have a working Maven 2 build environment (JCR-332) we should remove the old Maven 1
project files to avoid confusion and misunderstandings. The old Maven 1 build environment doesn't even
work anymore.

Unless anyone objects, I'll proceed to remove the project.xml, maven.xml, and project.properties files in a few days."
1,"ClassDescriptor.hasIdField() fails if id is declared in upper classorg.apache.jackrabbit.ocm.mapper.model.ClassDescriptor.hasIdField() looks up only current class and not the whole hierarchy, so it fails when the id field is declared in a upper class.

hasIdField should use getIdFieldDescriptor and not access idFieldDescriptor field directly, as follows :

    public boolean hasIdField() {
   		return (this.getIdFieldDescriptor() != null && this
    				.getIdFieldDescriptor().isId());
    }

Please find patch enclosed.

Sincerely yours,

Stphane Landelle"
0,"Improve the use of isDeleted in the indexing codeA spin off from here: http://www.nabble.com/Some-thoughts-around-the-use-of-reader.isDeleted-and-hasDeletions-td23931216.html.
Two changes:
# Optimize SegmentMerger work when a reader has no deletions.
# IndexReader.document() will no longer check if the document is deleted.

Will post a patch shortly"
0,"jcr-commons: Add utility to translate a string to a AuthorizableQuery and execute it on the user manager it would be convenient if jackrabbit-jcr-commons would provide a utility to generate authorizable
queries from a string."
0,"spi2dav: Drop Q*DefinitionImpl implementations and use spi-commons Q*DefinitionBuilderspi2dav provides separate implementations of the Q*Definition interfaces that apart from the construction just duplicate the code
present in spi-commons. Instead the Q*DefinitionBuilder helpers could be used to generate the definition instances."
0,VersionTest.testGetUUID() failsVersionTest.testGetUUID() fails due to inproper invlaidation of the successor properties after checkin.
1,"NTLM Authentication No Longer Working In Latest ReleaseOur application has been working fine using NTLM auth with HttpClient for 3 years.   We were most recently on 4.0.3.    Upon upgrading to 4.1.2, NTLM stopped working.

I tried both the new for 4.1 built-in NTLM and the ""old way"" of using JCIFS: client.getAuthSchemes().register(""ntlm"", new NTLMSchemeFactory()); 

Using wireshark I can see that NTLM auth is not even attempted using 4.1.2.    Rolling back to 4.0.3 immediately resolved this problem."
0,"[patch] javadoc and comment updates for BooleanClause.Javadoc and comment updates for BooleanClause, one minor code simplification."
0,"Too many open files when merging large index segmentsWhen large index segments are merged it may happen that lots of smaller index segments are created but have to wait until the large
index merge has completed. This may lead to a 'too many open files' exception on some system.

We should find a solution where large index merges are better decoupled from regular index operations."
0,"Support multi-selector OR constraints in join queriesOur current join implementation doesn't support OR constraints that refer to more than one selector. For example the following query is not possible:

    SELECT a.* FROM [my:type] AS a INNER JOIN [my:type] as b ON a.foo = b.bar WHERE a.baz = 'x' OR b.baz = 'y'

This limitation is a result of the way the join execution splits the query into per-selector components and merges the result based on the given join condition.

A simple but often inefficient solution would be to process such OR constraints as post-processing filters like we already do for some other more complex constraint types."
0,create test case to verify we support > 2.1B termsI created a test case for this... I'm leaving it as @Ignore because it takes more than four hours on a faaast machine (beast) to run.  I think we should run this before each release.
1,"Bundle binding deserialization problemI'm trying to upgrade from 1.3.x to jackrabbit 1.4.x (branch)  and have problems with existing repostories (probaly the same issue is with 1.5.x)

Caused by: org.apache.jackrabbit.core.state.ItemStateException: failed to read bundle: deadbeef-face-babe-cafe-babecafebabe: java.lang.IllegalArgumentException: invalid namespaceURI specified
 at org.apache.jackrabbit.core.persistence.bundle.BundleDbPersistenceManager.loadBundle(BundleDbPersistenceManager.java:1229)
 at org.apache.jackrabbit.core.persistence.bundle.BundleDbPersistenceManager.loadBundle(BundleDbPersistenceManager.java:1161)

It looks that issue was introduced by resolving JCR-1632"
1,"SQL-2 query returns more than the requested columnIf I do :

SELECT alias.[jcr:title] FROM [jnt:mainContent] as alias

and then iterate through the returned columns of the rows, I get the same result as for :

SELECT * FROM [jnt:mainContent] 

which is ALL the properties defined for jnt:mainContent.

Only if I use

SELECT alias.[jcr:title] as title FROM [jnt:mainContent] as alias

the result is limited to the title column."
0,SQL2: Implement LIKE support for node namesDoing a LIKE constraint on the local name of a node that throws javax.jcr.UnsupportedRepositoryOperationException.
0,"Promotion of SPI from ContribThis has been suggested by Jukka on the dev-list [1] and i would like to start the promotion now. Apart from 
the promotion itself the discussion regarding distribution of common classes [2] and the related issue JCR-996 somehow depends on the promotion and i consider the latter one to be important to follow.

So far nobody objected the promotion. If this is still true, i will start working on that.




[1] http://www.mail-archive.com/dev@jackrabbit.apache.org/msg06433.html
[2] http://www.mail-archive.com/dev@jackrabbit.apache.org/msg06698.html"
0,JCR2SPI: remove dependency to state-package within nodetype package
0,"[PATCH]character encoding handling is invalid at multipartHi,

Commons-Httpclient handle character encoding incorrect at multipart. This is 
significant problem for other than English people like me. Multipart has two 
encoding. First is header encoding which specify header of each part. Second 
is it's body encoding. Body encoding works well but header encoding is fixed 
as 'asc-ii'. This problem user following situation.

* upload file which file name is described by other than ""asc-ii"".
* use parameter which include other than ""asc-ii"" character.

Unfortunately , It seems RFC doesn't define header encoding for multipart but 
a lot of people needs set header encoding for thier own laungage. I attached
the patch. Please fix this problem.

regards,

Takashi Okamoto"
1,"ObjectConverterImpl.getObject(Session, Class, String) may not resolve mapping correctly for incompletely described mappingsWhen a node is mapped by calling the ObjectConverter.getObject(Session, Class, String) method and no discriminator property is configured the ObjectConverterImpl class tries to find a ""best"" mapping for the effective node. This is done by walking the class descriptor hierarchy starting at the descriptor for the selected class until a mapping for the node type is found.

In case the class descriptor hierarchy is incomplete because an improperly defined class descriptor would actually perfectly map the node but is not declared to extend (or implement) its parent classes/interfaces, the hierarchy walk down will not find the mapping and thus in the end, the originally requested class will be instantiated. If the class is abstract or an interface this of course fails.

If an exact class descriptor for the node type would be looked up directly, the mapping might be found immediately and the class of the descriptor can be verified it actually is assignement compatible with the requested class. If this would fail, we could still walk the hierarchy to see, whether we find another classdescriptor.

To clarify the issue consider the following example of an abstract base class and a concrete extension class with their node types

   AbstractBaseClass maps abstractly to AbstractBaseType
   BaseClass (extends AbstractBaseClass) maps to BaseType ( with supertype AbstractBaseType )

Note, that the BaseClass mapping does not declare to extend the AbstractBaseClass.

When calling ObjectConverterImpl.getObject(session, AbstractBaseClass.class, aBaseTypeNode), the descriptor fore the AbstractBaseClass is inspected agains the node and then it is decided to check the class descriptor hierarchy. Node mapping can be found by walking the hierarchy and hence the AbstractBaseClass is instantiated, which of course fails.

If the BaseClass mapping would be declared as extending the AbstractBaseClass mapping, everything would be fine."
0,"TestUTF32ToUTF8 can run foreverStress testing this particular test uncovered that the testRandomRanges testcase can run forever, depending on the random numbers picked..."
0,"move contrib/snowball to contrib/analyzersto fix bugs in some duplicate, handcoded impls of these stemmers (nl, fr, ru, etc) we should simply merge snowball and analyzers, and replace the buggy impls with the proper snowball stemfilters.
"
0,"Add preemptive authenticationWishlist request for preemptive authentication to be included in the API, like HttpClient 3.x had.  There is an example ClientPreemptiveBasicAuthentication.java that uses HttpRequestInterceptor which I had adapted to my application and it works fine."
0,Improved error reporting from JcrUtils.getRepositoryThe service provider mechanism and the null return value used by the RepositoryFactory API makes it a bit difficult to troubleshoot cases where a repository can not be accessed. It would be helpful if the JcrUtils.getRepository methods reported as accurate failure information as possible in case the requested repository is not found.
1,"Parameters 'idleTime' and 'queryClass' cause QueryHandler to failThis issue does not occur in a released jackrabbit-core version. With the changes from JCR-1462 jackrabbit now fails to startup if there is an unknown parameter in a bean configuration.

The parameters 'idleTime' and 'queryClass' are not used by the QueryHandler but by the SearchManager, which instantiates the QueryHandler. Therefore the parameters do not show up in the QueryHandler.

I suggest we introduce them in the common base class AbstractQueryHandler."
1,"FVH: slow performance on very large queriesThe change from HashSet to ArrayList for flatQueries in LUCENE-3019 resulted in very significant slowdown in some of our e-discovery queries after upgrade from 3.4.0 to 3.5.0. Our queries sometime contain tens of thousands of terms. As a result, major portion of execution time for such queries is now spent in the flatQueries.contains( sourceQuery ) method calls."
0,Implementation of a memory file systemI needed a memory file system for my test cases. A patch for a simple implementation the works well in my environment is attached
0,Basic support for fn:name()Add basic support for fn:name() in XPath queries. Jackrabbit should at least support the the fn:name() function within an equals expression.
0,"SPI: change param order with RepositoryService.createBatchall methods on RepositoryService that require a SessionInfo list the info as first parameter, except for 

RepositoryService.createBatch(ItemId, SessionInfo) 

unless someone objects i would refacter the method signature for consistency reasons.

new:

RepositoryService.createBatch(SessionInfo, ItemId)"
0,"Support all of unicode in StandardTokenizerStandardTokenizer currently only supports the BMP.

If it encounters characters outside of the BMP, it just discards them... 
it should instead implement fully implement UAX#29 across all of unicode."
1,"TaxonomyReader.refresh() is broken, replace its logic with reopen(), following IR.reopen patternWhen recreating the taxonomy index, TR's assumption that categories are only added does not hold anymore.
As result, calling TR.refresh() will be incorrect at best, but usually throw an AIOOBE."
0,"The repository-1.5.dtd is not well formedThe repository-1.5.dtd file at http://jackrabbit.apache.org/dtd/repository-1.5.dtd
is not well formed at the time of this writing 200/05/23 19:30GMT

1. It looks like a #REQUIRED is missing at line 173
2. Detected this while trying the 5minutes with ocm tutorial

Hope this helps,
 S."
1,"Finding Newest Segment In Empty IndexWhile extending the index writer, I discovered that its newestSegment method does not check to see if there are any segments before accessing the segment infos vector. Specifically, if you call the IndexWriter#newestSegment method on a brand-new index which is essentially empty, then it throws an java.lang.ArrayIndexOutOfBoundsException exception.

The proposed fix is to return null if no segments exist, as shown below:

--- lucene/src/java/org/apache/lucene/index/IndexWriter.java	(revision 930788)
+++ lucene/src/java/org/apache/lucene/index/IndexWriter.java	(working copy)
@@ -4587,7 +4587,7 @@
 
   // utility routines for tests
   SegmentInfo newestSegment() {
-    return segmentInfos.info(segmentInfos.size()-1);
+    return segmentInfos.size() > 0 ? segmentInfos.info(segmentInfos.size()-1) : null;
   }
"
1,"nonce-count in digest auth should not be quotedIn 3.0rc3 nonce-count (nc) is enclosed in quote marks. According to rfc2617 this
is wrong, nonce-count shouldn't be enclosed in quote marks.

> 3.2.2 The Authorization Request Header
> 
>    The client is expected to retry the request, passing an Authorization
>    header line, which is defined according to the framework above,
>    utilized as follows.
> 
>        credentials      = ""Digest"" digest-response
>        digest-response  = 1#( username | realm | nonce | digest-uri
>                        | response | [ algorithm ] | [cnonce] |
>                        [opaque] | [message-qop] |
>                            [nonce-count]  | [auth-param] )
> 
>        username         = ""username"" ""="" username-value
>        username-value   = quoted-string
>        digest-uri       = ""uri"" ""="" digest-uri-value
>        digest-uri-value = request-uri   ; As specified by HTTP/1.1
>        message-qop      = ""qop"" ""="" qop-value
>        cnonce           = ""cnonce"" ""="" cnonce-value
>        cnonce-value     = nonce-value
>        nonce-count      = ""nc"" ""="" nc-value
>        nc-value         = 8LHEX
>        response         = ""response"" ""="" request-digest
>        request-digest = <""> 32LHEX <"">
>        LHEX             =  ""0"" | ""1"" | ""2"" | ""3"" |
>                            ""4"" | ""5"" | ""6"" | ""7"" |
>                            ""8"" | ""9"" | ""a"" | ""b"" |
>                            ""c"" | ""d"" | ""e"" | ""f"""
0,"Enhance Ingres persistence bundle to handle unicodeTiny change to ingres.ddl for persistent bundles to handle unicode strings.

"
1,"CircularRedirectException encountered when using a proxy, but not when reaching the target directlyA CircularRedirectException is encountered when using a proxy (tinyproxy on a remote machine), whereas everything is fine when using no proxy. The target is a URL such as http://www.seoconsultants.com/w3c/status-codes/301.asp which has a 301 redirection.

The issue can be fixed by using ALLOW_CIRCULAR_REDIRECTS set to true (client params), but I can't consider this a ""real"" fix.

Here is a snippet of code that exemplifies the problem (use your own proxy):

---
String proxyHost = ""xyz.webfactional.com"";
int proxyPort = 7295;

DefaultHttpClient httpclient = new DefaultHttpClient();
// without a proxy it's OK!
httpclient.getParams().setParameter(ConnRoutePNames.DEFAULT_PROXY,
        new HttpHost(proxyHost, proxyPort, ""http""));

HttpParams params = httpclient.getParams();
HttpClientParams.setRedirecting(params, true);
HttpProtocolParams.setUserAgent(params,
        ""Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.0.10) Gecko/2009042315 Firefox/3.0.10"");

// OK, this fixes the problem, but at what cost / other problems ?
//httpclient.getParams().setParameter(ClientPNames.ALLOW_CIRCULAR_REDIRECTS, true);

String url = ""http://www.seoconsultants.com/w3c/status-codes/301.asp"";

HttpUriRequest request;
HttpResponse response;

request = new HttpGet(url);
System.out.println(""request = "" + request.getRequestLine());
response = httpclient.execute(request);
System.out.println(""status = "" + response.getStatusLine());
System.out.println(""headers = "" + Arrays.asList(response.getAllHeaders()));
---"
0,Remove ItemInfo.getName() since it is redundantI propose to remove the method getName() from org.apache.jackrabbit.spi.ItemInfo since it is redundant. The name is always the last element of the path which is available via the getPath() method.
0,"Move backwards compatibility tests to trunkAs discussed on dev@, I'd like to move the backwards compatibility tests from the Jackrabbit sandbox to trunk."
0,"CompactNodeTypeReader fails to explain why valid JCR names cause errorsfor example, you cannot use underscores in node type definitions:
[my:example_breaks2]

In fact only A-Z, a-z, 0-9, : are allowed, unless you quote the name. The error message you see when you make this mistake doesn't give any hint:
Missing ']' delimiter for end of node type name (nodetypes.cnd, line 8)

and the documentation on the website and the javadoc for CompactNodeTypeDefReader both just say:

 * unquoted_string ::= ...a string...

... not helpful. If you made this mistake, you end up needing to look at the source to figure out what you've done wrong. 

A few suggested solutions:
- change the documentation to say unquoted string is '[A-Za-z0-9:]+'
- change the error message to mention the token causing the problem, eg:
if (!currentTokenEquals(Lexer.END_NODE_TYPE_NAME)) {
            lexer.fail(""Missing '"" + Lexer.END_NODE_TYPE_NAME + ""' delimiter for end of node type name, found "" + currentToken);
}
- add ""st.wordChars('_','_');"" to the lexer, its probably going to be the most common cause, and doesnt conflict with other rules.
"
1,"webdav's PropertyDefinitionImpl's toXML doesn't seem to attach query operators element to the returned domPropertyDefinitionImpl.toXML does

        // JCR 2.0 extension
        Element qopElem = document.createElement(AVAILABLE_QUERY_OPERATORS_ELEMENT);
        String[] qops = getAvailableQueryOperators();
        for (int i = 0; i < qops.length; i++) {
            Element opElem = document.createElement(AVAILABLE_QUERY_OPERATOR_ELEMENT);
            DomUtil.setText(opElem, qops[i]);
            qopElem.appendChild(opElem);
        }

        return elem;

which doesn't attach the qopElem to the returned dom."
0,Keep WebDAV exception causesThe DavMethodBase and ExceptionConverter classes in jackrabbit-webdav and jackrabbit-spi2dav don't include the cause when throwing an exception based on some caught cause. This makes it harder to identify what is causing a  particular problem. The attached patch fixes that.
0,"Extend the client's redirect handling interface to allow control of the content of the redirectThe existing RedirectHandler interface provides the ability influence which situations cause redirects, but gives you no control over the content of the redirect itself.  For example, if you want the client follow the redirect of a POST request with a POST request to the new location, you can't do it.  DefaultRequestDirector decides what method will be used on the redirect request and as of the most recent patch, it's always either a HEAD or a GET.

One option for resolving this might be extending the RedirectHandler interface to be a factory for creating the redirect request object.  The the DefaultRequestDirector could then be changed to ask the RedirectHandler to create the appropriate request for the situation.

Thanks,
Ben"
1,"ConnectException not handled in DefaultHttpMethodRetryHandlerCopied from my mailing list post, Oleg suggested I post it to JIRA for 4.0 fix.

i am using commons-httpclient.3.0.1 and I am sending some requests
through https protocol. I have a problem with a long creation of
connection if ip address of remote service is not existing. I think
problem is in the situation when https connection is not created and
ConnectException is thrown after connection timeout. This exception is
catched in HttpMethodDirector.java in method executeWithRetry. Then
the DefaultHttpMethodRetryHandler is called to recognize whether
connection creation will be repeated or not.
I think, that special handling for ConnectException is missing in
retryMethod of DefaultHttpMethodRetryHandler, because exception is not
recognized and connetions are created again.
On the other hand, ConnectTimeoutException is thrown after connection
timeout for HTTP. This exception is handled in
DefaultHttpMethodRetryHandler and call is stopped.

These lines of code handle ConnectTimeoutException in retryMethod of
DefaultHttpMethodRetryHandler:
if (exception instanceof InterruptedIOException) {
            // Timeout
            return false;
        }

Probably this is missing for ConnectException:
if (exception instanceof InterruptedIOException || exception
instanceof ConnectException) {
            // Timeout
            return false;
        }

"
0,Add missing license headersThe RAT tool (http://code.google.com/p/arat/) points out a few files within Jackrabbit trunk that are currently missing the correct license header. We should fix those.
1,"JNDIDatabaseJournal doesn't work with ""oracle"" schema (or: unable to use OracleDatabaseJournal with a jndi datasource)Database journal works fine on oracle when using the OracleDatabaseJournal implementation; but when you need to use a jndi datasource you actually need to use org.apache.jackrabbit.core.journal.JNDIDatabaseJournal which doesn't work fine with the ""oracle"" schema.

With the following configuration:
<Cluster id=""node1"" syncDelay=""10"">
    <Journal class=""org.apache.jackrabbit.core.journal.JNDIDatabaseJournal"">
      <param name=""schema"" value=""oracle"" />

jackrabbit crashes at startup with a not well defined sql error. Investigating on the problem I see that the ""oracle.ddl"" file contains a ""tablespace"" variable that is replaced only by the OracleDatabaseJournal implementation.

As a workaround users can create a different ddl without a tablespace variable, but this should probably work better out of the box.

WDYT about one of the following solutions?
- make the base DatabaseJournal implementation support jndi datasource just like PersistenceManagers do (without a specific configuration property but specifying a jndi location in the url property)
- move the replacement of the tablespace variable (and maybe: add a generic replacement of *any* parameter found in the databaseJournal configuration) to the main DatabaseJournal implementation. This could be handy and it will make the OracleDatabaseJournal extension useless, but I see that at the moment there can be a problem with the MsSql implementation, since it adds ""on "" to the tablespace name only when it's not set to an empty string.





"
0,"remove DocsAndPositionsEnum.getPayloadLengthThis was an accidental leftover; now that getPayload returns a BytesRef, this method is not needed."
1,"leading wildcard's don't work with trailing wildcardAs reported by Antony Bowesman, leading wildcards don't work when there is a trailing wildcard character -- instead a PrefixQuery is constructed.


http://www.nabble.com/QueryParser-bug--tf3270956.html"
0,"customize handling of 302 redirectsI tried this with both the beta2 2.0 release, and the nightly build.  The
following code snippet describes what I am trying to do:

httpClient.getHostConfiguration().setHost(sHost, 80, ""http"");
HttpMethod method=null;
if (sMethod.indexOf(""POST"")!=-1) {
     method=new PostMethod(sURLInfo);
} else {
     method=new GetMethod(sURLInfo);
}
method.setFollowRedirects(true);
httpClient.executeMethod(method);

After this code executes, the ""getFollowRedirects"" method still returns false,
and any redirects which are sent by the webserver are not followed.  As a
temporary workaround, since I want all redirects followed, I commented out the
following code in the HttpMethodBase class in the ""processRedirectResponse"" method:

/*if (!getFollowRedirects()) {
     LOG.info(""Redirect requested but followRedirects is ""
     + ""disabled"");
     return false;
}*/

If this bug has already been reported, I apologize...I searched for and found
nothing related to this issue."
0,"Implement Connection TimeoutsI was writing test code to use the setSoTimeout(int millis) method to set a
timeout value when connecting to a URL.  It appears to me that no matter what I
set the timeout to be a HttpConnection will try to connect but uses some other
timeout value(I'm guessing the OS's default value).  I looked at the code for
HttpConnection and it uses the Socket(host,port) constructor which tries to
connect write away.  I'd like to suggest the following code below so the timeout
is set before first the connection is even made.

/* Compile the code as is and it should timeout within a sec.  If you
 * uncomment the first two lines after the try statement and comment
 * out the other socket connect statements and run the code again you will
 * notice write away that the timeout is something else because it connects
 * right away in the constructor.  Its like the timeout is worthless at this
 * point.  As a matter a fact the code should never get there.
 * This all assumes that 192.168.168.50 is not on your network.
 */

import java.io.*;
import java.net.*;

public class SocketTest {
    public static void main(String[] args) {
        long start = System.currentTimeMillis();

        try {
            //Socket socket = new Socket(""192.168.168.50"",80);
            //socket.setSoTimeout(1000);

            //Setting timeout before the connection is made.
            Socket socket = new Socket();
            InetSocketAddress sAddress =
                new InetSocketAddress(""192.168.168.50"",80);
            socket.connect(sAddress,1000);

        } catch (UnknownHostException e) {
            System.out.println(e);
        } catch (SocketException e) {
            System.out.println(e);
        } catch (IOException e) {
            System.out.println(e);
        }

        System.out.println(System.currentTimeMillis() - start);
    }
}"
0,"Changes.html formatting improvementsSome improvements to the Changes.html generated by the changes2html.pl script via the 'changes-to-html' ant task:

# Simplified the Simple stylesheet (removed monospace font specification) and made it the default.  The Fancy stylesheet is really hard for me to look at (yellow text on light blue background may provide high contrast with low eye strain, but IMHO it's ugly).
# Moved the monospace style from the Simple stylesheet to a new stylesheet named ""Fixed Width""
# Fixed syntax errors in the Fancy stylesheet, so that it displays as intended.
# Added <span style=""attrib"">  to change attributions.
# In the Fancy and Simple stylesheets, change attributions are colored dark green.
# Now properly handling change attributions in CHANGES.txt that have trailing periods.
# Clicking on an anchor to expand its children now changes the document location to show the children.
# Hovering over anchors now causes a tooltip to be displayed - either ""Click to expand"" or ""Click to collapse"" - the tooltip changes appropriately after a collapse or expansion."
1,"[PATCH] Loosing first matching document in BooleanQueryThis patch fixes loosing of first matching document when BooleanQuery
with BooleanClause.Occur.SHOULD is added to another BooleanQuery."
1,ChainedTermEnum omits initial termsThis is a regression caused by JCR-2393.
0,"TCK: NodeReadMethodsTest#testGetPrimaryItemItemNotFoundException selects wrong test dataMethod locateNodeWithoutPrimaryItem is used to locate recursively node which does not define a primary item, but this method calls internally locateNodeWithPrimaryItem instead of locateNodeWithoutPrimaryItem."
1,"primaryItemName is not inheritedif no primaryItemName is defined for a nodetype definition, it should be inherited from one of the supertypes. the spec is unclear about this, though it seems to be the natural behaviour.

for example when extending nt:resource, the subtype should not be force to redefine the jcr:data as primaryItemName.

"
0,"[PATCH] Decouple locking implementation from Directory implementationThis is a spinoff of http://issues.apache.org/jira/browse/LUCENE-305.

I've opened this new issue to capture that it's wider scope than
LUCENE-305.

This is a patch originally created by Jeff Patterson (see above link)
and then modified as described here:

  http://issues.apache.org/jira/browse/LUCENE-305#action_12418493

with some small additional changes:

  * For each FSDirectory.getDirectory(), I made a corresponding
    version that also accepts a LockFactory instance.  So, you can
    construct an FSDirectory with your own LockFactory.

  * Cascaded defaulting for FSDirectory's LockFactory implementation:
    if you pass in a LockFactory instance, it's used; else if
    setDisableLocks was called, we use NoLockFactory; else, if the
    system property ""org.apache.lucene.store.FSDirectoryLockFactoryClass""
    is defined, we use that; finally, we'll use the original locking
    implementation (SimpleFSLockFactory).

The gist is that all locking code has been moved out of *Directory and
into subclasses of a new abstract LockFactory class.  You can now set
the LockFactory of a Directory to change how it does locking.  For
example, you can create an FSDirectory but set its locking to
SingleInstanceLockFactory (if you know all writing/reading will take
place a single JVM).

The changes pass all unit tests (on Ubuntu Linux Sun Java 1.5 and
Windows XP Sun Java 1.4), and I added another TestCase to test the
LockFactory code.

Note that LockFactory defaults are not changed: FSDirectory defaults
to SimpleFSLockFactory and RAMDirectory defaults to
SingleInstanceLockFactory.

Next step (separate issue) is to create a LockFactory that uses the OS
native locks (through java.nio).
"
1,"Intermittent failure in TestIndexWriterMergePolicy.testMaxBufferedDocsChangeLast night's build failed from it: http://hudson.zones.apache.org/hudson/job/Lucene-trunk/1019/changes

Here's the exc:

{code}
    [junit] Testcase: testMaxBufferedDocsChange(org.apache.lucene.index.TestIndexWriterMergePolicy):	FAILED
    [junit] maxMergeDocs=2147483647; numSegments=11; upperBound=10; mergeFactor=10
    [junit] junit.framework.AssertionFailedError: maxMergeDocs=2147483647; numSegments=11; upperBound=10; mergeFactor=10
    [junit] 	at org.apache.lucene.index.TestIndexWriterMergePolicy.checkInvariants(TestIndexWriterMergePolicy.java:234)
    [junit] 	at org.apache.lucene.index.TestIndexWriterMergePolicy.testMaxBufferedDocsChange(TestIndexWriterMergePolicy.java:164)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:208)
{code}

Test doesn't fail if I run on opensolaris nor os X machines..."
0,"SystemSession#createSession should return SessionImpl againa long with the fix of  JCR-2890 (revision 1089436) the behavior of SystemSession#createSession has changed to
return a SystemSession instead of SessionImpl as it used to be.

while i basically consider this move to be correct and the better way of dealing with that session-cloning
mechanism as it prevents the user of this method to convert a SystemSession into a regular session
for extra writing operations (such as e.g. access control editing that is not supported with the
system session to prevent chicken-egg-problems on repo startup).

therefore i would like to revert that change for the 2.4 release in order to prevent regressions.

for the time after 2.4 i would however suggest that we finally take the time to clearly define the
usages, abilities and responsibilities of the system session and also review how and where we
expose them to the individual 'modules' of jackrabbit core..  i started working on this but decided
that this is definitely too risky for 2.4 whereas reverting the change mentioned above should
imo impose very limited risk as all usages of those sessions i am aware of use them as ""Session""
or ""SesssionImpl"", most of them not even having access to the SystemSession class."
0,Jackrabbit concurrency review and invariantsI've been working on reviewing and verifying the internal concurrency model of Jackrabbit in an attempt to proactively prevent the kinds of deadlock issues we've seen before. Overall things seem pretty good nowadays. I'll be updating the web site with some resulting design and review docs that should help guide future work in this area. I also have created some global invariant checks that I'll be adding to the codebase.
1,"Property.getValue() throws RepositoryException with internal errorRunning ConcurrentReadWriteTest (NUM_NODES=5, NUM_THREADS=3, RUN_NUM_SECONDS=120) resulted in a RepositoryException calling Property.getValue():

javax.jcr.RepositoryException: Internal error while retrieving value of b3fc1ea8-3364-4236-bcc7-dea0baf90640/{}test: null: null

Debugging shows that it is a NullPointerException:

java.lang.NullPointerException
	at org.apache.jackrabbit.core.PropertyImpl.getValue(PropertyImpl.java:481)
	at org.apache.jackrabbit.core.ConcurrentReadWriteTest$1$1.execute(ConcurrentReadWriteTest.java:68)
	at org.apache.jackrabbit.core.AbstractConcurrencyTest$Executor.run(AbstractConcurrencyTest.java:110)
	at java.lang.Thread.run(Thread.java:619)

It's probably the state which has been discarded after the sanityCheck()."
0,"Improve error messages for index aggregatesIn the case where an index aggregate fails because of a node that doesn't exist the logged warn messages contain a full stack-trace.
Besides the fact that this can be misleading (you may think that there's something wrong that you need to fix right away) it is also borderline useless.

The desired behavior would be to just log an ""info"" message mentioning that a certain node was skipped, similar to what the SeachManager does."
0,"Remove redundant RepositoryService.executeQuery() method There are currently two executeQuery() methods on RepositoryService. For simplicity we should remove the one that assumes default values for limit, offset and bind variable values."
0,"Add narrow API for loading stored fields, to replace FieldSelectorI think we should ""invert"" the FieldSelector API, with a ""push"" API
whereby FieldsReader invokes this API once per field in the document
being visited.

Implementations of the API can then do arbitrary things like save away
the field's size, load the field, clone the IndexInput for later lazy
loading, etc.

This very thin API would be a mirror image of the very thin index time
API we now have (IndexableField) and, importantly, it would have no
dependence on our ""user space"" Document/Field/FieldType impl, so apps
are free to do something totally custom.

After we have this, we should build the ""sugar"" API that rebuilds a
Document instance (ie IR.document(int docID)) on top of this new thin
API.  This'll also be a good test that the API is sufficient.

Relevant discussions from IRC this morning at
http://colabti.org/irclogger/irclogger_log/lucene-dev?date=2011-07-13#l76
"
0,"Introduce daily integration test suiteSome time ago we discussed integration tests that would be run on a daily basis. See also comments in issue JCR-1452. It seems we reached consensus that running a daily integration test suite is desirable.

Here's my proposal:

- Introduce a test suite org.apache.jackrabbit.core.integration.daily.DailyIntegrationTest which includes all tests that should be run on a daily basis.
- Configure our continuous integration system to run the test suite on a daily basis. e.g. mvn -Dtest=DailyIntegrationTest package

With this approach we don't need to introduce maven profiles or any other pom magic, yet it's easy for a developer to run the daily tests when needed."
0,IOContext should be part of the SegmentReader cache key Once IOContext (LUCENE-2793) is landed the IOContext should be part of the key used to cache that reader in the pool
0,EntryCollector may log warning for inexistent itemCurrently the EntryCollector may log a warning when the node reported in the event does not exist. This may happen when the repository runs in a cluster and a node is created and immediately removed again. This issue is related to JCR-3014. The call to Session.nodeExists() should actually return false when the identifier path cannot be resolved. Currently it throws a RepositoryException.
0,"Correct 2 minor javadoc mistakes in core, javadoc.access=privatePatches Token.java and TermVectorsReader.java"
0,"Lazy initialize ItemDefinitionThe item definition is currently set immediately when an ItemData is instantiated. Accessing nodes usually does not require reading the item definition, thus it is not necessary to load/set it that early.

Lazy initialization also has the benefit that content migration in an upgrade scenario becomes easier. Instead of throwing an exception early, jackrabbit could allow access to the item until an item definition is really required for the operation."
1,"JCR2SPI: remove node operation missing in submitted SPI batchIn JCR2SPI, the following sequence of operations seems to lead to an incorrect SPI batch being submitted:

1) remove ""/a""
2) add ""/a""
3) add ""/a/b""
4) session.save()

This seems to create an SPI batch where the first remove operation is missing.

Note that the problem only seems to occur when step 3 is part of the sequence.

Full Java source for test:

    try {
      if (session.getRepository().getDescriptor(Repository.LEVEL_2_SUPPORTED).equals(""true"")) {
        Node testnode;
        String name = ""delete-test"";
          
        Node root = session.getRootNode();
        
        // make sure it's there
        if (! root.hasNode(name)) {
          root.addNode(name, ""nt:folder"");
          session.save();
        }
        
        // now test remove/add in one batch
        if (root.hasNode(name)) {
          testnode = root.getNode(name);
          testnode.remove();
          // session.save(); // un-commenting this makes the test pass
        }
        
        testnode = root.addNode(name, ""nt:folder"");
        // add one child
        testnode.addNode(name, ""nt:folder""); // commenting this out makes the test pass
        
        session.save();
      }
    } finally {
      session.logout();
    }
    
    "
1,"Creating AccessControlEntryImpl from a base entry results in wrong restrictionsduring creation of a new AccessControlEntryImpl using a base entry the restrictions of the base entry are
not copied to the new instance."
0,"If tests fail, don't report about unclosed resourcesLuceneTestCase ensures in afterClass() if you closed all your directories, which in turn will check if you have closed any open files.

This is good, as a test will fail if we have resource leaks.

But if a test truly fails, this is just confusing, because its usually not going to make it to the part of its code where it would call .close()

So, if any tests fail, I think we should omit this check in afterClass()"
0,"JSR 283 Repository Descriptors- new methods returning Value objects (jcr2spi)
- check descriptor-report in jcr-server and corresponding handling on the client side
- etc.etc."
0,"Support system properties in ${...} vars in XML config filesThe variable replacement (${...}) in config files like repository.xml currently only allows for the special variables introduced by Jackrabbit, eg. ${wsp.name} or ${rep.home}. But it would be useful to support all java system properties here as it is some kind of a standard in Java XML config files (see Spring for an example).

This makes it easier to inject variables from outside the config file, eg. by setting them on the command line or injecting them programmatically in test cases. Typical parameters for that include database connection credentials, which one wants to avoid to put into repository.xml files that are often checked into SVN.

This is especially true for test cases, eg. I currently work on a persistence manager component and I want to include the repository.xml in the source tree (under applications/test) but without my specific credentials. These are applied by loading a user-specific properties file through the test case before the repository is started and the config is read.
"
0,"SPI-commons:  QValueTest.testDateValueEquality2 fails due to changes made with JCR-1018with the introduction of QValue.getCalendar() the internal value for DATE-properties is now a Calendar (was
String). however, the equals() method has not been adjusted."
0,"Add optional packing to FST buildingThe FSTs produced by Builder can be further shrunk if you are willing
to spend highish transient RAM to do so... our Builder today tries
hard not to use much RAM (and has options to tweak down the RAM usage,
in exchange for somewhat lager FST), even when building immense FSTs.

But for apps that can afford highish transient RAM to get a smaller
net FST, I think we should offer packing.
"
0,"TestConstantScoreRangeQuery does not compile with ecjTestConstantScoreRangeQuery has an assertEquals(String, Float, Float)
but most of the calls to assertEquals are (String, int, int).

ecj complains with the following error:
The method assertEquals(String, float, float) is ambiguous for the type TestConstantScoreRangeQuery

The simple solution is to supply an assertEquals(String, int, int) which calls Assert.assertEquals(String, int, int)

Patch to follow.
"
1,"recovery tool does not recover when version history can be instantiated, but root version can notJCR-2551 introduced a recovery mode which tries to instantiate the version history, and if this fails, disconnects the VH (version history) and makes the node unversioned.

However, it appears it can happen that the persistence is damaged such as getting the VH does indeed work, but subsequent operations fail due to other problems. One problem that has been seen is a missing frozenNode property of the root version (or a missing frozenNode itself).

As a quick fix, we may want to change the checker so that it actually also tries to get the rootVersion and it's frozenNode. Long term, depending on how frequent this problem is, we may have to think about a less drastic recovery than disconnecting the VH."
0,"wire logger skips empty lineWhen logging with 
org.apache.commons.logging.simplelog.log.httpclient.wire=debug, HttpConnection 
skips one line of server output in logs -- CRLF line between headers and body."
0,EnwikiQueryMaker
0,Data store garbage collection: log deleted files and total sizeThe data store garbage collection should list the names and the total size of all deleted files.
0,"NRTCachingDirectory, to buffer small segments in a RAMDirI created this simply Directory impl, whose goal is reduce IO
contention in a frequent reopen NRT use case.

The idea is, when reopening quickly, but not indexing that much
content, you wind up with many small files created with time, that can
possibly stress the IO system eg if merges, searching are also
fighting for IO.

So, NRTCachingDirectory puts these newly created files into a RAMDir,
and only when they are merged into a too-large segment, does it then
write-through to the real (delegate) directory.

This lets you spend some RAM to reduce I0.
"
1,"DOMException: NAMESPACE_ERR thrown when exporting document viewWhen I try to export some nodes with ExportDocumentView I get a DOMException with Jackrabbit 1.5.2. Version 1.4.6 works fine. Xerces version was 2.8.1.

Code:

Document document = documentBuilder.newDocument();
Element exportElement = (Element) document.appendChild(document.createElement(""Export""));
Result result = new DOMResult(exportElement);
TransformerHandler transformerHandler = saxTransformerFactory.newTransformerHandler();
transformerHandler.setResult(result);
session.exportDocumentView(workflowNode.getPath(), transformerHandler, true, false);

Exception:

org.w3c.dom.DOMException: NAMESPACE_ERR: An attempt is made to create or change an object in a way which is incorrect with regard to namespaces.
	at org.apache.xerces.dom.CoreDocumentImpl.checkDOMNSErr(Unknown Source)
	at org.apache.xerces.dom.AttrNSImpl.setName(Unknown Source)
	at org.apache.xerces.dom.AttrNSImpl.<init>(Unknown Source)
	at org.apache.xerces.dom.CoreDocumentImpl.createAttributeNS(Unknown Source)
	at org.apache.xerces.dom.ElementImpl.setAttributeNS(Unknown Source)
	at com.sun.org.apache.xalan.internal.xsltc.trax.SAX2DOM.startElement(SAX2DOM.java:194)
	at com.sun.org.apache.xml.internal.serializer.ToXMLSAXHandler.closeStartTag(ToXMLSAXHandler.java:204)
	at com.sun.org.apache.xml.internal.serializer.ToSAXHandler.flushPending(ToSAXHandler.java:277)
	at com.sun.org.apache.xml.internal.serializer.ToXMLSAXHandler.startElement(ToXMLSAXHandler.java:646)
	at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerHandlerImpl.startElement(TransformerHandlerImpl.java:263)
	at org.apache.jackrabbit.commons.xml.Exporter.startElement(Exporter.java:438)
	at org.apache.jackrabbit.commons.xml.DocumentViewExporter.exportNode(DocumentViewExporter.java:76)
	at org.apache.jackrabbit.commons.xml.Exporter.exportNode(Exporter.java:298)
	at org.apache.jackrabbit.commons.xml.Exporter.exportNodes(Exporter.java:214)
	at org.apache.jackrabbit.commons.xml.DocumentViewExporter.exportNode(DocumentViewExporter.java:77)
	at org.apache.jackrabbit.commons.xml.Exporter.exportNode(Exporter.java:295)
	at org.apache.jackrabbit.commons.xml.Exporter.export(Exporter.java:144)
	at org.apache.jackrabbit.commons.AbstractSession.export(AbstractSession.java:461)
	at org.apache.jackrabbit.commons.AbstractSession.exportDocumentView(AbstractSession.java:241)"
0,"remove support for event bundle IDsEvent bundle IDs currently are not used. We can re-add them later in case we need them.
"
0,"QueryParser throws new exceptions even if custom parsing logic threw a better oneWe have subclassed QueryParser and have various custom fields.  When these fields contain invalid values, we throw a subclass of ParseException which has a more useful message (and also a localised message.)

Problem is, Lucene's QueryParser is doing this:

{code}
    catch (ParseException tme) {
        // rethrow to include the original query:
        throw new ParseException(""Cannot parse '"" +query+ ""': "" + tme.getMessage());
    }
{code}

Thus, our nice and useful ParseException is thrown away, replaced by one with no information about what's actually wrong with the query (it does append getMessage() but that isn't localised.  And it also throws away the underlying cause for the exception.)

I am about to patch our copy to simply remove these four lines; the caller knows what the query string was (they have to have a copy of it because they are passing it in!) so having it in the error message itself is not useful.  Furthermore, when the query string is very big, what the user wants to know is not that the whole query was bad, but which part of it was bad.

"
0,"remove TermVectorsWriter (it's no longer used)We should remove TermVectorsWriter: it's no longer used now that
DocumentsWriter writes the term vectors directly to the index."
0,"FieldCacheImpl's getCacheEntries() is buggy as it uses WeakHashMap incorrectly and leads to ConcurrentModExceptionsThe way how WeakHashMap works internally leads to the fact that it is not allowed to iterate over a WHM.keySet() and then get() the value. As each get() operation inspects the ReferenceQueue of the weak keys, they may suddenly disappear. If you use the entrySet() iterator you get key and value and no need to call get(), contains(),... that inspects the ReferenceQueue."
0,"contrib/benchmark - few improvements and a bug fixBenchmark byTask was slightly improved:

1. fixed a bug in the ""child-should-not-report"" mechanism. If a task sequence contained only simple tasks it worked as expected (i.e. child tasks did not report times/memory) but if a child was a task sequence, then its children would report - they should not - this was fixed, so this property is now ""penetrating/inherited"" all the way down.

2. doc size control now possible also for the Reuters doc maker. (allowing to index N docs of size C characters each.)

3. TrecDocMaker was added - it reads as input the .gz files used in Trec - e.g. .gov data - this can be handy to benchmark Lucene on these large collections.  Similar to the Reuters collection, the doc-maker scans the input directory for all the files and extracts documents from the files.  Here there are multiple documents in each input file. Unlike the Reuters collection, we cannot provide a 'loader' for these collections - they are available from http://trec.nist.gov - for research purposes.

4. a new BasicDocMaker abstract class handles most of doc-maker tasks, including creating docs with specific size, so adding new doc-makers for other data is now much simpler."
0,"make similarities/term/collectionstats take long (for > 2B docs)As noted by Yonik and Andrzej on SOLR-1632, this would be useful for distributed scoring.

we can also add a sugar method add() to both of these to make it easier to sum."
1,"Clustering: race condition may cause duplicate entries in search indexThere seems to be a race condition that may cause duplicate search index entries. It is reproducible as follows (Jackrabbit 1.3):
1) Start clusternode 1 that just adds a single node of node type clustering:test.
2) Shutdown clusternode 1.
3) Start clusternode 2 with an empty search index.
4) Execute the query  //element(*, clustering:test).
4) Print the result of the query (UUIDs of nodes in the result set).

When I just run clusternode 2, then there is one node in the resultset, as expected. However, when I debug clusternode 2 and have a breakpoint (i.e., a pause of a few seconds at line 306 of RepositoryImpl.java - just before the clusternode is started), then the resultset contains two results, both with the same UUID.
"
0,"Unreferenced VersionHistory should be deleted automatically.since the creation of a VersionHistory is triggered by the creation of a mix:versionable node, the removal should happen automatically, as soon as no references to that version histroy exist anymore. this is the case, when all mix:versionable nodes (in all workspaces) belonging to that VH are deleted, and all the versions in the VH are removed i.e. only the jcr:rootVersion is left. At this point, the VH should be deleted aswell."
0,"avoid converting property values to stringsQValues currently can not expose properties of types LONG and DOUBLE in a parsed format. Thus, setting/retrieving properties of these types requires roundtripping through Strings, which we should avoid.

Proposal:

1) Add ""long getLong()"" and ""double getDouble()"" to QValue.

2) Add matching create methods to QValueFactory.

3) Take advantage of the new methods in JCR2SPI, for instance by allowing it's own Value implementation to internally just hold the QValue.

"
0,"Merge UUID to NodeIdThe current NodeId class is mostly just a wrapper around UUID, which causes two objects to be instantiated for each node identifier that the system uses. The memory and processing overhead is quite small, but given that there are tons of NodeId instances it would be good to eliminate that overhead.

There is also lots of code that just converts UUIDs to NodeIds and vice versa. We could simplify such code if we just used NodeId everywhere.

Also, we might want to open up the possibility of using non-UUID node identifiers at some point in future, so it would make a lot of sense to remove the NodeId.getUUID method and rely directly on NodeId and it's equals(), hashCode(), and toString() methods in many places where we currently use UUIDs."
0,"Change default write lock file location to index directory (not java.io.tmpdir)Now that readers are read-only, we no longer need to store lock files
in a different global lock directory than the index directory.  This
has been a source of confusion and caused problems to users in the
past.

Furthermore, once the write lock is stored in the index directory, it
no longer needs the big digest prefix that was previously required
to make sure lock files in the global lock directory, from different
indexes, did not conflict.

This way, all files related to an index will appear in a single
directory.  And you can easily list that directory to see if a
""write.lock"" is present to check whether a writer is open on the
index.

Note that this change just affects how FSDirectory creates its default
lockFactory if no lockFactory was specified.  It is still possible
(just no longer the default) to pick a different directory to store
your lock files by pre-instantiating your own LockFactory.

As part of this I would like to remove LOCK_DIR and the no-argument
constructor, in SimpleFSLockFactory and NativeFSLockFactory.  I don't
think we should have the notion of a global default lock directory
anymore.  This is actually an API change.  However, neither
SimpleFSLockFactory nor NativeFSLockFactory haver been released yet,
so I think this API removal is allowed?

Finally I want to deprecate (but not yet remove, because this has been
in the API for many releases) the static LOCK_DIR that's in
FSDirectory.  But it's now entirely unused.

See here for discussion leading to this:

  http://www.gossamer-threads.com/lists/lucene/java-dev/43940
"
1,"ParallelReader crashes when trying to merge into a new indexParallelReader causes a NullPointerException in
org.apache.lucene.index.ParallelReader$ParallelTermPositions.seek(ParallelReader.java:318)
when trying to merge into a new index.

See test case and sample output:

$ svn diff
Index: src/test/org/apache/lucene/index/TestParallelReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestParallelReader.java    (revision 179785)
+++ src/test/org/apache/lucene/index/TestParallelReader.java    (working copy)
@@ -57,6 +57,13 @@
 
   }
  
+  public void testMerge() throws Exception {
+    Directory dir = new RAMDirectory();
+    IndexWriter w = new IndexWriter(dir, new StandardAnalyzer(), true);
+    w.addIndexes(new IndexReader[] { ((IndexSearcher)
parallel).getIndexReader() });
+    w.close();
+  }
+
   private void queryTest(Query query) throws IOException {
     Hits parallelHits = parallel.search(query);
     Hits singleHits = single.search(query);
$ ant -Dtestcase=TestParallelReader test
Buildfile: build.xml
[...]
test:
    [mkdir] Created dir:
/Users/skirsch/text/lectures/da/thirdparty/lucene-trunk/build/test
    [junit] Testsuite: org.apache.lucene.index.TestParallelReader
    [junit] Tests run: 2, Failures: 0, Errors: 1, Time elapsed: 1.993 sec

    [junit] Testcase: testMerge(org.apache.lucene.index.TestParallelReader):  
Caused an ERROR
    [junit] null
    [junit] java.lang.NullPointerException
    [junit]     at
org.apache.lucene.index.ParallelReader$ParallelTermPositions.seek(ParallelReader.java:318)
    [junit]     at
org.apache.lucene.index.ParallelReader$ParallelTermDocs.seek(ParallelReader.java:294)
    [junit]     at
org.apache.lucene.index.SegmentMerger.appendPostings(SegmentMerger.java:325)
    [junit]     at
org.apache.lucene.index.SegmentMerger.mergeTermInfo(SegmentMerger.java:296)
    [junit]     at
org.apache.lucene.index.SegmentMerger.mergeTermInfos(SegmentMerger.java:270)
    [junit]     at
org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:234)
    [junit]     at
org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:96)
    [junit]     at
org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:596)
    [junit]     at
org.apache.lucene.index.TestParallelReader.testMerge(TestParallelReader.java:63)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)


    [junit] Test org.apache.lucene.index.TestParallelReader FAILED

BUILD FAILED
/Users/skirsch/text/lectures/da/thirdparty/lucene-trunk/common-build.xml:188:
Tests failed!

Total time: 16 seconds
$"
0,"Invert IR.getDelDocs -> IR.getLiveDocsSpinoff from LUCENE-1536, where we need to fix the low level filtering
we do for deleted docs to ""match"" Filters (ie, a set bit means the doc
is accepted) so that filters can be pushed all the way down to the
enums when possible/appropriate.

This change also inverts the meaning first arg to
TermsEnum.docs/AndPositions (renames from skipDocs to liveDocs).
"
0,"Handle Null Arguments consistantlyConsider throwing a NullPointerException or InvalidArgumentException for null
argument when they are not allowed.  Be consistant and document behaviour."
0,"sort missing string fields lastA SortComparatorSource for string fields that orders documents with the sort
field missing after documents with the field.  This is the reverse of the
default Lucene implementation.

The concept and first-pass implementation was done by Chris Hostetter."
1,checkindex fails if docfreq >= skipInterval and term is indexed more than once at same positionThis is a bad check in the skipping verification logic
0,"Better integration of the TestWebApp-HowTo into the documentationThe excellent webapp howto written by Olegolas needs to be integrated better
into httpclient documentation.  Currently it is in the docs directory as a html
file, but it would be better if it was in the xdocs directory as an xml file."
0,"Handle Returning Null consistantlyConsider returning empty arrays instead of null consistantly.  eg:
getResponseBody().  There may be good reason for both null and empty array
depending on the circumstannces."
0,"Allow customizing/subclassing of DirectoryReaderDirectoryReader is final and has only static factory methods. It is not possible to subclass it in any way.

The problem is mainly Solr, as Solr accesses directory(), IndexCommits,... and therefore cannot work on abstract IndexReader anymore. This should be changed, by e.g. handling reopening in the IRFactory, also versions, commits,... Currently its not possible to implement any other IRFactory that returns something else.

On the other hand, it should be possible to ""wrap"" a DirectoryReader / CompositeReader to handle filtering of collection based information (subreaders, reopening hooks,...). This can be done by making DirectoryReader abstract and let DirectoryReader.open return a internal hidden class ""StandardDirectoryReader"". This is similar to the relatinship between IndexReader and hidden DirectoryReader in the past.

DirectoryReader will have final implementations of most methods like getting document stored fields, global docFreq and other statistics, but allows hooking into doOpenIfChanged. Also it should not be limited to SegmentReaders as childs - any AtomicReader is fine. This allows users to create e.g. a Directory-based ParallelReader (see LUCENE-3736) that supports reopen and (partially commits)."
0,"Lower log level in o.a.j.jcr2spi.query.NodeIteratorImplNodeIteratorImpl.fetchNext() logs an error when it cannot load a node and skips that node. Since this is not an error condition (the node could have been deleted by another session), logging should occur at the warn level."
1,"TransientFileFactory may throw ConcurrentModificationException on shutdownWhen Jackrabbit is stopped the shutdown hook of the TransientFileFactory iterates over all tracked temp files and deletes them. At the same time the reaper thread may still remove file references from the list of tracked temp files. This may lead to a ConcurrentModificationException in the shutdown hook:

java.util.ConcurrentModificationException
	at java.util.AbstractList$Itr.checkForComodification(Unknown Source)
	at java.util.AbstractList$Itr.next(Unknown Source)
	at org.apache.jackrabbit.util.TransientFileFactory$1.run(TransientFileFactory.java:86)
"
0,"In IndexSearcher class, make subReader and docCount arrays protected so sub classes can access themPlease make these two member variables protected so subclasses can access them, e.g.:

  protected IndexReader[] subReaders;
  protected int[] docStarts;

Thanks"
1,"JCR2SPI: Workspace.getImportHandler creates a handler which doesn't work properly under JDK 1.4.JCR2SPI returns an import handler which delegates work to a SAXTransformerHandler. In JDK, that one has a known issue not processing namespace prefix mappings properly (will attach a separate test case).

Proposals:

- drop JDK 1.4 support
- tune the JCR2SPI handler to create namespace attributes when needed
- use an entirely different serializer

My personal preference would be just to drop JDK 1.4 support, but that may not be acceptable for everyone.
"
0,"tests should run checkIndex on indexes they createI think we should add a boolean checkIndexesOnClose (default=true) to MockDirectoryWrapper.

Only a very few tests need to disable this.
"
0,"Remove GData from trunk GData doesn't seem to be maintained anymore. We're going to remove it before we cut the 2.3 release unless there are negative votes.

In case someones jumps in in the future and starts to maintain it, we can re-add it to the trunk.

If anyone is using GData and needs it to be in 2.3 please let us know soon!"
0,"Eliminate class HostConfigurationRemove the target host attribute from the HostConfiguration class. This will allow one HostConfiguration object to be used for different targets.
The problem is that currently MultiThreadedHttpConnectionManager uses HostConfiguration objects as cache keys, which needs to be changed.

This is a followup to HTTPCLIENT-615.

cheers,
  Roland
"
0,"Fix pulsingcodec to reuse its enumsPulsingCodec currently doesnt always reuse its enums, which could lead to behavior like LUCENE-3515.

The problem is sometimes it returns the 'wrapped' enum, but other times it returns its 'pulsingenum' depending upon
whether terms are pulsed...

we can use the fact that these enums allow attributes to keep the reuse information for both so it can reuse when stepping through terms.
"
0,Add option to make sorting in user/group query case insensitiveSorting on string properties is currently case sensitive in the user/group search. There should be a way to specify whether sorting shuld be case (in)sensitive.
0,"Improvement to UndefinedTypeConverterImpl to map super types effectivelyImprovement to org.apache.jackrabbit.ocm.manager.atomictypeconverter.impl.UndefinedTypeConverterImpl's implementation of 
public Value getValue(ValueFactory valueFactory, Object propValue) , used equality check of class names to decide whether Object propValue is worthy of any attempt to map to an apropriate property.  Since the purpose of the class is to provide a 'best effort' attempt to map an Object of type java.lang.Object it will be better to use 'instanceof'.  This approach will convert the specific class as well as any inherited objects.  For example using instanceof will let us map a BufferedInputStream, and any other sub classes of InputStream to a Binary Property."
1,"Weird BooleanQuery behaviorHere's a simple OR-connected query.

T:files T:deleting C:thanks C:exists

The query above hits 1 document. But following *same* query only
with parenthesis results nothing.

(T:files T:deleting) (C:thanks C:exists)

Another combinations of MUST and SHOULD.

""T:files T:deleting +C:production +C:optimize"" hits 1 document.
""(T:files T:deleting) (+C:production +C:optimize)"" hits 1 document."
0,"ConstantScoreQuery should directly support wrapping Query and simply strip off scoresEspecially in MultiTermQuery rewrite modes we often simply need to strip off scores from Queries and make them constant score. Currently the code to do this looks quite ugly: new ConstantScoreQuery(new QueryWrapperFilter(query))

As the name says, QueryWrapperFilter should make any other Query constant score, so why does it not take a Query as ctor param? This question was aldso asked quite often by my customers and is simply correct, if you think about it.

Looking closer into the code, it is clear that this would also speed up MTQs:
- One additional wrapping and method calls can be removed
- Maybe we can even deprecate QueryWrapperFilter in 3.1 now (it's now only used in tests and the use-case for this class is not really available) and LUCENE-2831 does not need the stupid hack to make Simon's assertions pass
- CSQ now supports out-of-order scoring and topLevel scoring, so a CSQ on top-level now directly feeds the Collector. For that a small trick is used: The score(Collector) calls are directly delegated and the scores are stripped by wrapping the setScorer() method in Collector

During that I found a visibility bug in Scorer (LUCENE-2839): The method ""boolean score(Collector collector, int max, int firstDocID)"" should be public not protected, as its not solely intended to be overridden by subclasses and is called from other classes, too! This leads to no compiler bugs as the other classes that calls it is mainly BooleanScorer(2) and thats in same package, but visibility is wrong. I will open an issue for that and fix it at least in trunk where we have no backwards-requirement."
0,"Set default precisionStep for NumericField and NumericRangeFilterThis is a spinoff from LUCENE-1701.

A user using Numeric* should not need to understand what's
""under the hood"" in order to do their indexing & searching.

They should be able to simply:
{code}
doc.add(new NumericField(""price"", 15.50);
{code}

And have a decent default precisionStep selected for them.

Actually, if we add ctors to NumericField for each of the supported
types (so the above code works), we can set the default per-type.  I
think we should do that?

4 for int and 6 for long was proposed as good defaults.

The default need not be ""perfect"", as advanced users can always
optimize their precisionStep, and for users experiencing slow
RangeQuery performance, NumericRangeQuery with any of the defaults we
are discussing will be much faster.
"
0,"Extend the consistency check in BundleDbPersistenceManager's to fix child-parent relationsIt could happen that a child node is not in the ChildNodeEntries of its parent node.
You will get something like (javax.jcr.ItemNotFoundException: failed to build path of node1: parentNode has no child entry for node1) if you try to retrieve the path from node1.
We should handle such cases and fix it on consistency check"
0,"jcr2spi: use jcr names and path for log and exception messagein a couple of places jcr2spi adds the string representation of Path, Name and ItemId to the exception/error message. it would be convenient to convert them to jcr names and jcr path where ever possible."
0,"Add more unit on collection fieldscollection fields based on List are  supported not yet tested correctly.
Check if other kind collection are well tested"
1,CharsRef#append broken on trunk & 3.xCurrent impl. for append on CharsRef is broken - it overrides the actual content rather than append. its used in many places especially in solr so we might have some broken 
1,"DefaultRedirectHandler does not access correct HttpParamsIn the getLocationURI(HttpResponse, HttpContext) method, the HttpParams for determining REJECT_RELATIVE_REDIRECT and ALLOW_CIRCULAR_REDIRECTS are retrieved with:

HttpParams params = response.getParams();

The response HttpParams do not contain these values, however the request HttpParams do. The correct implementation is:

HttpRequest request = (HttpRequest) context.getAttribute(HttpExecutionContext.HTTP_REQUEST);
HttpParams params = request.getParams();

"
1,"jackrabbit-server.war is missing the slf4j-log4j12 libraryReported by Martin Perez:

But I found a bug on the .war file. It is missing the slf4j-log4j12-1.0.jar. It's in someway tricky to detect it because if you do not include it a ClassNotFoundException will be thrown but poiting to the JCR class with the log statement. Anyways, if you include the .jar file on the WEB-INF/lib directory, then the exception goes to exception's hell.

"
0,"Internal Timeout Handling in the TransactionContext is not XA Spec. conformThe problem here is that in a 2 phase transaction the xa spec does not  
permit a RB* return code in response to xa_commit().  The xa spec says  
the following about RB* return codes in the xa_commit() section:        
                                                                        
""The resource manager did not commit the work done on behalf of the     
transaction branch.  Upon return, the resource manager has rolled back  
the branch?s work and has released all held resources.  These values may
be returned only if TMONEPHASE is set in flags""                         
                                                                        
Essentially, the only two return codes from xa_commit that J2EE Containers can     
handle sensibly are XA_OK (normal case) and XA_RMFAIL.  RMFAIL will     
cause the containers to retry to commit the  transaction.  Any other return code will result in a heuristic          
transaction outcome (non-atomic).  

In a xa environment the TMONEPHASE is not set on the flags and so XA_RBTIMEOUT is 
not a permitted return code. A Container  transaction service cannot do anything to ensure an atomic     
outcome if an XAResource fails to honour its promise to be able to commit it made when it answer XA_OK in response to xa_prepare(). 

The internal timeout handling will rollback the Jackrabbit XAResource if the time exceeds between prepare and commit.
and in the commit Method will always throw a XA_RBTIMEOUT.

We should not handle the timeout internal because this should make the container in a 2 Phase transaction."
0,"Auto method retrial brokenFolks,
While working on the exception handling guide for the 3.0-alpha2 release I
stumbled upon a problem with HttpTimeoutException and its subclasses. In 3.0a1
HttpTimeoutException subclasses HttpRecoverableException which causes HTTP
methods failed due to a connect or socket timeout to be automatically retried. 

[INFO] HttpMethodDirector - -Recoverable exception caught when processing request
[INFO] HttpMethodDirector - -Recoverable exception caught when processing request
[INFO] HttpMethodDirector - -Recoverable exception caught when processing request
[INFO] HttpMethodDirector - -Recoverable exception caught when processing request
[WARN] HttpMethodDirector - -Recoverable exception caught but
MethodRetryHandler.retryMethod() returned false, rethrowing exception
org.apache.commons.httpclient.IOTimeoutException: Read timed out
	at
org.apache.commons.httpclient.HttpConnection$WrappedInputStream.handleException(HttpConnection.java:1350)
	at
org.apache.commons.httpclient.HttpConnection$WrappedInputStream.read(HttpConnection.java:1360)
	at java.io.FilterInputStream.read(FilterInputStream.java:66)
	at java.io.PushbackInputStream.read(PushbackInputStream.java:120)
	at org.apache.commons.httpclient.HttpParser.readRawLine(HttpParser.java:76)
	at org.apache.commons.httpclient.HttpParser.readLine(HttpParser.java:104)
	at org.apache.commons.httpclient.HttpConnection.readLine(HttpConnection.java:1054)
	at
org.apache.commons.httpclient.HttpMethodBase.readStatusLine(HttpMethodBase.java:1785)
	at
org.apache.commons.httpclient.HttpMethodBase.readResponse(HttpMethodBase.java:1546)
	at org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java:977)
	at
org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:383)
	at
org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:164)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:437)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:324)
	at Test.main(Test.java:13)
Caused by: java.net.SocketTimeoutException: Read timed out
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.read(SocketInputStream.java:129)
	at java.net.SocketInputStream.read(SocketInputStream.java:182)
	at
org.apache.commons.httpclient.HttpConnection$WrappedInputStream.read(HttpConnection.java:1358)
	... 13 more
Exception in thread ""main"" 

This probably is not what we want. Besides, for non-idempotent methods this may
simply be fatal and result in all sorts of unpleasant side-effects.

One possibilty that I personally favour is to make HttpTimeoutException class
extend IOException instead of HttpRecoverableException. There are others. The
question is whether timeout exceptions should be considered recoverable from the
conseptual standpoint. What do you think?

Oleg"
0," RAMDirectory(Directory dir, boolean closeDir)  constructor uses memory inefficiently.recently I found that  RAMDirectory(Directory dir, boolean closeDir)  constructor uses memory inefficiently.
files from source index are read entirely intro memory as single byte array which is after all is thrown away. And if I want to load my 200M optimized, compound format index to memory for faster search I should give JVM at least 400Mb memory limit. For larger indexes this can be an issue.

I've attached patch how to solve this problem."
1,"StandardQueryParser ignores AND operator for tokenized query termsThe standard query parser uses the default query operator for query clauses that are created from tokenization in the query parser instead of the actual operator for the source term.

here is an example:
{code}
StandardQueryParser parser = new StandardQueryParser(new StandardAnalyzer(Version.LUCENE_34));
parser.setDefaultOperator(Operator.OR);
System.out.println(((BooleanQuery)parser.parse(""_deleted:true AND title:"", ""f"")));
{code}

this should yield:
+_deleted:true +(title: title:)

as our former core query parser does but actually yields:
+_deleted:true title: title:

seems like a bug to me, looking at the tests seems we don't test for this kind of queries in the standard query parser tests too.
"
1,"SearchIndex class contains garbled StringSomehow during the switch to SL4J also a String literal in the SearchIndex class got garbled.

See:
http://svn.apache.org/viewcvs.cgi/incubator/jackrabbit/trunk/jackrabbit/src/main/java/org/apache/jackrabbit/core/query/lucene/SearchIndex.java?rev=385280&r1=378221&r2=385280

Since this is a low risk change I would like to get this included into the 1.0 branch."
0,"Extend apache parent pom for Apache wide configurationApache wide config is published in the apache parent pom, please use"
0,"UserManagement: membership cache default size too smallThe membership cache that has been introduced in JCR-2703 is making use of an LRUMap to cache group memberships (authorizable nodeId -> group nodeIds). In environments where users belong to more than 100 groups, the cache quickly becomes ineffective due to the default maximum size of the LRUMap.

Once the cache limit is hit, the rather expensive Node#getWeakReferences API calls resulting in search queries are executed again, leading to quite noticeable performance drops. Thus I'd suggest to either make the membership cache configurable or introduce some logic to let the cache grow dynamically as needed (still having some kind of hard limit to avoid memory issues)."
1,"Concurrent locking operations failI prepared simple test which tries to lock/unlock single node from many
threads. I expected only LockExceptions thrown by some threads which can
occur if node is already locked by other thread.

But I get incorrect effect sporadically. It looks like some thread
managed to acquire lock, but then can't release it.

Following exception is thrown then :

javax.jcr.InvalidItemStateException:
7c198c7b-76c8-47c8-96a8-d9dfefd4b387 has been modified externally
    at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1193)
    at org.apache.jackrabbit.core.NodeImpl.unlock(NodeImpl.java:3790)
    at JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:95)

additionally warning appears in log

org.apache.jackrabbit.core.lock.LockManagerImpl$LockInfo.loggingOut(LockManagerImpl.java:892)
- Unable to unlock session-scoped lock on node
'7c198c7b-76c8-47c8-96a8-d9dfefd4b387-W': Unable to unlock node. Node
has pending changes: /folder

In consequence node is left in locked state. It looks like a bug.
If one thread locked node successfully, then none other can modify it,
and the same thread should release lock without any problems.

Shouldn't be lock operation atomic itself ?

Przemo


package com.oyster.mom.contentserver.jcr.transaction;

import javax.jcr.Node;
import javax.jcr.RepositoryException;
import javax.jcr.Session;
import javax.jcr.SimpleCredentials;
import javax.jcr.lock.LockException;

import org.apache.jackrabbit.core.RepositoryImpl;
import org.apache.jackrabbit.core.config.RepositoryConfig;

public class JrTestConcurrentLocks extends Thread {

   private static final org.apache.commons.logging.Log log = org.apache.commons.logging.LogFactory.getLog(JrTestConcurrentLocks.class);

   public static String REPOSITORY_HOME = ""d:/repo/jackrabbit/"";

   public static String REPOSITORY_CONFIG = REPOSITORY_HOME + ""repository.xml"";

   public static void main(String[] args) throws Exception {

       JrTestConcurrentLocks test = new JrTestConcurrentLocks(-1);
       test.startup();

       JrTestConcurrentLocks tests[] = new JrTestConcurrentLocks[3];

       for (int i = 0; i < tests.length; i++) {
           JrTestConcurrentLocks x = new JrTestConcurrentLocks(i);
           x.setSession(repository.login(new SimpleCredentials(""admin"", ""admin"".toCharArray())));
           x.start();
           tests[i] = x;
       }

       for (int i = 0; i < tests.length; i++) {
           tests[i].join();
           tests[i].getSession().logout();
       }

       test.shutdown();
   }

   private static RepositoryImpl repository;

   private int id;

   private Session session;

   public void setSession(Session session) {
       this.session = session;
   }

   public Session getSession() {
       return this.session;
   }

   public JrTestConcurrentLocks(int i) {
       this.id = i;
   }

   public void startup() throws Exception {
       System.setProperty(""java.security.auth.login.config"", ""c:/jaas.config"");

       RepositoryConfig config = RepositoryConfig.create(REPOSITORY_CONFIG, REPOSITORY_HOME);
       repository = RepositoryImpl.create(config);

       Session session = repository.login(new SimpleCredentials(""admin"", ""admin"".toCharArray()));
       Node rootNode = session.getRootNode();
       if (!rootNode.hasNode(""folder"")) {
           Node folder = rootNode.addNode(""folder"");
           folder.addMixin(""mix:versionable"");
           folder.addMixin(""mix:lockable"");
           rootNode.save();
       }
       session.logout();
   }

   public void shutdown() throws RepositoryException {
       repository.shutdown();
   }

   public Node getFolder(Session session) throws RepositoryException {
       return session.getRootNode().getNode(""folder"");
   }

   public void run() {

       for (int i = 0; i < 10; i++) {
           log.info(""START id:"" + id + "", i="" + i);

           try {
               session.refresh(false);

               Node folder = getFolder(session);
               folder.lock(false, true);
               folder.unlock();

               log.info(""SUCCESS id:"" + id + "", i="" + i);
           }
           catch (LockException e) {
               log.info(""FAIL:"" + id + "", i="" + i);
           }
           catch (Exception e) {
               log.warn(""ERROR:"" + id + "", i="" + i, e);
           }


       }

   }
}


15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=0
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=0
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=0
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=0
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=1
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=1
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=0
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=1
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=1
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=2
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=2
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=3
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=3
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=4
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=2
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=2
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=3
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=3
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=4
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=4
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=5
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:97) - SUCCESS id:1, i=4
15:46:17 WARN  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:103) - ERROR:0, i=5
javax.jcr.ItemNotFoundException: 7c198c7b-76c8-47c8-96a8-d9dfefd4b387/{http://www.jcp.org/jcr/1.0}lockOwner
       at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:463)
       at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:319)
       at org.apache.jackrabbit.core.NodeImpl.getProperty(NodeImpl.java:1436)
       at org.apache.jackrabbit.core.NodeImpl.getOrCreateProperty(NodeImpl.java:428)
       at org.apache.jackrabbit.core.NodeImpl.internalSetProperty(NodeImpl.java:1267)
       at org.apache.jackrabbit.core.NodeImpl.lock(NodeImpl.java:3740)
       at JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:94)
15:46:17 WARN  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:103) - ERROR:2, i=0
javax.jcr.InvalidItemStateException: 7c198c7b-76c8-47c8-96a8-d9dfefd4b387 has been modified externally
       at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1193)
       at org.apache.jackrabbit.core.NodeImpl.unlock(NodeImpl.java:3790)
       at JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:95)
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=5
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=6
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=1
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=5
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=6
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=1
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=6
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=7
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=2
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=6
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=7
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=2
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=7
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=8
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=3
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=7
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=8
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=3
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=8
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=9
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=4
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=8
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=9
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=4
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=9
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=5
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=5
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=6
15:46:18 WARN  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:103) - ERROR:1, i=9
javax.jcr.InvalidItemStateException: /folder: the node cannot be saved because it has been modified externally.
       at org.apache.jackrabbit.core.NodeImpl.makePersistent(NodeImpl.java:908)
       at org.apache.jackrabbit.core.ItemImpl.persistTransientItems(ItemImpl.java:682)
       at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1173)
       at org.apache.jackrabbit.core.NodeImpl.lock(NodeImpl.java:3744)
       at JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:94)
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=6
15:46:18 WARN  org.apache.jackrabbit.core.lock.LockManagerImpl$LockInfo.loggingOut(LockManagerImpl.java:892) - Unable to unlock session-scoped lock on node '7c198c7b-76c8-47c8-96a8-d9dfefd4b387-W': Unable to unlock node. Node has pending changes: /folder
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=7
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=7
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=8
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=8
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=9
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=9

"
0,"Support for passing an SSLContext to the SSLSocketFactory of HttpClientWould it be possible to use an existing instance of SSLContext to initialise an SSLSocketFactory? This would allow using SSLContexts configured with more options, such as CRLs.

(This follows the thread of the httpclient-commons-dev list: http://marc.info/?l=httpclient-commons-dev&m=121737017814116&w=2 )."
0,"Change Term to use bytesin LUCENE-2426, the sort order was changed to codepoint order.

unfortunately, Term is still using string internally, and more importantly its compareTo() uses the wrong order [utf-16].
So MultiTermQuery, etc (especially its priority queues) are currently wrong.

By changing Term to use bytes, we can also support terms encoded as bytes such as numerics, instead of using
strange string encodings.
"
0,Some implementations require a save() after a mixin has been assignedSome test cases do not call save() after a mixin has been added.
1,"Deadlock in DefaultISMLockingThere seems to be a bug in DefaultISMLocking which was detected as part of JCR-2746.

1) The main thread gets a read lock.

2) The ObservationManager thread tries to lock for writing, which is blocked because there is still a read lock.

3) Then the main thread tries to get a second read lock, which is blocked because there is a waiting write lock.

The bug was introduced as part of JCR-2089 (Use java.util.concurrent), revisions 995411 and 995412. I think the safe solution is to revert those to commits, and add a test case. If the DefaultISMLocking is changed later on, more test cases are required. An efficient solution is relatively complicated.
"
0,"Collapse Searcher/Searchable/IndexSearcher; remove contrib/remote; merge PMS into IndexSearcherWe've discussed cleaning up our *Searcher stack for some time... I
think we should try to do this before releasing 4.0.

So I'm attaching an initial patch which:

  * Removes Searcher, Searchable, absorbing all their methods into IndexSearcher

  * Removes contrib/remote

  * Removes MultiSearcher

  * Absorbs ParallelMultiSearcher into IndexSearcher (ie you can now
    pass useThreads=true, or a custom ES to the ctor)

The patch is rough -- I just ripped stuff out, did search/replace to
IndexSearcher, etc.  EG nothing is directly testing using threads with
IndexSearcher, but before committing I think we should add a
newSearcher to LuceneTestCase, which randomly chooses whether the
searcher uses threads, and cutover tests to use this instead of making
their own IndexSearcher.

I think MultiSearcher has a useful purpose, but as it is today it's
too low-level, eg it shouldn't be involved in rewriting queries: the
Query.combine method is scary.  Maybe in its place we make a higher
level class, with limited API, that's able to federate search across
multiple IndexSearchers?  It'd also be able to optionally use thread
per IndexSearcher.
"
0,Enable DocValues by default for every CodecCurrently DocValues are enable with a wrapper Codec so each codec which needs DocValues must be wrapped by DocValuesCodec. The DocValues writer and reader should be moved to Codec to be enabled by default.
1,"A failure to connect to a MySQL database when JackRabbit starts a session leaves a .lock file in the repository. Subsequent sessions cannot be created by the same thread.I investigating the robustness of JackRabbit in the face of unexpected database errors, such as the database being unavailable. In my particular case, I am attempting to start a JackRabbit session using a TransientRepository while the database is not yet running. This correctly fails. However, if I attempt to create another session within the same thread after a short while, an exception occurs saying that the repository has already been locked. I would expect the repository folder not to be locked. Maybe the code meant to remove the .lock file was not triggered because of an uncaught exception.

Please see the attached files:
-a test class to reproduce the problem
-my repository.xml config
-the log file quantel.txt with details about the stack trace.
"
0,"Tests not executable for already present mixinsorg.apache.jackrabbit.test.api.NodeRemoveMixinTest.testCheckedIn() and 
org.apache.jackrabbit.test.api.NodeAddMixinTest.testCheckedIn() 

fail when the mixin being added is already present on the node. The tests should check for this and trow a NotExecutableException."
0,"toplevel exception cleanupHttpClient.execute should throw only one exception, for easier general use.
HttpMethod constructors (HttpGet, HttpPut, etc..) should throw IllegalArgumentException in the string constructor (imply the string is pre-checked).  People wanting to see a URIException can use 'new HttpGet(new URI(uri))' and trigger the exception from the explicit URI creation."
0,"o.a.jackrabbit.spi.commons.conversion.NameParser should not assume that namespace URI's are registeredaccording to JCR 2.0, ""3.4.3.4 Parsing Lexical Paths"":

<quote>
An otherwise valid path containing an expanded name with an unregistered 
namespace URI will always resolve into a valid internal representation of a path 
</quote>

the current implementation assumes that namespace URIs encountered in 
expanded form names are registered, otherwise the name is treated as
qualified name. "
1,"DbDatastore: Problems indexing pdf fileAs reported by Claus Kll:

When importing a pdf file into a repository configured with a DbDataStore the following exception occurs. This happens only when using the DbDataStore with copyWhenReading=true

java.io.IOException: Stream closed
       at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:156)
       at java.io.BufferedInputStream.read(BufferedInputStream.java:315)
       at org.apache.jackrabbit.core.data.db.TempFileInputStream.read(TempFileInputStream.java:107)
       at java.io.BufferedInputStream.read1(BufferedInputStream.java:265)
       at java.io.BufferedInputStream.read(BufferedInputStream.java:324)
       at java.io.BufferedInputStream.fill(BufferedInputStream.java:229)
       at java.io.BufferedInputStream.read(BufferedInputStream.java:246)
       at java.io.FilterInputStream.read(FilterInputStream.java:89)
       at java.io.PushbackInputStream.read(PushbackInputStream.java:141)
       at org.pdfbox.io.PushBackInputStream.peek(PushBackInputStream.java:71)
       at org.pdfbox.io.PushBackInputStream.isEOF(PushBackInputStream.java:88)
       at org.pdfbox.pdfparser.PDFParser.parseObject(PDFParser.java:370)
       at org.pdfbox.pdfparser.PDFParser.parse(PDFParser.java:176)"
1,"o.a.j.spi.commons.query.sql2.ParserTest uses platform encoding with non-ASCII charactersThe ParserTest class loads a series of test SQL statements from test.sql2.txt, which contains a few non-ASCII characters (good to test those!). Unfortunately the file is read using the default platform encoding, which breaks the Linux-based test builds.

I'll recode the file to UTF-8 and explicitly specify the encoding when the file is read."
0,"Fix for small syntax omission in TermQuery documentationA coding example, which could be cut'n'paste by a user, has unbalanced parenthesis.

This fix corrects the documentation, making no changes to functionality, only readability."
1,JCA build failure with J2EE 1.3The fix to JCR-736 introduced a similar problem as was previously reported in JCR-413. The fix in JCR-413 should apply also to this case.
1,"Benchmark does not close its Reader when OpenReader/CloseReader are not usedOnly the Searcher is closed, but because the reader is passed to the Searcher, the Searcher does not close the Reader, causing a resource leak."
0,"Deprecate IndexModifierSee discussion at http://www.gossamer-threads.com/lists/lucene/java-dev/52017?search_string=deprecating%20indexmodifier;#52017

This is to deprecate IndexModifier before 3.0 and remove it in 3.0.

This patch includes:
  1 IndexModifier and TestIndexModifier are deprecated.
  2 TestIndexWriterModify is added. It is similar to TestIndexModifer but uses IndexWriter and has a few other changes. The changes are because of the difference between IndexModifier and IndexWriter.
  3 TestIndexWriterLockRelease and TestStressIndexing are switched to use IndexWriter instead of IndexModifier."
0,"Incorrect slf4j-log4j12 dependency scope in spi-commonsThe slf4j-log4j12 dependency scope in jackrabbit-spi-commons is ""runtime"", when it should be ""test"". We don't want to impose a specific logging solution to downstream projects."
0,JSR 283: New Event Types
0,"GetReferencesNodeTest test assumptionsBad test assumptions in GetReferencesNodeTest:

1) In setUp(): there is a primary node type including mixin:versionable. Proposed fix: just create the node, try to add mixin:versionable, check the node type after save.

2) The repository supports non-protected reference properties. Proposed fix: check with AbstractJCRTest's ensureCanSetProperty method, and let NotExecutableException be thrown.
"
0,"Remove commons-collections and slf4j-api dependencies from jcr-commonsAs noted in JCR-1615 and discussed on the mailing list [1] it would be good if jackrabbit-jcr-commons didn't come with extra dependencies beyond the standard Java class libraries and the JCR API.

Currently jackrabbit-jcr-commons depends on both commons-collections and slf4j-api, but both dependencies are relatively isolated and could be dropped with relatively little effort. Both dependency changes may be backwards incompatible with existing clients, but since the impact is reasonably small and easy to resolve I'd be OK doing this in 1.5.

[1] http://markmail.org/message/724ruk4l7b5rjtan"
0,"Generify PriorityQueuePriority Queue should use generics like all other Java 5 Collection API classes. This very simple, but makes code more readable."
0,"Revise internal data structures of ThreadSafeClientConnManagerThreadSafeClientConnManager internal data structures can be improved:
- keep track of issued connections with weak references
- use class derived from WeakReference instead of a lookup table for callbacks from ReferenceThread
  (or drop ReferenceThread in favor of occasionally polling the issued connections for leaks)
"
0,"Log / trace wrapper for the JCR APII have implemented the log / trace mechanism for the JCR API. A short summary:

- A wrapper for a Repository. All other objects that where created directly or indirectly (Session, Node and so on) are wrapped as well. 
- The wrappers log all JCR method calls to a file and call the underlying methods. 
- Return values and calling method / line number can be logged as well (optional). 
- The log file itself is mainly Java source code and can be compiled and run.
- Included is a player to re-play log files (for example, if the log file is too big to be compiled).
"
0,"""ant dist"" no longer generates md5's for the top-level artifactsMark hit this for 2.9.0, and I just hit it again for 2.9.1.  It used to work..."
0,"ChildNodeEntriesImpl.update logs incorrect errorsThe ChildNodeEntriesImpl logs errors on a correct update.

""ChildInfo iterator contains multiple entries with the same name|index or uniqueID -> ignore ChildNodeInfo.""  (line 186)"
1,"leak in MultiThreadedHttpConnectionManager.ConnectionPool.mapHostsOnce entries are added to MultiThreadedHttpConnectionManager.ConnectionPool.mapHosts, they are never cleaned up unless MultiThreadedHttpConnectionManager is shutdown."
0,"Add toString() or getName() method to IndexReaderIt would be very useful for debugging if IndexReader either had a getName() method, or a toString() implementation that would get a string identification for the reader.

for SegmentReader, this would return the same as getSegmentName()
for Directory readers, this would return the ""generation id""?
for MultiReader, this could return something like ""multi(sub reader name, sub reader name, sub reader name, ...)

right now, i have to check instanceof for SegmentReader, then call getSegmentName(), and for all other IndexReader types, i would have to do something like get the IndexCommit and get the generation off it (and this may throw UnsupportedOperationException, at which point i have would have to recursively walk sub readers and try again)

I could work up a patch if others like this idea"
0,"need DOAP file for LuceneCan someone please draft a DOAP file for Lucene, so that we're listed at http://projects.apache.org/?

A DOAP generator is at:

http://projects.apache.org/create.html

Please attach it to this bug report.  Thanks."
0,"MTQ rewrite + weight/scorer init should be single passSpinoff of LUCENE-2690 (see the hacked patch on that issue)...

Once we fix MTQ rewrite to be per-segment, we should take it further and make weight/scorer init also run in the same single pass as rewrite."
0,"Optimize BlockTermsReader.seekWhen we seek, we first consult the terms index to find the right block
of 32 (default) terms that may hold the target term.  Then, we scan
that block looking for an exact match.

The scanning just uses next() and then compares the full term, but
this is actually rather wasteful.  First off, since all terms in the
block share a common prefix, we should compare the target against that
common prefix once, and then only compare the new suffix of each
term.  Second, since the term suffixes have already been read up front
into a byte[], we should do a no-copy comparison (vs today, where we
first read a copy into the local BytesRef and then compare).

With this opto, I removed the ability for BlockTermsWriter/Reader to
support arbitrary term sort order -- it's now hardwired to
BytesRef.utf8SortedAsUnicode.
"
1,"InternalValue.createCopy for binary properties (jcr:data) causes problemsRunning 1.4 with no data store configured, and option org.jackrabbit.useDataStore not set (i.e true), the following code gives 0 for the property length.

Node n = root.getNode(relPath);
session.getWorkspace().copy(n.getPath(), destPath);
Node contentNode = n.getNode(JcrConstants.JCR_CONTENT);
Property p = contentNode.getProperty(JcrConstants.JCR_DATA);
System.out.println(""length = ""+p.getLength());

InternalValue.createCopy checks USE_DATA_STORE and returns the same value for the source node's state. BundleBinding.writeState() calls BLOBInMemory.discard() when persisting the new node. This has now changed the value of the existing nodes property. Setting the option org.jackrabbit.useDataStore to false works fine. Possibly the check for binary property type in InternalValue.createCopy should be done first?"
0,"Disk full during addIndexes(Directory[]) can corrupt indexThis is a spinoff of LUCENE-555

If the disk fills up during this call then the committed segments file can reference segments that were not written.  Then the whole index becomes unusable.

Does anyone know of any other cases where disk full could corrupt the index?

I think disk full should worse lose the documents that were ""in flight"" at the time.  It shouldn't corrupt the index."
0,"Via NTLM proxy to SSL Apache/BasicAuth. - worked in may 22nd, but broken in beta1Hi there,

This morning I downloaded beta 1 and tried a small piece of code to connect to 
a SSLified apache server (using basic authentication) via a MS-Proxy 2.0 with 
NTLM enabled. The sourcecode of my crashme is based on the 1st attachment for 
HTTPCLIENT-153. It differs from the original in using basic authentication for the 
webserver instead of NTLM.

It failed with this error:

--
10-jun-2003 16:39:05 org.apache.commons.httpclient.HttpMethodBase 
processAuthenticationResponse
INFO: Already tried to authenticate to ""website#"" but still receiving 407.
Status: 407 : Proxy authentication required
--

Then I downloaded a fresh night build (commons-httpclient-20030605) which also 
failed :/

Then I went back to an old build from May (commons-httpclient-20030522) which 
worked like a charm!!!

Using MSIE I can succesfully connect to the apache server. I know it's not a 
problem with typos because I have MSIE ask me for all creds.

Seems somethings got broken along the way. If I can help, please ask!

Cheers."
0,"repository-1.5.dtd: change order of main elementsCurrently the order of elements in repository.xml is:
<!ELEMENT Repository (FileSystem,Security,Workspaces,Workspace,Versioning,SearchIndex?,Cluster?,DataStore?)>

I would like to change it to
<!ELEMENT Repository (Cluster?,FileSystem,DataStore?,Security,Workspaces,Workspace,Versioning,SearchIndex?)>
because I think that makes more sense.

Currently XML validation is disabled, and therefore the order of elements in the DTD does not need to match the repository.xml file. However as soon as XML validation is enabled, repository.xml files that use the wrong order will no longer work (the repository can not be started).

There is a request to enable XML validation at http://issues.apache.org/jira/browse/JCR-1462
"
0,"Stats for QueriesRe-enable the stats for queries, as they were disabled during the refactoring phase."
0,"Collapse nested OR expressionsExecuting a query with multiple OR expressions in a predicate leads to score values that depend on the order of the operands.

For example, the following query:

//*[jcr:contains(@prop1, 'foo') or jcr:contains(@prop2, 'foo') or jcr:contains(@prop3, 'foo')] order by @jcr:score descending

will return a slightly different result compared to:

//*[jcr:contains(@prop3, 'foo') or jcr:contains(@prop1, 'foo') or jcr:contains(@prop2, 'foo')] order by @jcr:score descending

Internally jackrabbit parses the predicate of the first query into a tree:

orExpr(orExpr(contains(prop1, 'foo'), contains(prop2, 'foo')), contains(prop3, 'foo'))

Lucene will calculate the score for the inner OR expression first and then for the outer, which is not equivalent with a nested expression that has property names in a different sequence.

The query should be translated internally into a single OR expression with three operands. That way, the score value is always the same, irrespective of the order of the operands."
1,"wrong eval order of access control entries within a single node (node-based ac)it seems to me that with the node-based access control the ac entries within a given node are currently collected in the wrong order.
if i remember correctly this worked before and i removed at some point (for reasons i don't recall exactly but have the vague idea that it
was related to the allow-only for groups).

anyway:
while playing around with the permission in our CRX recently i found, that the evaluation of the following setup didn't work as I would
have expected:

- user A is member of group B and C
- for both groups an ACE exists on a given node /a/b/c
- the acl looks like  { deny for B, allow for C }

I would have expected that the allow for C would have reverted the previous deny for B since - in the GUI - I read the ace eval order from first entry to last entry... in the order I added them."
0,Use the remote-resources-plugin to add LICENSE and NOTICE files to binariesSince JCRSITE-13 the remote resources plugin is configured to automatically add LICENSE and NOTICE files to all of our binary artifacts (including -sources and -javadoc jars). We should adapt the configuration so that these files get to include all the correct licensing metadata we currently maintain in src/main/resources/META-INF.
0,"Avoid exceptions thrown in finalize handler of RepositoryImpl constructorIf an exception happens during initialization of the repository, it might be overlayed by an exception thrown in the finalize handler of the RepositoryImpl constructor (see line 382 ff in [1]). The latter exception wins and the original exception is lost (if you don't have a log). This makes it hard to figure out the real problem.

This problem is actually a bit self-enforcing: if something goes wrong during startup, the code in the shutdown() method that is called is actually very prone to fail as it might not expect such a broken-startup state. In my case the overlaying NPE happened in ObservationManagerImpl.getRegisteredEventListeners, where this.dispatcher was unexpectedly null [2].

I think both places should be fixed (NPE guard in ObservationManagerImpl constructor for ""dispatcher"") and a try/catch block in the finalizer, just logging the exception:

    } finally {
        if (!succeeded) {
            try {
                // repository startup failed, clean up...
                shutdown();
            } catch (Throwable t) {
                // shutdown() likely to fail now, as startup was broken...
                log.error(""In addition to startup fail, another problem occurred while shutting down the repository again."", e);
            }
        }
    }


[1] http://svn.apache.org/viewvc/jackrabbit/trunk/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/RepositoryImpl.java?view=markup

[2] Overlaying exception's stacktrace:
Caused by: java.lang.NullPointerException
	at org.apache.jackrabbit.core.observation.ObservationManagerImpl.getRegisteredEventListeners(ObservationManagerImpl.java:143)
	at org.apache.jackrabbit.core.SessionImpl.removeRegisteredEventListeners(SessionImpl.java:1190)
	at org.apache.jackrabbit.core.SessionImpl.logout(SessionImpl.java:1215)
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.doDispose(RepositoryImpl.java:2153)
	at com.day.crx.core.CRXRepositoryImpl$CRXWorkspaceInfo.doDispose(CRXRepositoryImpl.java:1095)
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.dispose(RepositoryImpl.java:2108)
	at org.apache.jackrabbit.core.RepositoryImpl.doShutdown(RepositoryImpl.java:1146)
	at com.day.crx.core.CRXRepositoryImpl.doShutdown(CRXRepositoryImpl.java:845)
	at org.apache.jackrabbit.core.RepositoryImpl.shutdown(RepositoryImpl.java:1098)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:387)
	at com.day.crx.core.CRXRepositoryImpl.<init>(CRXRepositoryImpl.java:201)
	at com.day.crx.core.CRXRepositoryImpl.create(CRXRepositoryImpl.java:190)
	... 28 more
"
0,"wildcardquery rewrite improvementswildcardquery has logic to rewrite to termquery if there is no wildcard character, but
* it needs to pass along the boost if it does this
* if the user asked for a 'constant score' rewriteMethod, it should rewrite to a constant score query for consistency.

additionally, if the query is really a prefixquery, it would be nice to rewrite to prefix query.
both will enumerate the same number of terms, but prefixquery has a simpler comparison function."
0,"terms index should not store useless suffixesThis idea came up when discussing w/ Robert how to improve our terms index...

The terms dict index today simply grabs whatever term was at a 0 mod 128 index (by default).

But this is wasteful because you often don't need the suffix of the term at that point.

EG if the 127th term is aa and the 128th (indexed) term is abcd123456789, instead of storing that full term you only need to store ab.  The suffix is useless, and uses up RAM since we load the terms index into RAM.

The patch is very simple.  The optimization is particularly easy because terms are now byte[] and we sort in binary order.

I tested on first 10M 1KB Wikipedia docs, and this reduces the terms index (tii) file from 3.9 MB -> 3.3 MB = 16% smaller (using StandardAnalyzer, indexing body field tokenized but title / date fields untokenized).  I expect on noisier terms dicts, especially ones w/ bad terms accidentally indexed, that the savings will be even more.

In the future we could do crazier things.  EG there's no real reason why the indexed terms must be regular (every N terms), so, we could instead pick terms more carefully, say ""approximately"" every N, but favor terms that have a smaller net prefix.  We can also index more sparsely in regions where the net docFreq is lowish, since we can afford somewhat higher seek+scan time to these terms since enuming their docs will be much faster."
0,"Path.getAncestor and Path.isAncestor are not symmetricAlthough the method names refer to ancestors they operate on sub-paths. Consider:

PathFactory pf = PathFactoryImpl.getInstance();
Path.Element p = pf.getParentElement();

Path path = pf.create(new Path.Element[]{p, p});
Path ancestor = path.getAncestor(1);

assertFalse(ancestor.isAncestorOf(path) )  

This is not what one would expect from looking an the method signatures. 
I suggest to rename getAncestor to getSubPath, clarify the javadoc, and deprecate getAncestorCount. 

A patch follows.
"
0,"Fix 2.9 contrib builds to succeed when JDK 1.4 is used (leaving out contribs that require 1.5)When you build and test Lucene 2.9 with Java 1.4, building and testing of contrib fails. This patch fixes this to repect the current compiler version and disables all contribs that need Java 1.5 by checking their javac.source property.

This patch can be ported to 3.x or trunk, when 1.6 contribs will appear."
0,"Remove Serializable on ItemState classesItemStates are never directly serialized, which means they don't have to implement Serializable anymore.

See also: http://markmail.org/message/wsqnih2lembkcrdf"
0,"add IndexCommit.isOptimized methodSpinoff from here:

    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200807.mbox/%3C69de18140807010347s6269fea5r12c3212e0ec0a12a@mail.gmail.com%3E"
0,Add benchmark task for FastVectorHighlighter
0,"add concurrent merge policyProvide the ability to handle merges in one or more concurrent threads, i.e., concurrent with other IndexWriter operations.

I'm factoring the code from LUCENE-847 for this."
0,"JCR2SPI: error level logging when cleaning up session locks LockManagerImpl.loggingOut() tries to unlock nodes that have a session lock. If, while doing so, a RepositoryException is thrown, this gets locked on error level.

The TCK tests tearDown code removes test nodes using a separate session; thus we see RepositoryExceptions for the simple reason that the nodes have already been removed by somebody else.

Proposal: handle ItemNotFoundExc and PathNotFoundExc separately, not logging them.
"
0,"improve HttpRoute APISome of the constructors of HttpRoute have three boolean parameters.
Use enumerations to reduce the potential for confusion.

The flags for tunnelled and layered are not independent, since layered implies tunnelled.
These can be combined to a 3-valued enum.
"
0,"Add open ended range query syntax to QueryParserThe QueryParser fails to generate open ended range queries.
Parsing e.g. ""date:[1990 TO *]""  gives zero results,
but
ConstantRangeQuery(""date"",""1990"",null,true,true)
does produce the expected results.

""date:[* TO 1990]"" gives the same results as ConstantRangeQuery(""date"",null,""1990"",true,true)."
1,"RMI problems prevent proper startup of the Jackrabbit webappA trouble in binding the repository to a RMI registry will prevent the entire Jackrabbit webapp from starting properly. Since RMI is seldom the primary function of the webapp, it's more appropriate to simply log a warning in such cases."
1,"URI path resolution problems.URI does not completely conform to RFC 2396.  In particular it does not handle the following 
relative URIs correctly:

../../../g
../../../../g"
0,"Make QValueFactoryImpl extensibleThe class is currently final and other modules therefore copied code. This kind of duplication it hard to maintain and should be avoided.

If QValueFactoryImpl would be designed to be extensible then other classes could reuse much of the code."
1,"IndexWriter.numDocs doesn't take into account applied but not flushed deletesThe javadoc states that buffered deletes are not taken into account and so you must call commit first.

But, if you do that, and you're using CMS, and you're unlucky enough to have a background merge commit just after you call commit but before you call .numDocs, you can still get a wrong count back.

The fix is trivial -- numDocs should also consult any pooled readers for their current del count.

This is causing an intermittent failure in the new TestNRTThreads.
"
1,"DatabaseJournal improperly finds tables in external schemas when used on OracleThe DatabaseJournal currently calls database metadata to determine if the journal table has already been created.  It uses the following code to do so:

ResultSet rs = metaData.getTables(null, null, tableName, null);

The Oracle driver sometimes will return the table if it is in another schema on the same database.  Other DBMS code within JackRabbit has a specific Oracle version that handles this case.  In order for the journal table to be properly created, Oracle databases will need the schema name included in the getTables() call."
0,"Cookie.compare(...) uses single instance STRING_COLLATOR to do blocking comparesI am using a MultiThreadedHttpConnectionManager with a single HttpClient instance and multiple GetMethod objects.  I have a 500 thread max.  I recently noticed that all 500 threads are in the same place and seem to be blocking each other - the stack trace is below.  I dug into the Cookie.compare(...) method and saw that it is using STRING_COLLARTOR.compare(c1.getPath(), c2.getPath()).  STRING_COLLATOR is defined as a single instance object, 'private static final RuleBasedCollator STRING_COLLATOR = (RuleBasedCollator) RuleBasedCollator.getInstance(new Locale(""en"", ""US"", """"));'.  I also saw that RuleBasedCollator.compare is synchronized.  That means that every thread that is trying to make a request is getting blocked while it tries to add cookies to the request method.  I do not see a workaround because this is the same static final object in every Cookie instance.  So, the more threads, the more synchronized comparisons.  At times I am fetching URLs all from the same site so I am going through this code a lot.  I need it to be much faster than it currently is because all of my threads are getting eaten up on this call and backlogging my system.  Can a different RuleBasedCollator be used for each compare (use the RuleBasedCollator.getInstance() for every compare?  I think that would solve things.

Name: pool-1-thread-1443: 72.21.206.5
State: BLOCKED on java.text.RuleBasedCollator@190330a owned by: pool-1-thread-1867: 72.21.206.5
Total blocked: 9,598  Total waited: 381

Stack trace: 
java.text.RuleBasedCollator.compare(RuleBasedCollator.java:396)
org.apache.commons.httpclient.Cookie.compare(Cookie.java:484)
org.apache.commons.httpclient.cookie.CookieSpecBase.addInPathOrder(CookieSpecBase.java:578)
org.apache.commons.httpclient.cookie.CookieSpecBase.match(CookieSpecBase.java:557)
org.apache.commons.httpclient.HttpMethodBase.addCookieRequestHeader(HttpMethodBase.java:1179)
org.apache.commons.httpclient.HttpMethodBase.addRequestHeaders(HttpMethodBase.java:1305)
org.apache.commons.httpclient.HttpMethodBase.writeRequestHeaders(HttpMethodBase.java:2036)
org.apache.commons.httpclient.HttpMethodBase.writeRequest(HttpMethodBase.java:1919)
org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java:993)
org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:397)
org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:170)
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:396)
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:324)"
0,"Move PatternAnalyzer out of contrib/memory to contrib/analyzersin the memory index contrib there is a PatternAnalyzer.
i think this analyzer belongs in contrib/analyzers instead, it has no relation to memory index."
0,[PATCH] import cleanupThis patch just removes useless imports so you get less warnings in Eclipse.
1,"NPE in MultiReader.isCurrent() and getVersion()I'm attaching a fix for the NPE in MultiReader.isCurrent() plus a testcase. For getVersion(), we should throw a better exception that NPE. I will commit unless someone objects or has a better idea."
0,Add support for benchmarking CollectorsAs the title says.
0,"Support for OpenOffice text extractionHi, here is the patch.

>hi nicolas,
>
>thanks for your offer to contribute your openoffice textfilter, that's greatly appreciated!
>
>i suggest you post a jira 'Improvement' or ""New Feature' issue and attach your code as an svn patch. somebody will take care 
>of it (i assume marcel), i.e. review your contribution and provide feedback/further instructions.

>cheers
>stefan

>On 2/2/06, Nicolas Jouanin <nicolas.jouanin@gmail.com> wrote:
>>
>>
>>
>> Hi Stefan,
>>
>>
>>
>> I work with Martin Perez, main developper of jLibrary.
>>
>> Therefore, I developed a class which extracts metadata and text 
>> content from any openoffice file (that was not the hardest job). This 
>> class is already used into jLibrary.
>>
>> As Martin suggested me, I used this class to create a new TextFilter 
>> subclass into textfilters contrib project. I downloaded textfilters 
>> project from svn, created my class into the project tree and tested it 
>> with a test class , just like it was done with the other extractors.
>>
>> I can send you the code if you want to review it, or just tell me how 
>> I can commit it.
>>
>>
>>
>> Regards,
>>
>>
>>
>> Nicolas.
"
0,"Benchmark's ContentSource should not rely on file suffixes to be lower cased when detecting file type (gzip/bzip2/text)file.gz is correctly handled as gzip, but file.GZ handled as text which is wrong.
"
0,"HTTP Version configuration and trackingHTTP version tracking is currently oversimplified with a single http11 boolean. 
Extend this to handle any http version simply, and efficiently.

Possible suggestion:
> get rid of setHttp11() an isHttp11
> void setHttpVersion(String version)
> String getHttpVersion()
> boolean isHttpVersion(String version)"
0,"Unit and integration test cases for the new SimilaritiesWrite test cases to test the new Similarities added in [LUCENE-3220|https://issues.apache.org/jira/browse/LUCENE-3220]. Two types of test cases will be created:
 * unit tests, in which mock statistics are provided to the Similarities and the score is validated against hand calculations;
 * integration tests, in which a small collection is indexed and then searched using the Similarities.

Performance tests will be performed in a separate issue."
0,"[API Doc] Compile performance optimization guidePerformance optimization guide is long overdue and badly needed. The more people
start using HttpClient in all sorts of creative ways the more we are going to
need it.

Oleg"
0,"JSR 283: Create RepositoryFactory implementationJSR 283 specifies a RepositoryFactory to retrieve a repository instance based on a map of parameters. We should have the following implementations:

- local repository with repository home and repository configuration parameters
- repository obtained via JNDI
- repository obtained via RMI
"
0,"reorganize contrib modulesit would be nice to reorganize contrib modules, so that they are bundled together by functionality.

For example:
* the wikipedia contrib is a tokenizer, i think really belongs in contrib/analyzers
* there are two highlighters, i think could be one highlighters package.
* there are many queryparsers and queries in different places in contrib
"
1,"TransientRepository with LocalFileSystem eventually causes Repository data to be stored at path '/'I'm using a TransitoryRepository for my unit testing, with the repository's file system specified as:

    <FileSystem class=""org.apache.jackrabbit.core.fs.local.LocalFileSystem"">
        <param name=""path"" value=""${rep.home}/repository""/>
    </FileSystem>

I noticed today that when I run my unit tests Jackrabbit is creating four directories at the root of my hard drive: ""meta"", ""namespaces"", ""nodetypes"", and ""data"". I tracked the problem the fact that when a LocalFileSystem is closed, it sets the ""root"" to null - an invalid state. But when using a TransitoryRepository, the invalid state is never discovered because the LocalFileSystem object itself is not released, or re-initialized. It is simply used to create BasedFileSystem objects in RepositoryImpl. Calls to BasedFileSystem defer to the LocalFileSystem object that now has a null root. Inside the LocalFileSystem, all the calls to Java's io.File constructor have a ""null"" parent parameter, causing File to fall back to its single argument constructor which sees the path ""/meta"" and happily creates files at the root of the disk.

I'm not sure what the best solution is, but some thoughts I've had are:
- don't set the ""root"" property to null when closing a LocalFileSystem
- make RepositoryConfig re-init the FileSystem variable when it is accessed.
- don't cache the RepositoryConfig in TransitoryRepository (this might also require a new constructor that takes a class-path resource for the repository configuration file)"
0,FieldValueFitler should expose the field it usesFieldValueFitler should expose the field it uses. It currently hides this entirely.
1,"Abort Before Execute & Various Other Times FailsWith svn commit #639506, a few more scenarios become testable & can be fixed.  These are: aborting before HttpClient.execute is called, aborting between setting the connection request for aborting and setting the connection release trigger, and aborting after a redirected route uses a new connection request.  As of r639506, those three scenarios fail to abort correctly."
0,"Jcr2Spi: Avoid extra round trip to the SPI upon Node.getNode and Session.getItemUpon Session.getItem/itemExists and Node.getNode/hasNode JCR2SPI currently tries to load the Node from the persistent layer (SPI) if no corresponding entry exists in the hierarchy.

Since with JCR-1638 a flag has been introduced indicating if the child node entries are complete. In this case, the extra round trip could be omitted."
0,"improve performance when saving a node with a large number of child nodes (e.g. > 10k child node entries)JCR-307 brought a significant improvement WRT saving nodes with a large number of child nodes

unfortunately JCR-2579 broke part of the optimization (see NodeState.setChildNodeEntries(List))."
1,"jcr:successors property not persisted correctly within a transactionDuring a transaction, if you create a new version then read the version history the ""jcr:successors"" property is not updated. Note that ""jcr:predecessors"" is updated properly.

Also, the version history is sometimes not propertly read. During the transaction, it might appear empty. This behavior in not consistent from one execution to another.

After a restart of the repository, the version history and the ""jcr:successors"" property is read properly.

* Tests cases will follow shortly.

Thanks, 

Nicolas"
0,"Small speedups to DocumentsWriter's quickSortIn working on LUCENE-510 I found that DocumentsWriter's quickSort can
be further optimized to handle the common case of sorting only 2
values.

I ran with this alg:

  analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
  
  doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker
  
  docs.file=/Volumes/External/lucene/wiki.txt
  doc.stored = true
  doc.term.vector = true
  doc.add.log.step=2000
  doc.maker.forever = false
  
  directory=FSDirectory
  autocommit=false
  compound=false
  
  ram.flush.mb=64
  
  { ""Rounds""
    ResetSystemErase
    { ""BuildIndex""
      CreateIndex
      { ""AddDocs"" AddDoc > : 200000
      - CloseIndex
    }
    NewRound
  } : 5
  
  RepSumByPrefRound BuildIndex

Best of 5 was 857.3 docs/sec before the optimization and 881.6 after =
2.8% speedup, on a quad-core Mac Pro with 4-drive RAID 0 array.

The fix is trivial.  I will commit shortly.

"
0,"getDocValues should provide a MultiReader DocValues abstractionWhen scoring a ValueSourceQuery, the scoring code calls ValueSource.getValues(reader) on *each* leaf level subreader -- so DocValue instances are backed by the individual FieldCache entries of the subreaders -- but if Client code were to inadvertently  called getValues() on a MultiReader (or DirectoryReader) they would wind up using the ""outer"" FieldCache.

Since getValues(IndexReader) returns DocValues, we have an advantage here that we don't have with FieldCache API (which is required to provide direct array access). getValues(IndexReader) could be implimented so that *IF* some a caller inadvertently passes in a reader with non-null subReaders, getValues could generate a DocValues instance for each of the subReaders, and then wrap them in a composite ""MultiDocValues"".


"
1,"IndexWriter.addIndexes(IndexReader[]) fails to create compound filesEven if no exception is thrown while writing the compound file at the end of the 
addIndexes() call, the transaction is rolled back and the successfully written cfs 
file deleted. The fix is simple: There is just the 
{code:java}
success = true;
{code}
statement missing at the end of the try{} clause.

All tests pass. I'll commit this soon to trunk and 2.3.2."
0,"fileformats.xml doesn't document compound file streamsCurrent versions of Lucene generate segments in compound file stream format
files, but the fileformats documentation does not have any description of the
format for those files."
0,"Redefine HttpClient vs HttpMultiClient interface for 2.0In particular the HttpClient/HttpMultiClient issue must be resolved. 
HttpultiClient functionality should be prefered, but HttpClient is the most
suitable name.  Consider impact to other projects.  Is java1.1 compatability
really an issue anymore?"
0,"Fix typos in CHANGES.txt and contrib/CHANGES.txt prior to 2.9 releaseI noticed a few typos in CHANGES.txt and contrib/CHANGES.txt.  (Once they make it past a release, they're set in stone...)

Will attach a patch shortly."
0,"TokenWrapperAttributeFactory, CachingWrapperFilterHelper implements equals and so should also implement hashCodeits part of the contract of Object 

bq. If two objects are equal according to the equals(Object) method, then calling the hashCode method on each of the two objects must produce the same integer result."
1,"In case of SocketTimeoutException and using HttpRequestRetryHandler the execution is always +1If my request encounter a SocketTimeoutException, the HttpRequestRetryHandler#retryRequest will be called with an executionCount with a value +1."
1,"Add the org.apache.jackrabbit.rmi.jackrabbit package to the rmic generation From the UnicastRemoteObject's (ServerJackrabbitNodeTypeManager, ServerJackrabbitWorkspace) should be stubs generated.
"
0,"Content-Length & Transfer-Encoding request headers should be handled by entity enclosing methodsCurrently 'Content-Length' & 'Transfer-Encoding' request headers are handled by 
the HttpMethodBase class. This is conceptually wrong and error-prone in my 
opinion. Entity enclosing methods should control 'Content-Length' & 'Transfer-
Encoding' request headers instead, as they provide request content and 
encapsulate the requisite content transfer logic."
0,"QValueFactory improvements1) Allow all create methods to throw RepositoryException.

2) Further document that create(value,type) can throw ValueFormatException.

3) Remove special case create(File)
"
0,"Remove old byte[] norms api from IndexReaderFollowup to LUCENE-3628.

We should remove this api and just use docvalues everywhere, to allow for norms of arbitrary size in the future (not just byte[])"
0,"Populating exception message with InetSocketAddress.getHostName() can take a long timeIn the PlainSocketFactory class, when a SocketTimeoutException occurs a call is made to InetSocketAddress.getHostName() when generating the exception message. Unfortunately, this call can take a long time. In my case, the address I am specifying is an IP address, which InetSocketAddress attempts to perform a reverse-lookup on to determine the hostname; however, since  the address does not have a hostname assigned to it, the operation takes a long time to return.

I'm attaching a patch for trunk with my proposed fix. Viewing the source history, it looks like the code used to have the behavior I'm proposing, but it was changed in revision 1070943. Based on the source commits and linked issues, I cannot determine a specific reason for the change. If there is a reason the code needs to be the way it is, then I apologize for inconvenience I have caused."
0,"Query#mergeBooleanQueries argument should be of type BooleanQuery[] instead of Query[]The method #mergeBooleanQueries accepts Query[] and casts elements to BooleanQuery without checking. This will guarantee a ClassCastException if it is not a boolean query. We should enforce this by changing the signature. This won't really break back compat. as it only works with instances of BooleanQuery.

"
1,"Malformed excerpt if content contains markup and no highlights foundAny markup in content that is used in an excerpt is encoded with corresponding entity references. However, this process is broken when there are no highlights in the excerpt. In this case, the content is provided as is in the excerpt, which may lead to malformed HTML/XML."
1,"cannot PUT changes to a resource in the simple webdav serverwhen using the simple webdav server to PUT a resource, the ""versionable"" mixin node type is assigned to the new node without regard to whether the node type is already assigned to the node. this causes PUT requests that change existing resources to fail with 403 errors.

the fix is to augment AddMixinCommand to not try to add the mixin node type if the node already has it.
"
0,"Duplicate log of HTTP headerThe HTTP header line:

""HTTP/1.1 200 OK[\r][\n]"" 

is duplicated in the wire logs. Seems to be because the line is logged at:

HttpParser [line: 131] - readLine(InputStream, String)

and at:

HttpMethodBase [line: 1980] - readStatusLine(HttpState, HttpConnection)

It looks like the latter log should be removed?"
0,Add support for boolean values to QValueI suggest to add support for reading and writing boolean values to QValue and QValueFactory. I find it strange that there is such support for the other data types but booleans must be constructed via strings. 
1,ItemInfoBuilder fails to set correct path on propertiesThis only happens if the parent node's nodeId is id based (in contrast to path based). The build() method should not rely on the nodeId providing the full path. Instead it should us the parent node's getPath() method to construct the full path. 
0,"Index sorterA tool to sort index according to a float document weight. Documents with high weight are given low document numbers, which means that they will be first evaluated. When using a strategy of ""early termination"" of queries (see TimeLimitedCollector) such sorting significantly improves the quality of partial results.

(Originally this tool was created by Doug Cutting in Nutch, and used norms as document weights - thus the ordering was limited by the limited resolution of norms. This is a pure Lucene version of the tool, and it uses arbitrary floats from a specified stored field)."
0,"Source distribution packaging targets should make a tarball from ""svn export""Instead of picking and choosing which stuff to include from a local working copy, Lucene's dist-src/package-tgz-src target and Solr's package-src target should simply perform ""svn export"" with the same revision and URL as the local working copy."
1,"ContentEncodingHttpClient.execute(HttpGet, ResponseHandler<T>) throws IOException when reading compressed responseThe following snippet:

    String url = ""http://yahoo.com"";
    HttpClient httpClient = new ContentEncodingHttpClient();
    HttpGet get = new HttpGet(url);
    String content = httpClient.execute(get, new BasicResponseHandler());

throws:

java.io.IOException: Attempted read from closed stream.
	at org.apache.http.impl.io.ChunkedInputStream.read(ChunkedInputStream.java:126)
	at java.util.zip.CheckedInputStream.read(CheckedInputStream.java:42)
	at java.util.zip.GZIPInputStream.readUByte(GZIPInputStream.java:205)
	at java.util.zip.GZIPInputStream.readUShort(GZIPInputStream.java:197)
	at java.util.zip.GZIPInputStream.readHeader(GZIPInputStream.java:136)
	at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:58)
	at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:68)
	at org.apache.http.client.entity.GzipDecompressingEntity.getContent(GzipDecompressingEntity.java:63)
	at org.apache.http.conn.BasicManagedEntity.getContent(BasicManagedEntity.java:88)
	at org.apache.http.util.EntityUtils.consume(EntityUtils.java:65)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:974)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:919)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:910)
	at tv.adap.service.HttpPoolTest.testChunkedGzip(HttpPoolTest.java:41)

whereas the following snippet runs fine:

    String url = ""http://yahoo.com"";
    HttpClient httpClient = new ContentEncodingHttpClient();
    HttpGet get = new HttpGet(url);
    HttpResponse response = httpClient.execute(get);
    HttpEntity entity = response.getEntity();
    String content = EntityUtils.toString(entity);

These two snippets should be functionally the same (putting the entity body into content). Creating a JIRA per the recommendation of Oleg from httpclient-users."
0,"Allow indexingConfiguration to be loaded from the classpathThe ""indexingConfiguration"" attribute in the SearchIndex configuration (http://wiki.apache.org/jackrabbit/IndexingConfiguration) actually requires an absolute filesystem path.

It would be nice if SearchIndex would also accept a file available in the classpath... although you can use variables like ${wsp.home} or similar there are many scenarios where a classpath resource would help (for example when creating a new workspace the directory structure is automatically created by jackrabbit and doesn't need to be already available but the indexing configuration file does).

I am attaching a simple patch to SearchIndex that tries to load the file from the classpath if it has not been found. Since priority is given to the old behavior (file before classpath) so it's fully backward compatible.

Diff has been generated against trunk, it would be nice to have this patch also on the 2.0 branch.
 
 "
0,"Flexible query parser does not support open ranges and range queries with mixed inclusive and exclusive rangesFlexible query parser does not support open ranges and range queries with mixed inclusive and exclusive ranges.

These two problems were found while developing LUCENE-1768."
0,"Implement a cache to perform real request only when neededBrowsers may cache received content according to the values of different
response headers. It would be great if HttpClient could do the same."
1,"New socket timeout value wont have effect if connection is reusedReported by Teemu Tingander <Teemu.Tingander at tecnomen.fi> on The Jakarta
Commons HttpClient Developer List:

<snip>
Changing read timeout ()wont affect after successful method execution using
same connection.. 

This seems to be a bug in HttpClient class method
executeMethod(HostConfiguration ...)..

The problematic section seems to be if section checking if connection is
open
	
		method.setStrictMode(strictMode);
        		        
            if (!connection.isOpen()) {                
                connection.setConnectionTimeout(connectionTimeout);
-->		    connection.setSoTimeout(soTimeout);
                connection.open();
                if (connection.isProxied() && connection.isSecure()) {
                    method = new ConnectMethod(method);
                }
            }
 
Problem can be solved by moving the line out of if section

		method.setStrictMode(strictMode);

		connection.setSoTimeout(soTimeout);	
        		        
            if (!connection.isOpen()) {                
                connection.setConnectionTimeout(connectionTimeout);
                connection.open();
                if (connection.isProxied() && connection.isSecure()) {
                    method = new ConnectMethod(method);
                }
            }
</snip>"
0,"Add support for simple test casesAs discussed on the mailing list, I'd like to add a simple org.apache.jackrabbit.core.TestRepository helper class that could be used in a simple unit test template."
0,"Add getIndexCommit method to IndexReaderSpinoff from this thread:

  http://markmail.org/message/bojgqfgyxkkv4fyb

I think it makes sense ask an IndexReader for the commit point it has
open.  This enables the use case described in the above thread, which
is to create a deletion policy that is able to query all open readers
for what commit points they are using, and prevent deletion of them.

"
0,"Improve ArrayUtil/CollectionUtil.*Sort() methods to early-reaturn on empty or one-element lists/arraysIt might be a good idea to make CollectionUtil or ArrayUtil return early if the passed-in list or array's length <= 1 because sorting is unneeded then. This improves maybe automaton or other places, as for empty or one-element lists no SorterTermplate is created."
0,"Closing a session twice shouldn't write a warning in the logWhen closing a session twice the following warning is written to the log file as of JCR-2741:

""This session has already been closed. See the chained exception for a trace of where the session was closed.""

I think the second ""close()"" should simply be ignored, without warning.
"
0,JSR 283: Configurations and Baselines
0,Deprecated API called in o.a.l.store Directoriesjust ran into NIOFSDirectory and others still call getFile instead of getDirectory
0,Move extensions to the JSR 283 security API  from jackrabbit-core to jackrabbit-apiFor the 2.0.0 release i'd like to have the jackrabbit-specific extensions to the JSR 283 security API being part of jackrabbit-api.
0,Split the wire log into header and content parts. 
0,"contrib/javascript is not packaged into releasesthe contrib/javascript directory is (apparently) a collection of javascript utilities for lucene .. but it has not build files or any mechanism to package it, so it is excluded form releases.

"
1,"Check for correct content-type in URLEncodedUtils not working for encoding-suffixesDear DEV-Team,

i am developing an application with the httpclient. Today i found a small problem, related to URLEncodedUtils.

Our Tomcat-Server deliveres for Server-Requests, the HTTP-Header: ""Content-Type=application/x-www-form-urlencoded;charset=UTF-8"", but the httpclient only checks for: ""Content-Type=application/x-www-form-urlencoded"". This failing check results in an empty result of call to the method: URLEncodedUtils.parse(entity);

Following source-code causes the prob: 

public class URLEncodedUtils {

    /**
     * Returns true if the entity's Content-Type header is
     * <code>application/x-www-form-urlencoded</code>.
     */
    public static boolean isEncoded (final HttpEntity entity) {
        final Header contentType = entity.getContentType();
        return (contentType != null && contentType.getValue().equalsIgnoreCase(CONTENT_TYPE));
    }
}

IMO the method should be changed to:


public class URLEncodedUtils {

    /**
     * Returns true if the entity's Content-Type header is
     * <code>application/x-www-form-urlencoded</code>.
     */
    public static boolean isEncoded (final HttpEntity entity) {
        final Header contentType = entity.getContentType();
        return (contentType != null && contentType.getValue().startsWith(CONTENT_TYPE + "";""));
    }
}

Best Regards,"
0,"TestThreadSafety.testLazyLoadThreadSafety test failureTestThreadSafety.testLazyLoadThreadSafety failed with this error:

unable to create new native thread

Maybe because of SimpleText

Here is the stacktrace:
{noformat}
 [junit] Testsuite: org.apache.lucene.search.TestThreadSafe
    [junit] Testcase: testLazyLoadThreadSafety(org.apache.lucene.search.TestThreadSafe):	Caused an ERROR
    [junit] unable to create new native thread
    [junit] java.lang.OutOfMemoryError: unable to create new native thread
    [junit] 	at java.lang.Thread.start0(Native Method)
    [junit] 	at java.lang.Thread.start(Thread.java:614)
    [junit] 	at org.apache.lucene.search.TestThreadSafe.doTest(TestThreadSafe.java:129)
    [junit] 	at org.apache.lucene.search.TestThreadSafe.testLazyLoadThreadSafety(TestThreadSafe.java:148)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit] 
    [junit] 
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 6.051 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestThreadSafe -Dtestmethod=testLazyLoadThreadSafety -Dtests.seed=-277698010445513699:-89599297372877779
    [junit] NOTE: test params are: codec=SimpleText, locale=zh_SG, timezone=Pacific/Tongatapu
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.search.TestThreadSafe FAILED
{noformat}"
0,"don't download/extract 20,000 files when doing the buildWhen you build lucene, it downloads and extracts some data for contrib/benchmark, especially the 20,000+ files for the reuters corpus.
this is only needed for one test, and these 20,000 files drive IDEs and such crazy.
instead of doing this by default, we should only download/extract data if you specifically ask (like wikipedia, collation do, etc)

for the qualityrun test, instead use a linedoc formatted 587-line text file, similar to reuters.first20.lines.txt already used by benchmark.
"
0,"overhaul connection manager and associated connection interfaceMultiThreadedHttpConnectionManager/HttpHostConnection needs to be overhauled to provide a layer on top of OperatedClientConnection.
Preliminary working names: ThreadSafeClientConnManager/ManagedClientConnection

This implies some work on former HttpMethodDirector and HttpClient to verify completeness of the new connection management API.
"
0,MultiPhraseQuery should allow access to terms
1,GC resources in TermInfosReader when exception occurs in its constructorI replaced IndexModifier with IndexWriter in test case TestStressIndexing and noticed the test failed from time to time because some .tis file is still open when MockRAMDirectory.close() is called. It turns out it is because .tis file is not closed if an exception occurs in TermInfosReader's constructor.
0,"upgrade icu to 4.8we should upgrade from 4.6 to 4.8.

some internal methods became public, also a package-private reflection hack can be removed."
0,"New method to add an array of parameters to PostMethodWhen posting a form a web page may have many parameters to post to the 
webserver.  Currently in PostMethod, if you wanted to add multiple parameters, 
you would have to call addParameter(name, value) for each one.

A new convinence method should be added to allow for simpler client code by 
taking an array of NameValuePair objects and adding all parameters in a single 
function call.

void addParameters(NameValuePair[] parameters) 

Also, the comments for PostMethod functions that deal with parameters 
state ""Override method of HttpMethodBase ..."" which is incorrect.  More 
informative comments should be added to this public API."
0,"Header adding in org.apache.http.client.protocol.RequestAcceptEncoding should be conditionalorg.apache.http.client.protocol.RequestAcceptEncoding adds a header in any case. Any chance to do it conditional (like in RequestClientConnControl)? The code would be something like
if (!request.containsHeader(""Accept-Encoding"")) {
    request.addHeader(""Accept-Encoding"", ""gzip,deflate"");
}

In our app this header may be added before request intercepting, so would be great if this fact is checked.
"
0,"Eliminate unnecessary uses of Hashtable and VectorLucene uses Vector, Hashtable and Enumeration when it doesn't need to. Changing to ArrayList and HashMap may provide better performance.

There are a few places Vector shows up in the API. IMHO, List should have been used for parameters and return values.

There are a few distinct usages of these classes:
# internal but with ArrayList or HashMap would do as well. These can simply be replaced.
# internal and synchronization is required. Either leave as is or use a collections synchronization wrapper.
# As a parameter to a method where List or Map would do as well. For contrib, just replace. For core, deprecate current and add new method signature.
# Generated by JavaCC. (All *.jj files.) Nothing to be done here.
# As a base class. Not sure what to do here. (Only applies to SegmentInfos extends Vector, but it is not used in a safe manner in all places. Perhaps, implements List would be better.)
# As a return value from a package protected method, but synchronization is not used. Change return type.
# As a return value to a final method. Change to List or Map.

In using a Vector the following iteration pattern is frequently used.
for (int i = 0; i < v.size(); i++) {
  Object o = v.elementAt(i);
}

This is an indication that synchronization is unimportant. The list could change during iteration.

"
1,"BundleDbPersistenceManager consistencyFix doesn't fix missing non system childnode  entries of the root nodeThe bundle check/fix mechanism completely skips the checks on the root node, but the root node can also have non system child node entries which can be broken/missing. The attached patch makes the check only check the non system child node entries of the root node. It would be nice if this patch (if/when accepted) could also be backported to the 1.5 and 1.6 branches.
"
0,"DavMethods.POST should be public, not privateDavMethods.POST is declared as a private constant, when it should be public. attached is a patch to fix this."
0,"ClientPNames.VIRTUAL_HOST is used as is; if not provided, the port should be derived from the target URLThe parameter ClientPNames.VIRTUAL_HOST allows the default Host header to be overridden.

Currently the code uses the HttpHost entry as provided, and does not automatically add the port suffix.
This means that user code has to provide the port - but only if it's not the default for the protocol.

It would be simpler for the user if the port were automatically added.

If the user does not provide the port, the code should derive it from the target URL.

If the user does provide a port number, then that should be used (as is done currently). 
This allows the user to override the port (if that should ever prove necessary)."
0,"Abstract JCR base classesImplement and use a set of abstract AbstractSession, AbstractItem, etc. classes that implement as much of the respective JCR interfaces using nothing else but calls to other JCR methods. These would be just like the AbstractMap, etc. classes in java.util.

(See the related discussion at http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/10583)"
0,"Make WordDelimiterFilter's instantiation more readableCurrently WordDelimiterFilter's constructor is:

{code}
public WordDelimiterFilter(TokenStream in,
	                             byte[] charTypeTable,
	                             int generateWordParts,
	                             int generateNumberParts,
	                             int catenateWords,
	                             int catenateNumbers,
	                             int catenateAll,
	                             int splitOnCaseChange,
	                             int preserveOriginal,
	                             int splitOnNumerics,
	                             int stemEnglishPossessive,
	                             CharArraySet protWords) {
{code}

which means its instantiation is an unreadable combination of 1s and 0s.  

We should improve this by either using a Builder, 'int flags' or an EnumSet."
0,Land DWPT on trunkWith LUCENE-2956 we have resolved the last remaining issue for LUCENE-2324 so we can proceed landing the DWPT development on trunk soon. I think one of the bigger issues here is to make sure that all JavaDocs for IW etc. are still correct though. I will start going through that first.
0,Add import-export toolWe at <GX> creative online development would like to contribute our command-line import-export tool to the Apache Jackrabbit project. This tool is capable of exporting and importing all kinds of repository content (including custom nodetypes and namespace mappings) in a persistence-layer independent way. 
1,"WebDAV server should treat non-wellformed XML in request bodies as errorThe WebDAV server should treat non-wellformed XML request bodies as errors (instead of treating the request as if the request body was missing).

(causes Litmus test suite failure in test case propfind_invalid)"
0,"Add FileBody constructor with explicit filenameFileBody does not allow the filename field in the Content-Disposition header to be overriden, the filename taken from the File object - I have software that creates temporary files and needs to assign an implicit logical filename."
1,"ConcurrentModificationException thrown in MultiThreaded codeNow seeing this error.  This is with default cookie settings.  Happening rarely, however the web sites we're talking to do not use cookies very much.


java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1010)
        at org.apache.http.client.protocol.RequestAddCookies.process(RequestAddCookies.java:152)
        at org.apache.http.protocol.BasicHttpProcessor.process(BasicHttpProcessor.java:290)
        at org.apache.http.protocol.HttpRequestExecutor.preProcess(HttpRequestExecutor.java:160)
        at org.apache.http.impl.client.DefaultClientRequestDirector.execute(DefaultClientRequestDirector.java:355)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:501)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:456)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:422)
        at com.hi5.os.Hi5RemoteContentFetcher.fetch(Hi5RemoteContentFetcher.java:279)"
0,Convenience method to Or multiple values with a single filterAdded a convenience method to add Or Filter for the same property with multiple values. This is to simulate an IN Clause in JackRabbit. 
0,"Maintain the cluster revision tableThe revision table in which cluster nodes write their changes can potentially become very large. If all cluster nodes are up to date to a certain revision number, then it seems unnecessary to keep the revisions with a lower number."
0,"improve documentation of SPI Batch addPropertyClarify that Batch.addProperty should succeed even though the property already exists.


(See mailing list thread starting with: http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/200801.mbox/%3c47A1E1C1.2050107@gmx.de%3e)"
0,"Put everything in jackrabbit-spi-commons under org.apache.jackrabbit.spi.commonsTo avoid confusion and naming conflicts, we should put all classes and packages in jackrabbit-spi-commons under org.apache.jackrabbit.spi.commons."
0,"Change defaultValues format in NodeTypes XML to jcr valuecurrently, the defaultValues serialization in the nodetypes.xml is the only one that uses internal value serialization, rather than the jcr string serialization.
eg:

<propertyDef name=""jcr:requiredPrimaryTypes"" ..... >
  <defaultValues>
    <defaultValue>{http://www.jcp.org/jcr/nt/1.0}base</defaultValue>
  </defaultValues>
</propertyDef>

this in not very handy, when the custom_nodetypes.xml should be written automatically.
i suggest to change the serialization to use the jcr value one:

<propertyDef name=""jcr:requiredPrimaryTypes"" ..... >
  <defaultValues>
    <defaultValue>nt:base</defaultValue>
  </defaultValues>
</propertyDef>
"
0,"SharedFieldCache$StringIndex memory leak causing OOM's SharedFieldCache$StringIndex is not working properly. It is meant to cache the docnumbers in lucene along with the term to sort on. The issue is twofold. I have a solution for the second one, the first one is not really solvable from jr pov, because lucene index readers are already heavily caching Terms. 

Explanation of the problem:

For *each* unique property where is sorted on, a new lucene ScoreDocComparator is created (see SharedFieldComparator newComparator). This new comparator creates *per* lucene indexreader  SharedFieldCache.StringIndex which is stored in a WeakHashMap with as key, the indexreader . As this indexreader  almost *never* can be garbage collected (only if it is merged and thus unused after), the SharedFieldCache.StringIndex are there to be the rest of the jvm life (which is sometime short, as can be seen from the simple unittest attached).  Obviously, this results pretty fast in OOM.

1) issue one:  The cached terms[] in SharedFieldCache.StringIndex can become huge when you sort on a common property (date) which is present in a lot of nodes. It you sort on large properties, like 'title' this SharedFieldCache.StringIndex  will quickly use hundreds of Mb for a couple of hundred of thousand of nodes with a title. This issue is already a lucene issue, as lucene already caches the terms. OTOH, I really doubt whether we should index long string values as UNTOKENIZED in lucene at all. A half working solution might be a two-step solution, where the first sort is on the first 10 chars, and only if the comparator returns 0, take the entire string to sort on

2) issue two:  The cached terms[] in SharedFieldCache.StringIndex is frequently sparse, consuming an incredible amount of memory for string arrays containing mainly null values. For example (see attached unit test):

- add 1.000.000 nodes
- do a query and sort on a non existing property
- you'll loose 1.000.000 * 4 bytes ~ 4 Mb of memory
- sort on another non existing prop : another 4 Mb is lost
- do it 100 times --> 400 Mb is lost, and can't be reclaimed

I'll attach a solution which works really fine for me, still having the almost unavoidable memory absorption, but makes it much smaller. The solution is, that if < 10% of the String array is filled, i consider the array already sparse, and move to a HashMap solution. Performance does not decrease much (and in case of large sparsity increases because less memory consumption --> less gc, etc). 

Perhaps it does not seem to be a common issue (certainly the unit test) but our production environments memory snapshots indicate most memory being held by the SharedFieldCache$StringIndex (and the lucene Terms, which is harder to avoid)

I'd like to see this in the 1.5.1 if others are ok with it


"
0,"MultipartPostMethod does not check for error messagesIf a MultipartPost request is sent to a server which requires authentication, 
the server may respond to the request with an unauthorized header and close the 
connection before all of the data is sent.  HttpClient should monitor the 
incoming stream and cease transmitting the body if an error message is received 
(section 8.2.2 of rfc2616, see below).

At the very least HttpClient should check for a response when catching the 
HttpRecoverableException and retrying.  This probably should be done in 
HttpMethodBase so that we are in a known state when starting to retry the 
connection (ie: there isn't an existing response in the socket buffer to cause 
problems).

Ideally, HttpClient should also implement the 100 (Continue) status as 
specified in section 8.2.3 of rfc2616.

Finally, PostMethod should be tested to ensure that it does not exhibit this 
bug as well.

-------------
8.2.2 Monitoring Connections for Error Status Messages

   An HTTP/1.1 (or later) client sending a message-body SHOULD monitor
   the network connection for an error status while it is transmitting
   the request. If the client sees an error status, it SHOULD
   immediately cease transmitting the body. If the body is being sent
   using a ""chunked"" encoding (section 3.6), a zero length chunk and
   empty trailer MAY be used to prematurely mark the end of the message.
   If the body was preceded by a Content-Length header, the client MUST
   close the connection."
1,"spi2dav: Overwrite header T specified for MOVE and COPY causes failure if some API testsfailing tests are:

org.apache.jackrabbit.test.api.WorkspaceCopySameNameSibsTest#testCopyNodesNodeExistsAtDestPath
org.apache.jackrabbit.test.api.WorkspaceMoveSameNameSibsTest#testMoveNodesNodeExistsAtDestPath

those would be fixed by setting the overwrite header to F(alse)... however, this doesn't fit those cases where same-same
siblings would be allowed and the copy/move to a destination with existing item would succeed in JCR."
0,"Add a waitForMerges() method to IndexWriterIt would be very useful to have a waitForMerges() method on the IndexWriter.

Right now, the only way i can see to achieve this is to call IndexWriter.close()

ideally, there would be a method on the IndexWriter to wait for merges without actually closing the index.
This would make it so that background merges (or optimize) can be waited for without closing the IndexWriter, and then reopening a new IndexWriter

the close() reopen IndexWriter method can be problematic if the close() fails as the write lock won't be released
this could then result in the following sequence:
* close() - fails
* force unlock the write lock (per close() documentation)
* new IndexWriter() (acquires write lock)
* finalize() on old IndexWriter releases the write lock
* Index is now not locked, and another IndexWriter pointing to the same directory could be opened

If you don't force unlock the write lock, opening a new IndexWriter will fail until garbage collection calls finalize() the old IndexWriter

If the waitForMerges() method is available, i would likely never need to close() the IndexWriter until right before the process being shutdown, so this issue would not occur (worst case scenario, the waitForMerges() fails)


"
0,"HostnameVerifier shouldn't shadow simple name of implemented interfacepublic interface HostnameVerifier extends javax.net.ssl.HostnameVerifier.

As Findbugs says:

Class names shouldn't shadow simple name of implemented interface

This class/interface has a simple name that is identical to that of an implemented/extended interface, except that the interface is in a different package (e.g., alpha.Foo extends beta.Foo). This can be exceptionally confusing, create lots of situations in which you have to look at import statements to resolve references and creates many opportunities to accidently define methods that do not override methods in their superclasses. 
"
0,"dependencies for route planner implementationsThe implementations of HttpRoutePlanner that we have depend on the ConnectionManager, but use it only to look up the SchemeRegistry. Consider to depend only on the SchemeRegistry.

"
1,"Jackrabbit logs a NullPointerException on shutdown if the version manager wasn't initializedIf opening the repository fails, and the version manager was not initialized, then the shutdown method logs a NullPointerException when trying to close the version manager. This is a nuisance."
1,"MultiThreadedHttpConnectionManager setMaxTotalConnections() method doesn't workThe deprecated setMaxTotalConnections() method in the
MultiThreadedHttpConnectionManager seems like it has no effect:

Here is the source code in the current version:

    public void setMaxTotalConnections(int maxTotalConnections) {
        this.params.getMaxTotalConnections();
    }

Shouldn't it look more like this?

    public void setMaxTotalConnections(int maxTotalConnections) {
        this.params.setMaxTotalConnections(maxTotalConnections);
    }"
0,"ResponseContentEncoding should also handle x-gzip, compress and x-compressResponseContentEncoding should also handle x-gzip, compress and x-compress encodings according to specs (http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html, 3.5 Content Codings).

Also RequestAcceptEncoding should set Accept-Encoding to ""gzip,deflate,identity"". I am not sure about x-gzip, compress and x-compress here though.

Thanks"
0,"Optimization for FieldDocSortedHitQueueWhen updating core for generics,  I found the following as a optimization of FieldDocSortedHitQueue:

All FieldDoc values are Compareables (also the score or docid, if they
appear as SortField in a MultiSearcher or ParallelMultiSearcher). The code
of lessThan seems very ineffective, as it has a big switch statement on the
SortField type, then casts the value to the underlying numeric type Object,
calls Number.xxxValue() & co for it and then compares manually. As
j.l.Number is itself Comparable, I see no reason to do this. Just call
compareTo on the Comparable interface and we are happy. The big deal is that
it prevents casting and the two method calls xxxValue(), as Number.compareTo
works more efficient internally.

The only special cases are String sort, where the Locale may be used and the
score sorting which is backwards. But these are two if statements instead of
the whole switch.

I had not tested it now for performance, but in my opinion it should be
faster for MultiSearchers. All tests still pass (because they should).
"
0,"JCA project tests assume Windows pathsThe org.apache.jackrabbit.jca package in the top-level jca directory has unit tests that assume a Windows environment. It should be fixed to work in any environment. The best solution may be to use a test repository configuration file in the current directory.

The following is the start of the test case failures that I got running on MacOS X.

Testsuite: org.apache.jackrabbit.jca.test.ConnectionFactoryTest
Tests run: 3, Failures: 0, Errors: 3, Time elapsed: 0.778 sec

Testcase: testAllocation(org.apache.jackrabbit.jca.test.ConnectionFactoryTest): Caused an ERROR
org.apache.jackrabbit.core.config.ConfigurationException: Configuration file could not be read.: /Users/mark/Documents/school/whisper/jackrabbit/jackrabbit-trunk/jca/c:\dev\jcr\repository.xml (N
o such file or directory): /Users/mark/Documents/school/whisper/jackrabbit/jackrabbit-trunk/jca/c:\dev\jcr\repository.xml (No such file or directory)
org.apache.jackrabbit.core.config.ConfigurationException: Configuration file could not be read.: /Users/mark/Documents/school/whisper/jackrabbit/jackrabbit-trunk/jca/c:\dev\jcr\repository.xml (N
o such file or directory): /Users/mark/Documents/school/whisper/jackrabbit/jackrabbit-trunk/jca/c:\dev\jcr\repository.xml (No such file or directory)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createRepository(JCAManagedConnectionFactory.java:278)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createConnectionFactory(JCAManagedConnectionFactory.java:116)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createConnectionFactory(JCAManagedConnectionFactory.java:108)
        at org.apache.jackrabbit.jca.test.ConnectionFactoryTest.testAllocation(ConnectionFactoryTest.java:43)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
"
0,"Deprecating StopAnalyzer ENGLISH_STOP_WORDS - General replacement with an immutable SetStopAnalyzer and StandartAnalyzer are using the static final array ENGLISH_STOP_WORDS by default in various places. Internally this array is converted into a mutable set which looks kind of weird to me. 
I think the way to go is to deprecate all use of the static final array and replace it with an immutable implementation of CharArraySet. Inside an analyzer it does not make sense to have a mutable set anyway and we could prevent set creation each time an analyzer is created. In the case of an immutable set we won't have multithreading issues either. 
in essence we get rid of a fair bit of ""converting string array to set"" code, do not have a PUBLIC static reference to an array (which is mutable) and reduce the overhead of analyzer creation.

let me know what you think and I create a patch for it.

simon"
0,"Make FieldSortedHitQueue publicCurrently, those who utilize the ""advanced"" search API cannot sort results using
the handy FieldSortedHitQueue. I suggest making this class public to facilitate
this use, as I can't think of a reason not to."
0,"Add Apache RAT (Release Audit Tool) target to build.xml
Apache RAT is a useful tool to check for common mistakes in our source code (eg missing copyright headers):

    http://incubator.apache.org/rat/

I'm just copying the patch Grant worked out for Solr (SOLR-762).  I plan to commit to 2.4 & 2.9."
0,"Audit logJCR-2031 added the user name and path in debug logs for audit purposes. There are some problems with the fix that I had outlined in the comments for JCR-2031 and provided a patch. Additionally, it would use useful to add an update counter and size information to the debug log as well. Something like this:

17.03.2009 14:43:37 [1] 18216140 admin@/apps/acme/templates/contentpage/thumbnail.png (12343) 
17.03.2009 14:43:37 [2] 18216141 admin@/apps/acme/templates/contentpage/my.png (123) 
17.03.2009 14:43:37 [3] 18216142 admin@/apps/acme/templates/contentpage/blah.png (1423) 
17.03.2009 14:43:37 [4] 18216143 admin@/apps/acme/templates/contentpage/test.png (123423) 
17.03.2009 14:43:37 [5] 18216144 admin@/apps/acme/templates/contentpage/test2.png (123423) 

<date> <time> [<counter>] <txid> <userid>@<path> (<size>)

We should also think about whether we want this log as part of regular jackrabbit log or in a separate audit log. 
"
1,"DateUtil#formatDate uses default locale instead of USProblem reported by Yannick <yannick at meudal.net> on the httpclient-user list

==================================================================
Hello,

This is a bug report.

I'm using Commons HTTPClient (rc2) for generating HTTP requests. I put in 
headers some specific header, like the If-Modified-Since attribute. 
When I generate the date through DateUtil.formatDate method, I get a 
localized date, in french. Example: 
If-Modified-Since: dim., 10 avr. 2005 05:04:08 CEST

I get problems on my http server during parsing the received date. This is 
not a RFC 2616 compliant date format. It should be:
If-Modified-Since: Sun, 10 Apr 2005 05:04:08 CEST

A patch should be applied, by creating a new SimpleDateFormat(pattern, 
Locale.US) instead of SimpleDateFormat(pattern) (like it is done in the 
parse method, line #159).

org.apache.commons.httpclient.util.DateUtil, line #205:

    public static String formatDate(Date date, String pattern) {
        if (date == null) throw new IllegalArgumentException(""date is 
null"");
        if (pattern == null) throw new IllegalArgumentException(""pattern 
is null"");
 
        SimpleDateFormat formatter = new SimpleDateFormat(pattern, 
Locale.US);
        return formatter.format(date);
    }


Regards,

Yannick."
1,"String properties with invalid XML characters export as invalid XMLAs noted in the current JCR 1.0.1 maintenance draft, sections 6.4.1,
6.4.2.6, XML export of string properties that contain invalid XML
characters isn't well-defined currently, since those characters are
not permissible in XML.  The proposed fix is to use base64
encoding for such values in System View.

Most characters below #x20 are examples of this.  Currently, these
are escaped numerically in output (such as (amp)#0; )  but
such escape sequences can't be parsed by the XML
import methods.

The current behavior is particularly problematic, because the user
doesn't know the output is corrupt until later, when they try to import it
and get InvalidSerializedDataException.

If for some reason the base64 option is delayed, it might
make sense, as an interim solution, to fail on export
or to somehow patch import to relax its parsing and allow
these escape codes."
1,"Escape colon in statement of jcr:containsThe colon is a special character in the lucene query parser and allows to prefix query terms with an optional field name. JCR does not specify such a feature, thus a colon in the fulltext statement should be treated as a regular character. "
0,"TopDocsCollector should be abstract super class that is the real ""TopDocsCollector"" contract, a subclass should implement the priority-queue logic. e.g. PQTopDocsCollectorTopDocsCollector is both an abstract interface for producing TopDocs as well as a PriorityQueue based implementation.
Not all Collectors that could produce TopDocs must use a PriorityQueue, and it would be advantageous to allow the TopDocsCollector to be an ""interface"" type abstract class, with a PQTopDocsCollector sub-class.
While doing this, it'd be good to clean up the generics uses in these classes. As it's odd to create a TopFieldCollector and have to case the TopDocs object, when this can be fixed with generics."
0,"please log allocation of new connections to support debugging, testingI'd like to suggest that the MultiThreaded connection manager emit a trace-level log when it 
allocates a new HttpConnection to support debugging and testing.  I added one while working on 
my integration in Apache Axis (see org.apache.axis.transport.http.CommonsHTTPSender) and 
figured this would be of general use.  I'll attach a patch with the oh-so-minor addition after 
submitting this enhancement request."
1,InstantiatedIndexReader does not implement getFieldNames properlyCauses error in org.apache.lucene.index.SegmentMerger.mergeFields
0,"Make termInfosIndexDivisor configurableWorkspaces with large indexes may consume considerable heap memory. Lucene implements multi level skip lists for terms in the index. The first level of the skip list is kept in memory. This is usually not an issue, but when terms consist of long Strings the memory consumption increases drastically. Jackrabbit not just tokenizes string properties, but it also creates a single term, based on the complete string property value (needed for jcr:like function). These long terms are the reason for the increased memory consumption."
0,"The native FS lock used in test-framework's o.a.l.util.LuceneJUnitResultFormatter prohibits testing on a multi-user system{{LuceneJUnitResultFormatter}} uses a lock to buffer test suites' output, so that when run in parallel, they don't interrupt each other when they are displayed on the console.

The current implementation uses a fixed directory ({{lucene_junit_lock/}} in {{java.io.tmpdir}} (by default {{/tmp/}} on Unix/Linux systems) as the location of this lock.  This functionality was introduced on SOLR-1835.

As Shawn Heisey reported on SOLR-2739, some tests fail when run as root, but succeed when run as a non-root user.  

On #lucene IRC today, Shawn wrote:
{quote}
(2:06:07 PM) elyograg: Now that I know I can't run the tests as root, I have discovered /tmp/lucene_junit_lock.  Once you run the tests as user A, you cannot run them again as user B until that directory is deleted, and only root or the original user can do so.
{quote}
"
0,"Use Commons IO 1.4Commons IO contains a number of utility classes and methods for working with files and streams. Many of those utilities would be quite useful in Jackrabbit, so I'd like to introduce commons-io 1.4 as a dependency to jackrabbit-core.

"
0,"Lucene-core 2.9.0 missing from Maven Central RepositorySub-projects like lucene-demos, lucene-contrib, etc. exist in central, and depend on 2.9.0 of lucene-core. However, the lucene-core 2.9.0 artifact itself is missing."
0,JSR 283: Simple versioning
0,Remove duplicate code in QValueFactoryImpl (spi2dav)QValueFactoryImpl in spi2dav contains code duplicated from spi-commons QValueFactoryImpl. Once JCR-2245 has been applied the spi2dav variant can extend from the factory in spi-commons and we can simply the first.
0,"ISOLatin1AccentFilter a bit slowThe ISOLatin1AccentFilter is a bit slow giving 300+ ms responses when used in a highligher for output responses.

Patch to follow"
0,"Resolve JUnit assert deprecationsMany tests use assertEquals methods which have been deprecated.  The culprits are assertEquals(float, float), assertEquals(double, double) and assertEquals(Object[], Object[]).  Although not a big issue, they annoy me every time I see them so I'm going to fix them."
0,"ClientConnectionManager should throw InterruptedExceptionFor historical reasons, ThreadSafeClientConnectionManager throws an IllegalThreadStateException instead of an InterruptedException if the waiting thread is interrupted from outside. This design was chosen since adding InterruptedException to the HttpConnectionManager in 3.x would have broken the API. This is not a concern for HttpClient 4.0.
"
0,"OpenBitSet.prevSetBit()Find a previous set bit in an OpenBitSet.
Useful for parent testing in nested document query execution LUCENE-2454 ."
1,"BasicCookieStore treats cookies of the same name from the same host as duplicates, even if they have different pathsThe DefaultHttpClient is not handling cookies correctly when a single host returns multiple cookies of the same name but with separate paths.  For example, if a single instance of the client is used to access two different webapps on the same server, it may receive two different JSESSIONID cookies:

Cookie: [version: 0][name: JSESSIONID][value: F832C01D23F501CE5EEB296B602700C1][domain: lglom139.example.com][path: /msa-adrenalina][expiry: null]
Cookie: [version: 0][name: JSESSIONID][value: 0FC660347391B93267168F84F2B520F5][domain: lglom139.example.com][path: /maps][expiry: null]

Because the CookieIdentityComparator class does not test the cookie path when determining equality, each new JSESSIONID received replaces the previous one instead of adding a new cookie to the store.  This results in ""disconnecting"" the client from its sessions on the prior webapps.

I've confirmed that adding a path test to CookieIdentityComparator resolves this problem."
0,"DisjunctionScorerThis disjunction scorer can match a minimum nr. of docs, 
it provides skipTo() and it uses skipTo() on the subscorers. 
The score() method is abstract in DisjunctionScorer and implemented 
in DisjunctionSumScorer as an example."
0,"Extract JDBC Connection InitAn intermediate step to allowing a PM to be easily configurable through JNDI would be to extract the connection init. This will allow system integrators to subclass/wrap and dynamically configure a customized Simple PM. In org.apache.jackrabbit.core.state.db.SimpleDbPersistenceManager:

Replace lines (296-298) with

        initConnection();

Add:
	/**
	* Initialize the JDBC connection
	**/
	protected void initConnection() throws Exception {
            Class.forName(driver);
            con = DriverManager.getConnection(url, user, password);
            con.setAutoCommit(false);
	}"
1,"Enabling wire logging changes isEof/isStale behaviorIf you enable wire logging, DefaultClientConnection wraps the SocketInputBuffer with a LoggingSessionInputBuffer. This hides the EofSensor interface implemented by SocketInputBuffer (but not LoggingSessionInputBuffer), which makes at least AbstractHttpClientConnection.isEof() and isStale() methods behave differently.

(That is, stale connection checks won't really work as intended if wire logging is enabled. Which makes it a bit difficult to debug problems related to stale connections...)

Proposed fix: implement EofSensor interface in LoggingSessionInputBuffer (delegating it to wrapped buffer).
"
0,"replace collation/lib/icu4j.jar with a smaller icu jarCollation does not need all the icu data.
we can shrink the jar file a bit by using the data customizer, and excluding things like character set conversion tables."
0,"Change defaults in IndexWriter to maximize ""out of the box"" performanceThis is follow-through from LUCENE-845, LUCENE-847 and LUCENE-870;
I'll commit this once those three are committed.

Out of the box performance of IndexWriter is maximized when flushing
by RAM instead of a fixed document count (the default today) because
documents can vary greatly in size.

Likewise, merging performance should be faster when merging by net
segment size since, to minimize the net IO cost of merging segments
over time, you want to merge segments of equal byte size.

Finally, ConcurrentMergeScheduler improves indexing speed
substantially (25% in a simple initial test in LUCENE-870) because it
runs the merges in the backround and doesn't block
add/update/deleteDocument calls.  Most machines have concurrency
between CPU and IO and so it makes sense to default to this
MergeScheduler.

Note that these changes will break users of ParallelReader because the
parallel indices will no longer have matching docIDs.  Such users need
to switch IndexWriter back to flushing by doc count, and switch the
MergePolicy back to LogDocMergePolicy.  It's likely also necessary to
switch the MergeScheduler back to SerialMergeScheduler to ensure
deterministic docID assignment.

I think the combination of these three default changes, plus other
performance improvements for indexing (LUCENE-966, LUCENE-843,
LUCENE-963, LUCENE-969, LUCENE-871, etc.) should make for some sizable
performance gains Lucene 2.3!"
1,"Workspace.clone() fails the second time, if cloning referenceablesthe following testcode fails with the 2nd clone. please note, that if the 'folder' node is not made
referenceable, the test passes (copied an adapted from test in WorkspaceCloneTest).

    public void testCloneNodesTwice() throws RepositoryException {
        // clone referenceable node below non-referenceable node
        String dstAbsPath = node2W2.getPath() + ""/"" + node1.getName();

        Node folder = node1.addNode(""folder"");
        folder.addMixin(""mix:referenceable"");
        node1.save();
        workspaceW2.clone(workspace.getName(), node1.getPath(), dstAbsPath, true);
        workspaceW2.clone(workspace.getName(), node1.getPath(), dstAbsPath, true);

        // there should not be any pending changes after clone
        assertFalse(superuserW2.hasPendingChanges());
    }

"
0,"isHttp11 should have HttpClient scope-----Original Message-----
From: Kalnichevski, Oleg [mailto:oleg.kalnichevski@bearingpoint.com] 
Sent: Wednesday, January 15, 2003 8:24 AM
To: Commons HttpClient Project
Cc: Rob Owen
Subject: RE: isHttp11 and HTTP/1.0 servers 

Rob
You are basically right hands down. It does make sense for the HTTP version 
flag to have HttpClient scope. We should address this shortcoming as a part of 
the post-2.0-release redesign

Feel free to file a bug report to make sure the issue does not go forgotten

http://nagoya.apache.org/bugzilla/enter_bug.cgi?product=Commons

Many thanks for bring it up

Cheers

Oleg

-----Original Message-----
From: Rob Owen [mailto:Rob.Owen@sas.com]
Sent: Monday, January 13, 2003 18:31
To: Commons HttpClient Project
Subject: isHttp11 and HTTP/1.0 servers 


The boolean variable http11 is set on a method by method basis. For PutMethod, 
decisions (eg. Expect: 100-continue request header) are made prior to 
determining the value for Http11 (chicken and egg problem) and so the default 
(true) is used to produce the request. An HTTP/1.0 server hangs waiting for 
the extra data on the PUT method body. 

For applications that are using HttpClient (ie. they do not manipulate the 
HTTP methods directly and cannot be expected to set the value of Http11 for 
each method instance), shouldn't http11 have HttpClient scope ? This would 
allow an interaction (eg. OPTIONS) to set http11 and all methods thereafter 
would use this setting?
  
------
Rob Owen
SAS Institute Inc.
email: Rob.Owen@sas.com"
1,"Problem importing node with binary property in a repository configured with datastoreUsing the importXML method of workspace to import some node containing binary properties the nodes are imported correctly and the value of the binary data property is imported
However the binary data goes to the db (persistenceManager) an not to the datastore.

Creating a new node of the same type using the api, the binary data go to the datastore."
0,"Optimize TermsEnum.seek when caller doesn't need next termSome codecs are able to save CPU if the caller is only interested in
exact matches.  EG, Memory codec and SimpleText can do more efficient
FSTEnum lookup if they know the caller doesn't need to know the term
following the seek term.

We have cases like this in Lucene, eg when IW deletes documents by
Term, if the term is not found in a given segment then it doesn't need
to know the ceiling term.  Likewise when TermQuery looks up the term
in each segment.

I had done this change as part of LUCENE-3030, which is a new terms
index that's able to save seeking for exact-only lookups, but now that
we have Memory codec that can also save CPU I think we should commit
this today.

The change adds a ""boolean onlyExact"" param to seek(BytesRef).
"
0,"ParallelReader is now atomic, rename to ParallelAtomicReader and also add a ParallelCompositeReader (that requires LogDocMergePolicy to have identical subreader structure)The plan is:
- Move all subreaders to ctor (builder-like API. First build reader-set, then call build)
- Rename ParallelReader to ParallelAtomicReader
- Add a ParallelCompositeReader with same builder API, but taking any CompositeReader-set and checks them that they are aligned (docStarts identical). The subreaders are ParallelAtomicReaders."
1,"IndexReader.indexExists sometimes returns true when an index isn't presentIf you open a writer on a new dir and prepareCommit but don't finish the commit, IndexReader.indexExists incorrectly returns true, because it just checks for whether a segments_N file is present and not whether it can be successfully read."
0,"Remove ImportContextImpl#getDetectorthe method ImportContextImpl#getDetector refers an interface method on ImportContext that does not exist.
according to jukka that is a leftover. since the method is not used at all i would therefore suggest to remove it altogether, remove the instance field and deprecate the constructor taking the detector param."
0,"Improve parallel testsAs mentioned on the dev@ mailing list here: http://www.lucidimagination.com/search/document/93432a677917b9bd/lucenejunitresultformatter_sometimes_fails_to_lock

It would be useful to not create a lockfactory for each test suite (As they are run sequentially in the same separate JVM).
Additionally, we create a lot of JVMs (26) for each batch, because we have to run one for each letter.
Instead, we use a technique here to divide up the tests with a custom selector: http://blog.code-cop.org/2009/09/parallel-junit.html
(I emailed the blog author and received permission to use this code)

This gives a nice boost to the speed of overall tests, especially Solr tests, as many start with an ""S"", but this is no longer a problem.

"
1,"Findbugs reports and fixesRan findbugs 0.94.rc1 on 3.0RC4. 
Fixed a few of the obvious ones (patches to follow) and made notes on the 
remainder - see the //TODO markers in code.
Also created a findbugs target in build.xml - see appropriate patch file"
0,"Allow means force a Repository to synchronize with the clusterBased on the thread on the user mailing list I'm logging this to propose adding a sync() method to force cluster synchronization using the JackrabbitRepository extension API.

The purpose of the method is such that in a distributed clustered environment sometime cluster synchronization does or has not occurred such that certain repositories are in a stale state.  This method would provide a means to force a repository to update pull in possible changes made by other Jackrabbit repositories.

"
0,"Incorrect error message in AnalyzingQueryParser.getPrefixQueryThe error message of  getPrefixQuery is incorrect when tokens were added, for example by a stemmer. The message is ""token was consumed"" even if tokens were added.
Attached is a patch, which when applied gives a better description of what actually happened."
0,"SpecialOperations.isFinite can have TERRIBLE TERRIBLE runtime in certain situationsin an application of mine, i experienced some very slow query times with finite automata (all the DFAs are acyclic)

It turned out, the slowdown is some terrible runtime in SpecialOperations.isFinite <-- this is used to determine if the DFA is acyclic or not.

(in this case I am talking about even up to minutes of cpu).
"
0,"Change default value for maxMergeDocsThis is actually a left over from the time before JCR-197 was implemented. Back then index merges were performed with the client thread and would hold up execution for a long time if a large number of nodes were merged. The default value for maxMergeDocs limited this to 100'000 nodes, resulting in a couple of seconds for the merge operation.

This default value does not make sense anymore because index merges are performed in a background thread and may take a long time without an effect on regular workspace operations. If a workspace grows large it may cause performance degradation because the number of index segments increases linearly when there are more than 100'000 nodes.

I propose to set the new default to Integer.MAX_VALUE"
0,support offsets in MemoryPostingsReally we should add this for Sep & Pulsing too... but this is one more
0,"TCK: NodeTest#testAddNodeItemExistsException fails if validation deferred until saveThe test expects addNode to fail if a same-name sibling already exists.  JSR-170 allows this validation to be deferred until save.

Proposal: call save in the ""try"" block.

--- NodeTest.java       (revision 422074)
+++ NodeTest.java       (working copy)
@@ -380,6 +391,7 @@
         try {
             // try to add a node with same name again
             defaultTestNode.addNode(nodeName3, testNodeType);
+            defaultRootNode.save();
             fail(""Adding a node to a location where same name siblings are not allowed, but a node with same name"" +
                     "" already exists should throw ItemExistsException "");
         } catch (ItemExistsException e) {
"
0,"Grouped total countWhen grouping currently you can get two counts:
* Total hit count. Which counts all documents that matched the query.
* Total grouped hit count. Which counts all documents that have been grouped in the top N groups.

Since the end user gets groups in his search result instead of plain documents with grouping. The total number of groups as total count makes more sense in many situations. "
1,"MinPayloadFunction returns 0 when only one payload is presentIn some experiments with payload scoring through PayloadTermQuery, I'm seeing 0 returned when using MinPayloadFunction.  I believe there is a bug there.  No time at the moment to flesh out a unit test, but wanted to report it for tracking."
1,"MultiReader does not propagate readerFinishedListeners to clones/reopened readersWhile working on refactoring MultiReader/DirectoryReader in trunk, I found out that MultiReader does not correctly pass readerFinishedListeners to its clones and reopened readers."
0,Stop text extraction when the maxFieldLength limit is reachedWhen indexing large documents the text extraction often takes quite a while and uses lots of memory even if only the first maxFieldLength (by default 10000) tokens are used. I'd like to add a maxExtractLength parameter that can be used to set the maximum number of characters to extract from a binary. The default value of this parameter could be something like ten times the maxFieldLength setting.
0,JSR 283: Retention & Hold Management
1,"IndexReader.open(String|File) may incorrectly throw AlreadyClosedExceptionSpinoff from here:

    http://www.nabble.com/Runtime-exception-when-creating-IndexSearcher-to20226279.html

If you open an IndexSearcher/Reader, passing in String or File, then
closeDirectory is set to true in the reader.

If the index has a single segment, then SegmentReader.get is used to
open the index.  If an IOException is hit in there, the SegmentReader
closes itself and then closes the directory since closeDirectory is
true.

The problem is, the retry logic in SegmentInfos (to look for another
segments_N to try) kicks in and hits an AlreadyClosedException,
masking the original root cause.

Workaround is to separately get the Directory using
FSDirectory.getDirectory, and then instantiate IndexSearcher/Reader
from that.

This manifests as masking the root cause of a corrupted single-segment
index with a confusing AlreadyClosedException.  You could also hit
the false exception if the writer was in the process of committing
(ie, a retry was really needed) or if there is some transient IO
problem opening the index (eg too many open files).
"
0,"deprecate Directory.renameFile()Copied from my mailing list post so this issue can be tracked (if necessary). I will commit a patch.

I see that Directory.renameFile() isn't used anymore. I assume it has only 
been public for technical reasons, not because we expect this to be used 
from outside of Lucene? Should we deprecate this method? Its 
implementation e.g. in FSDirectory looks a bit scary anyway (the comment 
correctly says ""This is not atomic"" while the abstract class says ""This 
replacement should be atomic"").
"
1,"Incorrect sort by Numeric values for documents missing the sorting fieldWhile sorting results over a numeric field, documents which do not contain a value for the sorting field seem to get 0 (ZERO) value in the sort. (Tested against Double, Float, Int & Long numeric fields ascending and descending order).
This behavior is unexpected, as zero is ""comparable"" to the rest of the values. A better solution would either be allowing the user to define such a ""non-value"" default, or always bring those document results as the last ones.

Example scenario:
Adding 3 documents, 1st with value 3.5d, 2nd with -10d, and 3rd without any value.
Searching with MatchAllDocsQuery, with sort over that field in descending order yields the docid results of 0, 2, 1.

Asking for the top 2 documents brings the document without any value as the 2nd result - which seems as a bug?"
0,"Use of google-code-prettify for Lucene/Solr JavadocMy company, RONDHUIT uses google-code-prettify (Apache License 2.0) in Javadoc for syntax highlighting:

http://www.rondhuit-demo.com/RCSS/api/com/rondhuit/solr/analysis/JaReadingSynonymFilterFactory.html

I think we can use it for Lucene javadoc (java sample code in overview.html etc) and Solr javadoc (Analyzer Factories etc) to improve or simplify our life."
0,Add Amazon S3 authentication supportAdd support for the the Amazon S3 authentication scheme as defined by the online document: http://docs.amazonwebservices.com/AmazonS3/latest/index.html?RESTAuthentication.html
0,"Subclasses do not have write access to StatusLineHttpMethodBase provides the readStatusLine method explicitly designed for
subclasses to override. However, any attempt to do so quickly encounters issues
since the subclass does not have access to the statusLine member variable in
HttpMethodBase. The same holds true for several other member variables as well.

Recommend that all access to member variables occur through accessors and that
mutators be provided to set them. See patch below.
----------------------------------------------------------

Index: HttpMethodBase.java
===================================================================
--- HttpMethodBase.java	(revision 390815)
+++ HttpMethodBase.java	(working copy)
@@ -563,7 +563,7 @@
      * @return the status code associated with the latest response.
      */
     public int getStatusCode() {
-        return statusLine.getStatusCode();
+        return getStatusLine().getStatusCode();
     }
 
     /**
@@ -577,6 +577,13 @@
     }
 
     /**
+     * @param statusLine The statusLine to set.
+     */
+    protected final void setStatusLine(StatusLine statusLine) {
+        this.statusLine = statusLine;
+    }
+
+    /**
      * Checks if response data is available.
      * @return <tt>true</tt> if response data is available, <tt>false</tt>
otherwise.
      */
@@ -798,7 +805,7 @@
      * @return The status text.
      */
     public String getStatusText() {
-        return statusLine.getReasonPhrase();
+        return getStatusLine().getReasonPhrase();
     }
 
     /**
@@ -920,16 +927,16 @@
         }
         LOG.debug(""Resorting to protocol version default close connection policy"");
         // missing or invalid connection header, do the default
-        if (this.effectiveVersion.greaterEquals(HttpVersion.HTTP_1_1)) {
+        if (getEffectiveVersion().greaterEquals(HttpVersion.HTTP_1_1)) {
             if (LOG.isDebugEnabled()) {
-                LOG.debug(""Should NOT close connection, using "" +
this.effectiveVersion.toString());
+                LOG.debug(""Should NOT close connection, using "" +
getEffectiveVersion().toString());
             }
         } else {
             if (LOG.isDebugEnabled()) {
-                LOG.debug(""Should close connection, using "" +
this.effectiveVersion.toString());
+                LOG.debug(""Should close connection, using "" +
getEffectiveVersion().toString());
             }
         }
-        return this.effectiveVersion.lessEquals(HttpVersion.HTTP_1_0);
+        return getEffectiveVersion().lessEquals(HttpVersion.HTTP_1_0);
     }
     
     /**
@@ -980,14 +987,14 @@
         this.responseConnection = conn;
 
         checkExecuteConditions(state, conn);
-        this.statusLine = null;
+        setStatusLine(null);
         this.connectionCloseForced = false;
 
         conn.setLastResponseInputStream(null);
 
         // determine the effective protocol version
-        if (this.effectiveVersion == null) {
-            this.effectiveVersion = this.params.getVersion(); 
+        if (getEffectiveVersion() == null) {
+            setEffectiveVersion(this.params.getVersion()); 
         }
 
         writeRequest(state, conn);
@@ -996,7 +1003,7 @@
         // the method has successfully executed
         used = true; 
 
-        return statusLine.getStatusCode();
+        return getStatusCode();
     }
 
     /**
@@ -1048,8 +1055,8 @@
         getRequestHeaderGroup().clear();
         getResponseHeaderGroup().clear();
         getResponseTrailerHeaderGroup().clear();
-        statusLine = null;
-        effectiveVersion = null;
+        setStatusLine(null);
+        setEffectiveVersion(null);
         aborted = false;
         used = false;
         params = new HttpMethodParams();
@@ -1586,18 +1593,18 @@
         ""enter HttpMethodBase.readResponse(HttpState, HttpConnection)"");
         // Status line & line may have already been received
         // if 'expect - continue' handshake has been used
-        while (this.statusLine == null) {
+        while (getStatusLine() == null) {
             readStatusLine(state, conn);
             processStatusLine(state, conn);
             readResponseHeaders(state, conn);
             processResponseHeaders(state, conn);
             
-            int status = this.statusLine.getStatusCode();
+            int status = getStatusCode(); 
             if ((status >= 100) && (status < 200)) {
                 if (LOG.isInfoEnabled()) {
-                    LOG.info(""Discarding unexpected response: "" +
this.statusLine.toString()); 
+                    LOG.info(""Discarding unexpected response: "" +
getStatusLine().toString()); 
                 }
-                this.statusLine = null;
+                setStatusLine(null);
             }
         }
         readResponseBody(state, conn);
@@ -1675,7 +1682,7 @@
         if (Wire.CONTENT_WIRE.enabled()) {
             is = new WireLogInputStream(is, Wire.CONTENT_WIRE);
         }
-        boolean canHaveBody = canResponseHaveBody(statusLine.getStatusCode());
+        boolean canHaveBody = canResponseHaveBody(getStatusCode());
         InputStream result = null;
         Header transferEncodingHeader =
responseHeaders.getFirstHeader(""Transfer-Encoding"");
         // We use Transfer-Encoding if present and ignore Content-Length.
@@ -1714,7 +1721,7 @@
         } else {
             long expectedLength = getResponseContentLength();
             if (expectedLength == -1) {
-                if (canHaveBody &&
this.effectiveVersion.greaterEquals(HttpVersion.HTTP_1_1)) {
+                if (canHaveBody &&
getEffectiveVersion().greaterEquals(HttpVersion.HTTP_1_1)) {
                     Header connectionHeader =
responseHeaders.getFirstHeader(""Connection"");
                     String connectionDirective = null;
                     if (connectionHeader != null) {
@@ -1850,19 +1857,19 @@
         } while(true);
 
         //create the status line from the status string
-        statusLine = new StatusLine(s);
+        setStatusLine(new StatusLine(s));
 
         //check for a valid HTTP-Version
-        String versionStr = statusLine.getHttpVersion();
+        String versionStr = getStatusLine().getHttpVersion();
         if (getParams().isParameterFalse(HttpMethodParams.UNAMBIGUOUS_STATUS_LINE) 
            && versionStr.equals(""HTTP"")) {
             getParams().setVersion(HttpVersion.HTTP_1_0);
             if (LOG.isWarnEnabled()) {
                 LOG.warn(""Ambiguous status line (HTTP protocol version missing):"" +
-                statusLine.toString());
+                getStatusLine().toString());
             }
         } else {
-            this.effectiveVersion = HttpVersion.parse(versionStr);
+            setEffectiveVersion(HttpVersion.parse(versionStr));
         }
 
     }
@@ -1943,9 +1950,9 @@
                     readResponseHeaders(state, conn);
                     processResponseHeaders(state, conn);
 
-                    if (this.statusLine.getStatusCode() ==
HttpStatus.SC_CONTINUE) {
+                    if (getStatusCode() == HttpStatus.SC_CONTINUE) {
                         // Discard status line
-                        this.statusLine = null;
+                        setStatusLine(null);
                         LOG.debug(""OK to continue received"");
                     } else {
                         return;
@@ -2087,7 +2094,7 @@
      */
     private String getRequestLine(HttpConnection conn) {
         return  HttpMethodBase.generateRequestLine(conn, getName(),
-                getPath(), getQueryString(), this.effectiveVersion.toString());
+                getPath(), getQueryString(), getEffectiveVersion().toString());
     }
 
     /**
@@ -2128,6 +2135,13 @@
     }
 
     /**
+     * @param effectiveVersion The effectiveVersion to set.
+     */
+    protected final void setEffectiveVersion(HttpVersion effectiveVersion) {
+        this.effectiveVersion = effectiveVersion;
+    }
+
+    /**
      * Per RFC 2616 section 4.3, some response can never contain a message
      * body.
      *
@@ -2358,7 +2372,7 @@
     ) {
         // set used so that the response can be read
         this.used = true;
-        this.statusLine = statusline;
+        setStatusLine(statusline);
         this.responseHeaders = responseheaders;
         this.responseBody = null;
         this.responseStream = responseStream;"
0,"Refactor Searchable to not have RMI Remote dependencyPer http://lucene.markmail.org/message/fu34tuomnqejchfj?q=RemoteSearchable

We should refactor Searchable slightly so that it doesn't extend the java.rmi.Remote marker interface.  I believe the same could be achieved by just marking the RemoteSearchable and refactoring the RMI implementation out of core and into a contrib.

If we do this, we should deprecate/denote it for 2.9 and then move it for 3.0"
0,spi2dav: create RepositoryFactory implementation
0,"add LuceneTestCase.newSearcher()Most tests in the search package don't care about what kind of searcher they use.

we should randomly use MultiSearcher or ParallelMultiSearcher sometimes in tests."
1,"Using MultiSearcher and ParallelMultiSearcher can change the sort order.When using multiple sort criteria the first criterium that indicates a difference should be used.
When a field does not exist for a given document, special rules apply.
From what I see in the code, it is sorted as 0 for integer and float fields, and null Strings are sorted before others.

This works correctly in both Lucene 1.4.3 and in trunk as long as you use a single IndexSearcher (except perhaps in special cases, see other bug reports like LUCENE-374).

However, in MultiSearcher and ParallelMultiSearcher, the results of the separate IndexSearchers are merged and there an error occurs.
The bug is located in FieldDocSortedHitQueue.

It can even be demonstrated by passing a single indexSearcher to a MultiSearcher.

TestCase and patch follow."
1,FilteredQuery ignores boostFiltered query ignores it's own boost.
0,"some legal jcr names cause unneccessary server-roundtrips assume the following legal qualified jcr names:

""{foo}""
""{foo} bar""

when items with such names are read from the spi layer, they are first interpreted as expanded form names.
a prefix lookup for namespace 'foo' fails and the name is treated as qualified jcr name.

=> depending on the spi implementation, a server-roundtrip is required in order to determine that 'foo' is not a
registered namespace. "
1,"Registering nodetypes with empty namespace prefix causes a namespace exception in sync nodeRegistering a nodetype with empty namespace prefix causes a namespace exception in sync node. Stacktrace looks as follows:

03.03.2008 15:33:50 *ERROR* ClusterNode: Unable to read revision '10618'. (ClusterNode.java, line 1051)
o.a.j.core.journal.JournalException: Parse error while reading node type definition.
        at o.a.j.core.journal.AbstractRecord.readNodeTypeDef(AbstractRecord.java:256)
        at o.a.j.core.cluster.ClusterNode.consume(ClusterNode.java:1026)
        at o.a.j.core.journal.AbstractJournal.doSync(AbstractJournal.java:198)
        at o.a.j.core.journal.AbstractJournal.sync(AbstractJournal.java:173)
        at o.a.j.core.cluster.ClusterNode.sync(ClusterNode.java:303)
        at o.a.j.core.cluster.ClusterNode.run(ClusterNode.java:274)
        at java.lang.Thread.run(Thread.java:595)
Caused by: o.a.j.core.nodetype.compact.ParseException: Error while parsing 'bla' ((internal), line 3)
        at o.a.j.core.nodetype.compact.Lexer.fail(Lexer.java:152)
        at o.a.j.core.nodetype.compact.CompactNodeTypeDefReader.toQName(CompactNodeTypeDefReader.java:653)
        at o.a.j.core.nodetype.compact.CompactNodeTypeDefReader.doNodeTypeName(CompactNodeTypeDefReader.java:265)
        at o.a.j.core.nodetype.compact.CompactNodeTypeDefReader.parse(CompactNodeTypeDefReader.java:215)
        at o.a.j.core.nodetype.compact.CompactNodeTypeDefReader.<init>(CompactNodeTypeDefReader.java:178)
        at o.a.j.core.nodetype.compact.CompactNodeTypeDefReader.<init>(CompactNodeTypeDefReader.java:162)
        at o.a.j.core.journal.AbstractRecord.readNodeTypeDef(AbstractRecord.java:248)
        ... 6 more
Caused by: javax.jcr.NamespaceException: No URI for pefix '' declared.
        at o.a.j.spi.commons.namespace.NamespaceMapping.getURI(NamespaceMapping.java:74)
        at o.a.j.spi.commons.conversion.NameParser.parse(NameParser.java:116)
        at o.a.j.spi.commons.conversion.ParsingNameResolver.getQName(ParsingNameResolver.java:62)
        at o.a.j.spi.commons.conversion.DefaultNamePathResolver.getQName(DefaultNamePathResolver.java:61)
        at o.a.j.core.nodetype.compact.CompactNodeTypeDefReader.toQName(CompactNodeTypeDefReader.java:646)
        ... 11 more

"
0,Improve NodeTypeRegistry.effectiveNodeType()The current getEffectiveNodeType() implementation has a minor bug that prevents from proper caching for certain nodetype combinations. further performance enhancements can be made to the effective node type cache.
0,"SQL2 parser may infer type for UncastLiteral from static analysisThe spec says:

""An UncastLiteral is always interpreted as a Value of property type STRING. A CastLiteral, on the other hand, is interpreted as the string form of a Value of the PropertyType indicated.""

There are also two test cases in NodeNameTest that need to be fixed accordingly: testLongLiteral and testBooleanLiteral
"
0,"Setup nightly build website links and docsPer discussion on mailing list, we are going to setup a Nightly Build link on the website linking to the docs (and javadocs) generated by the nightly build process.  The build process may need to be modified to complete this task.

Going forward, the main website will, for the most part, only be updated per releases (I imagine exceptions will be made for News items and per committer's discretion).  The Javadocs linked to from the main website will always be for the latest release."
0,"Logger (Category) names don't follow common patternThe Wire class uses two loggers named unexpected. The ""org.apache.commons.""
prefix is missing - so you can't mute all debug level statements with a
one-liner in you log4j.properties for example:

  log4j.logger.org.apache.commons.httpclient INFO

You have to add this, too:

  log4j.logger.httpclient INFO

Please prepend the ""org.apache.commons."" before both names.

Cheers,
Christian

<code>
class Wire {

    public static Wire HEADER_WIRE = new
Wire(LogFactory.getLog(""httpclient.wire.header""));
    
    public static Wire CONTENT_WIRE = new
Wire(LogFactory.getLog(""httpclient.wire.content""));

</code>

http://svn.apache.org/viewcvs.cgi/jakarta/commons/proper/httpclient/trunk/src/java/org/apache/commons/httpclient/Wire.java?rev=155418&view=markup"
0,"Move QueryParsers from contrib/queryparser to queryparser moduleEach of the QueryParsers will be ported across.

Those which use the flexible parsing framework will be placed under the package flexible.  The StandardQueryParser will be renamed to FlexibleQueryParser and surround.QueryParser will be renamed to SurroundQueryParser."
0,"Synchronization bottleneck in FieldSortedHitQueue with many concurrent readersThe below is from a post by (my colleague) Paul Smith to the java-users list:

---

Hi ho peoples.

We have an application that is internationalized, and stores data from many languages (each project has it's own index, mostly aligned with a single language, maybe 2).

Anyway, I've noticed during some thread dumps diagnosing some performance issues, that there appears to be a _potential_ synchronization bottleneck using Locale-based sorting of Strings.  I don't think this problem is the root cause of our performance problem, but I thought I'd mention it here.  Here's the stack dump of a thread waiting:

""http-1001-Processor245"" daemon prio=1 tid=0x31434da0 nid=0x3744 waiting for monitor entry [0x2cd44000..0x2cd45f30]
        at java.text.RuleBasedCollator.compare(RuleBasedCollator.java)
        - waiting to lock <0x6b1e8c68> (a java.text.RuleBasedCollator)
        at org.apache.lucene.search.FieldSortedHitQueue$4.compare(FieldSortedHitQueue.java:320)
        at org.apache.lucene.search.FieldSortedHitQueue.lessThan(FieldSortedHitQueue.java:114)
        at org.apache.lucene.util.PriorityQueue.upHeap(PriorityQueue.java:120)
        at org.apache.lucene.util.PriorityQueue.put(PriorityQueue.java:47)
        at org.apache.lucene.util.PriorityQueue.insert(PriorityQueue.java:58)
        at org.apache.lucene.search.FieldSortedHitQueue.insert(FieldSortedHitQueue.java:90)
        at org.apache.lucene.search.FieldSortedHitQueue.insert(FieldSortedHitQueue.java:97)
        at org.apache.lucene.search.TopFieldDocCollector.collect(TopFieldDocCollector.java:47)
        at org.apache.lucene.search.BooleanScorer2.score(BooleanScorer2.java:291)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:132)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:110)
        at com.aconex.index.search.FastLocaleSortIndexSearcher.search(FastLocaleSortIndexSearcher.java:90)
.....

In our case we had 12 threads waiting like this, while one thread had the lock on the RuleBasedCollator.  Turns out RuleBasedCollator's.compare(...) method is synchronized.  I wonder if a ThreadLocal based collator would be better here... ?  There doesn't appear to be a reason for other threads searching the same index to wait on this sort.  Be just as easy to use their own.  (Is RuleBasedCollator a ""heavy"" object memory wise?  Wouldn't have thought so, per thread)

Thoughts?

---

I've investigated this somewhat, and agree that this is a potential problem with a series of possible workarounds. Further discussion (including proof-of-concept patch) to follow."
0,"FilteredQuery should have getFilter()Unless you are in the same package, you can't access the filter in a FilteredQuery.
A getFilter() method should be added."
0,"Use correct version number in repository descriptorThe repository descriptor 'jcr.repository.version' always shows 1.0-dev.

The value should reflect the current jackrabbit version."
0,"JavaDoc getConnection methods in Connection ManagersThe JavaDoc for the getConnection() methods in the Simple and MultiThreaded
Connection managers is taken from the interface, and so is too generic.

The Javadoc for the doGetConnection() method in the MultiThreaded manager is
fine, but is not visible in the JavaDoc

The Simple Mangager JavaDoc could likewise be improved

[I hope to provide patches shortly]"
0,"Drop Maven 1 compatibilityWe migrated from Maven 1 to Maven 2 as the build system in Jackrabbit 1.2, but we kept compatibility with related Maven 1 build with the maven-one-plugin that deployed all builds also to the local Maven 1 repository.

Hardly any downstream project uses Maven 1 anymore, so it's safe for us to simplify our build now by dropping the use of the maven-one-plugin."
0,"Changes.html should be visible to users for closed releasesChanges.html is currently available only in the dev page, for trunk. 
See LUCENE-1157 for discussion on where exactly to expose this."
1,Incorrect results from joins on multivalued propertiesIt looks like join conditions on multivalued properties only use one of the multiple values for the comparison.
1,"Custom LoginModule configurations broken in 1.5.0Upgrading Jackrabbit from 1.4.5 to 1.5 has created an LDAP exception.  The configuration file which has not changed (except for the adding the new SimpleSecurityManager as required) is the default with the following substituted for the LoginModule:

        <LoginModule class=""com.sun.security.auth.module.LdapLoginModule"">
            <param name=""userProvider"" value=""ldap://localhost/ou=people,dc=example,dc=com"" />
            <param name=""userFilter"" value=""(&amp;(uid={USERNAME})(objectClass=inetOrgPerson))"" />
            <param name=""authzIdentity"" value=""{USERNAME}"" />
            <param name=""debug"" value=""true"" />
        </LoginModule>

This configuration worked correctly and I was able to authenticate properly with Jackrabbit 1.4.5
The same configuration with 1.5 throws the following exception:

javax.jcr.LoginException: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider
        at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1414)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.openSession(JCAManagedConnectionFactory.java:140)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createManagedConnection(JCAManagedConnectionFactory.java:176)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createManagedConnection(JCAManagedConnectionFactory.java:168)
        at com.sun.enterprise.resource.ConnectorAllocator.createResource(ConnectorAllocator.java:136)
        at com.sun.enterprise.resource.AbstractResourcePool.createSingleResource(AbstractResourcePool.java:891)
        at com.sun.enterprise.resource.AbstractResourcePool.createResourceAndAddToPool(AbstractResourcePool.java:1752)
        at com.sun.enterprise.resource.AbstractResourcePool.createResources(AbstractResourcePool.java:917)
        at com.sun.enterprise.resource.AbstractResourcePool.initPool(AbstractResourcePool.java:225)
        at com.sun.enterprise.resource.AbstractResourcePool.internalGetResource(AbstractResourcePool.java:516)
        at com.sun.enterprise.resource.AbstractResourcePool.getResource(AbstractResourcePool.java:443)
        at com.sun.enterprise.resource.PoolManagerImpl.getResourceFromPool(PoolManagerImpl.java:248)
        at com.sun.enterprise.resource.PoolManagerImpl.getResource(PoolManagerImpl.java:176)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.internalGetConnection(ConnectionManagerImpl.java:337)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.allocateConnection(ConnectionManagerImpl.java:189)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.allocateConnection(ConnectionManagerImpl.java:165)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.allocateConnection(ConnectionManagerImpl.java:158)
        at org.apache.jackrabbit.jca.JCARepositoryHandle.login(JCARepositoryHandle.java:98)
        at org.apache.jackrabbit.jca.JCARepositoryHandle.login(JCARepositoryHandle.java:89)
        at org.apache.jackrabbit.jca.JCARepositoryHandle.login(JCARepositoryHandle.java:73)
        at com.threesl.Sapphire.CradleJCR.login(CradleJCR.java:44)   

 try {
            InitialContext ctx = new InitialContext();
            repository = (Repository) ctx.lookup(""jcr/repository"");
            session = repository.login(credentials);
        } catch (Exception e) {

        at com.threesl.Sapphire.CradleWS.doLogin(CradleWS.java:68)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jersey.impl.model.method.dispatch.EntityParamDispatchProvider$TypeOutInvoker._dispatch(EntityParamDispatchProvider.java:136)
        at com.sun.jersey.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:85)
        at com.sun.jersey.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:123)
        at com.sun.jersey.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:111)
        at com.sun.jersey.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:71)
        at com.sun.jersey.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:111)
        at com.sun.jersey.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:63)
        at com.sun.jersey.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:722)
        at com.sun.jersey.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:692)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:344)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:831)
        at org.apache.catalina.core.ApplicationFilterChain.servletService(ApplicationFilterChain.java:411)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:290)
        at org.apache.catalina.core.StandardContextValve.invokeInternal(StandardContextValve.java:271)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:202)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:632)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:577)
        at com.sun.enterprise.web.WebPipeline.invoke(WebPipeline.java:94)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:206)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:632)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:577)
        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:571)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:1080)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:150)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:632)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:577)
        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:571)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:1080)
        at org.apache.coyote.tomcat5.CoyoteAdapter.service(CoyoteAdapter.java:272)
        at com.sun.enterprise.web.connector.grizzly.DefaultProcessorTask.invokeAdapter(DefaultProcessorTask.java:637)
        at com.sun.enterprise.web.connector.grizzly.DefaultProcessorTask.doProcess(DefaultProcessorTask.java:568)
        at com.sun.enterprise.web.connector.grizzly.DefaultProcessorTask.process(DefaultProcessorTask.java:813)
        at com.sun.enterprise.web.connector.grizzly.DefaultReadTask.executeProcessorTask(DefaultReadTask.java:341)
        at com.sun.enterprise.web.connector.grizzly.DefaultReadTask.doTask(DefaultReadTask.java:263)
        at com.sun.enterprise.web.connector.grizzly.DefaultReadTask.doTask(DefaultReadTask.java:214)
        at com.sun.enterprise.web.connector.grizzly.TaskBase.run(TaskBase.java:265)
        at com.sun.enterprise.web.connector.grizzly.ssl.SSLWorkerThread.run(SSLWorkerThread.java:106)
Caused by: javax.security.auth.login.LoginException: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider
        at org.apache.jackrabbit.core.security.authentication.LocalAuthContext.login(LocalAuthContext.java:68)
        at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1407)
        ... 62 more
javax.security.auth.login.LoginException: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider
        at org.apache.jackrabbit.core.security.authentication.LocalAuthContext.login(LocalAuthContext.java:68)
        at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1407)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.openSession(JCAManagedConnectionFactory.java:140)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createManagedConnection(JCAManagedConnectionFactory.java:176)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createManagedConnection(JCAManagedConnectionFactory.java:168)
        at com.sun.enterprise.resource.ConnectorAllocator.createResource(ConnectorAllocator.java:136)
        at com.sun.enterprise.resource.AbstractResourcePool.createSingleResource(AbstractResourcePool.java:891)
        at com.sun.enterprise.resource.AbstractResourcePool.createResourceAndAddToPool(AbstractResourcePool.java:1752)
        at com.sun.enterprise.resource.AbstractResourcePool.createResources(AbstractResourcePool.java:917)
        at com.sun.enterprise.resource.AbstractResourcePool.initPool(AbstractResourcePool.java:225)
        at com.sun.enterprise.resource.AbstractResourcePool.internalGetResource(AbstractResourcePool.java:516)
        at com.sun.enterprise.resource.AbstractResourcePool.getResource(AbstractResourcePool.java:443)
        at com.sun.enterprise.resource.PoolManagerImpl.getResourceFromPool(PoolManagerImpl.java:248)
        at com.sun.enterprise.resource.PoolManagerImpl.getResource(PoolManagerImpl.java:176)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.internalGetConnection(ConnectionManagerImpl.java:337)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.allocateConnection(ConnectionManagerImpl.java:189)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.allocateConnection(ConnectionManagerImpl.java:165)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.allocateConnection(ConnectionManagerImpl.java:158)
        at org.apache.jackrabbit.jca.JCARepositoryHandle.login(JCARepositoryHandle.java:98)
        at org.apache.jackrabbit.jca.JCARepositoryHandle.login(JCARepositoryHandle.java:89)
        at org.apache.jackrabbit.jca.JCARepositoryHandle.login(JCARepositoryHandle.java:73)
        at com.threesl.Sapphire.CradleJCR.login(CradleJCR.java:44)
        at com.threesl.Sapphire.CradleWS.doLogin(CradleWS.java:68)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jersey.impl.model.method.dispatch.EntityParamDispatchProvider$TypeOutInvoker._dispatch(EntityParamDispatchProvider.java:136)
        at com.sun.jersey.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:85)
        at com.sun.jersey.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:123)
        at com.sun.jersey.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:111)
        at com.sun.jersey.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:71)
        at com.sun.jersey.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:111)
        at com.sun.jersey.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:63)
        at com.sun.jersey.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:722)
        at com.sun.jersey.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:692)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:344)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:831)
        at org.apache.catalina.core.ApplicationFilterChain.servletService(ApplicationFilterChain.java:411)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:290)
        at org.apache.catalina.core.StandardContextValve.invokeInternal(StandardContextValve.java:271)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:202)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:632)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:577)
        at com.sun.enterprise.web.WebPipeline.invoke(WebPipeline.java:94)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:206)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:632)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:577)
        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:571)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:1080)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:150)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:632)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:577)
        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:571)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:1080)
        at org.apache.coyote.tomcat5.CoyoteAdapter.service(CoyoteAdapter.java:272)
        at com.sun.enterprise.web.connector.grizzly.DefaultProcessorTask.invokeAdapter(DefaultProcessorTask.java:637)
        at com.sun.enterprise.web.connector.grizzly.DefaultProcessorTask.doProcess(DefaultProcessorTask.java:568)
        at com.sun.enterprise.web.connector.grizzly.DefaultProcessorTask.process(DefaultProcessorTask.java:813)
        at com.sun.enterprise.web.connector.grizzly.DefaultReadTask.executeProcessorTask(DefaultReadTask.java:341)
        at com.sun.enterprise.web.connector.grizzly.DefaultReadTask.doTask(DefaultReadTask.java:263)
        at com.sun.enterprise.web.connector.grizzly.DefaultReadTask.doTask(DefaultReadTask.java:214)
        at com.sun.enterprise.web.connector.grizzly.TaskBase.run(TaskBase.java:265)
        at com.sun.enterprise.web.connector.grizzly.ssl.SSLWorkerThread.run(SSLWorkerThread.java:106)
RAR5117 : Failed to obtain/create connection from connection pool [ jackrabbit-connection-pool ]. Reason : Failed to create session: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider

"
0,"Log level for message should be debug instead of error.In method org.apache.commons.httpclient.HttpMethodBase.getResponseBody() Log
message should be logged as debug instead of error. 

717             } catch (IOException e) {
718                 LOG.error(""I/O failure reading response body"", e);
719                 this.responseBody = null;
720             }

According to HTTPCLIENT-57:
2) Only/always log exception stack traces at the debug level
        } catch (Exception ex) {
            log.debug"
0,"""System Properties"" doc lists ""lockDir"" instead of ""lockdir""The ""System Properties"" documentation page states that the lock file directory
can be set with the system property ""org.apache.lucene.lockDir"".  However, as
implemented in org.apache.lucene.store.FSDirectory, line 56, the property name
is actually ""org.apache.lucene.lockdir"" (lower case ""d"" in ""lockdir""). 
Recommend changing documentation to match code."
0,"SetPropertyAssumeTypeTest check for non-protected string array propertySetPropertyAssumeTypeTest.testValuesConstraintViolationExceptionBecauseOfInvalidTypeParameter tries to find a property definition for a writable, multivalued string property. It consults NodeTypeUtil.locatePropertyDef() for that purpose.

In my setup, the property definition being returned is for jcr:valueConstraints, defined on nt:propertyDefinition. Nodes of that type in turn can not be created on the test node, thus the test fails already when trying to create the node.

It seems the test suite tries to be too smart here. Can we change this so that the node type and the property name are configuration parameters?"
0,"Change contrib/spatial to use trie's NumericUtils, and remove NumberUtilsCurrently spatial contrib includes a copy of NumberUtils from solr (otherwise it would depend on solr)

Once LUCENE-1496 is sorted out, this copy should be removed."
0,"move HttpRoute and related classes to separate packageThe route-related stuff in o.a.h.conn is detached from the rest of the connection management API.
Move HttpRoute, RouteTracker, HttpRouteDirector, HttpRoutePlanner to o.a.h.conn.route or ...routing.
Implementation classes have a dependency on Scheme and SchemeRegistry in o.a.h.conn,
but that does not introduce a recursive dependency between packages.
"
0,"spi2dav : EventJournal not  implementedi didn't look at the details just realized that all EventJournalTest of the TCK fail in the setup
jcr2spi - spi2dav(ex) - jcr-server.
i assume that this is due to missing implementation (the corresponding SPI method throws UnsupportedRepositoryOperationException)."
0,"[PATCH] Use entrySet iterators to avoid map look ups in loopsCode uses a keySet iterator in a loop, then does a map look up using the key retrieved from the iterator. 

Might as well use an entrySet iterator to avoid n map lookups.

Patch does this."
1,"AccessManager + CachingHierarchyManager problemThe problem we have is the implementation of the CachingHierarchyManager,
to which the SimpleAccessManager holds a reference.

Let's consider following example:
i add 3 subnodes (a,b,c) to a node and after that i reorder b and c ..
so i have a,c,b. in the process of reordering (using the function
orderBefore of javax.jcr.Node) our AccessManager is called several times to check the permissions of the nodes. In this AccessManager we use some
functions of the CachingHierarchyManager, f.ex.

Path itemPath = hierMgr.getPath(id);
return itemPath.denotesRoot();

or

Path itemPath = hierMgr.getPath(itemId);
Path parentPath = itemPath.getAncestor(1);
return hierMgr.resolvePath(parentPath);

the problem is, that when calling the methods of the
CachingHierarchyManager the nodes i ask for will be cached in the idCache in a wrong state (i. e.: before actually reordering the elements).
so if i want f.ex. delete the node b after reordering, the node will
be looked up in the idCache. in the cache the index of node b is still 2
(actually it should be 3) and so the wrong node will be deleted! "
0,"read/write .del as d-gaps when the deleted bit vector is sufficiently sparse.del file of a segment maintains info on deleted documents in that segment. The file exists only for segments having deleted docs, so it does not exists for newly created segments (e.g. resulted from merge). Each time closing an index reader that deleted any document, the .del file is rewritten. In fact, since the lock-less commits change a new (generation of) .del file is created in each such occasion.

For small indexes there is no real problem with current situation. But for very large indexes, each time such an index reader is closed, creating such new bit-vector seems like unnecessary overhead in cases that the bit vector is sparse (just a few docs were deleted). For instance, for an index with a segment of 1M docs, the sequence: {open reader; delete 1 doc from that segment; close reader;} would write a file of ~128KB. Repeat this sequence 8 times: 8 new files of total size of 1MB are written to disk.

Whether this is a bottleneck or not depends on the application deletes pattern, but for the case that deleted docs are sparse, writing just the d-gaps would save space and time. 

I have this (simple) change to BitVector running and currently trying some performance tests to, yet, convince myself on the worthiness of this.

"
1,"IndexWriter does not release its write lock when trying to open an index which does not yet existIn version 2.0.0, the private IndexWriter constructor does not properly remove its write lock in the event of an error. This can be seen when one attempts to open (not create) an index in a directory which exists, but in which there is no segments file. Here is the offending code:

    247   private IndexWriter(Directory d, Analyzer a, final boolean create, boolean closeDir)
    248     throws IOException {
    249       this.closeDir = closeDir;
    250       directory = d;
    251       analyzer = a;
    252 
    253       Lock writeLock = directory.makeLock(IndexWriter.WRITE_LOCK_NAME);
    254       if (!writeLock.obtain(writeLockTimeout)) // obtain write lock
    255         throw new IOException(""Index locked for write: "" + writeLock);
    256       this.writeLock = writeLock;                   // save it
    257 
    258       synchronized (directory) {        // in- & inter-process sync
    259         new Lock.With(directory.makeLock(IndexWriter.COMMIT_LOCK_NAME), commitLockTimeout) {
    260             public Object doBody() throws IOException {
    261               if (create)
    262                 segmentInfos.write(directory);
    263               else
    264                 segmentInfos.read(directory);
    265               return null;
    266             }
    267           }.run();
    268       }
    269   }

On line 254, a write lock is obtained by the constructor. If an exception is raised inside the doBody() method (on line 260), then that exception is propagated, the constructor will fail, but the lock is not released until the object is garbage collected. This is typically an issue except when using the IndexModifier class.

As of the filing of this bug, this has not yet been fixed in the trunk (IndexWriter.java#472959):

    251   private IndexWriter(Directory d, Analyzer a, final boolean create, boolean closeDir)
    252     throws IOException {
    253       this.closeDir = closeDir;
    254       directory = d;
    255       analyzer = a;
    256 
    257       Lock writeLock = directory.makeLock(IndexWriter.WRITE_LOCK_NAME);
    258       if (!writeLock.obtain(writeLockTimeout)) // obtain write lock
    259         throw new IOException(""Index locked for write: "" + writeLock);
    260       this.writeLock = writeLock;                   // save it
    261 
    262       synchronized (directory) {        // in- & inter-process sync
    263         new Lock.With(directory.makeLock(IndexWriter.COMMIT_LOCK_NAME), commitLockTimeout) {
    264             public Object doBody() throws IOException {
    265               if (create)
    266                 segmentInfos.write(directory);
    267               else
    268                 segmentInfos.read(directory);
    269               return null;
    270             }
    271           }.run();
    272       }
    273   }"
0,"Surround query languageThis is a copy of what I posted about a year ago. 
 
The whole thing is hereby licenced under the Apache Licence 2.0, 
copyright 2005 Apache Software Foundation. 
 
For inclusion in Lucene (sandbox perhaps?) it will need 
at least the following adaptations: 
- renaming of package names 
  (org.surround to somewhere org.apache.lucene ) 
- moves of the source files to corresponding directories 
 
Although it uses the identifier sncf in some places 
I'm not associated with French railroads, but I like the TGV. 
 
Regards, 
Paul Elschot"
1,"JCR2SPI: potential race condition in event listener registrationThere's a potential race condition when the first event listener is registered (ObservationManager.addEventListener). The observation manager should only start listening for events after the new SPI event filter has been created.

(Note there's a related problem when an *additional* event listener is getting registered, while a RepositoryService.getEvents call is already in progress).
"
0,"Move FunctionQuery, ValueSources and DocValues to Queries moduleHaving resolved the FunctionQuery sorting issue and moved the MutableValue classes, we can now move FunctionQuery, ValueSources and DocValues to a Queries module."
1,"CVE-2009-0026: Cross site scripting issues in webappSome of the jackrabbit-webapp forms don't properly escape user input when displaying it in the resulting HTML page. This leads to potential cross site scripting issues. For example:

    search.jsp?q=%25%22%3Cscript%3Ealert(1)%3C/script%3E
    swr.jsp?q=%25""<script>alert(1)</script>&swrnum=1

The CVE id for this issue is CVE-2009-0026. This issue was reported by the Red Hat Security Response Team."
0,"Helper Method to escape illegal XPath Search TermIf you try to perform a search like this

//element(*, nt:base)[jcr:contains(., 'test!')]

you get this exception

javax.jcr.RepositoryException: Exception building query: org.apache.jackrabbit.core.query.lucene.fulltext.ParseException: Encountered ""<EOF>"" at line 1, column 6.
"
0,"LengthFilter and other TokenFilters that skip tokens ignore relative positionIncrementSee for reference:
http://www.nabble.com/WordDelimiterFilter%2BLenghtFilter-results-in-termPosition%3D%3D-1-td16306788.html
and http://www.nabble.com/Lucene---Java-f24284.html

It seems that LengthFilter (at least) could produce a stream in which the first Token has a positionIncrement of 0, which make CheckIndex and Luke function ""Reconstruct&Edit"" to generate exception.

Should something be done to avoid this situation, or could the error be ignored (by allowing Term with a position of -1, and relaxing CheckIndex checks?)
"
1,"CacheBehaviour Observation brokenWhile trying to fix JCR-2293 I discovered that CacheBehaviour Observation is broken:

- HierarchyEventListener.onEvent ignores local event (despite the comment saying otherwise). Not sure which way it should be. However with local events being ignored, JCR-2293 will most probably also occur with CacheBehaviour Observation. 

- NodeEntryImpl.refresh(Event) does not set its child node entries to incomplete when a node/property was added.

- After tentatively fixing above issues, I discovered that NodeEntryImpl.refresh(Event) and my own event listener operate on different NodeEntryImpl and ChildNodeEntryImpl instances. That is, even though I set childNodeEntries.complete to false in NodeEntryImpl.refresh(Event), when my own event listener retrieves that node (entry), it gets a different instance which has childNodeEntries.complete still set to true.
"
1,"jcr-server should respect child node definition of jcr:contentWhen creating a new file, jcr:content defaults to nt:unstructured. This causes file creation to fail when the underlying persistent store (i.e. SPI implementation) does not support nt:unstructured for jcr:content. 

I suggest to check whether the underlying implementation provides its own node type for jcr:content and use that one. If not, default to nt:unstructured."
0,"FieldInfos should be read-only if loaded from diskCurrently FieldInfos create a private FieldNumberBiMap when they are loaded from a directory which is necessary due to some limitation we need to face with IW#addIndexes(Dir). If we add an index via a directory to an existing index field number can conflict with the global field numbers in the IW receiving the directories. Those field number conflicts will remain until those segments are merged and we stabilize again based on the IW global field numbers. Yet, we unnecessarily creating a BiMap here where we actually should enforce read-only semantics since nobody should modify this FieldInfos instance we loaded from the directory. If somebody needs to get a modifiable copy they should simply create a new one and all all FieldInfo instances to it.

"
0,PropertyTypeRegistry should also yield if property is multi-valuedCurrently only the PropertyType is available for a certain property name. In some cases it is also required to know if the property is single- or multi-valued.
0,"PostMethod Java doc refers to wrong section of RFC1945""The HTTP POST method is defined in section 9.5 of RFC1945"" should read ""The
HTTP POST method is defined in section 8.3 of RFC1945""

Change 9.5 to 8.3."
0,"Optimize concurrent queriesThere are a number of bottlenecks that prevent scalability of concurrent queries:

- Fake norms are created repeatedly because a new SearchIndex$CombinedIndexReader is created for each query. This prevents caching of fake norms on the level of the CombinedIndexReader. Creating fake norms for index readers that span multiple sub reader is inefficient and should be avoided. Like with other Jackrabbit specific queries, there should be one for TermQuery, which is aware of sub readers. Its weight should then create one scorer for each sub reader. This effectively reuses the fake norms on the sub reader.

- There should be a  UUID cache that maps document number to UUID. This is basically the inverse of the existing DocNumberCache. UUID lookup is regularly a bottleneck in the SegmentReader where the method document() is synchronized and does I/O.

- Queries often contain constraints that limit the result to nodes with a certain flag set to a literal. These constraints should be cached in the query handler."
1,"ArrayIndexOutOfBoundsException in NodeTypeDefDiffIt appears that the code for building diffs in child node definitions loops incorrectly, opening the possibility for an ArrayIndexOutOfBounds exception. The offending portion is in the ""buildChildNodeDefDiffs"" method:

<<
NodeDef[] cnda2 = newDef.getChildNodeDefs();
HashMap defs2 = new HashMap();
for (int i = 0; i < cnda1.length; i++) {
    defs2.put(cnda2[i].getId(), cnda2[i]);
}
>>

It seems like simply changing the length check to be cnda2 (as it is in ""buildPropDefsDiff"") would suffice."
0,Only load item definition when requiredSome item definitions are loaded when an item state is constructed. Whenever possible this should be delayed to a time when the definition is actually used.
0,FieldCache should support longs and doublesWould be nice if FieldCache supported longs and doubles
1,"BundleDbPersistenceManager.checkConsistency() only fixes inconsistency if consistencyFix is enabled in configurationThe method has a parameter that explicitly tells whether an inconsistency should be fixed, thus the configuration parameter should be ignored.

Suggested patch:

Index: BundleDbPersistenceManager.java
===================================================================
--- BundleDbPersistenceManager.java	(revision 648657)
+++ BundleDbPersistenceManager.java	(working copy)
@@ -864,7 +864,7 @@
         }
 
         // repair collected broken bundles
-        if (consistencyFix && !modifications.isEmpty()) {
+        if (fix && !modifications.isEmpty()) {
             log.info(name + "": Fixing "" + modifications.size() + "" inconsistent bundle(s)..."");
             Iterator iterator = modifications.iterator();
             while (iterator.hasNext()) {
"
0,"Remove support for pre-3.0 indexesWe should remove support for 2.x (and 1.9) indexes in 4.0. It seems that nothing can be done in 3x because there is no special code which handles 1.9, so we'll leave it there. This issue should cover:
# Remove the .zip indexes
# Remove the unnecessary code from SegmentInfo and SegmentInfos. Mike suggests we compare the version headers at the top of SegmentInfos, in 2.9.x vs 3.0.x, to see which ones can go.
# remove FORMAT_PRE from FieldInfos
# Remove old format from TermVectorsReader

If you know of other places where code can be removed, then please post a comment here.

I don't know when I'll have time to handle it, definitely not in the next few days. So if someone wants to take a stab at it, be my guest.
"
0,"Occasional IndexingQueueTest failuresEvery now and then, when doing a clean build of the latest jackrabbit trunk I see the following test failure in jackrabbit-core:

-------------------------------------------------------------------------------
Test set: org.apache.jackrabbit.core.query.lucene.TestAll
-------------------------------------------------------------------------------
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.665 sec <<< FAILURE!
testQueue(org.apache.jackrabbit.core.query.lucene.IndexingQueueTest)  Time elapsed: 1.654 sec  <<< FAILURE!
junit.framework.AssertionFailedError
        at junit.framework.Assert.fail(Assert.java:47)
        at junit.framework.Assert.assertTrue(Assert.java:20)
        at junit.framework.Assert.assertTrue(Assert.java:27)
        at org.apache.jackrabbit.core.query.lucene.IndexingQueueTest.testQueue(IndexingQueueTest.java:69)

Typically the problem disappears when I rebuild, but the test should still not have failed."
0,"JCRTest.java (First Steps example code) creates a StringValue with ""new""The JCRTest.java file described in the First Steps document (http://incubator.apache.org/jackrabbit/firststeps.html) on the jackrabbit incubator website contains a line that attempts to create a StringValue using new, rather than using the ValueFactory interface. This causes the code to fail to compile - perhaps an initiative test, but could be off-putting...

Simple fix is to swap the line:

 n.setProperty(""testprop"", new StringValue(""Hello, World.""));

to 

n.setProperty(""testprop"", session.getValueFactory().createValue(""Hello, World.""));

"
0,"implement PerFieldAnalyzerWrapper.getOffsetGapPerFieldAnalyzerWrapper does not delegates calls to getOffsetGap(Fieldable), instead it returns the default values from the implementation of Analyzer. (Similar to LUCENE-659 ""PerFieldAnalyzerWrapper fails to implement getPositionIncrementGap"")"
1,"NoSuchItemStateException on removing node (no versioning)I'm using jackrabbit 1.2.1
with no versioning
with a very simple SimpleAccessManager (this try to compute the path of the passed ItemId and verify permissions over that path)

when I remove a node (nt:file or nt:folder),  calling session.save() I obtains the exception reported below.

is it really a bug or am i wrong?
thanks

the following is the code I'm using to build the path
----------------------------------------- CODE START
public String getStringPath(ItemId id) throws ItemNotFoundException, RepositoryException, NoPrefixDeclaredException
	{
		String p = """";
		NamespaceResolver nsResolver = ((HierarchyManagerImpl) hierMgr).getNamespaceResolver();
		Path path = hierMgr.getPath(id);
		PathElement[] pe = path.getElements();
		for (int i = 0; i < pe.length; i++)
		{
			if (pe[i].denotesName())
				p += ""/"" + pe[i].toJCRName(nsResolver);
		}
		return p;
	}
----------------------------------------- CODE END


----------------------------------------- START
javax.jcr.ItemNotFoundException: failed to build path of d688e92f-26ae-4f7c-aba7-aaff1df62c2c: d688e92f-26ae-4f7c-aba7-aaff1df62c2c: d688e92f-26ae-4f7c-aba7-aaff1df62c2c
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:362)
	at org.apache.jackrabbit.core.CachingHierarchyManager.getPath(CachingHierarchyManager.java:224)
	at it.unict.faq.jackrabbit.SimpleAccessManager.getStringPath(SimpleAccessManager.java:238)
	at it.unict.faq.jackrabbit.SimpleAccessManager.controllo(SimpleAccessManager.java:215)
	at it.unict.faq.jackrabbit.SimpleAccessManager.isGranted(SimpleAccessManager.java:183)
	at org.apache.jackrabbit.core.ItemImpl.validateTransientItems(ItemImpl.java:645)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1162)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:821)
	at it.unict.faq.driver.manager.impl.DAO.JcrDAO.CancellaNodo(JcrDAO.java:638)
	at it.unict.faq.driver.manager.impl.DocumentServerManager.ds_del(DocumentServerManager.java:58)
	at elearn.portal.action.ds_del_portal.execute(ds_del_portal.java:28)
	at org.apache.struts.action.RequestProcessor.processActionPerform(RequestProcessor.java:421)
	at org.apache.struts.action.RequestProcessor.process(RequestProcessor.java:226)
	at org.apache.struts.action.ActionServlet.process(ActionServlet.java:1158)
	at org.apache.struts.action.ActionServlet.doPost(ActionServlet.java:415)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:709)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:252)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:173)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:264)
	at org.acegisecurity.intercept.web.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:107)
	at org.acegisecurity.intercept.web.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:72)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:110)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.providers.anonymous.AnonymousProcessingFilter.doFilter(AnonymousProcessingFilter.java:125)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.rememberme.RememberMeProcessingFilter.doFilter(RememberMeProcessingFilter.java:142)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.wrapper.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:81)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.AbstractProcessingFilter.doFilter(AbstractProcessingFilter.java:217)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.logout.LogoutFilter.doFilter(LogoutFilter.java:108)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.context.HttpSessionContextIntegrationFilter.doFilter(HttpSessionContextIntegrationFilter.java:193)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.util.FilterChainProxy.doFilter(FilterChainProxy.java:148)
	at org.acegisecurity.util.FilterToBeanProxy.doFilter(FilterToBeanProxy.java:98)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:202)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:173)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:213)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:178)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:126)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:105)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:107)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:148)
	at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:869)
	at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.processConnection(Http11BaseProtocol.java:664)
	at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpoint.java:527)
	at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFollowerWorkerThread.java:80)
	at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:684)
	at java.lang.Thread.run(Thread.java:595)
Caused by: org.apache.jackrabbit.core.state.NoSuchItemStateException: d688e92f-26ae-4f7c-aba7-aaff1df62c2c
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getTransientItemState(SessionItemStateManager.java:323)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:154)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getItemState(HierarchyManagerImpl.java:120)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:357)
	... 52 more
org.apache.jackrabbit.core.state.NoSuchItemStateException: d688e92f-26ae-4f7c-aba7-aaff1df62c2c
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getTransientItemState(SessionItemStateManager.java:323)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:154)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getItemState(HierarchyManagerImpl.java:120)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:357)
	at org.apache.jackrabbit.core.CachingHierarchyManager.getPath(CachingHierarchyManager.java:224)
	at it.unict.faq.jackrabbit.SimpleAccessManager.getStringPath(SimpleAccessManager.java:238)
	at it.unict.faq.jackrabbit.SimpleAccessManager.controllo(SimpleAccessManager.java:215)
	at it.unict.faq.jackrabbit.SimpleAccessManager.isGranted(SimpleAccessManager.java:183)
	at org.apache.jackrabbit.core.ItemImpl.validateTransientItems(ItemImpl.java:645)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1162)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:821)
	at it.unict.faq.driver.manager.impl.DAO.JcrDAO.CancellaNodo(JcrDAO.java:638)
	at it.unict.faq.driver.manager.impl.DocumentServerManager.ds_del(DocumentServerManager.java:58)
	at elearn.portal.action.ds_del_portal.execute(ds_del_portal.java:28)
	at org.apache.struts.action.RequestProcessor.processActionPerform(RequestProcessor.java:421)
	at org.apache.struts.action.RequestProcessor.process(RequestProcessor.java:226)
	at org.apache.struts.action.ActionServlet.process(ActionServlet.java:1158)
	at org.apache.struts.action.ActionServlet.doPost(ActionServlet.java:415)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:709)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:252)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:173)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:264)
	at org.acegisecurity.intercept.web.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:107)
	at org.acegisecurity.intercept.web.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:72)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:110)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.providers.anonymous.AnonymousProcessingFilter.doFilter(AnonymousProcessingFilter.java:125)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.rememberme.RememberMeProcessingFilter.doFilter(RememberMeProcessingFilter.java:142)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.wrapper.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:81)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.AbstractProcessingFilter.doFilter(AbstractProcessingFilter.java:217)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.logout.LogoutFilter.doFilter(LogoutFilter.java:108)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.context.HttpSessionContextIntegrationFilter.doFilter(HttpSessionContextIntegrationFilter.java:193)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.util.FilterChainProxy.doFilter(FilterChainProxy.java:148)
	at org.acegisecurity.util.FilterToBeanProxy.doFilter(FilterToBeanProxy.java:98)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:202)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:173)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:213)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:178)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:126)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:105)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:107)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:148)
	at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:869)
	at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.processConnection(Http11BaseProtocol.java:664)
	at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpoint.java:527)
	at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFollowerWorkerThread.java:80)
	at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:684)
	at java.lang.Thread.run(Thread.java:595)
----------------------------------------- END"
0,"random analyzer testswe have been finding+fixing lots of bugs by randomizing lucene tests.
in r966878 I added a variant of random unicode string that gives you a random string within the same unicode block (for other purposes)

I think we should use this to test the analyzers better, for example we should pound tons of random greek strings against the greek analyzer and at least make sure there aren't exceptions.
"
0,[PATCH] don't use the reflective form of {Collection}.toArrayPassing a prototype array into {Collection}.toArray that is too small makes the toArray call expend alot of effort using reflection to do it's job. It is more performant to just pass in a correctly sized prototype. This patch does this.
1,"derelativizing of relative URIs with a scheme is incorrectURI constructor ""public URI(URI base, URI relative) throws URIException"" assumes that if given 'relative' URI has a scheme, it should provide an authority and complete path to the constructed URI. However, a URI can have a scheme but still be relative, requiring the authority and base path of the 'base' URI. 

Demonstration code:

URI base = new URI(""http://www.example.com/some/page"");
URI rel = new URI(""http:boo"");
URI derel = new URI(base,rel);
derel.toString();
(java.lang.String) http:boo

In fact, derel should be ""http://www.example.com/some/boo"". 

RFC2396 is a little confused about this; section 3.1 states """"Relative URI references are distinguished from absolute URI in that they do not begin with a scheme name."" But, in section 5, there are several sentences talking about relative URIs that begin with schemes (and how this prevents using relative URIs that have leading path segments that look like scheme identifiers). 

RFC3896, which supercedes RFC2396, removes the implication a relative URI cannot begin with a scheme, leaving the other text explcitly discussing relative URIs with schemes. 

Both Firefox (1.5) and IE (6.0) treat ""http:boo"" the same as ""boo"" for purposes of derelativization against an HTTP base URI, which would give the final URI ""http://www.example.com/some/boo"" in the example above. 

Even relative URIs like ""http:../../boo"" are explicitly legal. 

"
0,"warn on invalid set-cookie headerI had a problem on a WS server that comes from some proxy misconfiguration...
resulting in this reponse beeing received by HTTPclient :
17:26:36,489 DEBUG [header] << ""HTTP/1.1 200 OK[\r][\n]""
17:26:36,489 DEBUG [header] << ""Set-Cookie: =f448bb59feedbaaabaee; path=/[\r][\n]""
17:26:36,489 DEBUG [header] << ""Date: Tue, 15 Nov 2005 16:26:36 GMT[\r][\n]""
17:26:36,489 DEBUG [header] << ""Server: Apache[\r][\n]""
17:26:36,489 DEBUG [header] << ""Connection: close[\r][\n]""
17:26:36,489 DEBUG [header] << ""Transfer-Encoding: chunked[\r][\n]""
17:26:36,489 DEBUG [header] << ""Content-Type: text/xml;charset=utf-8[\r][\n]""

The set-cookie header is malformed, as cookie has no name, so the HTTP head may
be considered invalid.

This results in an error when building the NEXT request. I'd expect httpclient
to WARN on malformed header and drop it."
0,"Multi-level skipping on posting listsTo accelerate posting list skips (TermDocs.skipTo(int)) Lucene uses skip lists. 
The default skip interval is set to 16. If we want to skip e. g. 100 documents, 
then it is not necessary to read 100 entries from the posting list, but only 
100/16 = 6 skip list entries plus 100%16 = 4 entries from the posting list. This 
speeds up conjunction (AND) and phrase queries significantly.

However, the skip interval is always a compromise. If you have a very big index 
with huge posting lists and you want to skip over lets say 100k documents, then 
it is still necessary to read 100k/16 = 6250 entries from the skip list. For big 
indexes the skip interval could be set to a higher value, but then after a big 
skip a long scan to the target doc might be necessary.

A solution for this compromise is to have multi-level skip lists that guarantee a 
logarithmic amount of skips to any target in the posting list. This patch 
implements such an approach in the following way:

  Example for skipInterval = 3:
                                                      c            (skip level 2)
                  c                 c                 c            (skip level 1) 
      x     x     x     x     x     x     x     x     x     x      (skip level 0)
  d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d  (posting list)
      3     6     9     12    15    18    21    24    27    30     (df)
 
  d - document
  x - skip data
  c - skip data with child pointer
 
Skip level i contains every skipInterval-th entry from skip level i-1. Therefore the 
number of entries on level i is: floor(df / ((skipInterval ^ (i + 1))).
 
Each skip entry on a level i>0 contains a pointer to the corresponding skip entry in 
list i-1. This guarantees a logarithmic amount of skips to find the target document.


Implementations details:

   * I factored the skipping code out of SegmentMerger and SegmentTermDocs to 
     simplify those classes. The two new classes AbstractSkipListReader and 
	 AbstractSkipListWriter implement the skipping functionality.
   * While AbstractSkipListReader and Writer take care of writing and reading the 
     multiple skip levels, they do not implement an actual skip data format. The two 
	 new subclasses DefaultSkipListReader and Writer implement the skip data format 
	 that is currently used in Lucene (with two file pointers for the freq and prox 
	 file and with payload length information). I added this extra layer to be 
	 prepared for flexible indexing and different posting list formats. 
      
   
File format changes: 

   * I added the new parameter 'maxSkipLevels' to the term dictionary and increased the
     version of this file. If maxSkipLevels is set to one, then the format of the freq 
	 file does not change at all, because we only have one skip level as before. For 
	 backwards compatibility maxSkipLevels is set to one automatically if an index 
	 without the new parameter is read. 
   * In case maxSkipLevels > 1, then the frq file changes as follows:
     FreqFile (.frq) --> <TermFreqs, SkipData>^TermCount
	 SkipData        --> <<SkipLevelLength, SkipLevel>^(Min(maxSkipLevels, 
	                       floor(log(DocFreq/log(skipInterval))) - 1)>, SkipLevel>
	 SkipLevel       --> <SkipDatum>^DocFreq/(SkipInterval^(Level + 1))

	 Remark: The length of the SkipLevel is not stored for level 0, because 1) it is not 
	 needed, and 2) the format of this file does not change for maxSkipLevels=1 then.
	 
	 
All unit tests pass with this patch."
0,spi2davex: reduce memory footprint of Node/PropertyInfoImplthe in-memory footprint of o.a.jackrabbit.spi2davex.NodeinfoImpl & PropertyInfoImp is quite big. 
0,"Tokenizers (which are the source of Tokens) should call AttributeSource.clearAttributes() firstThis is a followup for LUCENE-1796:
{quote}
Token.clear() used to be called by the consumer... but then it was switched to the producer here: LUCENE-1101 
I don't know if all of the Tokenizers in lucene were ever changed, but in any case it looks like at least some of these bugs were introduced with the switch to the attribute API - for example StandardTokenizer did clear it's reusableToken... and now it doesn't.
{quote}

As alternative to changing all core/contrib Tokenizers to call clearAttributes first, we could do this in the indexer, what would be a overhead for old token streams that itsself clear their reusable token. This issue should also update the Javadocs, to clearly state inside Tokenizer.java, that the source TokenStream (normally the Tokenizer) should clear *all* Attributes. If it does not do it and e.g. the positionIncrement is changed to 0 by any TokenFilter, but the filter does not change it back to 1, the TokenStream would stay with 0. If the TokenFilter would call PositionIncrementAttribute.clear() (because he is responsible), it could also break the TokenStream, because clear() is a general method for the whole attribute instance. If e.g. Token is used as AttributeImpl, a call to clear() would also clear offsets and termLength, which is not wanted. So the source of the Tokenization should rest the attributes to default values.

LUCENE-1796 removed the iterator creation cost, so clearAttributes should run fast, but is an additional cost during Tokenization, as it was not done consistently before, so a small speed degradion is caused by this, but has nothing to do with the new TokenStream API."
0,"Merge jcr-benchmark into the performance test suiteThe jackrabbit-jcr-benchmark component currently lives in the JCR Commons area, but there have been no active plans to release the component and AFAIUI it's so far only been used for the performance test suite we set up in JCR-2695. To avoid the extra complexity of spreading the test code over multiple components and trunks, I'd like to merge the jcr-benchmark component back to Jackrabbit trunk into the performance test suite we have in tests/performance."
0,"when checking tvx/fdx size mismatch, also include whether the file existsIndexWriter checks, during flush and during merge, that the size of the index file for stored fields (*.fdx) and term vectors (*.tvx) matches how many bytes it has just written.

This originally was added for LUCENE-1282, ie, as a safety to catch the nasty ""off by 1"" JRE hotspot bug that would otherwise silently corrupt the index.

However, this check also seems to catch a different case, where the size of the file is zero.   The most recent example is LUCENE-1521.  I'd like to improve the message in the exception to include whether or not the file exists, to help understand why users are sometimes hitting this exception.  My best theory at this point is something external is removing the file out from under the IndexWriter.
"
1,"PUT method blocks against older serversTo reproduce, attempt a PUT request against an appropriate servlet under TC3.2
(yes I know that needs an upgrade - sigh)

RFC 2616 says:
""Because of the presence of older implementations, the protocol allows ambiguous
situations in which a client may send ""Expect: 100- continue"" without receiving
either a 417 (Expectation Failed) status or a 100 (Continue) status. Therefore,
when a client sends this header field to an origin server (possibly via a proxy)
from which it has never seen a 100 (Continue) status, the client SHOULD NOT wait
for an indefinite period before sending the request body.""

This isn't how HttpClient behaves. After sending the headers,
PutMethod.writeRequestBody() returns false. HttpMethodBase then calls
readStatusCode(), which blocks waiting for a read (or I guess you could time out
the whole request). Right now this makes it impossible to use HttpClient to PUT
to older Http 1.1 implementations.

A suggested resolution: since the spec allows for clients to avoid waiting if
they know the 100 response will not arrive, why not simply provide a boolean
flag to allow the 'wait for 100' behaviour in PutMethod.writeResponseBody() to
be turned off, on a per-request basis? This solution puts the burden of knowing
""origin server[s]...from which it has never seen a 100 (Continue) status"" on the
user of HttpClient. Less than perfect as you can only find out that this has
happened by trial and error.

A more correct solution, is to maintain a list of servers that ignore the Expect
header in PutMethod, and override PutMethod.readStatusCode() to time out, send
the body, remember this server is buggy, and read the status code again."
1,"DatabaseFileSystem's logger references the wrong classIn DatabaseFileSystem, the logger is constructed as
private static Logger log = LoggerFactory.getLogger(DbFileSystem.class);

It should be constructed as:
private static Logger log = LoggerFactory.getLogger(DatabaseFileSystem.class);"
1,"HTTPS Post Does Not WorkUsing Java 1.4.1_01 on Windows 2000. An HTTPS Post results in HTTP/100-Continue 
messages. The same code posting to a non HTTPS URL works. The code populates 
the request body using a NameValuePair array."
0,"Make BooleanWeight and DisjunctionMaxWeight protectedCurrently, BooleanWeight is private, yet it has 2 protected members (similarity, weights) which are unaccessible from custom code

i have some use cases where it would be very useful to crawl a BooleanWeight to get at the sub Weight objects

however, since BooleanWeight is private, i have no way of doing this

If BooleanWeight is protected, then i can subclass BooleanQuery to hook in and wrap BooleanWeight with a subclass to facilitate this walking of the Weight objects

Would also want DisjunctionMaxWeight to be protected, along with its ""weights"" member

Would be even better if these Weights were made public with accessors to their sub ""weights"" objects (then no subclassing would be necessary on my part)

this should be really trivial and would be great if it can get into 2.9

more generally, it would be nice if all Weight classes were public with nice accessors to relevant ""sub weights""/etc so custom code can get its hooks in where and when desired"
0,"Wire log is incomplete if HttpParser detects an errorIf HttpParser detects an error in any of the headers, it throws a ProtocolException

Although the failing header is included in the Exception detail, the headers leading up to the failure are not logged, which makes it hard to debug (and is quite confusing, as the PE does not appear to be related to the data that has been received).

This is because the wire-logging is done in the caller (HttpMethodDirector) which only logs the header if the parse succeeds.

Perhaps the Wire logging should be done at the point where the HttpParser reads the line."
1,"TimeLimitingCollector's TimeExceededException contains useless relative docidWe found another bug with the RandomIndexWriter: When TimeLimitingCollector breaks collection after timeout, it records the last/next collected docid. It does this without rebasing, so the docid is useless. TestTimeLimitingCollector checks the docid, but correctly rebases it (as only this makes sense). Because the RandomIndexWriter uses different merge settings, the index is now sometimes not optimized and so the test fails (which is correct, as the docid is useless for non-optimized index).

Attached is a patch that fixes this. Please tell me if I should backport to 2.9 and 3.0!"
0,"Further parallelizaton of ParallelMultiSearcherWhen calling {{search(Query, Filter, int)}} on a ParallelMultiSearcher, the {{createWeights}} function of MultiSearcher is called, and sequentially calls {{docFreqs()}} on every sub-searcher. This can take a significant amount of time when there are lots of remote sub-searchers.

"
1,"javax.jcr.RepositoryException when a JOIN SQL2 query is send via Davex and has resultssee the following thread for details:
http://www.mail-archive.com/users@jackrabbit.apache.org/msg17975.html

assuming a data structure as follows:
/foo [nt:unstructured]
/foo/bar [nt:unstructured]
/foo/bar@lala = huii (lala is string property of bar)
/ding [nt:unstructured]
/ding@dong = ##barUUID### (dong is a property of type ""Reference"")

then the following code will throw an exception:

DavexClient Client = new DavexClient(url);
Repository repo = Client.getRepository();
Credentials sc = new SimpleCredentials(""admin"",""admin"".toCharArray());
Session s = repo.login(sc,workspace);

QueryManager qm = s.getWorkspace().getQueryManager();

String sql = ""SELECT data.* FROM [nt:unstructured] AS data WHERE data.lala= 'huii'"";
sql = ""SELECT * FROM [nt:unstructured] AS data INNER JOIN [nt:unstructured] AS referring ON referring.[dong] = data.[jcr:uuid] WHERE data.lala = 'huii'"";
sql = ""SELECT * FROM [nt:unstructured] AS data INNER JOIN [nt:unstructured] AS referring ON ISDESCENDANTNODE(data, referring) WHERE data.lala = 'huii'"";
Query query = qm.createQuery(sql, Query.JCR_SQL2);
QueryResult qr = query.execute();

The first query works just fine and I can iterate over the result. Neither the second nor the third query works.
In both cases I end up with a javax.jcr.RepositoryException. Note the exception only happens if the query returns results. Aka a join will work just fine if it matches no rows."
1,"need to ensure that sims that use collection-level stats (e.g. sumTotalTermFreq) handle non-existent fieldBecause of things like queryNorm, unfortunately similarities have to handle the case where they are asked to computeStats() for a term, where the field does not exist at all.
(Note they will never have to actually score anything, but unless we break how queryNorm works for TFIDF, we have to deal with this case).

I noticed this while doing some benchmarking, so i created a test to test some cases like this across all the sims."
1,MSSql and MySQL bunlde PM schemas missing definition for name indexthe mssql and mysql ddl files of the respective bundle persistence managers are missing the definitions for the name index.
1,"IW.optimize() can do too many merges at the very endThis was fixed on trunk in LUCENE-1044 but I'd like to separately
backport it to 2.3.

With ConcurrentMergeScheduler there is a bug, only when CFS is on,
whereby after the final merge of an optimize has finished and while
it's building its CFS, the merge policy may incorrectly ask for
another merge to collapse that segment into a compound file.  The net
effect is optimize can spend many extra iterations unecessarily
merging a single segment to collapse it to compound file.

I believe the case is rare (hard to hit), and maybe only if you have
multiple threads calling optimize at once (the TestThreadedOptimize
test can hit it), but it's a low-risk fix so I plan to commit to 2.3
shortly.

"
0,"cache revalidation of variants does not update original variant entryWhen the cache stories multiple variant entries due to Vary headers in responses, the cache correctly sends a conditional request containing the etags of any existing variants on a ""variant miss"" (incoming request does not match the request variants already cached). In addition, when it receives a 304 response, it correctly returns the indicated variant to the request that causes the variant miss. However, it does not update the pre-existing variant cache entry as recommended by RFC 2616.

For example:

request 1, User-Agent: agent1 results in a 200 OK with Etag: etag1 and Vary: User-Agent.
request 2, User-Agent: agent2 causes an If-None-Match to the origin; if it returns 304 Not Modified with Etag: etag1
request 3, User-Agent: agent1 results in a 200 OK but gets the (outdated) entry that resulted from request 1

in other words, the origin response from request 2 does not update the variant for ""agent1"".

This does not cause incorrect behavior (this is a SHOULD) but does miss out on some caching opportunities here.
"
0,"TCK: Test that expect that modifications made by Session1 are automatically visible to Session2While changes made by session1 are automatically visible to any other session2 with the RI, this is not required by the
specification. Therefore i would suggest to modify the following test cases:

- NodeUUIDTest.testSaveMovedRefNode()
- SessionUUIDTest.testSaveMovedRefNode()

-> no patch. sorry.

- NodeTest.testRemoveInvalidItemStateException()

-> see patch."
0,"Enhanced JCR remoting (extending webdav SPI impl, basic remoting servlet)"
0,"Check if a DAV-Request has a Label in the header, before checking if it's version-controlledWhen looking at our MySQL logs, I realized that jackrabbit on each DAV Request calls the VERSION table every time I get a new node (which is not cached yet), even if I only do a simple getNode. 
As a versioning table can get pretty large, this may have a performance impact.

I found out, that DavResourceFactoryImpl checks, if a node is versioned to decide, if we have to check for the Label header to later check out another version for the GET request. I re-ordered those checks now so that it first checks, if there's an http Label-header and only then checks, if the node is versioned. The check for a Label header should be much faster than checking a DB, if it's versioned (and scale much better, too)



"
0,Index update overhead on cluster slave due to JCR-905JCR-905 is a quick and dirty fix and causes overhead on a cluster slave node when it processes revisions.
0,"OracleBundlePersistenceManager needs special blob handling for JDBC drivers prior to oracle 10the new oracle bundle persistence manager (see JCR-755) needs special blob handling for oracle jdbc drivers prior to version 10. since the pm works for newer versions, i suggest to add a separate pm eg: Oracle9PersistenceManager that contains this special blob handling."
0,"jackrabbit-webapp faceliftStill before 1.4, I meant to make the jackrabbit-webapp look a bit nicer. I'm taking the skin from JCR-1236 and applying it to jackrabbit-webapp."
0,"contrib.ssl.HostConfigurationWithHostFactoryI'd like to contribute an example specialized HostConfiguration, to replace the one I contributed in HTTPCLIENT-634."
0,"All spatial contrib shape classes implement equals but not hashCodeviolates contract - at a min, need to implement return constant."
1,"[patch] fix uppercase/lowercase handling for not equal tocode is missing breaks in switch statements, which causes both uppercase and lowercase terms to the not equal to lucene search. patch fixes."
1,FilterIndexReader should overwrite isOptimized()A call of FilterIndexReader.isOptimized() results in a NPE because FilterIndexReader does not overwrite isOptimized().
1,"o.a.l.analysis.de.GermanStemmer crashes on some inputsSee the tests from LUCENE-2560. 

GermanAnalyzer no longer uses this stemmer by default, but we should fix it."
0,"client cache may be a shared cache but is caching responses to requests with Authorization headers""      When a shared cache (see section 13.7) receives a request
      containing an Authorization field, it MUST NOT return the
      corresponding response as a reply to any other request, unless one
      of the following specific exceptions holds:

      1. If the response includes the ""s-maxage"" cache-control
         directive, the cache MAY use that response in replying to a
         subsequent request. But (if the specified maximum age has
         passed) a proxy cache MUST first revalidate it with the origin
         server, using the request-headers from the new request to allow
         the origin server to authenticate the new request. (This is the
         defined behavior for s-maxage.) If the response includes ""s-
         maxage=0"", the proxy MUST always revalidate it before re-using
         it.

      2. If the response includes the ""must-revalidate"" cache-control
         directive, the cache MAY use that response in replying to a
         subsequent request. But if the response is stale, all caches
         MUST first revalidate it with the origin server, using the
         request-headers from the new request to allow the origin server
         to authenticate the new request.

      3. If the response includes the ""public"" cache-control directive,
         it MAY be returned in reply to any subsequent request.""

http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.8

It isn't clear whether the CachingHttpClient is a shared cache or not (it depends on where it gets used), so the conservative compliant behavior is to assume we are a shared cache. The current implementation is caching responses regardless of whether the original requests had Authorization headers or not.

Patch and discussion forthcoming.

"
0,"Static index pruning by in-document term frequency (Carmel pruning)This module provides tools to produce a subset of input indexes by removing postings data for those terms where their in-document frequency is below a specified threshold. The net effect of this processing is a much smaller index that for common types of queries returns nearly identical top-N results as compared with the original index, but with increased performance. 

Optionally, stored values and term vectors can also be removed. This functionality is largely independent, so it can be used without term pruning (when term freq. threshold is set to 1).

As the threshold value increases, the total size of the index decreases, search performance increases, and recall decreases (i.e. search quality deteriorates). NOTE: especially phrase recall deteriorates significantly at higher threshold values. 

Primary purpose of this class is to produce small first-tier indexes that fit completely in RAM, and store these indexes using IndexWriter.addIndexes(IndexReader[]). Usually the performance of this class will not be sufficient to use the resulting index view for on-the-fly pruning and searching. 

NOTE: If the input index is optimized (i.e. doesn't contain deletions) then the index produced via IndexWriter.addIndexes(IndexReader[]) will preserve internal document id-s so that they are in sync with the original index. This means that all other auxiliary information not necessary for first-tier processing, such as some stored fields, can also be removed, to be quickly retrieved on-demand from the original index using the same internal document id. 

Threshold values can be specified globally (for terms in all fields) using defaultThreshold parameter, and can be overriden using per-field or per-term values supplied in a thresholds map. Keys in this map are either field names, or terms in field:text format. The precedence of these values is the following: first a per-term threshold is used if present, then per-field threshold if present, and finally the default threshold.

A command-line tool (PruningTool) is provided for convenience. At this moment it doesn't support all functionality available through API."
0,"Swap URL+Email recognizing StandardTokenizer and UAX29TokenizerCurrently, in addition to implementing the UAX#29 word boundary rules, StandardTokenizer recognizes email adresses and URLs, but doesn't provide a way to turn this behavior off and/or provide overlapping tokens with the components (username from email address, hostname from URL, etc.).

UAX29Tokenizer should become StandardTokenizer, and current StandardTokenizer should be renamed to something like UAX29TokenizerPlusPlus (or something like that).

For rationale, see [the discussion at the reopened LUCENE-2167|https://issues.apache.org/jira/browse/LUCENE-2167?focusedCommentId=12929325&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12929325]."
0,Include the README file in the generated jar filesThe Incubator would prefer if we had the incubation notice included in the binary jar files we release. It should be a simple Maven configuration change to get the README.txt file included in the binary jars.
1,"Test failures with spi2jcr in AddEventListenerTestTwo tests fail:

- AddEventListenerTest.testUUID
- AddEventListenerTest.testNodeType"
0,"Remove noLockHack in SharedItemStateManagerWith the increased test coverage, specifically the recently added multi-threaded tests, I'm reasonably confident that the noLockHack in SharedItemStateManager is not needed anymore.

Attached patch removes the hack. All tests still pass, including the daily integration tests."
0,"EnwikiDocMaker id fieldThe EnwikiDocMaker is fairly usable outside of the benchmarking class, but it would benefit from indexing the ID field of the docs.

Patch to follow that adds an ID field."
0,"Enhance indexing of binary contentIndexing of binary content should be enhanced in order to allow either configuration what fields are indexed or provide better support for custom NodeIndexer implementations.

The current design has a couple of flaws that should be addressed at the same time:
- Reader instances are requested from the text filters even though the reader might never be used
- only jcr:data properties of nt:resource nodes are fulltext indexed
- It is up to the text filter implementation to decide the lucene field name for the text representation, responsibility should be moved to the NodeIndexer. A text filter should only provide a Reader instance.

With those changes a custom NodeIndexer can then decide if a binary property has one or more representations in the index."
0,"Some files are missing the license headersJukka provided the following list of files that are missing the license headers.
In addition there might be other files (like build scripts) that don't have the headers.

src/java/org/apache/lucene/document/MapFieldSelector.java
src/java/org/apache/lucene/search/PrefixFilter.java
src/test/org/apache/lucene/TestHitIterator.java
src/test/org/apache/lucene/analysis/TestISOLatin1AccentFilter.java
src/test/org/apache/lucene/index/TestAddIndexesNoOptimize.java
src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
src/test/org/apache/lucene/index/TestFieldInfos.java
src/test/org/apache/lucene/index/TestIndexFileDeleter.java
src/test/org/apache/lucene/index/TestIndexWriter.java
src/test/org/apache/lucene/index/TestIndexWriterDelete.java
src/test/org/apache/lucene/index/TestIndexWriterLockRelease.java
src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java
src/test/org/apache/lucene/index/TestNorms.java
src/test/org/apache/lucene/index/TestParallelTermEnum.java
src/test/org/apache/lucene/index/TestSegmentTermEnum.java
src/test/org/apache/lucene/index/TestTerm.java
src/test/org/apache/lucene/index/TestTermVectorsReader.java
src/test/org/apache/lucene/search/TestRangeQuery.java
src/test/org/apache/lucene/search/TestTermScorer.java
src/test/org/apache/lucene/store/TestBufferedIndexInput.java
src/test/org/apache/lucene/store/TestWindowsMMap.java
src/test/org/apache/lucene/store/_TestHelper.java
src/test/org/apache/lucene/util/_TestUtil.java
contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker.java
contrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/server/FeedNotFoundException.java
contrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/server/registry/ComponentType.java
contrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/server/registry/RegistryException.java
contrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/storage/lucenestorage/StorageAccountWrapper.java
contrib/gdata-server/src/core/src/test/org/apache/lucene/gdata/storage/lucenestorage/TestModifiedEntryFilter.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/AtomUriElementTest.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMEntryImplTest.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMFeedImplTest.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMGenereatorImplTest.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMSourceImplTest.java
contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java
contrib/javascript/queryConstructor/luceneQueryConstructor.js
contrib/javascript/queryEscaper/luceneQueryEscaper.js
contrib/javascript/queryValidator/luceneQueryValidator.js
contrib/queries/src/java/org/apache/lucene/search/BooleanFilter.java
contrib/queries/src/java/org/apache/lucene/search/BoostingQuery.java
contrib/queries/src/java/org/apache/lucene/search/FilterClause.java
contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery.java
contrib/queries/src/java/org/apache/lucene/search/TermsFilter.java
contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThisQuery.java
contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java
contrib/regex/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java
contrib/snowball/src/java/net/sf/snowball/Among.java
contrib/snowball/src/java/net/sf/snowball/SnowballProgram.java
contrib/snowball/src/java/net/sf/snowball/TestApp.java
contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestSpellChecker.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/BooleanQueryTst.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/ExceptionQueryTst.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/SingleFieldTestDb.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/Test01Exceptions.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/Test02Boolean.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/Test03Distance.java
contrib/wordnet/src/java/org/apache/lucene/wordnet/SynExpand.java
contrib/wordnet/src/java/org/apache/lucene/wordnet/SynLookup.java
contrib/wordnet/src/java/org/apache/lucene/wordnet/Syns2Index.java
"
0,"Supporting deleteDocuments in IndexWriter (Code and Performance Results Provided)Today, applications have to open/close an IndexWriter and open/close an
IndexReader directly or indirectly (via IndexModifier) in order to handle a
mix of inserts and deletes. This performs well when inserts and deletes
come in fairly large batches. However, the performance can degrade
dramatically when inserts and deletes are interleaved in small batches.
This is because the ramDirectory is flushed to disk whenever an IndexWriter
is closed, causing a lot of small segments to be created on disk, which
eventually need to be merged.

We would like to propose a small API change to eliminate this problem. We
are aware that this kind change has come up in discusions before. See
http://www.gossamer-threads.com/lists/lucene/java-dev/23049?search_string=indexwriter%20delete;#23049
. The difference this time is that we have implemented the change and
tested its performance, as described below.

API Changes
-----------
We propose adding a ""deleteDocuments(Term term)"" method to IndexWriter.
Using this method, inserts and deletes can be interleaved using the same
IndexWriter.

Note that, with this change it would be very easy to add another method to
IndexWriter for updating documents, allowing applications to avoid a
separate delete and insert to update a document.

Also note that this change can co-exist with the existing APIs for deleting
documents using an IndexReader. But if our proposal is accepted, we think
those APIs should probably be deprecated.

Coding Changes
--------------
Coding changes are localized to IndexWriter. Internally, the new
deleteDocuments() method works by buffering the terms to be deleted.
Deletes are deferred until the ramDirectory is flushed to disk, either
because it becomes full or because the IndexWriter is closed. Using Java
synchronization, care is taken to ensure that an interleaved sequence of
inserts and deletes for the same document are properly serialized.

We have attached a modified version of IndexWriter in Release 1.9.1 with
these changes. Only a few hundred lines of coding changes are needed. All
changes are commented by ""CHANGE"". We have also attached a modified version
of an example from Chapter 2.2 of Lucene in Action.

Performance Results
-------------------
To test the performance our proposed changes, we ran some experiments using
the TREC WT 10G dataset. The experiments were run on a dual 2.4 Ghz Intel
Xeon server running Linux. The disk storage was configured as RAID0 array
with 5 drives. Before indexes were built, the input documents were parsed
to remove the HTML from them (i.e., only the text was indexed). This was
done to minimize the impact of parsing on performance. A simple
WhitespaceAnalyzer was used during index build.

We experimented with three workloads:
  - Insert only. 1.6M documents were inserted and the final
    index size was 2.3GB.
  - Insert/delete (big batches). The same documents were
    inserted, but 25% were deleted. 1000 documents were
    deleted for every 4000 inserted.
  - Insert/delete (small batches). In this case, 5 documents
    were deleted for every 20 inserted.

                                current       current          new
Workload                      IndexWriter  IndexModifier   IndexWriter
-----------------------------------------------------------------------
Insert only                     116 min       119 min        116 min
Insert/delete (big batches)       --          135 min        125 min
Insert/delete (small batches)     --          338 min        134 min

As the experiments show, with the proposed changes, the performance
improved by 60% when inserts and deletes were interleaved in small batches.


Regards,
Ning


Ning Li
Search Technologies
IBM Almaden Research Center
650 Harry Road
San Jose, CA 95120"
1,"trunk tests hang/deadlock TestIndexWriterWithThreadstrunk tests have been hanging often lately in hudson, this time i was careful to kill and get a good stacktrace:"
1,"ClassCastException org.apache.xerces.parsers.XIncludeAwareParserConfiguration cannot be cast to org.apache.xerces.xni.parser.XMLParserConfiguration when deploying in JBoss 5.1I tried to follow the steps given on http://wiki.apache.org/jackrabbit/JackrabbitOnJBoss
To get over an exception I had to use jcr-2.0.jar (instead of jcr-1.0.jar)
The following exception happens when the jboss server is started.
=======================================================================

2009-10-02 11:49:05,630 ERROR [org.jboss.web.tomcat.service.deployers.JBossContextConfig] (main) XML error parsing: context.xml
org.jboss.xb.binding.JBossXBRuntimeException: Failed to create a new SAX parser
	at org.jboss.xb.binding.UnmarshallerFactory$UnmarshallerFactoryImpl.newUnmarshaller(UnmarshallerFactory.java:100)
	at org.jboss.web.tomcat.service.deployers.JBossContextConfig.processContextConfig(JBossContextConfig.java:549)
	at org.jboss.web.tomcat.service.deployers.JBossContextConfig.init(JBossContextConfig.java:536)
	at org.apache.catalina.startup.ContextConfig.lifecycleEvent(ContextConfig.java:279)
	at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:117)
	at org.apache.catalina.core.StandardContext.init(StandardContext.java:5436)
	at org.apache.catalina.core.StandardContext.start(StandardContext.java:4148)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeployInternal(TomcatDeployment.java:310)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeploy(TomcatDeployment.java:142)
	at org.jboss.web.deployers.AbstractWarDeployment.start(AbstractWarDeployment.java:461)
	at org.jboss.web.deployers.WebModule.startModule(WebModule.java:118)
	at org.jboss.web.deployers.WebModule.start(WebModule.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java:157)
	at org.jboss.mx.server.Invocation.dispatch(Invocation.java:96)
	at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
	at org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:264)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:668)
	at org.jboss.system.microcontainer.ServiceProxy.invoke(ServiceProxy.java:206)
	at $Proxy38.start(Unknown Source)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:42)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:37)
	at org.jboss.dependency.plugins.action.SimpleControllerContextAction.simpleInstallAction(SimpleControllerContextAction.java:62)
	at org.jboss.dependency.plugins.action.AccessControllerContextAction.install(AccessControllerContextAction.java:71)
	at org.jboss.dependency.plugins.AbstractControllerContextActions.install(AbstractControllerContextActions.java:51)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.system.microcontainer.ServiceControllerContext.install(ServiceControllerContext.java:286)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.ServiceController.doChange(ServiceController.java:688)
	at org.jboss.system.ServiceController.start(ServiceController.java:460)
	at org.jboss.system.deployers.ServiceDeployer.start(ServiceDeployer.java:163)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:99)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:46)
	at org.jboss.deployers.spi.deployer.helpers.AbstractSimpleRealDeployer.internalDeploy(AbstractSimpleRealDeployer.java:62)
	at org.jboss.deployers.spi.deployer.helpers.AbstractRealDeployer.deploy(AbstractRealDeployer.java:50)
	at org.jboss.deployers.plugins.deployers.DeployerWrapper.deploy(DeployerWrapper.java:171)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doDeploy(DeployersImpl.java:1439)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1157)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1178)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.install(DeployersImpl.java:1098)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.process(DeployersImpl.java:781)
	at org.jboss.deployers.plugins.main.MainDeployerImpl.process(MainDeployerImpl.java:702)
	at org.jboss.system.server.profileservice.repository.MainDeployerAdapter.process(MainDeployerAdapter.java:117)
	at org.jboss.system.server.profileservice.repository.ProfileDeployAction.install(ProfileDeployAction.java:70)
	at org.jboss.system.server.profileservice.repository.AbstractProfileAction.install(AbstractProfileAction.java:53)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.install(AbstractProfileService.java:361)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.activateProfile(AbstractProfileService.java:306)
	at org.jboss.system.server.profileservice.ProfileServiceBootstrap.start(ProfileServiceBootstrap.java:271)
	at org.jboss.bootstrap.AbstractServerImpl.start(AbstractServerImpl.java:461)
	at org.jboss.Main.boot(Main.java:221)
	at org.jboss.Main$1.run(Main.java:556)
	at java.lang.Thread.run(Thread.java:619)
Caused by: org.jboss.xb.binding.JBossXBException: Failed to create a new SAX parser
	at org.jboss.xb.binding.parser.sax.SaxJBossXBParser.<init>(SaxJBossXBParser.java:97)
	at org.jboss.xb.binding.UnmarshallerImpl.<init>(UnmarshallerImpl.java:56)
	at org.jboss.xb.binding.UnmarshallerFactory$UnmarshallerFactoryImpl.newUnmarshaller(UnmarshallerFactory.java:96)
	... 73 more
Caused by: java.lang.ClassCastException: org.apache.xerces.parsers.XIncludeAwareParserConfiguration cannot be cast to org.apache.xerces.xni.parser.XMLParserConfiguration
	at org.apache.xerces.parsers.SAXParser.<init>(Unknown Source)
	at org.apache.xerces.parsers.SAXParser.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserFactoryImpl.newSAXParser(Unknown Source)
	at org.jboss.xb.binding.parser.sax.SaxJBossXBParser.<init>(SaxJBossXBParser.java:92)
	... 75 more
2009-10-02 11:49:05,645 ERROR [org.jboss.web.tomcat.service.deployers.JBossContextConfig] (main) XML error parsing: jboss.web/localhost/context.xml.default
org.jboss.xb.binding.JBossXBRuntimeException: Failed to create a new SAX parser
	at org.jboss.xb.binding.UnmarshallerFactory$UnmarshallerFactoryImpl.newUnmarshaller(UnmarshallerFactory.java:100)
	at org.jboss.web.tomcat.service.deployers.JBossContextConfig.processContextConfig(JBossContextConfig.java:549)
	at org.jboss.web.tomcat.service.deployers.JBossContextConfig.init(JBossContextConfig.java:537)
	at org.apache.catalina.startup.ContextConfig.lifecycleEvent(ContextConfig.java:279)
	at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:117)
	at org.apache.catalina.core.StandardContext.init(StandardContext.java:5436)
	at org.apache.catalina.core.StandardContext.start(StandardContext.java:4148)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeployInternal(TomcatDeployment.java:310)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeploy(TomcatDeployment.java:142)
	at org.jboss.web.deployers.AbstractWarDeployment.start(AbstractWarDeployment.java:461)
	at org.jboss.web.deployers.WebModule.startModule(WebModule.java:118)
	at org.jboss.web.deployers.WebModule.start(WebModule.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java:157)
	at org.jboss.mx.server.Invocation.dispatch(Invocation.java:96)
	at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
	at org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:264)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:668)
	at org.jboss.system.microcontainer.ServiceProxy.invoke(ServiceProxy.java:206)
	at $Proxy38.start(Unknown Source)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:42)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:37)
	at org.jboss.dependency.plugins.action.SimpleControllerContextAction.simpleInstallAction(SimpleControllerContextAction.java:62)
	at org.jboss.dependency.plugins.action.AccessControllerContextAction.install(AccessControllerContextAction.java:71)
	at org.jboss.dependency.plugins.AbstractControllerContextActions.install(AbstractControllerContextActions.java:51)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.system.microcontainer.ServiceControllerContext.install(ServiceControllerContext.java:286)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.ServiceController.doChange(ServiceController.java:688)
	at org.jboss.system.ServiceController.start(ServiceController.java:460)
	at org.jboss.system.deployers.ServiceDeployer.start(ServiceDeployer.java:163)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:99)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:46)
	at org.jboss.deployers.spi.deployer.helpers.AbstractSimpleRealDeployer.internalDeploy(AbstractSimpleRealDeployer.java:62)
	at org.jboss.deployers.spi.deployer.helpers.AbstractRealDeployer.deploy(AbstractRealDeployer.java:50)
	at org.jboss.deployers.plugins.deployers.DeployerWrapper.deploy(DeployerWrapper.java:171)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doDeploy(DeployersImpl.java:1439)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1157)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1178)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.install(DeployersImpl.java:1098)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.process(DeployersImpl.java:781)
	at org.jboss.deployers.plugins.main.MainDeployerImpl.process(MainDeployerImpl.java:702)
	at org.jboss.system.server.profileservice.repository.MainDeployerAdapter.process(MainDeployerAdapter.java:117)
	at org.jboss.system.server.profileservice.repository.ProfileDeployAction.install(ProfileDeployAction.java:70)
	at org.jboss.system.server.profileservice.repository.AbstractProfileAction.install(AbstractProfileAction.java:53)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.install(AbstractProfileService.java:361)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.activateProfile(AbstractProfileService.java:306)
	at org.jboss.system.server.profileservice.ProfileServiceBootstrap.start(ProfileServiceBootstrap.java:271)
	at org.jboss.bootstrap.AbstractServerImpl.start(AbstractServerImpl.java:461)
	at org.jboss.Main.boot(Main.java:221)
	at org.jboss.Main$1.run(Main.java:556)
	at java.lang.Thread.run(Thread.java:619)
Caused by: org.jboss.xb.binding.JBossXBException: Failed to create a new SAX parser
	at org.jboss.xb.binding.parser.sax.SaxJBossXBParser.<init>(SaxJBossXBParser.java:97)
	at org.jboss.xb.binding.UnmarshallerImpl.<init>(UnmarshallerImpl.java:56)
	at org.jboss.xb.binding.UnmarshallerFactory$UnmarshallerFactoryImpl.newUnmarshaller(UnmarshallerFactory.java:96)
	... 73 more
Caused by: java.lang.ClassCastException: org.apache.xerces.parsers.XIncludeAwareParserConfiguration cannot be cast to org.apache.xerces.xni.parser.XMLParserConfiguration
	at org.apache.xerces.parsers.SAXParser.<init>(Unknown Source)
	at org.apache.xerces.parsers.SAXParser.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserFactoryImpl.newSAXParser(Unknown Source)
	at org.jboss.xb.binding.parser.sax.SaxJBossXBParser.<init>(SaxJBossXBParser.java:92)
	... 75 more
2009-10-02 11:49:05,645 ERROR [org.jboss.web.tomcat.service.deployers.JBossContextConfig] (main) XML error parsing: WEB-INF/context.xml
org.jboss.xb.binding.JBossXBRuntimeException: Failed to create a new SAX parser
	at org.jboss.xb.binding.UnmarshallerFactory$UnmarshallerFactoryImpl.newUnmarshaller(UnmarshallerFactory.java:100)
	at org.jboss.web.tomcat.service.deployers.JBossContextConfig.processContextConfig(JBossContextConfig.java:549)
	at org.jboss.web.tomcat.service.deployers.JBossContextConfig.init(JBossContextConfig.java:540)
	at org.apache.catalina.startup.ContextConfig.lifecycleEvent(ContextConfig.java:279)
	at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:117)
	at org.apache.catalina.core.StandardContext.init(StandardContext.java:5436)
	at org.apache.catalina.core.StandardContext.start(StandardContext.java:4148)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeployInternal(TomcatDeployment.java:310)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeploy(TomcatDeployment.java:142)
	at org.jboss.web.deployers.AbstractWarDeployment.start(AbstractWarDeployment.java:461)
	at org.jboss.web.deployers.WebModule.startModule(WebModule.java:118)
	at org.jboss.web.deployers.WebModule.start(WebModule.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java:157)
	at org.jboss.mx.server.Invocation.dispatch(Invocation.java:96)
	at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
	at org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:264)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:668)
	at org.jboss.system.microcontainer.ServiceProxy.invoke(ServiceProxy.java:206)
	at $Proxy38.start(Unknown Source)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:42)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:37)
	at org.jboss.dependency.plugins.action.SimpleControllerContextAction.simpleInstallAction(SimpleControllerContextAction.java:62)
	at org.jboss.dependency.plugins.action.AccessControllerContextAction.install(AccessControllerContextAction.java:71)
	at org.jboss.dependency.plugins.AbstractControllerContextActions.install(AbstractControllerContextActions.java:51)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.system.microcontainer.ServiceControllerContext.install(ServiceControllerContext.java:286)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.ServiceController.doChange(ServiceController.java:688)
	at org.jboss.system.ServiceController.start(ServiceController.java:460)
	at org.jboss.system.deployers.ServiceDeployer.start(ServiceDeployer.java:163)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:99)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:46)
	at org.jboss.deployers.spi.deployer.helpers.AbstractSimpleRealDeployer.internalDeploy(AbstractSimpleRealDeployer.java:62)
	at org.jboss.deployers.spi.deployer.helpers.AbstractRealDeployer.deploy(AbstractRealDeployer.java:50)
	at org.jboss.deployers.plugins.deployers.DeployerWrapper.deploy(DeployerWrapper.java:171)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doDeploy(DeployersImpl.java:1439)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1157)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1178)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.install(DeployersImpl.java:1098)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.process(DeployersImpl.java:781)
	at org.jboss.deployers.plugins.main.MainDeployerImpl.process(MainDeployerImpl.java:702)
	at org.jboss.system.server.profileservice.repository.MainDeployerAdapter.process(MainDeployerAdapter.java:117)
	at org.jboss.system.server.profileservice.repository.ProfileDeployAction.install(ProfileDeployAction.java:70)
	at org.jboss.system.server.profileservice.repository.AbstractProfileAction.install(AbstractProfileAction.java:53)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.install(AbstractProfileService.java:361)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.activateProfile(AbstractProfileService.java:306)
	at org.jboss.system.server.profileservice.ProfileServiceBootstrap.start(ProfileServiceBootstrap.java:271)
	at org.jboss.bootstrap.AbstractServerImpl.start(AbstractServerImpl.java:461)
	at org.jboss.Main.boot(Main.java:221)
	at org.jboss.Main$1.run(Main.java:556)
	at java.lang.Thread.run(Thread.java:619)
Caused by: org.jboss.xb.binding.JBossXBException: Failed to create a new SAX parser
	at org.jboss.xb.binding.parser.sax.SaxJBossXBParser.<init>(SaxJBossXBParser.java:97)
	at org.jboss.xb.binding.UnmarshallerImpl.<init>(UnmarshallerImpl.java:56)
	at org.jboss.xb.binding.UnmarshallerFactory$UnmarshallerFactoryImpl.newUnmarshaller(UnmarshallerFactory.java:96)
	... 73 more
Caused by: java.lang.ClassCastException: org.apache.xerces.parsers.XIncludeAwareParserConfiguration cannot be cast to org.apache.xerces.xni.parser.XMLParserConfiguration
	at org.apache.xerces.parsers.SAXParser.<init>(Unknown Source)
	at org.apache.xerces.parsers.SAXParser.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserFactoryImpl.newSAXParser(Unknown Source)
	at org.jboss.xb.binding.parser.sax.SaxJBossXBParser.<init>(SaxJBossXBParser.java:92)
	... 75 more
2009-10-02 11:49:05,786 ERROR [org.apache.catalina.startup.ContextConfig] (main) Marking this application unavailable due to previous error(s)
2009-10-02 11:49:05,786 ERROR [org.apache.catalina.core.StandardContext] (main) Context [/jackrabbit-webapp-2.0-alpha9] startup failed due to previous errors
2009-10-02 11:49:06,473 ERROR [org.jboss.kernel.plugins.dependency.AbstractKernelController] (main) Error installing to Start: name=jboss.web.deployment:war=/jackrabbit-webapp-2.0-alpha9 state=Create mode=Manual requiredState=Installed
org.jboss.deployers.spi.DeploymentException: URL file:/C:/applications/jboss-5.1.0.GA/server/default/tmp/ahn1p-6tv4p6-g0bacatm-1-g0bageil-9t/jackrabbit-webapp-2.0-alpha9.war/ deployment failed
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeployInternal(TomcatDeployment.java:331)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeploy(TomcatDeployment.java:142)
	at org.jboss.web.deployers.AbstractWarDeployment.start(AbstractWarDeployment.java:461)
	at org.jboss.web.deployers.WebModule.startModule(WebModule.java:118)
	at org.jboss.web.deployers.WebModule.start(WebModule.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java:157)
	at org.jboss.mx.server.Invocation.dispatch(Invocation.java:96)
	at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
	at org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:264)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:668)
	at org.jboss.system.microcontainer.ServiceProxy.invoke(ServiceProxy.java:206)
	at $Proxy38.start(Unknown Source)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:42)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:37)
	at org.jboss.dependency.plugins.action.SimpleControllerContextAction.simpleInstallAction(SimpleControllerContextAction.java:62)
	at org.jboss.dependency.plugins.action.AccessControllerContextAction.install(AccessControllerContextAction.java:71)
	at org.jboss.dependency.plugins.AbstractControllerContextActions.install(AbstractControllerContextActions.java:51)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.system.microcontainer.ServiceControllerContext.install(ServiceControllerContext.java:286)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.ServiceController.doChange(ServiceController.java:688)
	at org.jboss.system.ServiceController.start(ServiceController.java:460)
	at org.jboss.system.deployers.ServiceDeployer.start(ServiceDeployer.java:163)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:99)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:46)
	at org.jboss.deployers.spi.deployer.helpers.AbstractSimpleRealDeployer.internalDeploy(AbstractSimpleRealDeployer.java:62)
	at org.jboss.deployers.spi.deployer.helpers.AbstractRealDeployer.deploy(AbstractRealDeployer.java:50)
	at org.jboss.deployers.plugins.deployers.DeployerWrapper.deploy(DeployerWrapper.java:171)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doDeploy(DeployersImpl.java:1439)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1157)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1178)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.install(DeployersImpl.java:1098)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.process(DeployersImpl.java:781)
	at org.jboss.deployers.plugins.main.MainDeployerImpl.process(MainDeployerImpl.java:702)
	at org.jboss.system.server.profileservice.repository.MainDeployerAdapter.process(MainDeployerAdapter.java:117)
	at org.jboss.system.server.profileservice.repository.ProfileDeployAction.install(ProfileDeployAction.java:70)
	at org.jboss.system.server.profileservice.repository.AbstractProfileAction.install(AbstractProfileAction.java:53)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.install(AbstractProfileService.java:361)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.activateProfile(AbstractProfileService.java:306)
	at org.jboss.system.server.profileservice.ProfileServiceBootstrap.start(ProfileServiceBootstrap.java:271)
	at org.jboss.bootstrap.AbstractServerImpl.start(AbstractServerImpl.java:461)
	at org.jboss.Main.boot(Main.java:221)
	at org.jboss.Main$1.run(Main.java:556)
	at java.lang.Thread.run(Thread.java:619)
2009-10-02 11:49:06,598 ERROR [org.jboss.kernel.plugins.dependency.AbstractKernelController] (main) Error installing to Real: name=vfszip:/C:/applications/jboss-5.1.0.GA/server/default/deploy/jackrabbit-webapp-2.0-alpha9.war/ state=PreReal mode=Manual requiredState=Real
org.jboss.deployers.spi.DeploymentException: URL file:/C:/applications/jboss-5.1.0.GA/server/default/tmp/ahn1p-6tv4p6-g0bacatm-1-g0bageil-9t/jackrabbit-webapp-2.0-alpha9.war/ deployment failed
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeployInternal(TomcatDeployment.java:331)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeploy(TomcatDeployment.java:142)
	at org.jboss.web.deployers.AbstractWarDeployment.start(AbstractWarDeployment.java:461)
	at org.jboss.web.deployers.WebModule.startModule(WebModule.java:118)
	at org.jboss.web.deployers.WebModule.start(WebModule.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java:157)
	at org.jboss.mx.server.Invocation.dispatch(Invocation.java:96)
	at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
	at org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:264)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:668)
	at org.jboss.system.microcontainer.ServiceProxy.invoke(ServiceProxy.java:206)
	at $Proxy38.start(Unknown Source)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:42)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:37)
	at org.jboss.dependency.plugins.action.SimpleControllerContextAction.simpleInstallAction(SimpleControllerContextAction.java:62)
	at org.jboss.dependency.plugins.action.AccessControllerContextAction.install(AccessControllerContextAction.java:71)
	at org.jboss.dependency.plugins.AbstractControllerContextActions.install(AbstractControllerContextActions.java:51)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.system.microcontainer.ServiceControllerContext.install(ServiceControllerContext.java:286)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.ServiceController.doChange(ServiceController.java:688)
	at org.jboss.system.ServiceController.start(ServiceController.java:460)
	at org.jboss.system.deployers.ServiceDeployer.start(ServiceDeployer.java:163)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:99)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:46)
	at org.jboss.deployers.spi.deployer.helpers.AbstractSimpleRealDeployer.internalDeploy(AbstractSimpleRealDeployer.java:62)
	at org.jboss.deployers.spi.deployer.helpers.AbstractRealDeployer.deploy(AbstractRealDeployer.java:50)
	at org.jboss.deployers.plugins.deployers.DeployerWrapper.deploy(DeployerWrapper.java:171)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doDeploy(DeployersImpl.java:1439)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1157)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1178)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.install(DeployersImpl.java:1098)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.process(DeployersImpl.java:781)
	at org.jboss.deployers.plugins.main.MainDeployerImpl.process(MainDeployerImpl.java:702)
	at org.jboss.system.server.profileservice.repository.MainDeployerAdapter.process(MainDeployerAdapter.java:117)
	at org.jboss.system.server.profileservice.repository.ProfileDeployAction.install(ProfileDeployAction.java:70)
	at org.jboss.system.server.profileservice.repository.AbstractProfileAction.install(AbstractProfileAction.java:53)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.install(AbstractProfileService.java:361)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.activateProfile(AbstractProfileService.java:306)
	at org.jboss.system.server.profileservice.ProfileServiceBootstrap.start(ProfileServiceBootstrap.java:271)
	at org.jboss.bootstrap.AbstractServerImpl.start(AbstractServerImpl.java:461)
	at org.jboss.Main.boot(Main.java:221)
	at org.jboss.Main$1.run(Main.java:556)
	at java.lang.Thread.run(Thread.java:619)
"
0,"Prepare CharArraySet for Unicode 4.0CharArraySet does lowercaseing if created with the correspondent flag. This causes that  String / char[] with uncode 4 chars which are in the set can not be retrieved in ""ignorecase"" mode.
"
0,"Use Java 5 enumsReplace the use of o.a.l.util.Parameter with Java 5 enums, deprecating Parameter.

Replace other custom enum patterns with Java 5 enums."
1,"When using QueryImpl.setLimit() and QueryImpl.setOffset(), then NodeIterator.getSize() reports wrong sizeWhen using QueryImpl.setLimit() and QueryImpl.setOffset(), then NodeIterator.getSize() reports wrong size. Returned size seems to be allways the same as the limit."
0,"Use common base classes in jackrabbit-core and jcr2spiAs part of JCR-742 I've implemented a number of generic JCR base classes and adapters in org.apache.jackrabbit.commons. These classes are based on existing code in jackrabbit-core.

To encourage code reuse across jackrabbit-core and jcr2spi, I'd like to make both components use these generic base classes.

"
1,"No equals operation for Credentials implementationsI tripped across a scenario where I wanted to compare credentials, so I could
know to discard connection state (and thus any associated cookies).

Patch to follow shortly."
0,"David Spencer Spell Checker improvedhy,
i developed a SpellChecker based on the David Spencer code (DSc) but more flexible.
the structure of the index is inspired of the DSc (for a 3-4 gram):
word:
gram3:
gram4:
 
3start:
4start:
..
3end:
4end:
..
transposition:
 
This index is a dictonary so there isn't the ""freq"" field like with DSc version.
it's independant of the user index. So we can add words becoming to several
fields of several index for example or, why not, to a file with a list of words.
The suggestSimilar method return a list of suggests word sorted by the
Levenshtein distance and optionaly to the popularity of the word for a specific
field in a user index. More of that, this list can be restricted only to words
present in a specific field of a user index.
 
See the test case.
 
i hope this code will be put in the lucene sandbox. 
 
Nicolas Maisonneuve"
1,"TopFieldCollector throws AIOOBE if numHits is 0See solr-user thread ""ArrayIndexOutOfBoundsException for query with rows=0 and sort param"".

I think we should just create a null collector (only tallies up totalHits) if numHits is 0?"
0,"ServerQuery does not use RemoteAdapterFactory for creating ServerQueryResultThe ServerQuery sould use the Factory for creating ServerQueryResult.

Siehe the method ServerQuery.execute():

{code}
public RemoteQueryResult execute() throws RepositoryException, RemoteException {
        return new ServerQueryResult(query.execute(), getFactory());
    }
{code}

it should be:
{code}
    public RemoteQueryResult execute() throws RepositoryException, RemoteException {
        return getFactory().getRemoteQueryResult(this.query.execute());
    }
{code}"
0,"Change SortField types to an EnumWhen updating my SOLR-2533 patch, one issue was that the int value I had given my new type had been used by another change in the mean time.  Since we don't use these fields in a bitset kind of way, we can convert them to an enum."
0,"RFC4918 feature: absolute paths in ""Destination"" and ""If"" headersRFC4918 allows absolute paths (instead of absolute URIs) in the ""Destination"" and ""If"" headers (<http://greenbytes.de/tech/webdav/rfc4918.html#rfc.section.14.8>). This makes it simpler to deal with situations where reverse proxies are involved (because those usually are not aware of WebDAV request headers and do not rewrite them)."
0,Text.escapeIllegalJCRChars should be adjusted to match the 2.0 set of illegal charsText.escapeIllegalJCRChars still contains chars that were illegal in JCR 1.0 but were removed from the set for JCR 2.0
0,"Document order of result nodes should be configurableQueries without an order by clause are performed with document order for the result nodes. This is a quite expensive operation, because the document order is available in the search index itself. The document order is calculated with the help of the ItemStateManager and requires loading of all result node states including their ancestors.

Queries with a lot of result nodes become quite expensive, even though the actual query execution is fast. Because most use cases will not care for the document order, this feature should be made configurable. Some parameter for the QueryHandler that disables the document order on result nodes."
0,"TCK: NodeReadMethodsTest.testGetName fails with NPE if  'testroot' has no child nodeThe 'testGetName' does not assert, that  the childnode field has been populated during setup.
if  for whatever reason  the test data don't provide a single childnode below the test root, this test will fail with nullpointer
exception.

i would like to suggest to use the same assertion as in testGetPath and throw a NotExecutableException in case of
missing child node.

patch attached.

"
0,"IndexWriter should prune 100% deleted segs even in the NRT caseWe now prune 100% deleted segs on commit from IW or IR (LUCENE-2010),
but this isn't quite aggressive enough, because in the NRT case you
rarely call commit.

Instead, the moment we delete the last doc of a segment, it should be
pruned from the in-memory segmentInfos.  This way, if you open an NRT
reader, or a merge kicks off, or commit is called, the 100% deleted
segment is already gone.
"
0,"IndexWriter commits unnecessarily on fresh DirectoryI've noticed IndexWriter's ctor commits a first commit (empty one) if a fresh Directory is passed, w/ OpenMode.CREATE or CREATE_OR_APPEND. This seems unnecessarily, and kind of brings back an autoCommit mode, in a strange way ... why do we need that commit? Do we really expect people to open an IndexReader on an empty Directory which they just passed to an IW w/ create=true? If they want, they can simply call commit() right away on the IW they created.

I ran into this when writing a test which committed N times, then compared the number of commits (via IndexReader.listCommits) and was surprised to see N+1 commits.

Tried to change doCommit to false in IW ctor, but it got IndexFileDeleter jumping on me .. so the change might not be that simple. But I think it's manageable, so I'll try to attack it (and IFD specifically !) back :)."
0,"Extract a generic framework for running randomized tests.{color:red}The work on this issue is temporarily at github{color} (lots of experiments and tweaking):
https://github.com/carrotsearch/randomizedtesting
Or directly: git clone git://github.com/carrotsearch/randomizedtesting.git
{color}
----

RandomizedRunner is a JUnit runner, so it is capable of running @Test-annotated test cases. It
respects regular lifecycle hooks such as @Before, @After, @BeforeClass or @AfterClass, but it
also adds the following:

Randomized, but repeatable execution and infrastructure for dealing with randomness:

- uses pseudo-randomness (so that a given run can be repeated if given the same starting seed)
  for many things called ""random"" below,
- randomly shuffles test methods to ensure they don't depend on each other,
- randomly shuffles hooks (within a given class) to ensure they don't depend on each other,
- base class RandomizedTest provides a number of methods for generating random numbers, strings
  and picking random objects from collections (again, this is fully repeatable given the
  initial seed if there are no race conditions),
- the runner provides infrastructure to augment stack traces with information about the initial
  seeds used for running the test, so that it can be repeated (or it can be determined that
  the test is not repeatable -- this indicates a problem with the test case itself).

Thread control:

- any threads created as part of a test case are assigned the same initial random seed 
  (repeatability),
- tracks and attempts to terminate any Threads that are created and not terminated inside 
  a test case (not cleaning up causes a test failure),
- tracks and attempts to terminate test cases that run for too long (default timeout: 60 seconds,
  adjustable using global property or annotations),

Improved validation and lifecycle support:

- RandomizedRunner uses relaxed contracts of hook methods' accessibility (hook methods _can_ be
  private). This helps in avoiding problems with method shadowing (static hooks) or overrides
  that require tedious super.() chaining). Private hooks are always executed and don't affect
  subclasses in any way, period.
- @Listeners annotation on a test class allows it to hook into the execution progress and listen
  to events.
- @Validators annotation allows a test class to provide custom validation strategies 
  (project-specific). For example a base class can request specific test case naming strategy
  (or reject JUnit3-like methods, for instance).
- RandomizedRunner does not ""chain"" or ""suppress"" exceptions happening during execution of 
  a test case (including hooks). All exceptions are reported as soon as they happened and multiple
  failure reports can occur. Most environments we know of then display these failures sequentially
  allowing a clearer understanding of what actually happened first.
"
0,"SessionImpl#isSupportedOption: Skip descriptor evaluation if descriptor has not been loadedfollowup issue for JCR-3076.

as jukka stated changing the jcr-server to serve the repository-descriptor without mandating a successful login would
require quite some changes on the server side as the current flow demands a successful repository login in order
to be access any resource including the root resource that acts as parent for all (available) workspaces. since the
repository-descriptor report has be requested one of the resources it also mandates a successful login although
retrieving descriptors on the jcr-level is possible when just having the repository at hand.

on the other hand i would assume that the descriptor functionality present on the Repository is rarely used.
therefore i would suggest to just relax the check for supported options in the jcr2spi session implementation
and skip the evaluation if the descriptor isn't available at all. consequently the failure of a non-supported
feature would be postponed to the point it reaches the SPI (instead of informing the API consumer upfront). 
on the other hand supported operations would not fail just because the descriptors have not been loaded."
0,"Query path constraints like foo//*/bar do not scaleTo resolve the * step the LuceneQueryBuilder currently creates a MatchAllQuery and checks every node for a foo ancestor. Instead, it should search for bar nodes and check for foo ancestors with at least one arbitrary hierarchy level in between."
0,"Allow whitespaces in base64 encoded binary fields of XML import filesWhen importing files using Session.importXML(), the Binary property values are Base64 encoded.  However you cannot put whitespaces in them, and XML files with binaries in them become very long lines.  The files are more manageable if whilespaces could be put in them, as is common to do in base base64 encoded files."
0,"Improve architecture of FieldSortedHitQueuePer the discussion (quite some time ago) on issue LUCENE-806, I'd like to propose an architecture change to the way FieldSortedHitQueue works, and in particular the way it creates SortComparatorSources. I think (I hope) that anyone who looks at the FSHQ code will agree that the class does a lot and much of it's repetitive stuff that really has no business being in that class.

I am about to attach a patch which, in and of itself, doesn't really achieve much that's concrete but does tidy things up a great deal and makes it easier to plug in different behaviours. I then have a subsequent patch which provides a fairly simple and flexible example of how you might replace an implementation, in this case the field-local-String-comparator version from LUCENE-806.

The downside to this patch is that it involved changing the signature of SortComparatorSource.newComparator to take a Locale. There would be ways around this (letting FieldSortedHitQueue take in either a SortComparatorSource or some new, improved interface which takes a Locale (and possibly extends SortComparatorSource). I'm open to this but personally I think that the Locale version makes sense and would suggest that the code would be nicer by breaking the API (and hence targeting this to, presumably, 3.0 at a minimum).

This code does not include specific tests (I will add these, if people like the general idea I'm proposing here) but all current tests pass with this change.

Patch to follow."
1,"ExportSysViewTest fails with: System property org.xml.sax.driver not specifiedThe ExportSysViewTest class uses the XMLReaderFactory.createXMLReader() method that depends on the system property ""org.xml.sax.driver"" being specified. Apparently using a TransformerFactory works around this issue in some way, as the problem only appeared once we changed the XML export feature to use a custom serializer class instead of a JAXP Transformer for serialization (see JCR-1952).

The current workaround is to explicitly force a Transformer to be loaded, but we really should fix the cause of this issue for example by replacing the XMLReader instance with a SAXParser."
1,"when opening the merged SegmentReader, IW attempts to open store files that were deletedThe issue happens when a merge runs that does not merge the doc stores, those doc stores are still being written to, IW is using CFS, and while the merge is running the doc stores get closed and turned into a cfx file.

When we then try to open the reader (for warming), which as of LUCENE-2311 will now [correctly] open the doc stores, we hit FNFE because the SegmentInfo for the merge does not realize that the doc stores were turned into  a cfx.

This issue does affect trunk; if you crank up the #docs in the test, it happens consistently (I will tie this to _TestUtil.getRandomMultiplier!)."
0,"DirectoryTaxonomyWriter should throw a proper exception if it was closedDirTaxoWriter may throw random exceptions (NPE, Already Closed - depend on what API you call) after it has been closed/rollback. We should detect up front that it is already closed, and throw AlreadyClosedException.

Also, on LUCENE-3573 Doron pointed out a problem with DTW.rollback -- it should call close() rather than refreshReader. I will fix that as well in this issue."
0,Using explain may double ram reqs for fieldcaches when using ValueSourceQuery/CustomScoreQuery or for ConstantScoreQuerys that use a caching Filter.
1,"Tika regressions in 0.8There are a few notable problems in Tika 0.8, namely TIKA-548 and TIKA-556, that may adversely affect users of Jackrabbit 2.2.

Since we don't have a Tika 0.9 release available yet, I'll add workarounds for these issues in Jackrabbit."
1,"sysview import does not resolve referenceswhen importing a sysview with references, those are not resolved against the new uuids of the mix:referenceable nodes"
0,"Journal: Use buffered input / output streamsThe journal should use buffered input / output streams wherever possible. Currently there are some places where bytes are directly written to the journal file, which degrades performance."
1,"SpellChecker.clearIndex calls unlock inappropriatelyAs noted in LUCENE-1050, fixing a bug in SimpleLockFactory related to not reporting success/filure of lock file deletion has surfaced bad behavior in SpellChecker.clearIndex...

Grant...
{quote}
It seems the SpellChecker is telling the IndexReader to delete the lockFile, but the lockFile doesn't exist.
  ...
I don't know much about the locking mechanism, but it seems like this should check to see if the lockFile exists before trying to delete it.
{quote}

Hoss...
{quote}
Grant: my take on this is that SpellChecker.clearIndex is in the wrong. it shouldn't be calling unlock unless it has reason to think there is a ""stale lock"" that needs to be closed - ie: this is a bug in SpellChecker that you have only discovered because this bug LUCENE-1050 was fixed.

I would suggest a new issue for tracking, and a patch in which SpellChecker.clearIndex doesn't call unlock unless isLocked returns true. Even then, it might make sense to catch and ignore LockReleaseFailedException and let whatever resulting exception may originate from ""new IndexWriter"" be returned.
{quote}

marking for 2.3 since it seems like a fairly trivial fix, and if we don't deal with it now it will be a bug introduced in 2.3.

"
1,"Problems with custom nodes in journalI have an application that uses custom node types and I am having problems in a clustered configuration.

Issue 1: the following definition in a nodetype is incorrectly read from the journal:
  + * (nt:hierarchyNode) version

The * is stored in the journal as _x002a_ since it should be a QName and it gets escaped.
When read, the code ...core.nodetype.compact.CompactNodeTypeDefReader.doChildNodeDefinition does the following test:

        if (currentTokenEquals('*')) {
            ndi.setName(ItemDef.ANY_NAME); 
        } else {
            ndi.setName(toQName(currentToken));
        }

Since currentToken is _x002a_ and not * toQName(currentToken) is called but it fails.
I changed the test to:
        if (currentTokenEquals('*') || currentTokenEquals(""_x002a_""))
            ....
and that fixes the problem.

Issue 2: when storing a nodeType in the journal the superclass nt:base is not store, but when reading I get an error saying the node should be a subclass of nt:base.

The code in...core.nodetype.compact.CompactNodeTypeDefWriter.writeSupertypes skips nt:base when writing the node.

When reading the nodetype definition from the journal the following exception is thrown:

Unable to deliver node type operation: [{http://namespace/app/repository/1.0}resource] all primary node types except nt:base itself must be (directly or indirectly) derived from nt:base

probably because nt:base is not re-added to the nodetype definition

 "
0,"Includes new (old) mimetypes that OpenOfficeTextExtractor can handleThe following patch adds the old openoffice (1.0 version) mimetypes to have their contents extracted. 
I've tested with simple files and it worked here. 


$ cat OpenOfficeTextExtractor-mimetype.patch
--- jackrabbit-1.4/jackrabbit-text-extractors/src/main/java/org/apache/jackrabbit/extractor/OpenOfficeTextExtractor.java    2007-12-19 12:57:58.000000000 -0200
+++ jackrabbit-1.4-modified/jackrabbit-text-extractors/src/main/java/org/apache/jackrabbit/extractor/OpenOfficeTextExtractor.java  2008-07-24 15:01:08.000000000 -0300
@@ -54,7 +54,11 @@
                            ""application/vnd.oasis.opendocument.graphics"",
                            ""application/vnd.oasis.opendocument.presentation"",
                            ""application/vnd.oasis.opendocument.spreadsheet"",
-                           ""application/vnd.oasis.opendocument.text""});
+                           ""application/vnd.oasis.opendocument.text"",
+                           ""application/vnd.sun.xml.calc"",
+                           ""application/vnd.sun.xml.draw"",
+                           ""application/vnd.sun.xml.impress"",
+                           ""application/vnd.sun.xml.writer""});
     }

     //-------------------------------------------------------< TextExtractor >
"
0,"Add Searcher.search(Query, int)Now that we've deprecated Hits (LUCENE-1290), I think we should add this trivial convenience method to Searcher, which is just sugar for Searcher.search(Query, null, int) ie null filter, returning a TopDocs.

This way there is a simple API for users to retrieve the top N results for a Query.
"
0,"Remove SortField.AUTOI'd like to remove SortField.AUTO... it's dangerous for Lucene to
guess the type of your field, based on the first term it encounters.
It can easily be wrong, and, whether it's wrong or right could
suddenly change as you index different documents.

It unexepctedly binds SortField to needing an IndexReader to do the
guessing.

It's caused various problems in the past (most recently, for me on
LUCENE-1656) as we fix other issues/make improvements.

I'd prefer that users of Lucene's field sort be explicit about the
type that Lucene should cast the field to.  Someday, if we have
optional strong[er] typing of Lucene's fields, such type information
would already be known.  But in the meantime, I think users should be
explicit.
"
0,"[PATCH] MultiFieldQueryParser and BooleanQuery do not provide adequate support for queries across multiple fieldsThe attached test case demonstrates this problem and provides a fix:
  1.  Use a custom similarity to eliminate all tf and idf effects, just to 
isolate what is being tested.
  2.  Create two documents doc1 and doc2, each with two fields title and 
description.  doc1 has ""elephant"" in title and ""elephant"" in description.  
doc2 has ""elephant"" in title and ""albino"" in description.
  3.  Express query for ""albino elephant"" against both fields.
Problems:
      a.  MultiFieldQueryParser won't recognize either document as containing 
both terms, due to the way it expands the query across fields.
      b.  Expressing query as ""title:albino description:albino title:elephant 
description:elephant"" will score both documents equivalently, since each 
matches two query terms.
  4.  Comparison to MaxDisjunctionQuery and my method for expanding queries 
across fields.  Using notation that () represents a BooleanQuery and ( | ) 
represents a MaxDisjunctionQuery, ""albino elephant"" expands to:
        ( (title:albino | description:albino)
          (title:elephant | description:elephant) )
This will recognize that doc2 has both terms matched while doc1 only has 1 
term matched, score doc2 over doc1.

Refinement note:  the actual expansion for ""albino query"" that I use is:
        ( (title:albino | description:albino)~0.1
          (title:elephant | description:elephant)~0.1 )
This causes the score of each MaxDisjunctionQuery to be the score of highest 
scoring MDQ subclause plus 0.1 times the sum of the scores of the other MDQ 
subclauses.  Thus, doc1 gets some credit for also having ""elephant"" in the 
description but only 1/10 as much as doc2 gets for covering another query term 
in its description.  If doc3 has ""elephant"" in title and both ""albino"" 
and ""elephant"" in the description, then with the actual refined expansion, it 
gets the highest score of all (whereas with pure max, without the 0.1, it 
would get the same score as doc2).

In real apps, tf's and idf's also come into play of course, but can affect 
these either way (i.e., mitigate this fundamental problem or exacerbate it)."
0,"Remove SVN.exe and revision numbers from build.xml by svn-copy the backwards branch and linking snowball tests by svn:externalsAs we often need to update backwards tests together with trunk and always have to update the branch first, record rev no, and update build xml, I would simply like to do a svn copy/move of the backwards branch.

After a release, this is simply also done:
{code}
svn rm backwards
svn cp releasebranch backwards
{code}

By this we can simply commit in one pass, create patches in one pass.

The snowball tests are currently downloaded by svn.exe, too. These need a fixed version for checkout. I would like to change this to use svn:externals. Will provide patch, soon."
0,Use only one scheduler for repository tasksThere are still a few Timer instances being used by Jackrabbit. It would be better if all tasks were scheduled by the central ScheduledExecutorService thread pool of the repository.
0,"Make Jackrabbit compile on Java 7Compiling on Java 7 fails with the following error:

    jackrabbit-core/src/main/java/org/apache/jackrabbit/core/util/db/DataSourceWrapper.java:[30,7] error:
    DataSourceWrapper is not abstract and does not override abstract method getParentLogger() in CommonDataSource

We should fix that."
1,"HttpClient does not compile 'out of the box' in IBM's VisualAge IDEThis was observed with IBM VisualAge 3.5, which runs JDK1.2.2:

Importing the HTTPClient source code into the IDE brings up a
compilation error in 

org.apache.commons.httpclient.HttpMethodBase.

The initialization of ""private ResponseConsumedWatcher m_responseWatcher""
using an anyonymous inner class seems to cause some trouble. Implicated code:

private ResponseConsumedWatcher m_responseWatcher = new ResponseConsumedWatcher
() {
	public void responseConsumed() {
		responseBodyConsumed();
	}
};

The error message is: ""Field initialization: The constructor invoked to create
org.apache.commons.httpclient.HttpMethodBase$1 with arguments () is not defined""

...but only in the context of HttpMethodBase(String uri) constructor, i.e.
the HttpMethodBase() constructor *can* be compiled, HttpMethodBase(String uri)
*cannot* be compiled with error ""Cannot create constructor due to incorrect
field initialization"".

I interpret this to mean that the compiler is looking for a parameterless
constructor for the anonymous class in the context of 
HttpMethodBase(String uri). The message did not really make sense to me. 
Checked the syntax, checked in the Language Definition whether setting up an
anonymous class like that is permitted; found nothing obviously wrong.

Fix:

The code above is equivalent to constructing the instance at the beginning
of each constructor of the enclosing class. A copy and paste of the
construction code into each of the two constructors fixes things...until
the next update."
0,"Method to create default RepositoryConfig from just the repository directoryIt would be useful to have a static method like RepositoryConfig.create(File) that would take the repository directory and expect to find the repository configuration in a ""repository.xml"" file inside that directory.

If the directory does not exist, it would be created. And if the repository configuration file does not exist, then it would be created from the default configuration included in Jackrabbit."
1,"Text.unescape(""%"") throws a StringIndexOutOfBoundsExceptionYou get the following exception:

java.lang.StringIndexOutOfBoundsException: String index out of range: 3
	at java.lang.String.substring(String.java:1935)
	at org.apache.jackrabbit.util.Text.unescape(Text.java:407)
	at org.apache.jackrabbit.util.Text.unescape(Text.java:438)

It would be better if it failed with IllegalArgumentException."
1,"it is not possible to register an event listener which listens to mixin nodetypesit would be a nice enhancement if one could as well define mixin nodetypes to be listened:
...
om.addEventListener(this,
                        Event.PROPERTY_ADDED | Event.PROPERTY_CHANGED | Event.PROPERTY_REMOVED,
                        ""/"",
                        true,
                        null,
                        new String[]{""mix:Custom""},
                        false);
..."
0,bug form doesn't list latest version 
0,"Rate-limit IO used by mergingLarge merges can mess up searches and increase NRT reopen time (see
http://blog.mikemccandless.com/2011/06/lucenes-near-real-time-search-is-fast.html).

A simple rate limiter improves the spikey NRT reopen times during big
merges, so I think we should somehow make this possible.  Likely this
would reduce impact on searches as well.

Typically apps that do indexing and searching on same box are in no
rush to see the merges complete so this is a good tradeoff.
"
0,"+ - operators allow any amount of whitespaceAs an example, (foo - bar) is treated like (foo -bar).
It seems like for +- to be treated as unary operators, they should be immediately followed by the operand."
0,"[RFE] Allow streaming of POST methods via chunked transfer encoding.This is an RFE with a possible implementation attached. The implementation does
not modify any existing code.

We're using HTTP POST to send a large amount of data with an unknown size. We
don't want to buffer the entire request, so we implemented a streaming POST
method. The implementation has 3 classes: StreamedPostMethod,
BufferedChunkedOutputStream and OutputStreamWriter. The bulk of the code is in
the BufferedChunkedOutputStream, which may be a good target for replacing
ChunkedOutputStream from the main distribution.

BufferedChunkedOutputStream has the following charactersitics:
1) It has an internal 2K buffer. Without the buffer, chunk sizes would be too
small in many cases (e.g. ObjectOutputStream likes to call write(byte[]) with 4
byte long arguments). 2K was chosen to minimize the chunk overhead to less than 1%.
2) If the entire entity body fits within the 2K buffer, it does not use
chunking. This implies that the headers are only sent out when the first chunk
(or the entire body) has to be written, but no sooner.
3) The chunk size is not limited to 2K: if write(byte[]) is called with a large
argument, the internal buffer and the new request are sent out as a single chunk.
4) Because of (2) it's tightly coupled to StreamedPostMethod.reallyWriteHeaders.
5) StreamedPostMethod calls BufferedChunkedOutputStream.finish() to write the
last buffer and ending chunk.

Because of 4 and 5, we didn't want to touch ChunkedOutputStream. Interestingly,
EntityEnclosingMethod is already tightly coupled to ChunkedOutputStream because
it has to call writeClosingChunk. There is probably some room for refactoring here.

The package is just a suggestion; feel free to move the files as appropirate.
This code was written against 2.0rc2. We're hoping it will get included in time
for the 2.1 release.

To use the code, you must implement OutputStreamWriter and pass it to
StreamedPostMethod's constructor. Execute the method as usual.

Caveats: StreamedPostMethod does not implement Expect/continue logic. We had no
way to test this. It is also strictly for POST. In general, the same methodology
is applicable to PUT, etc. It should be fairly simple to generalize.

Legal: Goldman, Sachs & Co. is making this code available under the Apache License."
1,CompactNodeTypeDefReader does not recognise MIXIN ORDERABLE sequencethe code in 'doOptions' misses to set the setOrderableChildNodes flag if the order of the tokens is MIXIN ORDERABLE.
0,"Patch for ShingleFilter.enablePositions (or PositionFilter)Make it possible for *all* words and shingles to be placed at the same position, that is for _all_ shingles (and unigrams if included) to be treated as synonyms of each other.

Today the shingles generated are synonyms only to the first term in the shingle.
For example the query ""abcd efgh ijkl"" results in:
   (""abcd"" ""abcd efgh"" ""abcd efgh ijkl"") (""efgh"" efgh ijkl"") (""ijkl"")

where ""abcd efgh"" and ""abcd efgh ijkl"" are synonyms of ""abcd"", and ""efgh ijkl"" is a synonym of ""efgh"".

There exists no way today to alter which token a particular shingle is a synonym for.
This patch takes the first step in making it possible to make all shingles (and unigrams if included) synonyms of each other.

See http://comments.gmane.org/gmane.comp.jakarta.lucene.user/34746 for mailing list thread."
0,"Optimization for IndexWriter.addIndexes()One big performance problem with IndexWriter.addIndexes() is that it has to optimize the index both before and after adding the segments.  When you have a very large index, to which you are adding batches of small updates, these calls to optimize make using addIndexes() impossible.  It makes parallel updates very frustrating.

Here is an optimized function that helps out by calling mergeSegments only on the newly added documents.  It will try to avoid calling mergeSegments until the end, unless you're adding a lot of documents at once.

I also have an extensive unit test that verifies that this function works correctly if people are interested.  I gave it a different name because it has very different performance characteristics which can make querying take longer."
1,"SpellChecker does not work properly on calling indexDictionary after clearIndexWe have to call clearIndex and indexDictionary to rebuild dictionary from fresh. The call to SpellChecker clearIndex() function does not reset the searcher. Hence, when we call indexDictionary after that, many entries that are already in the stale searcher will not be indexed.

Also, I see that IndexReader reader is used for the sole purpose of obtaining the docFreq of a given term in exist() function. This functionality can also be obtained by using just the searcher by calling searcher.docFreq. Thus, can we get away completely with reader and code associated with it like
      if (IndexReader.isLocked(spellIndex)){
	IndexReader.unlock(spellIndex);
      }
and the reader related code in finalize?

"
0,"Use AtomicReaderContext also for CustomScoreProviderWhen moving to AtomicReaderContext, one place was not changed to use it: CustomScoreQuery's CustomScoreProvider. It should also take AtomicReaderContext instead of IndexReader, as this may help users to effectively implement custom scoring there absolute DocIds are needed."
1,"intermittent failure in TestIndexWriter. testRandomIWReaderRarely, this test (which was added with LUCENE-1516) fails in MockRAMDirectory.close because some files were not closed, eg:
{code}
   [junit] NOTE: random seed of testcase 'testRandomIWReader' was: -5001333286299627079
   [junit] ------------- ---------------- ---------------
   [junit] Testcase: testRandomIWReader(org.apache.lucene.index.TestStressIndexing2):        Caused an ERROR
   [junit] MockRAMDirectory: cannot close: there are still open files: {_cq.tvx=3, _cq.fdx=3, _cq.tvf=3, _cq.tvd=3, _cq.fdt=3}
   [junit] java.lang.RuntimeException: MockRAMDirectory: cannot close: there are still open files: {_cq.tvx=3, _cq.fdx=3, _cq.tvf=3, _cq.tvd=3, _cq.fdt=3}
   [junit]     at org.apache.lucene.store.MockRAMDirectory.close(MockRAMDirectory.java:292)
   [junit]     at org.apache.lucene.index.TestStressIndexing2.testRandomIWReader(TestStressIndexing2.java:66)
   [junit]     at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)
{code}"
0,"Misleading method names in SetValueBinaryTestSome of the method names in SetValueBinaryTest say ""Boolean"" when they should say ""Binary"" (copy&paste error from SetValueBooleanTest?).

"
0,"nightly builds depend on cloveras reported by Michael Pelz Sherman on java-dev@lucene and solr-user@lucene the nightly builds coming out of hudson current depend on clover...

  [root@crm.test.bbhmedia.net tmp]# strings lucene-core-nightly.jar | grep -i clover|more
org/apache/lucene/LucenePackage$__CLOVER_0_0.class
org/apache/lucene/analysis/Analyzer$__CLOVER_1_0.class
...

the old nightly.sh dealt with this by running ant nightly twice, first without clover to get the jars and then with clover to get the report.  it loks like maybe this logic never made it into the hudson setup.

someone with hudson admin access/knowledge will need to look into this."
0,"Add an explicit method to invoke IndexDeletionPolicyToday, if one uses an IDP which holds onto segments, such as SnapshotDeletionPolicy, or any other IDP in the tests, those segments are left in the index even if the IDP no longer references them, until IW.commit() is called (and actually does something). I'd like to add a specific method to IW which will invoke the IDP's logic and get rid of the unused segments w/o forcing the user to call IW.commit(). There are a couple of reasons for that:

* Segments take up sometimes valuable HD space, and the application may wish to reclaim that space immediately. In some scenarios, the index is updated once in several hours (or even days), and waiting until then may not be acceptable.
* I think it's a cleaner solution than waiting for the next commit() to happen. One can still wait for it if one wants, but otherwise it will give you the ability to immediately get rid of those segments.
* TestSnapshotDeletionPolicy includes this code, which only strengthens (IMO) the need for such method:
{code}
// Add one more document to force writer to commit a
// final segment, so deletion policy has a chance to
// delete again:
Document doc = new Document();
doc.add(new Field(""content"", ""aaa"", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
writer.addDocument(doc);
{code}

If IW had an explicit method, that code would not need to exist there at all ...

Here comes the fun part - naming the baby:
* invokeDeletionPolicy -- describes exactly what is going to happen. However, if the user did not set IDP at all (relying on default, which I think many do), users won't understand what is it.
* deleteUnusedSegments - more user-friendly, assuming users understand what 'segments' are.

BTW, IW already has deleteUnusedFiles() which only tries to delete unreferenced files that failed to delete before (such as on Windows, due to e.g. open readers). Perhaps instead of inventing a new name, we can change IW.deleteUnusedFiles to call IndexFileDeleter.checkpoint (instead of deletePendingFiles) which deletes those files + calls IDP.onCommit()."
0,Hide ugly repository init code for OCMHide repository namespace registration and ocm:discriminator node type registration in implementation of OCM
1,"NullPointerException when iterating over propertiesRunning ConcurrentReadWriteTest (NUM_NODES=5, NUM_THREADS=3, RUN_NUM_SECONDS=120) resulted in a NullPointerException:

Exception in thread ""Thread-11"" java.lang.NullPointerException
	at org.apache.commons.collections.map.AbstractReferenceMap$ReferenceEntry.getValue(AbstractReferenceMap.java:596)
	at org.apache.commons.collections.map.AbstractReferenceMap.containsKey(AbstractReferenceMap.java:204)
	at org.apache.jackrabbit.core.state.ItemStateMap.contains(ItemStateMap.java:66)
	at org.apache.jackrabbit.core.state.ItemStateReferenceCache.isCached(ItemStateReferenceCache.java:91)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.hasItemState(LocalItemStateManager.java:173)
	at org.apache.jackrabbit.core.state.XAItemStateManager.hasItemState(XAItemStateManager.java:252)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:174)
	at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:495)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:326)
	at org.apache.jackrabbit.core.LazyItemIterator.prefetchNext(LazyItemIterator.java:90)
	at org.apache.jackrabbit.core.LazyItemIterator.next(LazyItemIterator.java:203)
	at org.apache.jackrabbit.core.LazyItemIterator.nextProperty(LazyItemIterator.java:118)
	at org.apache.jackrabbit.core.ConcurrentReadWriteTest$1$1.execute(ConcurrentReadWriteTest.java:64)
	at org.apache.jackrabbit.core.AbstractConcurrencyTest$Executor.run(AbstractConcurrencyTest.java:110)
	at java.lang.Thread.run(Thread.java:619)

The cache is not synchronized and is accessed at the same time by the current thread and another thread that notified ItemStates about changes."
0,"HttpClient throws java.net.SocketException instead of org.apache.http.conn.ConnectionTimeoutException when connection timeout occursWhen sending an http request a connection timeout occurs, the HttpClient.execute method throws a java.net.SocketException instead of a org.apache.http.conn.ConnectionTimeoutException.

java.net.SocketTimeoutException: connect timed out
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:519)
        at org.apache.http.conn.scheme.PlainSocketFactory.connectSocket(PlainSocketFactory.java:119)
        at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:129)
        at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:164)
        at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:119)
        at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:349)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:555)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:487)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:465)
"
0,Jackrabbit JCR Nodemanagement API implementationThere needs to be a Jackrabbit implementation of the org.apache.portals.graffito.jcr.nodemanagement.NodeTypeManager interface.
1,"IndexMerger blocks client threads when obsolete index segments are deletedWhen index segments have been merged, the obsolete indexes are replaced with the new one an deleted afterwards. Currently deleting the obsolete segments is inside a MultiIndex synchronized block, which may block other threads from updating the index concurrently."
0,"[API Doc] Improve the description of the preemptive authenticationHttpClient authentication guide does not reflect the fact that preemptive
authentication requires default credentials to be set. It should also mention
the security implications of preemptive authentication (default credentials sent
with EVERY request to ANY target / proxy server)"
0,"cache module does not recognize equivalent URIshttp://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.2.3

""When comparing two URIs to decide if they match or not, a client SHOULD use a case-sensitive octet-by-octet comparison of the entire URIs, with these exceptions:
  * A port that is empty or not given is equivalent to the default port for that URI-reference;
  * Comparisons of host names MUST be case-insensitive;
  * Comparisons of scheme names MUST be case-insensitive;
  * An empty abs_path is equivalent to an abs_path of ""/"".
Characters other than those in the ""reserved"" and ""unsafe"" sets (see RFC 2396 [42]) are equivalent to their """"%"" HEX HEX"" encoding.

For example, the following three URIs are equivalent:
      http://abc.com:80/~smith/home.html
      http://ABC.com/%7Esmith/home.html
      http://ABC.com:/%7esmith/home.html""

The current implementation does not canonicalize the URIs it uses for cache keys, and so is missing potential cache hits. More importantly, though, required invalidations due to PUT/POST/DELETE to a URI (as well as those mentioned in Location or Content-Location headers) may not occur properly due to this bug."
0,"Support for handling large files should be addedYou should probably change the EntityEnclosingMethod content length field 
to a long in a future release. Currently it's an int."
1,"bad normalization in sorted search returning TopDocsFieldSortedHitQueue.maxscore is maintained in the lessThan method (which never gets called if a single document is added to the queue).

I've checked in a test to TestSort.testTopDocsScores() with the final assertion commented out."
0,"Allow basic regexp in namespace prefix of index-ruleCurrently a regular expression is limited to the local name, which makes fallback declarations that should match everything else difficult to write. I.e. you have to write a line per namespace in the node type registry, which bloats the index-rule unnecessarily.

Currently:

<property isRegexp=""true"">.*</property>

will only match properties with the empty namespace URI.

I propose we allow a basic regular expression in the prefix. That is the match all pattern: '.*' (dot star).

The following would match any property, including any namespace:

<property isRegexp=""true"">.*:.*</property>
"
0,"misleading lack of javadoc in StringRequestEntityWhen using httpclient2, we were doing the following:

	// Add the Content-type header.  This sets the charset to UTF-8.
	method.setRequestHeader( ""Content-type"", ""text/xml; charset=UTF-8"" );
	// The given string is converted internally by the post method into
	// a UTF-8 encoded byte array.
	method.setRequestBody( xmlstring );

The comments show that this was the way we used to obtain a UTF-8 encoded XML
document (if this was wrong, that may be the origin of the problem?).


When upgrading to httpclient3 and killing deprecated code, this was converted to:

	// Add the Content-type header.  This sets the charset to UTF-8.
	method.setRequestHeader( ""Content-type"", ""text/xml; charset=UTF-8"" );
	// The given string is converted internally by the post method into
	// a UTF-8 encoded byte array.
        method.setRequestEntity( new StringRequestEntity( xmlstring ) );

which went without problem during the tests on my machine and on test production
machine.. because platforms charset were UTF-8, which is not the case for
production machines :(

I think the javadoc of the used StringRequestEntity constructor should strongly
state that it uses String#getBytes for the content, which uses the platform
charset. Also, I didn't notice any ""upgrade to 3.x"" documentation which would
have helped me :/"
0,"Data Store: remove kill switch ""InternalValue.USE_DATA_STORE""There is still a ""kill switch"" (public static final boolean USE_DATA_STORE) in the class org.apache.jackrabbit.core.value.InternalValue. In version 2.0 this constant should be removed. Also, the system property ""org.jackrabbit.useDataStore"" will no longer be used. 

It is still possible to disable the DataStore (don't include a DataStore configuration in repository.xml)."
0,"Include diagnostics per-segment when writing a new segmentIt would be very helpful if each segment in an index included
diagnostic information, such as the current version of Lucene.

EG, in LUCENE-1474 this would be very helpful to see if certain
segments were written under 2.4.0.

We can start with just the current version.

We could also consider making this extensible, so you could provide
your own arbitrary diagnostics, but SegmentInfo/s is not public so I
think such an API would be ""one-way"" in that you'd have to use
CheckIndex to check on it later.  Or we could wait on such extensibility
until we provide some consistent way to access per-segment details
in the index.
"
0,"Add tests.iter.min to improve controlling tests.iter's behaviorAs discussed here: http://lucene.472066.n3.nabble.com/Stop-iterating-if-testsFailed-td2747426.html, this issue proposes to add tests.iter.min in order to allow one better control over how many iterations are run:

* Keep tests.iter as it is today
* Add tests.iter.min (default to tests.iter) to denote that at least N instances of the test should run until there's either a failure or tests.iter is reached.

If one wants to run until the first failure, he can set tests.iter.min=1 and tests.iter=X -- up to X instances of the test will run, until the first failure.

Similarly, one can set tests.iter=N to denote that at least N instances should run, regardless if there were failures, but if after N runs a failure occurred, the test should stop.

Note: unlike what's proposed on the thread, tests.iter.max is dropped from this proposal as it's exactly like tests.iter, so no point in having two similar parameters.

I will work on a patch tomorrow."
1,"Formatting error in ReportTask in contrib/benchmarkI am building a new Task, AnalyzerTask, that lets you change the Analyzer in the loop, thus allowing for the comparison of the same Analyzers over the set of documents.

My algorithm declaration looks like:
NewAnalyzer(WhitespaceAnalyzer, SimpleAnalyzer, StopAnalyzer, standard.StandardAnalyzer)

And it could be longer.

The exception is:
Error: cannot execute the algorithm! String index out of range: 85
java.lang.StringIndexOutOfBoundsException: String index out of range: 85
	at java.lang.String.substring(String.java:1765)
	at org.apache.lucene.benchmark.byTask.utils.Format.format(Format.java:85)
	at org.apache.lucene.benchmark.byTask.tasks.ReportTask.tableTitle(ReportTask.java:85)
	at org.apache.lucene.benchmark.byTask.tasks.ReportTask.genPartialReport(ReportTask.java:140)
	at org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask.reportSumByName(RepSumByNameTask.java:77)
	at org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask.doLogic(RepSumByNameTask.java:39)
	at org.apache.lucene.benchmark.byTask.tasks.PerfTask.runAndMaybeStats(PerfTask.java:83)
	at org.apache.lucene.benchmark.byTask.tasks.TaskSequence.doSerialTasks(TaskSequence.java:112)
	at org.apache.lucene.benchmark.byTask.tasks.TaskSequence.doLogic(TaskSequence.java:93)
	at org.apache.lucene.benchmark.byTask.utils.Algorithm.execute(Algorithm.java:228)
	at org.apache.lucene.benchmark.byTask.Benchmark.execute(Benchmark.java:73)
	at org.apache.lucene.benchmark.byTask.Benchmark.main(Benchmark.java:109)

The error seems to be caused by the fact that ReportTask uses the OP (operation) column for the String, but then uses the length of the algorithm declaration to index into the String, resulting in the index out of bounds exception.

The line in question is:
return (s + padd).substring(0, col.length());

And probably should be changed to something like:
    String s1 = (s + padd);
    return s1.substring(0, Math.min(col.length(), s1.length()));

Either that or the column should be trimmed.  The workaround is to explicitly name the task.

If no objections, I will make the change, tomorrow.  "
0,Update Lucene to 3.0Lucene 3.0 was released on 2009/11/25. They migrated to Java 1.5 as Jackrabbit is doing with 2.0. Also they added some new optimizations. It would be nice if Jackrabbit could switch to the new lucene version too.
0,"Root exception not logged in ClusterNode for ClusterExceptionWhen our MySQL server is down or failed queries we have the following log :
ERROR (ClusterNode-node1) [org.apache.jackrabbit.core.cluster.ClusterNode] Periodic sync of journal failed: Unable to return record iterater.

So the root exception (SQLException in my case) is missing from the log and this prevent me from quickly finding the reason."
1,"Version.isSame(Object) not workingVersion interface is implemented (on the frontend) by the VersionImpl class (extending NodeWrapper), which delegates to an internal NodeImpl class, which in turn extends ItemImpl.

Say you have :
      Node node = // at Version 1.0
      Version version = // retrieved as 1.0 for the node
      Version baseVersion = node.getBaseVersion()

You now expect
      baseVersion.isSame(version)
even if
      baseVersion != version

This fails, because VersionImpl delegates the isSame call to its delegatee, thus above call becomes
      ((VersionImpl) baseVersion).delegatee.isSame(version)
where this method is implemented by the ItemImpl class from which the delegatee NodeImpl extends.

That latter implementation ItemImpl.isSame() only returns true if the other is an ItemImpl, too. But this is not the case because VersionImpl is a Version, NodeWrapper, Node but not an ItemImpl.

Probably the best solution would be for NodeImpl.isSame() to check whether the otherItem is a NodeWrapper und use ((NodeWrapper) otherItem).delegatee as the otherItem for the delegatee call.

On another track: ItemImpl.isSame() should probably do a fast check whether the otherItem is actually the same instance to prevent type checks..."
1,"NullPointerException in ItemManagerWe have a lot of these occurring:
java.lang.NullPointerException
	at org.apache.jackrabbit.core.ItemManager.getDefinition(ItemManager.java:206)
	at org.apache.jackrabbit.core.ItemData.getDefinition(ItemData.java:99)
	at org.apache.jackrabbit.core.AbstractNodeData.getNodeDefinition(AbstractNodeData.java:73)
	at org.apache.jackrabbit.core.NodeImpl.getDefinition(NodeImpl.java:2430)
	at org.apache.jackrabbit.core.ItemValidator.isProtected(ItemValidator.java:373)
	at org.apache.jackrabbit.core.ItemValidator.checkCondition(ItemValidator.java:273)
	at org.apache.jackrabbit.core.ItemValidator.checkRemove(ItemValidator.java:254)
	at org.apache.jackrabbit.core.ItemRemoveOperation.perform(ItemRemoveOperation.java:63)
	at org.apache.jackrabbit.core.session.SessionState.perform(SessionState.java:200)
	at org.apache.jackrabbit.core.ItemImpl.perform(ItemImpl.java:91)
	at org.apache.jackrabbit.core.ItemImpl.remove(ItemImpl.java:322)
	at org.apache.jackrabbit.core.NPEandCMETest$TestTask.run(NPEandCMETest.java:87)
	at java.lang.Thread.run(Thread.java:679)

I'll attach a junit test to reproduce this exception."
0,"SpanQueryFilter additionSimilar to the QueryFilter (or whatever it is called now) the SpanQueryFilter is a regular Lucene Filter, but it also can return Spans-like information.  This is useful if you not only want to filter based on a Query, but you then want to be able to compare how a given match from a new query compared to the positions of the filtered SpanQuery.  Patch to come shortly also contains a caching mechanism for the SpanQueryFilter"
1,"DirectoryReader ignores NRT SegmentInfos in #isOptimized()DirectoryReader  only takes shared (with IW) SegmentInfos into account in DirectoryReader#isOptimized(). This can return true even if the actual realtime reader sees more than one segments. 

{code}
public boolean isOptimized() {
    ensureOpen();
   // if segmentsInfos changes in IW this can return false positive
    return segmentInfos.size() == 1 && !hasDeletions();
  }
{code}

DirectoryReader should check if this reader has a non-nul segmentInfosStart and use that instead"
1,"Calling size method of a ManageableArrayList causes NullPointerExceptionWhen using the NTCollectionConverterImpl with proxy=""true"" a call on the size () method of a ManageableArrayList causes a NullPointerException if there is no underlying List. LazyCollectionLoader doLoad returns null because there is are no children.

The ManageableArrayList is created because the isNull method of the NTCollectionConverterImpl class always returns false. 
According to the comment line this is done because the getCollectionNodes always returns a list. 
But after the fix for JCR-882 this is not correct anymore.

The attached fix corrects this. 

The only question remaining is how to differ between an empty list and a null-value for the field containing the list."
0,"don't spawn thread statically in FSDirectory on Mac OS Xon the Mac, creating the digester starts up a PKCS11 thread.

I do not think threads should be created statically (I have this same issue with TimeLimitedCollector and also FilterManager).

Uwe discussed removing this md5 digester, I don't care if we remove it or not, just as long as it doesn't create a thread,
and just as long as it doesn't use the system default locale."
0,"[PATCH] Efficiently retrieve sizes of field valuesSometimes an application would like to know how large a document is before retrieving it.  This can be important for memory management or choosing between algorithms, especially in cases where documents might be very large.

This patch extends the existing FieldSelector mechanism with two new FieldSelectorResults:  SIZE and SIZE_AND_BREAK.  SIZE creates fields on the retrieved document that store field sizes instead of actual values.  SIZE_AND_BREAK is especially efficient if one field comprises the bulk of the document size (e.g., the body field) and can thus be used as a reasonable size approximation.
"
0,"CharArraySet cannot be made generic, because it violates the Set<char[]> interfaceI tried to make CharArraySet using generics (extends AbstractSet<char[]>) but this is not possible, as it e.g. returns sometimes String instances in the Iterator instead of []. Also its addAll method accepts both String and char[]. I think this class is a complete mis-design and violates almost everything (sorry).

What to do? Make it Set<?> or just place a big @SuppressWarnings(""unchecked""> in front of it?

Because of this problem also a lot of Set declarations inside StopAnalyzer cannot be made generic as you never know whats inside."
1,"Query parser builds invalid parse treeCalling org.apache.jackrabbit.spi.commons.query.QueryParser.parse on 

SELECT prop1 FROM nt:unstructured WHERE prop1 IS NOT NULL ORDER BY prop1 ASC 

results in the following parse tree

+ Select properties: {}prop1
 + PathQueryNode
   + LocationStepQueryNode:  NodeTest=* Descendants=true Index=NONE
     + RelationQueryNode: Op: NOT NULL Prop=@{}prop1 Type=STRING Value=%
     + NodeTypeQueryNode:  Prop={http://www.jcp.org/jcr/1.0}primaryType Value={http://www.jcp.org/jcr/nt/1.0}unstructured
 + OrderQueryNode
   {}prop1 asc=true

The RelationQueryNode should not have a second operand since the NOT NULL operator is unary.

"
1,Update monitor is not releasedWhen the timer thread in MultiIndex commits the volatile index after some idle time it does not release / reset the updateInProgress flag. This results in queries that hang until another thread writes to the workspace.
1,"IndexWriter.synced  field accumulates data leading to a Memory LeakI am running into a strange OutOfMemoryError. My small test application does
index and delete some few files. This is repeated for 60k times. Optimization
is run from every 2k times a file is indexed. Index size is 50KB. I did analyze
the HeapDumpFile and realized that IndexWriter.synced field occupied more than
half of the heap. That field is a private HashSet without a getter. Its task is
to hold files which have been synced already.

There are two calls to addAll and one call to add on synced but no remove or
clear throughout the lifecycle of the IndexWriter instance.

According to the Eclipse Memory Analyzer synced contains 32618 entries which
look like file names ""_e065_1.del"" or ""_e067.cfs""

The index directory contains 10 files only.

I guess synced is holding obsolete data "
0,"Use a System.arraycopy more than a forIn org.apache.lucene.index.DocumentWriter. The patch will explain by itself. I didn't make any performance test, but I think it is obvious that it will be faster.
All tests passed."
0,"extensibility patch for simple WebDAV servletattaching a patch that makes the simple WebDAV servlet more extensible - subclasses can now provide their own support objects such as DavResourceFactory and DavLocatorFactory. also patches DavResourceImpl to make importXml and importFile methods protected, for subclass use.
"
0,"Open access modifier for RepositoryImpl.doShutdown()This is required for a subclass of RepositoryImpl that wants to run additional code on shutdown, otherwise a deadlock may occur because the sequence of lock acquisition cannot be ensured.

Jackrabbit requires that the shutdownLock is first acquired and then the actual shutdown code is executed."
1,"Yet another race in IW#nrtIsCurrentIn IW#nrtIsCurrent looks like this:

{code}
  synchronized boolean nrtIsCurrent(SegmentInfos infos) {
    ensureOpen();
    return infos.version == segmentInfos.version && !docWriter.anyChanges() && !bufferedDeletesStream.any();
  }
{code}

* the version changes once we checkpoint the IW
* docWriter has changes if there are any docs in ram or any deletes in the delQueue
* bufferedDeletes contain all frozen del packages from the delQueue

yet, what happens is 1. we decrement the numDocsInRam in DWPT#doAfterFlush (which is executed during DWPT#flush) but before we checkpoint. 2. if we freeze deletes (empty the delQueue) we put them in the flushQueue to maintain the order.  This means they are not yet in the bufferedDeleteStream.

Bottom line, there is a window where we could see IW#nrtIsCurrent returning true if we check within this particular window. Phew, I am not 100% sure if that is the reason for our latest failure in SOLR-2861 but from what the logs look like this could be what happens. If we randomly hit low values for maxBufferedDocs & maxBufferedDeleteTerms this is absolutely possible."
0,"The 1.5.0 webapp points to 1.4 javadocsThere's a ""Jackrabbit API"" link on the Jackrabbit webapp 1.5.0 that points to http://jackrabbit.apache.org/api/1.4/. It should be updated to point to http://jackrabbit.apache.org/api/1.5/."
0,"Add set/getLocalAddress methods to HostConfigurationOn clustered or multi-homed systems, there's a need to specify the local bind
address of sockets, to ensure that they're created on the right interface.  To
do this, the local address needs to be passed to the 4-argument version of
ProtcolSocketFactory.createSocket.

After discussion on the mailing list, the best approach for this seems to be
adding the local address as a property on HostConfiguration and HttpConnection.  

I've attached a patch which does the following:
- Add public set/getLocalAddress methods to HostConfiguration and HttpConnection.
- HttpConnection uses the local address when opening connections.
- Modify HostConfiguration.equals and hostEquals to compare the local address too.
- SimpleHttpConnectionManager uses the local address from the provided config. 
I also cleaned up its getConnection method a bit.
- HttpClient.executeMethod uses the local address from its default
HostConfiguration if the method's config doesn't specify one."
1,"PriorityQueue is inheriently broken if subclass attempts to use ""heap"" w/generic T bound to anything other then ""Object""as discovered in SOLR-2410 the fact that the protected ""heap"" variable in PriorityQueue is initialized using an Object[] makes it impossible for subclasses of PriorityQueue to exist and access the ""heap"" array unless they bind the generic to Object."
0,"Deprecate all String/File ctors/opens in IndexReader/IndexWriter/IndexSearcherDuring investigation of LUCENE-1658, I found out, that even LUCENE-1453 is not completely fixed.
As 1658 deprecates all FSDirectory.getDirectory() static factories, we should not use them anymore. As the user is now free to choose the correct directory implementation using direct instantiation or using FSDir.open() he should no longer use all ctors/methods in IndexWriter/IndexReader/IndexSearcher & Co. that simply take path names as String or File and always instantiate the Directory himself.

LUCENE-1453 currently works for the cached directory implementations from FSDir.getDirectory, but not with uncached, non refcounting FSDirs. Sometime reopen() closes the directory (as far as I see, when a SegmentReader changes to a MultiSegmentReader and/or deletes apply). This is hard to track. In Lucene 3.0 we then can remove the whole bunch of closeDirectory parameters/fields in these classes and simply do not care anymore about closing directories.

To remove this closeDirectory parameter now (before 3.0) and also fix 1453 correctly, an additional idea would be to change these factories that take the File/String to return the IndexReader wrapped by a FilteredIndexReader, that keeps track on closing the underlying directory after close and reopen. This is simplier than passing this boolean between different DirectoryIndexReader instances. The small performance impact by wrapping with FilterIndexReader should not be so bad, because the method is deprecated and we can state, that it is better to user the factory method with Directory parameter."
1,"org.apache.lucene.analysis.cn.ChineseTokenizer missing offset decrementApparently, in ChineseTokenizer, offset should be decremented like bufferIndex
when Character is OTHER_LETTER.  This directly affects startOffset and endOffset
values.

This is critical to have Highlighter working correctly because Highlighter marks
matching text based on these offset values."
0,"PayloadTermQuery refers to a deprecated documentation for redirection When PayloadTermQuery refers to the function for scoring Similarity - it refers to override the deprecated method - 

Similarity#scorePayload(String, byte[],int,int) . 

That method has been deprecated by  Similarity#scorePayload(int, String, int, int, byte[],int,int) . 


This javadoc patch addresses the class level javadoc for the class to provide the right signature in Similarity to be overridden. "
0,standard codec's terms dict seek should only scan if new term is in same index blockTermInfosReader in trunk already optimizes for this case... just need to do the same on flex.
0,"add IndexWriter.removeUnferencedFiles, so apps can more immediately delete index files when readers are closedThis has come up several times on the user's list.

On Windows, which prevents deletion of still-open files, IndexWriter cannot remove files that are in-use by open IndexReaders.  This is fine, and IndexWriter periodically retries the delete, but it doesn't retry very often (only on open, on flushing a new segment, and on committing a merge).  So it lacks immediacy.

With this expert method, apps that want faster deletion can call this method."

label,summmarydescription
0,"Remove some synchronization on CachingNamespaceResolverThe methods getQName() and getJCRName() are unnecessarily synchronized and cause monitor contention with concurrent calls to the methods of the NameCache interface (those are also synchronized).

I propose the following change:

Index: CachingNamespaceResolver.java
===================================================================
--- CachingNamespaceResolver.java	(revision 488245)
+++ CachingNamespaceResolver.java	(working copy)
@@ -84,7 +84,7 @@
     /**
      * @deprecated use {@link NameFormat#parse(String, NamespaceResolver)}
      */
-    public synchronized QName getQName(String name)
+    public QName getQName(String name)
             throws IllegalNameException, UnknownPrefixException {
         return NameFormat.parse(name, this);
     }
@@ -92,7 +92,7 @@
     /**
      * @deprecated use {@link NameFormat#format(QName, NamespaceResolver)}
      */
-    public synchronized String getJCRName(QName name)
+    public String getJCRName(QName name)
             throws NoPrefixDeclaredException {
         return NameFormat.format(name, this);
     }
"
1,"XPath relative path support missing for ""is null"" and ""is not null""I believe the change for issue JCR-247 is incomplete, for instance

  //*[@x]

and

  //*[foo/@x]

are parsed into the same query tree."
0,"Provide More of Lucene For MavenPlease provide javadoc & source jars for lucene-core.  Also, please provide the rest of lucene (the jars inside of ""contrib"" in the download bundle) if possible."
1,"ChunkedInputStream broken (2 bugs + fixes, 1 suggestion)Bug 1.

In the     

read(byte[] b, int off, int len)  method

of ChunkedInputStream, the number of bytes to read from the underlying 
InputStream is calculated wrongly. In the code this is done by

len = Math.min(len, chunkSize);

This could (and will) cause the server (also Apache) to indeed serve that 
number of bytes (let's say chunkSize), but it may be that we already had a 
number of bytes on the first read. The result is that the input is now NOT 
positioned on the end of a chunk and the rest of the reader fails because it 
cannot find CRLF or a valid chunksize.

Proposed fix (works, tested)
len = Math.min(len, chunkSize-pos);

Bug 2.

In the calculation of the chunkSize (method getChunkSizeFromInputStream) the 
conversion to int is done by calling             

result = Integer.parseInt(dataString, 16);

This is not robust and causes the occasional crash. The fix is simple and in 
fact implements what is done when the chunkSize is commented (see lines in code 
above)

result = Integer.parseInt(dataString.trim(), 16);

Tested and works.



Suggestion:
Same routine, input state machine. Perhaps just being pedantic..change while 
loop to:

        while (state != 2) {
            int b = in.read();
            if (b == -1) throw new IOException(""chunked stream ended 
unexpectedly"");
            switch (state) {
                case 0:
                    if (b == '\r')
                      state = 1;
                    else
                      baos.write(b);
                    break;
                case 1:
                    if (b == '\n')
                      state = 2;
                    else{
                     // this was not CRLF, so now write '\r' + this char
                      baos.write('\r');
                      baos.write(b);
                      state = 0;
                    }
                    break;
                default: throw new RuntimeException(""assertion failed"");
            }
        }.

In the     

read(byte[] b, int off, int len)  method

of ChunkedInputStream, the number of bytes to read from the underlying 
InputStream is calculated wrongly. In the code this is done by

len = Math.min(len, chunkSize);

This could (and will) cause the server (also Apache) to indeed serve that 
number of bytes (let's say chunkSize), but it may be that we already had a 
number of bytes on the first read. The result is that the input is now NOT 
positioned on the end of a chunk and the rest of the reader fails because it 
cannot find CRLF or a valid chunksize.

Proposed fix (works, tested)
len = Math.min(len, chunkSize-pos);

Bug 1.

In the     

read(byte[] b, int off, int len)  method

of ChunkedInputStream, the number of bytes to read from the underlying 
InputStream is calculated wrongly. In the code this is done by

len = Math.min(len, chunkSize);

This could (and will) cause the server (also Apache) to indeed serve that 
number of bytes (let's say chunkSize), but it may be that we already had a 
number of bytes on the first read. The result is that the input is now NOT 
positioned on the end of a chunk and the rest of the reader fails because it 
cannot find CRLF or a valid chunksize.

Proposed fix (works, tested)
len = Math.min(len, chunkSize-pos);

Bug 2.

In the calculation of the chunkSize (method getChunkSizeFromInputStream) the 
conversion to int is done by calling             

result = Integer.parseInt(dataString, 16);

This is not robust and causes the occasional crash. The fix is simple and in 
fact implements what is done when the chunkSize is commented (see lines in code 
above)

result = Integer.parseInt(dataString.trim(), 16);

Tested and works.



Suggestion:
Same routine, input state machine. Perhaps just being pedantic..change while 
loop to:

        while (state != 2) {
            int b = in.read();
            if (b == -1) throw new IOException(""chunked stream ended 
unexpectedly"");
            switch (state) {
                case 0:
                    if (b == '\r')
                      state = 1;
                    else
                      baos.write(b);
                    break;
                case 1:
                    if (b == '\n')
                      state = 2;
                    else{
                     // this was not CRLF, so now write '\r' + this char
                      baos.write('\r');
                      baos.write(b);
                      state = 0;
                    }
                    break;
                default: throw new RuntimeException(""assertion failed"");
            }
        }.

In the calculation of the chunkSize (method getChunkSizeFromInputStream) the 
conversion to int is done by calling             

result = Integer.parseInt(dataString, 16);

This is not robust and causes the occasional crash. The fix is simple and in 
fact implements what is done when the chunkSize is commented (see lines in code 
above)

result = Integer.parseInt(dataString.trim(), 16);

Tested and works.



Suggestion:
Same routine, input state machine. Perhaps just being pedantic..change while 
loop to:

        while (state != 2) {
            int b = in.read();
            if (b == -1) throw new IOException(""chunked stream ended 
unexpectedly"");
            switch (state) {
                case 0:
                    if (b == '\r')
                      state = 1;
                    else
                      baos.write(b);
                    break;
                case 1:
                    if (b == '\n')
                      state = 2;
                    else{
                     // this was not CRLF, so now write '\r' + this char
                      baos.write('\r');
                      baos.write(b);
                      state = 0;
                    }
                    break;
                default: throw new RuntimeException(""assertion failed"");
            }
        }"
0,refactor consistency checks in BundleDBPersistenceManager into a standalone class that could be re-used for other PMssee subject
0,"Index SplitterIf an index has multiple segments, this tool allows splitting those segments into separate directories.  "
0,"A tokenfilter to decompose compound wordsA tokenfilter to decompose compound words you find in many germanic languages (like German, Swedish, ...) into single tokens.

An example: Donaudampfschiff would be decomposed to Donau, dampf, schiff so that you can find the word even when you only enter ""Schiff"".

I use the hyphenation code from the Apache XML project FOP (http://xmlgraphics.apache.org/fop/) to do the first step of decomposition. Currently I use the FOP jars directly. I only use a handful of classes from the FOP project.

My question now:
Would it be OK to copy this classes over to the Lucene project (renaming the packages of course) or should I stick with the dependency to the FOP jars? The FOP code uses the ASF V2 license as well.

What do you think?"
0,"Wrap IllegalArgumentException from UUID when bad ID passed to Session.getNodeByUUIDHi,

On 6/30/06, David Kennedy <davek@us.ibm.com> wrote:
> When invoking session.getNodeByUUID and passing an invalid ID, an
> IllegalArgumentException is thrown.  Should this be wrapped in an
> ItemNotFoundException or RepositoryException by SessionImpl?

Good point, an ItemNotFoundException would probably be best. Could you
please file a Jira issue for this?

BR,

Jukka Zitting"
0,"TCK vs available property typesThe TCK tests allow configuration of node type / property names to tests specific property types, but they do not take into account that a given repository may not support a specific property type (this is similar to issue JCR-801 about multiple workspace support).

JSR-170 is a bit fuzzy here: it requires all types, but does not require that every type actually exists on a settable node type. In practice, a repository may support reference properties on the builtin nodetypes for version storage, but nowhere else.

Thus, there should be a way to configure the tests so that specific property type tests are left out. Again, there are a few possibilities to do that:

1) reserve a special property name for the case where the test should be skipped (""PROPERTY_TYPE_NOT_SUPPORT""), or

2) add new config flags.

The latter arguably is the cleaner approach, the former avoids introducing new configuration parameters. Thus, I'm leaning to 2). Feedback appreciated.

"
0,"Documentation on SingleClientConnManager(SchemeRegistry schreg) constructor is wrongSeems that the documentation for single-arg constructor SingleClientConnManager(SchemeRegistry schreg) is wrong.

Documentation says that incoming SchemeRegistry parameter can be null:
    schreg - the scheme registry, or null for the default registry

However, the constructor throws an exception in incoming schreg param is null:

    /**
     * Creates a new simple connection manager.
     *
     * @param params    the parameters for this manager
     * @param schreg    the scheme registry, or
     *                  <code>null</code> for the default registry
     */
    public SingleClientConnManager(HttpParams params,
                                   SchemeRegistry schreg) {
        if (schreg == null) {
            throw new IllegalArgumentException
                (""Scheme registry must not be null."");
        }


So this is likely a documentation bug..."
0,Avoid item state reads during Session.logout()Local item states are discarded during Session.logout(). Currently the CachingHierarchyManager is still registered as a item state listener at that time and will cause numerous ItemStateManager.hasItemState() calls. This is unnecessary and just adds overhead to the logout call. In addition it will also contribute to a potential lock contention on the SharedItemStateManager.
0,Namespace handling in AbstractSession should be synchronizedThe AbstractSession base class in o.a.j.commons implicitly assume that the session is never accessed concurrently from more than one thread and thus doesn't synchronize access to the namespace map. This causes problems when the session *is* accessed concurrently. Instead of relying on client code we should enforce thread-safety by explicitly synchronizing potentially unsafe operations on the session instance.
0,"Build should enable unchecked warnings in javacJust have to uncomment this:
{code}
        <!-- for generics in Java 1.5: -->
        <!--<compilerarg line=""-Xlint:unchecked""/>-->
{code}
in common-build.xml.  Test & core are clean, but contrib still has many warnings.  Either we fix contrib with this issue, or, conditionalize this (anyone anty who can do this?) so contrib is off until we can fix it."
0,"make it possible to use searchermanager with distributed statsLUCENE-3555 added explicit stats methods to indexsearcher, but you must
subclass to override this (e.g. populate with distributed stats).

Its also impossible to then do this with SearcherManager.

One idea is make this a factory method (or similar) on IndexSearcher instead,
so you don't need to subclass it to override.

Then you can initialize this in a SearcherWarmer, except there is currently
a lot of hair in what this warming should be. This is a prime example where
Searcher has different meaning from Reader, we should clean this up.

Otherwise, lets make NRT/SearcherManager subclassable in such a way that 
you can return a custom indexsearcher."
1,"Checking of stale connections is brokenHttpConnections that went stale (dropped by server) throw SocketExceptions
instead of silently re-opening themselves, as has been the case with earlier
versions of HttpClient.

I think the problem for this can be found in HttpConnection:

  public boolean closeIfStale() throws IOException {
    if (used && isOpen && isStale()) {
      LOG.debug(""Connection is stale, closing..."");
      close();
      return true;
    }
    return false;
  }

staleness is only checked if used = true, but there is no code in HttpConnection
that sets the used flag. In other words: used is always false and isStale() is
never called."
0,"LayeredSchemeSocketFactory.createLayeredSocket() should have access to HttpParamsWe use a custom implementation of LayeredSchemeSocketFactory that manages a keystore location through HttpParams. That allows us to use different keystores on a per connection basis.

When a proxy is used LayeredSchemeSocketFactory.createLayeredSocket() is invoked which does not have a parameter that passes the HttpParams along. In consequence certificate authentication fails in our implementation. Is there a reason why all other factory methods in the super class have an HttpParams parameter except for LayeredSchemeSocketFactory.createLayeredSocket()?

The downstream bug is here:

369805: certificate authentication with custom keystore fails behind proxy
https://bugs.eclipse.org/bugs/show_bug.cgi?id=369805

Any input would be greatly appreciated."
1,"HttpClient 4.1 ignores request retry handler and stops retrying when a read timeout is followed by a connection refusalI encountered an issue while writing unit tests for the RestBackup(tm) API Client Library, https://github.com/mleonhard/restbackup-java .  HttpClient 4.1 is failing to retry when it encounters a read timeout followed by a connection refusal.  This problem occurs on Windows but not on Linux.  Below is a short program that reproduces the problem.  It performs the expected 5 request attempts on Linux but only 2 on Windows.

My Windows environment is a laptop with Windows 7 Ultimate 64-bit and Oracle Java SE Development Kit Update 21 32-bit.  My Linux environment is Amazon EC2 with Ubuntu 10.04 LTS 32-bit and Oracle Java SE Development Kit Update 21 32-bit.

This is my first bug report to an Apache project.  I'd like to add that I'm a big fan of the Commons libraries and Http Components.

Sincerely,
-Michael

=== RetryBug.java ===

import java.io.IOException;
import java.net.ServerSocket;
import java.util.logging.Logger;

import org.apache.http.client.HttpRequestRetryHandler;
import org.apache.http.client.methods.HttpGet;
import org.apache.http.impl.client.DefaultHttpClient;
import org.apache.http.params.CoreConnectionPNames;
import org.apache.http.protocol.HttpContext;

public class RetryBug {
    private static final Logger _log = Logger.getLogger(RetryBug.class.getName());

    public static void main(String[] args) throws IOException {
        ServerSocket serverSocket = new ServerSocket(0, 1);
        DefaultHttpClient httpClient = new DefaultHttpClient();
        HttpRequestRetryHandler retryHandler = new HttpRequestRetryHandler() {
                public boolean retryRequest(IOException e, int count, HttpContext context) {
                    _log.info(""count="" + count + "" "" + e.toString());
                    return count < 5;
                }
            };
        httpClient.setHttpRequestRetryHandler(retryHandler);
        httpClient.getParams().setIntParameter(CoreConnectionPNames.SO_TIMEOUT, 100);
        try {
            String url = ""http://127.0.0.1:"" + serverSocket.getLocalPort() + ""/"";
            httpClient.execute(new HttpGet(url));
        } finally {
            serverSocket.close();
        }
    }
}


=== Windows 7 ===

C:\RetryBug>md5sum httpcomponents-client-4.1-bin.zip
008ad15560249bcde42cfe34fdb4e858 *httpcomponents-client-4.1-bin.zip

C:\RetryBug>""c:\Program Files (x86)\Java\jdk1.6.0_21\bin\java.exe"" -version
java version ""1.6.0_21""
Java(TM) SE Runtime Environment (build 1.6.0_21-b06)
Java HotSpot(TM) Client VM (build 17.0-b16, mixed mode)

C:\RetryBug>""c:\Program Files (x86)\Java\jdk1.6.0_21\bin\javac.exe"" -cp httpcomponents-client-4.1\lib\commons-codec-1.4.jar;httpcomponents-client-4.1\lib\commons-logging-1.1.1.jar;httpcomponents-client-4.1\lib\httpclient-4.1.jar;httpcomponents-client-4.1\lib\httpcore-4.1.jar RetryBug.java

C:\RetryBug>""c:\Program Files (x86)\Java\jdk1.6.0_21\bin\java.exe"" -cp httpcomponents-client-4.1\lib\commons-codec-1.4.jar;httpcomponents-client-4.1\lib\commons-logging-1.1.1.jar;httpcomponents-client-4.1\lib\httpclient-4.1.jar;httpcomponents-client-4.1\lib\httpcore-4.1.jar;. RetryBug
Mar 9, 2011 9:14:36 PM RetryBug$1 retryRequest
INFO: count=1 java.net.SocketTimeoutException: Read timed out
Mar 9, 2011 9:14:36 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: I/O exception (java.net.SocketTimeoutException) caught when processing request: Read timed out
Mar 9, 2011 9:14:36 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: Retrying request
Exception in thread ""main"" org.apache.http.conn.HttpHostConnectException: Connection to http://127.0.0.1:56361 refused
        at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:158)
        at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:149)
        at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:121)
        at org.apache.http.impl.client.DefaultRequestDirector.tryExecute(DefaultRequestDirector.java:650)
        at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:454)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:820)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:754)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:732)
        at RetryBug.main(RetryBug.java:27)
Caused by: java.net.ConnectException: Connection refused: connect
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:529)
        at org.apache.http.conn.scheme.PlainSocketFactory.connectSocket(PlainSocketFactory.java:120)
        at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:148)
        ... 8 more

C:\RetryBug>


=== Ubuntu 10 ===

$ md5sum httpcomponents-client-4.1-bin.tar.gz
f043c1cc016cb3b720be9fb020bfa755  httpcomponents-client-4.1-bin.tar.gz
$ ~/jdk1.6.0_21/bin/java -version
java version ""1.6.0_21""
Java(TM) SE Runtime Environment (build 1.6.0_21-b06)
Java HotSpot(TM) Client VM (build 17.0-b16, mixed mode, sharing)
$ ~/jdk1.6.0_21/bin/javac -cp httpcomponents-client-4.1/lib/httpclient-cache-4.1.jar:httpcomponents-client-4.1/lib/commons-logging-1.1.1.jar:httpcomponents-client-4.1/lib/httpcore-4.1.jar:httpcomponents-client-4.1/lib/httpclient-4.1.jar:httpcomponents-client-4.1/lib/httpmime-4.1.jar:httpcomponents-client-4.1/lib/commons-codec-1.4.jar RetryBug.java
$ ~/jdk1.6.0_21/bin/java -cp httpcomponents-client-4.1/lib/httpclient-cache-4.1.jar:httpcomponents-client-4.1/lib/commons-logging-1.1.1.jar:httpcomponents-client-4.1/lib/httpcore-4.1.jar:httpcomponents-client-4.1/lib/httpclient-4.1.jar:httpcomponents-client-4.1/lib/httpmime-4.1.jar:httpcomponents-client-4.1/lib/commons-codec-1.4.jar:. RetryBug
Mar 9, 2011 1:09:42 PM RetryBug$1 retryRequest
INFO: count=1 java.net.SocketTimeoutException: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: I/O exception (java.net.SocketTimeoutException) caught when processing request: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: Retrying request
Mar 9, 2011 1:09:42 PM RetryBug$1 retryRequest
INFO: count=2 java.net.SocketTimeoutException: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: I/O exception (java.net.SocketTimeoutException) caught when processing request: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: Retrying request
Mar 9, 2011 1:09:42 PM RetryBug$1 retryRequest
INFO: count=3 java.net.SocketTimeoutException: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: I/O exception (java.net.SocketTimeoutException) caught when processing request: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: Retrying request
Mar 9, 2011 1:09:42 PM RetryBug$1 retryRequest
INFO: count=4 java.net.SocketTimeoutException: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: I/O exception (java.net.SocketTimeoutException) caught when processing request: Read timed out
Mar 9, 2011 1:09:42 PM org.apache.http.impl.client.DefaultRequestDirector tryExecute
INFO: Retrying request
Mar 9, 2011 1:09:51 PM RetryBug$1 retryRequest
INFO: count=5 java.net.SocketTimeoutException: Read timed out
Exception in thread ""main"" java.net.SocketTimeoutException: Read timed out
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:129)
        at org.apache.http.impl.io.AbstractSessionInputBuffer.fillBuffer(AbstractSessionInputBuffer.java:149)
        at org.apache.http.impl.io.SocketInputBuffer.fillBuffer(SocketInputBuffer.java:110)
        at org.apache.http.impl.io.AbstractSessionInputBuffer.readLine(AbstractSessionInputBuffer.java:260)
        at org.apache.http.impl.conn.DefaultResponseParser.parseHead(DefaultResponseParser.java:98)
        at org.apache.http.impl.io.AbstractMessageParser.parse(AbstractMessageParser.java:252)
        at org.apache.http.impl.AbstractHttpClientConnection.receiveResponseHeader(AbstractHttpClientConnection.java:281)
        at org.apache.http.impl.conn.DefaultClientConnection.receiveResponseHeader(DefaultClientConnection.java:247)
        at org.apache.http.impl.conn.AbstractClientConnAdapter.receiveResponseHeader(AbstractClientConnAdapter.java:219)
        at org.apache.http.protocol.HttpRequestExecutor.doReceiveResponse(HttpRequestExecutor.java:298)
        at org.apache.http.protocol.HttpRequestExecutor.execute(HttpRequestExecutor.java:125)
        at org.apache.http.impl.client.DefaultRequestDirector.tryExecute(DefaultRequestDirector.java:622)
        at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:454)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:820)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:754)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:732)
        at RetryBug.main(RetryBug.java:27)
$"
0,"Support for single-workspace repositoriesThere should be a way to configure the test cases in a way such that NodeTest.java can pass although the repository implementation does not support multiple workspaces.

The cleanest approach probably would be to allow javax.jcr.tck.workspacename to stay undefined, and to skip the tests in that case. Alternatives would be a special name indicating lack of support for other workspaces, or an additional config variable.

"
1,WorkspaceInfo.dispose() does not deregister from obs dispatcherJCR-305 introduces an automatic disposing of idle workspaces. this can lead to memory leaks because the observation factory is not deregistered from the delegating one.
1,"JCR2SPI NamespaceRegistryImpl.unregisterNamespace passes prefix to storage when uri is expectedWhen trying to unregister a namespace through SPI, Jackrabbit throws a NamespaceException : <prefix>: is not a registered namespace uri.

javax.jcr.NamespaceRegistry.unregisterNamespace(String prefix) expects the namespace prefix. Though, org.apache.jackrabbit.jcr2spi.NamespaceRegistryImpl.unregisterNamespace(String prefix) calls directly org.apache.jackrabbit.jcr2spi.NamespaceStorage.unregisterNamespace(String uri), which expects the namespace uri.

The namespace registry should first retrieve the uri for the provided prefix."
0,"Javadoc in jackrabbit-jcr-rmi is missing an ending "">"" The javadoc file /jackrabbit-jcr-rmi/src/main/javadoc/apache/rmi/observation/package.html is missing the final "">"" from the ending body tag.
"
0,"Add some ligatures (ff, fi, fl, ft, st) to ISOLatin1AccentFilterISOLatin1AccentFilter removes common diacritics and some ligatures. This patch adds support for additional common ligatures: ff, fi, fl, ft, st."
0,"Implement a backup toolIssue for tracking the progress of the Google Summer of Code project assigned to Nicolas Toper.  The original project requirements are:

""Implement a tool for backing up and restoring content in an Apache Jackrabbit content repository. In addition to the basic content hierarchies, the tool should be able to efficiently manage binary content, node version histories, custom node types, and namespace mappings. Incremental or selective backups would be a nice addition, but not strictly necessary."""
0,"equals and hashCode implementation in org.apache.lucene.search.* packageI would like to talk about the implementation of equals and hashCode method  in org.apache.lucene.search.* package. 

Example One:

org.apache.lucene.search.spans.SpanTermQuery (Super Class)
	<- org.apache.lucene.search.payloads.BoostingTermQuery (Sub Class)

Observation:

* BoostingTermQuery defines equals but inherits hashCode from SpanTermQuery. Definition of equals is a code clone of SpanTermQuery with a change in class name. 

Intention:

I believe the intention of equals redefinition in BoostingTermQuery is not to make the objects of SpanTermQuery and BoostingTermQuery comparable. ie. spanTermQuery.equals(boostingTermQuery) == false && boostingTermQuery.equals(spanTermQuery) == false.


Problem:

With current implementation, the intention might not be respected as a result of symmetric property violation of equals contract i.e.
spanTermQuery.equals(boostingTermQuery) == true (can be) && boostingTermQuery.equals(spanTermQuery) == false. (always)
(Note: Provided their state variables are equal)

Solution:

Change implementation of equals in SpanTermQuery from:

{code:title=SpanTermQuery.java|borderStyle=solid}
  public boolean equals(Object o) {
    if (!(o instanceof SpanTermQuery))
      return false;
    SpanTermQuery other = (SpanTermQuery)o;
    return (this.getBoost() == other.getBoost())
      && this.term.equals(other.term);
  }
{code}

To:
{code:title=SpanTermQuery.java|borderStyle=solid}
  public boolean equals(Object o) {
  	if(o == this) return true;
  	if(o == null || o.getClass() != this.getClass()) return false;
//    if (!(o instanceof SpanTermQuery))
//      return false;
    SpanTermQuery other = (SpanTermQuery)o;
    return (this.getBoost() == other.getBoost())
      && this.term.equals(other.term);
  }
{code}

Advantage:

* BoostingTermQuery.equals and BoostingTermQuery.hashCode is not needed while still preserving the same intention as before.
 
* Any further subclassing that does not add new state variables in the extended classes of SpanTermQuery, does not have to redefine equals and hashCode. 

* Even if a new state variable is added in a subclass, the symmetric property of equals contract will still be respected irrespective of implementation (i.e. instanceof / getClass) of equals and hashCode in the subclasses.


Example Two:


org.apache.lucene.search.CachingWrapperFilter (Super Class)
	<- org.apache.lucene.search.CachingWrapperFilterHelper (Sub Class)

Observation:
Same as Example One.

Problem:
Same as Example one.

Solution:
Change equals in CachingWrapperFilter from:
{code:title=CachingWrapperFilter.java|borderStyle=solid}
  public boolean equals(Object o) {
    if (!(o instanceof CachingWrapperFilter)) return false;
    return this.filter.equals(((CachingWrapperFilter)o).filter);
  }
{code}

To:
{code:title=CachingWrapperFilter.java|borderStyle=solid}
  public boolean equals(Object o) {
//    if (!(o instanceof CachingWrapperFilter)) return false;
    if(o == this) return true;
    if(o == null || o.getClass() != this.getClass()) return false;
    return this.filter.equals(((CachingWrapperFilter)o).filter);
  }
{code}

Advantage:
Same as Example One. Here, CachingWrapperFilterHelper.equals and CachingWrapperFilterHelper.hashCode is not needed.


Example Three:

org.apache.lucene.search.MultiTermQuery (Abstract Parent)
	<- org.apache.lucene.search.FuzzyQuery (Concrete Sub)
	<- org.apache.lucene.search.WildcardQuery (Concrete Sub)

Observation (Not a problem):

* WildcardQuery defines equals but inherits hashCode from MultiTermQuery.
Definition of equals contains just super.equals invocation. 

* FuzzyQuery has few state variables added that are referenced in its equals and hashCode.
Intention:

I believe the intention here is not to make objects of FuzzyQuery and WildcardQuery comparable. ie. fuzzyQuery.equals(wildCardQuery) == false && wildCardQuery.equals(fuzzyQuery) == false.

Proposed Implementation:
How about changing the implementation of equals in MultiTermQuery from:

{code:title=MultiTermQuery.java|borderStyle=solid}
    public boolean equals(Object o) {
      if (this == o) return true;
      if (!(o instanceof MultiTermQuery)) return false;

      final MultiTermQuery multiTermQuery = (MultiTermQuery) o;

      if (!term.equals(multiTermQuery.term)) return false;

      return getBoost() == multiTermQuery.getBoost();
    }
{code}

To:
{code:title=MultiTermQuery.java|borderStyle=solid}
    public boolean equals(Object o) {
      if (this == o) return true;
//      if (!(o instanceof MultiTermQuery)) return false;
      if(o == null || o.getClass() != this.getClass()) return false;

      final MultiTermQuery multiTermQuery = (MultiTermQuery) o;

      if (!term.equals(multiTermQuery.term)) return false;

      return getBoost() == multiTermQuery.getBoost();
    }
{code}

Advantage:

Same as above. Here, WildcardQuery.equals is not needed as it does not define any new state. (FuzzyQuery.equals is still needed because FuzzyQuery defines new state.) 
"
0,"Catch exceptions in Threads created by JUnit tasksOn hudson we had several assertions failed in TestRAMDirectory, that were never caught by the error reportier in JUnit (as the test itsself did not fail). This patch adds a handler for uncaught exceptions to LuceneTestCase(J4) that let the test fail in tearDown()."
0,"warning: unmappable character for encoding UTF8There are a few non-ASCII characters in the Jackrabbit source files that cause warnings at least in my environment. It seems that all the warnings are caused by ""smart quote"" characters.

The exact warnings are: 

/home/hukka/workspace/Jackrabbit/src/java/org/apache/jackrabbit/core/NodeImpl.java:3068: warning: unmappable character for encoding UTF8
            // &#65533;newer&#65533; than N and therefore N should be updated to reflect N'.
               ^
/home/hukka/workspace/Jackrabbit/src/java/org/apache/jackrabbit/core/NodeImpl.java:3068: warning: unmappable character for encoding UTF8
            // &#65533;newer&#65533; than N and therefore N should be updated to reflect N'.
                     ^
/home/hukka/workspace/Jackrabbit/src/java/org/apache/jackrabbit/core/NodeImpl.java:3074: warning: unmappable character for encoding UTF8
            // N' is &#65533;older&#65533; or the &#65533;same age&#65533; as N and therefore N should be left alone.
                     ^
/home/hukka/workspace/Jackrabbit/src/java/org/apache/jackrabbit/core/NodeImpl.java:3074: warning: unmappable character for encoding UTF8
            // N' is &#65533;older&#65533; or the &#65533;same age&#65533; as N and therefore N should be left alone.
                           ^
/home/hukka/workspace/Jackrabbit/src/java/org/apache/jackrabbit/core/NodeImpl.java:3074: warning: unmappable character for encoding UTF8
            // N' is &#65533;older&#65533; or the &#65533;same age&#65533; as N and therefore N should be left alone.
                                    ^
/home/hukka/workspace/Jackrabbit/src/java/org/apache/jackrabbit/core/NodeImpl.java:3074: warning: unmappable character for encoding UTF8
            // N' is &#65533;older&#65533; or the &#65533;same age&#65533; as N and therefore N should be left alone.
                                             ^
/home/hukka/workspace/Jackrabbit/src/java/org/apache/jackrabbit/core/NodeImpl.java:3376: warning: unmappable character for encoding UTF8
        // 2. N&#65533;s jcr:baseVersion property will be changed to point to V.
               ^
/home/hukka/workspace/Jackrabbit/src/java/org/apache/jackrabbit/core/NodeImpl.java:3382: warning: unmappable character for encoding UTF8
        // 3. N&#65533;s jcr:isCheckedOut property is set to false.

/home/hukka/workspace/Jackrabbit/src/test/org/apache/jackrabbit/test/api/version/CheckinTest.java:80: warning: unmappable character for encoding UTF8
        assertEquals(""The versionable checked-out node&#65533;s jcr:predecessors property is copied to the new version on checkin."", Arrays.asList(nPredecessorsValue), Arrays.asList(vPredecessorsValue));
                                                      ^
/home/hukka/workspace/Jackrabbit/src/test/org/apache/jackrabbit/init/NodeTestData.java:95: warning: unmappable character for encoding UTF8
        writer.write(""Hello w&#65533;rld."");
"
0,"Optimize PhraseQueryLooking the scorers for PhraseQuery, I think there are some speedups
we could do:

  * The AND part of the scorer (which advances to the next doc that
    has all the terms), in PhraseScorer.doNext, should do the same
    optimizing as BooleanQuery's ConjunctionScorer, ie sort terms from
    rarest to most frequent.  I don't think it should use a linked
    list/firstToLast() that it does today.

  * We do way too much work now when .score() is not called, because
    we go and find all occurrences of the phrase in the doc, whereas
    we should stop only after finding the first and then go and count
    the rest if .score() is called.

  * For the exact case, I think we can use two int arrays to find the
    matches.  The first array holds the count of how many times a term
    in the phrase ""matched"" a phrase starting at that position.  When
    that count == the number of terms in the phrase, it's a match.
    The 2nd is a ""gen"" array (holds docID when that count was last
    touched), to avoid clearing.  Ie when incrementing the count, if
    the docID != gen, we reset count to 0.  I think this'd be faster
    than the PQ we now use.  Downside of this is if you have immense
    docs (position gets very large) we'd need 2 immense arrays.

It'd be great to do LUCENE-1252 along with this, ie factor
PhraseScorer into two AND'd sub-scorers (LUCENE-1252 is open for
this).  The first one should be ConjunctionScorer, and the 2nd one
checks the positions (ie, either the exact or sloppy scorers).  This
would mean if the PhraseQuery is AND'd w/ other clauses (or, a filter
is applied) we would save CPU by not checking the positions for a doc
unless all other AND'd clauses accepted the doc.
"
1,"NPE when copying nodes with Workspace.copy()I get a NullpointerException when using Workspace.copy():

java.lang.NullPointerException
at org.apache.jackrabbit.core.BatchedItemOperations.copyNodeState(BatchedItemOperations.java:1834)
at org.apache.jackrabbit.core.BatchedItemOperations.copyNodeState(BatchedItemOperations.java:1806)
at org.apache.jackrabbit.core.BatchedItemOperations.copyNodeState(BatchedItemOperations.java:1806)
at org.apache.jackrabbit.core.BatchedItemOperations.copy(BatchedItemOperations.java:423)
at org.apache.jackrabbit.core.WorkspaceImpl.internalCopy(WorkspaceImpl.java:444)
at org.apache.jackrabbit.core.WorkspaceImpl.copy(WorkspaceImpl.java:666)
at xxx.MyClass.myMeth(MyClass.java)

It seems that it happens not all the time. The error occurs since we use Jackrabbit 1.6.0. We do not get the error with previous versions. It seems that we only get the error when trying to copy nodes that were created with Jackrabbit 1.4 and copied with Jackrabbit 1.6."
1,"JCR-2523 break the transaction handling in container managed environmentduring the cleanup (returning to the pool) of an jca managed connection,  an new internal session is created in the object JCAManagedConnection in the method cleanup, this is supposed to fix JCR-2523, The sideeffect is, that the XA-Resource (variable-xaResource) in JCAManagedConnection is not anymore the same XASessionImpl Object like the session Object. Subsequent calls on this connection, lead that the internal session variable is not anymore informed about the current transaction context. (XAItemStateManager, variables tx and txLog are null), because only the xaResource is informed about the new transaction context. Result is that the complete transaction handling does not work anymore.
I attached a sample project which shows this behaviour.
"
0,Typo in message logged upon startup when repository is already in useAs per subject
1,"Missing synchronization in InternalVersionHistoryImplThe InternalVersionHistoryImpl objects can be accessed (and modified) concurrently by multiple sessions, which can in some rare cases result in corruption in the internal cache map data structures. Access to these cache maps should be properly synchronized."
0,"when tests fail, sometimes the testmethod in 'reproduce with' is nullan example is the recent fail: https://builds.apache.org/job/Lucene-3.x/680/

it would be better to not populate -Dtestmethod with anything here..."
1,"Don't commit an empty segments_N when IW is opened with create=trueIf IW is opened with create=true, it forcefully commits an empty
segments_N.  But really it should not: if autoCommit is false, nothing
should be committed until commit or close is explicitly called.

Spinoff from http://www.nabble.com/no-segments*-file-found:-files:-Error-on-opening-index-td23219520.html
"
1,"FSDirectory.openFile(String) causes ClassCastExceptionWhen you call FSDirectory.openFile(String) you get a ClassCastException since FSIndexInput is not an org.apache.lucene.store.InputStream

The workaround is to reimplement using openInput(String). I personally don't need this to be fixed but wanted to document it here in case anyone else runs into this for any reason.

The reason I'm calling this is that I have a requirement on my project to create read only indexes and name the index segments consistently from one build to the next. So, after creating and optimizing the index, I rename the files and rewrite the segments file. It would be nice if I had an API that would allow me to say ""I only want one segment and I want its name to be 'foo'"". For instance IndexWriter.optimize(String segmentName)"
0,PersistenceManager sanity checkLibrary that provides a framework for testing the repository consistency and repairing it if necessary.
0,"single norm file still uses up descriptorsThe new index file format with a single .nrm file for all norms does not decrease file descriptor usage.
The .nrm file is opened once for each field with norms in the index segment."
0,Jcr2Spi: Unneeded call to getPropertyInfo upon creating a new NodeStatecreating a new NodeState may result in additional (but unneeded) calls to getPropertyInfo if a jcr:uuid or jcr:mixinTypes property is present. This can be avoided since the corresponding property values are already present.
0,"Highlighter should try and use maxDocCharsToAnalyze in WeightedSpanTermExtractor when adding a new field to MemoryIndex as well as when using CachingTokenStreamhuge documents can be drastically slower than need be because the entire field is added to the memory index
this cost can be greatly reduced in many cases if we try and respect maxDocCharsToAnalyze

things can be improved even further by respecting this setting with CachingTokenStream

"
1,"IndexReader.clone can leave files openI hit this in working on LUCENE-1516.

When not using compound file format, if you clone an IndexReader, then close the original, then close the clone, the stored fields files (_X.fdt, _X.fdx) remain incorrectly open.

I have a test showing it; fix is trivial.  Will post patch & commit shortly."
1,"trunk TestRollingUpdates.testRollingUpdates seed failuretrunk r1152892
reproducable: always

{code}
junit-sequential:
    [junit] Testsuite: org.apache.lucene.index.TestRollingUpdates
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 1.168 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestRollingUpdates -Dtestmethod=testRollingUpdates -Dtests.seed=-5322802004404580273:-4001225075726350391
    [junit] WARNING: test method: 'testRollingUpdates' left thread running: merge thread: _c(4.0):cv3/2 _h(4.0):cv3 into _k
    [junit] RESOURCE LEAK: test method: 'testRollingUpdates' left 1 thread(s) running
    [junit] NOTE: test params are: codec=RandomCodecProvider: {docid=Standard, body=SimpleText, title=MockSep, titleTokenized=Pulsing(freqCutoff=20), date=MockFixedIntBlock(blockSize=1474)}, locale=lv_LV, timezone=Pacific/Fiji
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestRollingUpdates]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_26 (64-bit)/cpus=8,threads=1,free=128782656,total=158400512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testRollingUpdates(org.apache.lucene.index.TestRollingUpdates):   FAILED
    [junit] expected:<20> but was:<21>
    [junit] junit.framework.AssertionFailedError: expected:<20> but was:<21>
    [junit]     at org.apache.lucene.index.TestRollingUpdates.testRollingUpdates(TestRollingUpdates.java:76)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1522)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1427)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.TestRollingUpdates FAILED

{code}"
0,"New JcrUtils utility methodsI'd like to add the following new utility methods to JcrUtils:

    readFile(Node): returns an InputStream for reading file contents
    readFile(Node, OutputStream): writes file contents to the given stream
    getLastModified(Node): returns the jcr:lastModified value
    setLastModified(Node, Calendar): sets the jcr:lastModified value
    getPropertyType(String): like PropertyType.valueFromName(String), but case-insensitive
"
0,"variables should be accessed through gettersSome attention should be placed on classes who shared their variables directly (as opposed to through a getter). This is sometimes OK for subclasses, but rarely good for other classes that use the objects. There's a small number of classes that have non-private variables, especially in the impl.conn & impl.conn.tsccm packages.

See HTTPCLIENT-745 ."
0,"contrib intelligent Analyzer for ChineseI wrote a Analyzer for apache lucene for analyzing sentences in Chinese language. it's called ""imdict-chinese-analyzer"", the project on google code is here: http://code.google.com/p/imdict-chinese-analyzer/

In Chinese, ""我是中国人""(I am Chinese), should be tokenized as ""我""(I)   ""是""(am)   ""中国人""(Chinese), not ""我"" ""是中"" ""国人"". So the analyzer must handle each sentence properly, or there will be mis-understandings everywhere in the index constructed by Lucene, and the accuracy of the search engine will be affected seriously!

Although there are two analyzer packages in apache repository which can handle Chinese: ChineseAnalyzer and CJKAnalyzer, they take each character or every two adjoining characters as a single word, this is obviously not true in reality, also this strategy will increase the index size and hurt the performance baddly.

The algorithm of imdict-chinese-analyzer is based on Hidden Markov Model (HMM), so it can tokenize chinese sentence in a really intelligent way. Tokenizaion accuracy of this model is above 90% according to the paper ""HHMM-based Chinese Lexical analyzer ICTCLAL"" while other analyzer's is about 60%.

As imdict-chinese-analyzer is a really fast and intelligent. I want to contribute it to the apache lucene repository."
1,"Overwriting a reference property with different type corrupts rep- create node n1
- create node n2
- n2.setProperty(""prop"", n1)
- save()
- n2.setProperty(""prop"", ""hello, world."")
- save()
- n1.remove()
- save() --> exception

see also ReferencesTest case

btw: removing the property or overwriting with a different reference works."
0,"Typos in MultiThreadedHttpConnectionManagerI've done a review of the MultiThreadedHttpConnectionManager class in 3.0-beta1,
especially focussing on the documentation. In general, it could use a lot of
improvement, IMHO.

This bug report only deals with some typos I found in the class, and some
minimal style improvements that are compatible with the other classes.

I will attach a proposed patch."
0,"Fold AuthSSLProtocolSocketFactory into HttpClient properInclude the functionality of the AuthSSLProtocolSocketFactory class into the
main distribution of HttpClient

http://svn.apache.org/repos/asf/jakarta/commons/proper/httpclient/trunk/src/contrib/org/apache/commons/httpclient/contrib/ssl/"
0,"add svn ignores for eclipse artifactsBe nice to ignore the files eclipse puts into the project root as we do the .idea file for intellij.

The two files are

.project
.classpath

I'm gonna lie and say there's a patch available for this because an svn diff patch with propery changes can't be applied with patch anyway."
0,"Standardize on a common mocking framework (either EasyMock or Mockito)We are currently using EasyMock in the caching module and Mockito in the main module. While Mockito appears to have a somewhat nicer API, the sheer number of test cases based on EasyMock in the caching module makes it much simpler to replace Mockito with EasyMock than the other way around."
0,"FieldsInfo uses deprecated APIThe class FieldsInfo.java uses deprecated API in method ""public void add(Document doc)""
I rused the replacement and created the patch -> see attachment"
1,"After RepositoryImpl instance has been created and shut down, some classes cannot be unloadedI've built a simple web-application, which contains one servlet loaded at start-up. In its init() method an instance of RepositoryImpl() is created, in its destroy() method this instance is stopped (using shutdown()).
From the servlet code, only classes in jackrabbit-core, JCR API and Servlet API are referenced.
jackrabbit-core version is 1.4.5, and jackrabbit-jcr-commons version is 1.4.2. Other jackrabbit libs are all of 1.4 version.

Even if servlet's doGet() method never gets called, when the web-application is redeployed, all its classes still hang in memory, which produces a memory leak.

init() method is 

    public void init() throws ServletException {
        super.init();
        try {
            RepositoryConfig repoConfig = RepositoryConfig.create(getClass().getResourceAsStream(""repository.xml""), ""."");
            repo = RepositoryImpl.create(repoConfig);
        } catch (Exception e) {
            throw new ServletException(e);
        }
    }

while destroy() method is

    public void destroy() {
        repo.shutdown();
        super.destroy();
    }

Even when I applied patches from JCR-1636 and added TransientFileFactory.shutdown() call to destroy() method, nothing has changed.
Tested this in Jetty 6.1.9 and Tomcat 6.0.14."
1,"Missing support for lock timeout and ownerHint in jcr-servertrying to set the lock timeout when creating a lock seems not to work over the davex transport. the timeout is always 2147483.

this was my test code:

import javax.jcr.*;
import javax.jcr.lock.*;

import org.apache.jackrabbit.jcr2spi.RepositoryImpl;
import org.apache.jackrabbit.jcr2spi.config.RepositoryConfig;


String url = ""http://localhost:8080/server/"";
String workspace = ""tests"";

RepositoryConfig config = new RepositoryConfigImplTest(repoUrl);
Repository repo = RepositoryImpl.create(config);

Credentials sc = new SimpleCredentials(""admin"",""admin"".toCharArray());
Session s = repo.login(sc,workspace);

Node t;
if (s.getRootNode().hasNode(""test"")) {
    t = s.getRootNode().getNode(""test"");
} else {
    t = s.getRootNode().addNode(""test"", ""nt:unstructured"");
}
t.addMixin(""mix:lockable"");
s.save();
LockManager m = s.getWorkspace().getLockManager();
Lock l = m.lock(t.getPath(), false, true, 10, ""me"");
System.out.println(l.getSecondsRemaining());

and the output is 2147483


the relevant communication fragment is below, i attach the full trace in case i miss something.

LOCK /server/tests/jcr%3aroot/test HTTP/1.1
Timeout: Second-10
Depth: 0
Link: <urn:uuid0c740bb9-042a-4ef2-b019-1a6c52784c29>; rel=""http://www.day.com/jcr/webdav/1.0/session-id""
Authorization: Basic YWRtaW46YWRtaW4=
User-Agent: Jakarta Commons-HttpClient/3.0
Host: localhost:8080
Content-Length: 254
Content-Type: text/xml; charset=UTF-8

<?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?><D:lockinfo xmlns:D=""DAV:""><D:lockscope><dcr:exclusive-session-scoped xmlns:dcr=""http://www.day.com/jcr/webdav/1.0""/></D:lockscope><D:locktype><D:write/></D:locktype><D:owner>me</D:owner></D:lockinfo>

HTTP/1.1 200 OK
Content-Type: text/xml; charset=utf-8
Content-Length: 450
Lock-Token: <aa724c28-3c24-41e8-a3b4-9fc129adf732>
Server: Jetty(6.1.x)

<?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?><D:prop xmlns:D=""DAV:""><D:lockdiscovery><D:activelock><D:lockscope><dcr:exclusive-session-scoped xmlns:dcr=""http://www.day.com/jcr/webdav/1.0""/></D:lockscope><D:locktype><D:write/></D:locktype><D:depth>0</D:depth><D:timeout>Second-2147483</D:timeout><D:owner>admin</D:owner><D:locktoken><D:href>aa724c28-3c24-41e8-a3b4-9fc129adf732</D:href></D:locktoken></D:activelock></D:lockdiscovery></D:prop>



by the way: if i do not explicitly logout before the program exits, the lock is also not released even though it is session based. should the session not trigger a logout on destruction?"
0,"indexwriter creates unwanted termvector infoI noticed today that when I build a big index in Solr, I get some unwanted termvector info, even though I didn't request any.
This does not happen on 3x - not sure when it started happening on trunk."
0,"TSCCM code cleanupThe ThreadSafeClientConnectionManager, or rather it's ConnPoolByRoute, needs plenty of cleanup.
- use long + TimeUnit for timeout intervals (Java 5 style)
- compute timeout end date once instead of remaining interval
- review which methods should acquire the pool lock,
  and which should expect the caller to have done that
- use factory methods to instantiate some of the helper objects
"
0,"TRStringDistance uses way too much memory (with patch)The implementation of TRStringDistance is based on version 2.1 of org.apache.commons.lang.StringUtils#getLevenshteinDistance(String, String), which uses an un-optimized implementation of the Levenshtein Distance algorithm (it uses way too much memory). Please see Bug 38911 (http://issues.apache.org/bugzilla/show_bug.cgi?id=38911) for more information.

The commons-lang implementation has been heavily optimized as of version 2.2 (3x speed-up). I have reported the new implementation to TRStringDistance."
0,"need members of MultipartRequestEntity to be ""protected"" instead of ""private"" to make it extendable for multipart/relatedAs explained in the mailing-list[1], I'd like to have some of 
MultipartRequestEntity move from ""private"" visibility to ""protected"" visibility,
to be able to extend as MultipartRelatedRequestEntity. Namely, the attribute
""parts"" and the method ""getMultipartBoundary"" would be needed.

Thank you.

[1]
http://mail-archives.apache.org/mod_mbox/jakarta-httpclient-dev/200510.mbox/%3c87irw18ndm.fsf@meuh.mnc.ch%3e"
0,"optimize spanfirstquery, spanpositionrangequerySpanFirstQuery and SpanPositionRangeQuery (SpanFirst is just a special case of this), are currently inefficient.

Take this worst case example: SpanFirstQuery(""the"").
Currently the code reads all the positions for the term ""the"".

But when enumerating spans, once we have passed the allowable range we should move on to the next document (skipTo)
 "
0,"Add new top-level projects to the building documentationThe current building.xml file only mentions jackrabbit and contrib as being top level Jackrabbit projects. With the push towards 1.0, there are now several additional projects at the same level as jackrabbit and contrib. The building.xml page should be updated to mention and link to them. It should also provide a link into the subversion repository that is labeled as something like ""Current Jackrabbit project list""; even though the link location is the same as the link to the repository, the label will help readers know that's where to look for the most up-to-date list of Jackrabbit projects."
0,"improve performance of contrib/TestCompoundWordTokenFiltercontrib/analyzers/compound has some tests that use a hyphenation grammar file.

The tests are currently for german, and they actually are nice, they show how the combination of the hyphenation rules and dictionary work in tandem.
The issue is that the german grammar file is not apache licensed: http://offo.sourceforge.net/hyphenation/licenses.html
So the test must download the entire offo zip file from sourceforge to execute.

I happen to think the test is a great example of how this thing works (with a language where it matters), but we could consider using a different grammar file, for a language that is apache licensed.
This way it could be included in the source with the test and would be more practical.
"
0,"isValid should be invoked after analyze rather than before it so it can validate the output of analyzeThe Synonym map has a protected method String analyze(String word) designed for custom stemming.

However, before analyze is invoked on a word, boolean isValid(String str) is used to validate the word - which causes the program to discard words that maybe useable by the custom analyze method. 

I think that isValid should be invoked after analyze rather than before it so it can validate the output of analyze and allow implemters to decide what is valid for the overridden analyze method. (In fact, if you look at code snippet below, isValid should really go after the empty string check)

This is a two line change in org.apache.lucene.index.memory.SynonymMap

      /*
       * Part B: ignore phrases (with spaces and hyphens) and
       * non-alphabetic words, and let user customize word (e.g. do some
       * stemming)
       */
      if (!isValid(word)) continue; // ignore
      word = analyze(word);
      if (word == null || word.length() == 0) continue; // ignore"
0,Sessions are not logged out in case of exceptionsSome test cases do not logout sessions if an exception occurs.
1,"ConcurrentScheduleManager.addMyself() has wrong intedThis method has the wrong index for the 'size' variable, I think it should b allInstances.size.

{code:java}
private void addMyself() {
    synchronized(allInstances) {
      final int size=0;
      int upto = 0;
      for(int i=0;i<size;i++) {
        final ConcurrentMergeScheduler other = (ConcurrentMergeScheduler) allInstances.get(i);
        if (!(other.closed && 0 == other.mergeThreadCount()))
          // Keep this one for now: it still has threads or
          // may spawn new threads
          allInstances.set(upto++, other);
      }
      allInstances.subList(upto, allInstances.size()).clear();
      allInstances.add(this);
    }
  }
{code}"
0,Support SortedSource in MultiDocValuesMultiDocValues doesn't support Sorted variant ie. SortedSource but throws UnsupportedOperationException. This forces users to work per segment. For consistency we should support sortedsource also if we wrap the DocValues in MDV.
0,"FastVectorHighlighter: add a FragmentBuilder to return entire field contentsIn Highlightrer, there is a Nullfragmenter. There is a requirement its counterpart in FastVectorhighlighter."
1,[PATCH] DbDataStore: Make sure streams are closedStream isn't closed on end of use. this patch fixes it.
1,"FrenchAnalyzer's tokenStream method does not honour the contract of AnalyzerIn {{Analyzer}} :
{code}
/** Creates a TokenStream which tokenizes all the text in the provided
    Reader.  Default implementation forwards to tokenStream(Reader) for 
    compatibility with older version.  Override to allow Analyzer to choose 
    strategy based on document and/or field.  Must be able to handle null
    field name for backward compatibility. */
  public abstract TokenStream tokenStream(String fieldName, Reader reader);
{code}


and in {{FrenchAnalyzer}}

{code}
public final TokenStream tokenStream(String fieldName, Reader reader) {

    if (fieldName == null) throw new IllegalArgumentException(""fieldName must not be null"");
    if (reader == null) throw new IllegalArgumentException(""reader must not be null"");
{code}"
0,"Add a serializing content handlerBoth JCR-1310 and JCR-1343 need XML serialization functionality and we've also previously (JCR-367, JCR-1086) implemented something similar.

It would be good to centralize such code, and so I'd like to use the already referenced code from Cocoon [1] as the basis for a SerializingContentHandler class in jackrabbit-jcr-commons.

[1] https://svn.apache.org/repos/asf/cocoon/trunk/core/cocoon-pipeline/cocoon-pipeline-impl/src/main/java/org/apache/cocoon/serialization/AbstractTextSerializer.java"
0,"improved compound file handlingCurrently CompoundFileReader could use some improvements, i see the following problems
* its CSIndexInput extends bufferedindexinput, which is stupid for directories like mmap.
* it seeks on every readInternal
* its not possible for a directory to override or improve the handling of compound files.

for example: it seems if you were impl'ing this thing from scratch, you would just wrap the II directly (not extend BufferedIndexInput,
and add compound file offset X to seek() calls, and override length(). But of course, then you couldnt throw read past EOF always when you should,
as a user could read into the next file and be left unaware.

however, some directories could handle this better. for example MMapDirectory could return an indexinput that simply mmaps the 'slice' of the CFS file.
its underlying bytebuffer etc naturally does bounds checks already etc, so it wouldnt need to be buffered, not even needing to add any offsets to seek(),
as its position would just work.

So I think we should try to refactor this so that a Directory can customize how compound files are handled, the simplest 
case for the least code change would be to add this to Directory.java:

{code}
  public Directory openCompoundInput(String filename) {
    return new CompoundFileReader(this, filename);
  }
{code}

Because most code depends upon the fact compound files are implemented as a Directory and transparent. at least then a subclass could override...
but the 'recursion' is a little ugly... we could still label it expert+internal+experimental or whatever.
"
0,"DirectoryIndexReader finalize() holding TermInfosReader longer than necessaryDirectoryIndexReader has a finalize method, which causes the JDK to keep a reference to the object until it can be finalized.  SegmentReader and MultiSegmentReader are subclasses that contain references to, potentially, hundreds of megabytes of cached data in a TermInfosReader.

Some options would be removing finalize() from DirectoryIndexReader (it releases a write lock at the moment) or possibly nulling out references in various close() and doClose() methods throughout the class hierarchy so that the finalizable object doesn't references the Term arrays.

Original mailing list message:
http://mail-archives.apache.org/mod_mbox/lucene-java-user/200906.mbox/%3C7A5CB4A7BBCE0C40B81C5145C326C31301A62971@NUMEVP06.na.imtn.com%3E"
0,"introduce HttpRoutePlanner interfaceDefine an interface to determine a route for a given target host.
Create default implementation replacing DefaultHttpClient.determineRoute(...);
Implementations will need access to params and/or request.

The interface fits into HttpConn, but DHC.dR(...) uses client parameters.
Either move parameters to HttpConn, or keep default implementation in HttpClient.

Alternative implementations could evaluate Java system properties related to proxy settings.

"
0,"Wrong trailing index calculation in PatternReplaceCharFilterReimplementation of PatternReplaceCharFilter to pass randomized tests (used to throw exceptions previously). Simplified code, dropped boundary characters, full input buffered for pattern matching."
0,Include to jackrabbit-jcr-rmi and jackrabbit-jcr-servlet in main trunkJackrabbit 2.0 should include the 2.0 version of the RMI component and the related jcr-servlet updates.
0,[PATCH] Javadoc correction for Scorer.java 
0,"Make StopFilter.enablePositionIncrements explicitI think the default for this should be true, ie, do not lose
information when filtering (preserve the positions of the original
tokens).

But, we can't change this without breaking back-compat.

So, as workaround, we should make the parameter explicit so one must
decide up front.
"
0,"repository.xml DTD doesn't allow <DataStore> elementThe repository.xml DTD at http://jackrabbit.apache.org/dtd/repository-1.4.dtd conflicts with the instructions in the wiki page at http://wiki.apache.org/jackrabbit/DataStore

Adding the <DataStore> element as specified in the wiki page violates the DTD.

"
0,"re-sync client with changes in core alpha6 snapshotThere have been API changes in core since it's alpha5 release.
Client needs to be adapted so it's alpha2 (snapshot) builds and runs against the current core API.
"
1,"Bugs in org.apache.lucene.index.TermVectorsReader.clone()A couple of things:

- The implementation can return null which is not allowed.  It should throw a CloneNotSupportedException if that's the case.

- Part of the code reads:

    TermVectorsReader clone = null;
    try {
      clone = (TermVectorsReader) super.clone();
    } catch (CloneNotSupportedException e) {}

    clone.tvx = (IndexInput) tvx.clone();

If a CloneNotSupportedException is caught then ""clone"" will be null and the assignment to clone.tvx will fail with a null pointer exception."
0,"typos on FAQI found out the following typos on the FAQ (http://lucene.sourceforge.net/cgi-
bin/faq/faqmanager.cgi) of lucene:
in 8. Will Lucene work with my Java application ?
- felxible
- applciations"
0,"Contributing a High-performance single-document main memory Apache Lucene fulltext search index.Here is my contribution: a High-performance single-document main memory Apache Lucene fulltext 
search index. I'll try to attach the files, hoping for comments on how to proceed with this..."
1,"Deadlock caused by versioning operations within transactionDeadlock occurs, while running a very simple test, which is just trying
to checkout/checkin node within transaction concurrently from 2 threads.

Find enclosed thread dump, log and simple Java program.
I'm using UserTransaction implementation from jackrabbit test suite.

Regards
Przemo Pakulski
www.cognifide.com


Full thread dump Java HotSpot(TM) Client VM (1.4.2_08-b03 mixed mode):

""Thread-5"" prio=5 tid=0x03054c48 nid=0x180c in Object.wait() [355f000..355fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x1148ef20> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
       at java.lang.Object.wait(Object.java:429)
       at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock.acquire(Unknown Source)
       - locked <0x1148ef20> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.acquireWriteLock(SharedItemStateManager.java:1137)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.access$200(SharedItemStateManager.java:110)
       at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:456)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:651)
       at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:150)
       at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:128)
       - locked <0x11565ac8> (a org.apache.jackrabbit.core.TransactionContext)
       at org.apache.jackrabbit.core.XASessionImpl.prepare(XASessionImpl.java:300)
       at com.oyster.mom.contentserver.jcr.transaction.JackrabbitUserTransaction.commit(JackrabbitUserTransaction.java:102)
       at com.oyster.mom.contentserver.jcr.transaction.JrTestDeadlock.run(JrTestDeadlock.java:97)

""Thread-4"" prio=5 tid=0x0303b348 nid=0x9d0 in Object.wait() [351f000..351fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x1148ef20> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
       at java.lang.Object.wait(Object.java:429)
       at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock.acquire(Unknown Source)
       - locked <0x1148ef20> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.acquireWriteLock(SharedItemStateManager.java:1137)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.access$200(SharedItemStateManager.java:110)
       at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:456)
       at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:651)
       at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:150)
       at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:128)
       - locked <0x1156f558> (a org.apache.jackrabbit.core.TransactionContext)
       at org.apache.jackrabbit.core.XASessionImpl.prepare(XASessionImpl.java:300)
       at com.oyster.mom.contentserver.jcr.transaction.JackrabbitUserTransaction.commit(JackrabbitUserTransaction.java:102)
       at com.oyster.mom.contentserver.jcr.transaction.JrTestDeadlock.run(JrTestDeadlock.java:97)

""IndexMerger"" daemon prio=5 tid=0x030388b8 nid=0x1858 in Object.wait() [34df000..34dfd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x114fd280> (a org.apache.commons.collections.buffer.BlockingBuffer)
       at java.lang.Object.wait(Object.java:429)
       at org.apache.commons.collections.buffer.BlockingBuffer.remove(BlockingBuffer.java:107)
       - locked <0x114fd280> (a org.apache.commons.collections.buffer.BlockingBuffer)
       at org.apache.jackrabbit.core.query.lucene.IndexMerger.run(IndexMerger.java:235)

""Thread-2"" daemon prio=5 tid=0x0303a230 nid=0xe4c in Object.wait() [349f000..349fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x114fd2e0> (a java.util.TaskQueue)
       at java.util.TimerThread.mainLoop(Timer.java:429)
       - locked <0x114fd2e0> (a java.util.TaskQueue)
       at java.util.TimerThread.run(Timer.java:382)

""Thread-1"" daemon prio=5 tid=0x0301b7a0 nid=0x1a00 in Object.wait() [345f000..345fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x114f9058> (a java.util.TaskQueue)
       at java.lang.Object.wait(Object.java:429)
       at java.util.TimerThread.mainLoop(Timer.java:403)
       - locked <0x114f9058> (a java.util.TaskQueue)
       at java.util.TimerThread.run(Timer.java:382)

""ObservationManager"" daemon prio=5 tid=0x02ef6c50 nid=0x10d8 in Object.wait() [341f000..341fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x114f38e0> (a org.apache.commons.collections.buffer.BlockingBuffer)
       at java.lang.Object.wait(Object.java:429)
       at org.apache.commons.collections.buffer.BlockingBuffer.remove(BlockingBuffer.java:107)
       - locked <0x114f38e0> (a org.apache.commons.collections.buffer.BlockingBuffer)
       at org.apache.jackrabbit.core.observation.ObservationManagerFactory.run(ObservationManagerFactory.java:155)
       at java.lang.Thread.run(Thread.java:534)

""Signal Dispatcher"" daemon prio=10 tid=0x00a05590 nid=0x1914 waiting on condition [0..0]

""Finalizer"" daemon prio=9 tid=0x00a027f8 nid=0x17a4 in Object.wait() [2c9f000..2c9fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x113db118> (a java.lang.ref.ReferenceQueue$Lock)
       at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:111)
       - locked <0x113db118> (a java.lang.ref.ReferenceQueue$Lock)
       at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:127)
       at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)

""Reference Handler"" daemon prio=10 tid=0x00a01478 nid=0x16d4 in Object.wait() [2c5f000..2c5fd8c]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x113db180> (a java.lang.ref.Reference$Lock)
       at java.lang.Object.wait(Object.java:429)
       at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:115)
       - locked <0x113db180> (a java.lang.ref.Reference$Lock)

""main"" prio=5 tid=0x0003e6f0 nid=0x1470 in Object.wait() [7f000..7fc38]
       at java.lang.Object.wait(Native Method)
       - waiting on <0x11524f10> (a com.oyster.mom.contentserver.jcr.transaction.JrTestDeadlock)
       at java.lang.Thread.join(Thread.java:1001)
       - locked <0x11524f10> (a com.oyster.mom.contentserver.jcr.transaction.JrTestDeadlock)
       at java.lang.Thread.join(Thread.java:1054)
       at com.oyster.mom.contentserver.jcr.transaction.JrTestDeadlock.main(JrTestDeadlock.java:33)

""VM Thread"" prio=5 tid=0x00a42730 nid=0x17d0 runnable

""VM Periodic Task Thread"" prio=10 tid=0x00a45540 nid=0x1928 waiting on condition
""Suspend Checker Thread"" prio=10 tid=0x00a04af8 nid=0x17ac runnable

import javax.jcr.Node;
import javax.jcr.RepositoryException;
import javax.jcr.Session;
import javax.jcr.SimpleCredentials;

import org.apache.jackrabbit.core.RepositoryImpl;
import org.apache.jackrabbit.core.config.RepositoryConfig;

public class JrTestDeadlock extends Thread {

   private static final org.apache.commons.logging.Log log = org.apache.commons.logging.LogFactory.getLog(JrTestDeadlock.class);

   public static String REPOSITORY_HOME = ""d:/repo/jackrabbit/"";

   public static String REPOSITORY_CONFIG = REPOSITORY_HOME + ""repository.xml"";

   public static void main(String[] args) throws Exception {

       JrTestDeadlock test = new JrTestDeadlock(-1);
       test.startup();

       JrTestDeadlock tests[] = new JrTestDeadlock[2];

       for (int i = 0; i < tests.length; i++) {
           JrTestDeadlock x = new JrTestDeadlock(i);
           x.start();
           tests[i] = x;
       }

       for (int i = 0; i < tests.length; i++) {
           tests[i].join();
       }

       test.shutdown();
   }

   private static RepositoryImpl repository;

   private int id;

   public JrTestDeadlock(int i) {
       this.id = i;
   }

   public void startup() throws Exception {
       System.setProperty(""java.security.auth.login.config"", ""c:/jaas.config"");

       RepositoryConfig config = RepositoryConfig.create(REPOSITORY_CONFIG, REPOSITORY_HOME);
       repository = RepositoryImpl.create(config);

       Session session = repository.login(new SimpleCredentials(""admin"", ""admin"".toCharArray()));
       Node rootNode = session.getRootNode();
       if (!rootNode.hasNode(""folder"")) {
           Node folder = rootNode.addNode(""folder"");
           folder.addMixin(""mix:versionable"");
           folder.addMixin(""mix:lockable"");
           rootNode.save();
       }
       session.logout();
   }

   public void shutdown() throws RepositoryException {
       repository.shutdown();
   }

   public Node getFolder(Session session) throws RepositoryException {
       return session.getRootNode().getNode(""folder"");
   }

   public void run() {
       try {
           Session session = repository.login(new SimpleCredentials(""admin"", ""admin"".toCharArray()));
           for (int i = 0; i < 100; i++) {
               log.error(""START id:"" + id + "", i="" + i);

               boolean success = false;

               JackrabbitUserTransaction ut = new JackrabbitUserTransaction(session);
               try {
                   ut.begin();

                   Node folder = getFolder(session);
                   folder.checkout();
                   folder.checkin();

                   success = true;
                   log.info(""SUCCESS id:"" + id + "", i="" + i);
               }
               catch (Exception e) {
                   log.warn(""FAIL:"" + id + "", i="" + i, e);
               }
               finally {
                   try {
                       if (success) {
                           ut.commit();
                       }
                       else {
                           ut.rollback();
                       }
                   }
                   catch (Exception e) {
                       log.fatal(e);
                   }
               }
           }
           session.logout();
       }
       catch (RepositoryException e) {
           e.printStackTrace();
       }
   }
}


13:46 ERROR JrTestDeadlock.run(JrTestDeadlock.java:76) - START id:0, i=0
13:46 ERROR JrTestDeadlock.run(JrTestDeadlock.java:76) - START id:1, i=0
13:46 INFO  JrTestDeadlock.run(JrTestDeadlock.java:89) - SUCCESS id:0, i=0
13:46 INFO  JrTestDeadlock.run(JrTestDeadlock.java:89) - SUCCESS id:1, i=0
13:46 ERROR org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:156) - org.apache.jackrabbit.core.state.StaleItemStateException: 233e656f-79f8-414d-9e37-3fce865b492d/{http://www.jcp.org/jcr/1.0}isCheckedOut has been modified externally
13:46 FATAL JrTestDeadlock.run(JrTestDeadlock.java:104) - javax.transaction.RollbackException: Transaction rolled back: XA_ERR=104
13:46 ERROR JrTestDeadlock.run(JrTestDeadlock.java:76) - START id:1, i=1
13:46 WARN  JrTestDeadlock.run(JrTestDeadlock.java:92) - FAIL:1, i=1
ax.jcr.InvalidItemStateException: f83a830b-abbf-4ab2-8625-b9e2c4802316: the item does not exist anymore
    at org.apache.jackrabbit.core.version.XAVersion.sanityCheck(XAVersion.java:81)
    at org.apache.jackrabbit.core.version.XAVersion.getInternalVersion(XAVersion.java:70)
    at org.apache.jackrabbit.core.version.AbstractVersion.getUUID(AbstractVersion.java:107)
    at org.apache.jackrabbit.core.NodeImpl.checkout(NodeImpl.java:2759)
    at JrTestDeadlock.run(JrTestDeadlock.java:85)
13:46 ERROR JrTestDeadlock.run(JrTestDeadlock.java:76) - START id:1, i=2
13:46 INFO  JrTestDeadlock.run(JrTestDeadlock.java:89) - SUCCESS id:1, i=2
13:51 WARN  org.apache.jackrabbit.core.TransactionContext.run(TransactionContext.java:239) - Transaction rolled back because timeout expired.

"
0,"Test failures when running ""mvn cobertura:check""It looks like the bytecode instrumentation done by Cobertura interferes with the rather complex XPathTokenManager class produced by JavaCC.

The easiest workaround seems to be to simply exclude XPathTokenManager from being instrumented by Cobertura."
0,"Rethink LocalizedTestCaseRunner with JUnit 4 - Clover OOMAs a spinn off from this [conversation|http://www.lucidimagination.com/search/document/ae20885bf5baedc5/build_failed_in_hudson_lucene_3_x_116#7ed351341152ee2d] we should rethink the way how we execute testcases with different locals since glover reports appears to throw OOM errors b/c Junit treats each local as a single test case run.

Here are some options:
* select the local at random only run the test with a single local
* set the local via system property -Dtest.locale=en.EN
* run with the default locale only -Dtest.skiplocale=true
* one from the above but only if instrumented with clover (let common tests run all the locale)

"
1,"URIResolverImpl: use of bitwise instead of logical AND operatorURIResolverImpl, line 111: 

                if (path != null & cache.containsItemId(uuidId)) {
"
0,"User configurable cookie policySome user configurable how cookies are handled.  Emulate cookie options in web
browsers."
0,"FST.BYTE2 should save as fixed 2 byte not as vIntWe currently write BYTE1 as a single byte, but BYTE2/4 as vInt, but I think that's confusing.  Also, for the FST for the new Kuromoji analyzer (LUCENE-3305), writing as 2 bytes instead shrank the FST and ran faster, presumably because more values were >= 16384 than were < 128.

Separately the whole INPUT_TYPE is very confusing... really all it's doing is ""declaring"" the allowed range of the characters of the input alphabet, and then the only thing that uses that is the write/readLabel methods (well and some confusing sugar methods in Builder!).  Not sure how to fix that yet...

It's a simple change but it changes the FST binary format so any users w/ FSTs out there will have to rebuild (FST is marked experimental...).
"
0,WriteLineDocTask should write gzip/bzip2/txt according to the extension of specified output file nameSince the readers behave this way it would be nice and handy if also this line writer would.
1,"Merging implemented by codecs must catch aborted mergesThis is a regression (we lost functionality on landing flex).

When you close IW with ""false"" (meaning abort all running merges), IW asks the merge threads to abort.  The threads are supposed to periodically check if they are aborted and throw an exception if so.

But on the cutover to flex, where the codec can override how merging is done (but a default impl is in the base enum classes), we lost this."
0,"Misleading exception message when re-index failsE.g. the log may say:

19.06.2007 11:25:42 *ERROR* RepositoryImpl: Failed to initialize workspace 'default' (RepositoryImpl.java, line 382)
javax.jcr.RepositoryException: Error indexing root node: 10022d38-c449-4751-b8f0-9d07ac45ead5:
[...]

The mentioned uuid is not the root node and the root cause is missing."
0,"Per socket SOCKS proxiesHttpClient requires a way of allowing a SOCKS proxy to be used on some
connections without requiring that all created Sockets go through the proxy."
0,"MockDirectoryWrapper should track open file handles of IndexOutput tooMockDirectoryWrapper currently tracks open file handles of IndexInput only. Therefore IO files that are not closed do not fail our tests, which can then lead to test directories fail to delete on Windows. We should make sure all open files are tracked and if they are left open, fail the test. I'll attach a patch shortly."
0,"Analysis for IrishAdds analysis for Irish.

The stemmer is generated from a snowball stemmer. I've sent it to Martin Porter, who says it will be added during the week."
0,"Rename BaseMultiReader class to BaseCompositeReader and make publicCurrently the abstract DirectoryReader and MultiReader and ParallelCompositeReader extend a package private class. Users that want to implement a composite reader, should be able to subclass this pkg-private class, as it implements lots of abstract methods, useful for own implementations. In fact MultiReader is a shallow subclass only implementing correct closing&refCounting.

By making it public after the rename, the generics problems (type parameter R is not correctly displayed) in the JavaDocs are solved, too."
0,"Provide Programmatic Access to CheckIndexWould be nice to have programmatic access to the CheckIndex tool, so that it can be used in applications like Solr.  

See SOLR-566"
0,"SQL Server support in clustering moduleI realize the clustering module doesn't specifically support SQL Server yet (there's no mssql.ddl), but I still tried to run the repository against SQL Server with clustering enabled in the hope that the default schema (default.ddl) would suffice. Apparently, it doesn't (unless I'm doing something very wrong), since I kept getting the following error whenever a write operation was attempted:

2007-05-25 14:48:06,757 WARN  [org.apache.jackrabbit.core.journal.DatabaseJournal] Error while rolling back connection: You cannot rollback with autocommit set!
2007-05-25 14:48:06,757 ERROR [org.apache.jackrabbit.core.cluster.ClusterNode] Unable to commit log entry.
org.apache.jackrabbit.core.journal.JournalException: Unable to append revision 1090.
	at org.apache.jackrabbit.core.journal.DatabaseJournal.append
	at org.apache.jackrabbit.core.journal.AppendRecord.update(AppendRecord.java:242)
	at org.apache.jackrabbit.core.cluster.ClusterNode$WorkspaceUpdateChannel.updateCommitted(ClusterNode.java:530)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:725)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:855)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:302)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:306)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1214)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:849)
Caused by: java.sql.DataTruncation: Data truncation
	at net.sourceforge.jtds.jdbc.SQLDiagnostic.addDiagnostic(SQLDiagnostic.java:379)
	at net.sourceforge.jtds.jdbc.TdsCore.tdsErrorToken(TdsCore.java:2781)
	at net.sourceforge.jtds.jdbc.TdsCore.nextToken(TdsCore.java:2224)
	at net.sourceforge.jtds.jdbc.TdsCore.getMoreResults(TdsCore.java:628)
	at net.sourceforge.jtds.jdbc.JtdsStatement.processResults(JtdsStatement.java:525)
	at net.sourceforge.jtds.jdbc.JtdsStatement.executeSQL(JtdsStatement.java:487)
	at net.sourceforge.jtds.jdbc.JtdsPreparedStatement.execute(JtdsPreparedStatement.java:475)
	at org.jboss.resource.adapter.jdbc.WrappedPreparedStatement.execute(WrappedPreparedStatement.java:183)
	at org.apache.jackrabbit.core.journal.DatabaseJournal.append
	... 58 more

However, I think I got things working by using a modified version of default.ddl, with the only change being the type of the REVISION_DATA field (varbinary -> IMAGE)."
1,"Deadlock in DBCP when accessing nodeI found a deadlock situation using JR 2.2.10, the problem is with DBCP 1.2.2 and is fixed in DBCP 1.3, JR trunk also uses DBCP 1.2.2 and should also be updated

The ticket in dbcp is #DBCP-270, related tickets are #DBCP-65 #DBCP-281 #DBCP-271

Stack trace of where my call is stalled:
{code}
main@1, prio=5, in group 'main', status: 'MONITOR'
	 blocks Timer-1@2545
	 waiting for Timer-1@2545 to release lock on {1}
	  at org.apache.commons.pool.impl.GenericObjectPool.addObjectToPool(GenericObjectPool.java:1137)
	  at org.apache.commons.pool.impl.GenericObjectPool.returnObject(GenericObjectPool.java:1076)
	  at org.apache.commons.dbcp.PoolableConnection.close(PoolableConnection.java:87)
	  at org.apache.commons.dbcp.PoolingDataSource$PoolGuardConnectionWrapper.close(PoolingDataSource.java:181)
	  at org.apache.jackrabbit.core.util.db.DbUtility.close(DbUtility.java:75)
	  at org.apache.jackrabbit.core.util.db.ResultSetWrapper.invoke(ResultSetWrapper.java:63)
	  at $Proxy12.close(Unknown Source:-1)
	  at org.apache.jackrabbit.core.persistence.pool.BundleDbPersistenceManager.loadBundle(BundleDbPersistenceManager.java:1042)
	  at org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.getBundle(AbstractBundlePersistenceManager.java:669)
	  at org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.load(AbstractBundlePersistenceManager.java:415)
	  at org.apache.jackrabbit.core.state.SharedItemStateManager.loadItemState(SharedItemStateManager.java:1830)
	  at org.apache.jackrabbit.core.state.SharedItemStateManager.getNonVirtualItemState(SharedItemStateManager.java:1750)
	  at org.apache.jackrabbit.core.state.SharedItemStateManager.getItemState(SharedItemStateManager.java:265)
	  at org.apache.jackrabbit.core.state.LocalItemStateManager.getNodeState(LocalItemStateManager.java:109)
	  at org.apache.jackrabbit.core.state.LocalItemStateManager.getItemState(LocalItemStateManager.java:174)
	  at org.apache.jackrabbit.core.state.XAItemStateManager.getItemState(XAItemStateManager.java:260)
	  at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:161)
	  at org.apache.jackrabbit.core.ItemManager.getItemData(ItemManager.java:382)
	  at org.apache.jackrabbit.core.ItemManager.getNode(ItemManager.java:669)
	  at org.apache.jackrabbit.core.ItemManager.getNode(ItemManager.java:647)
	  at org.apache.jackrabbit.core.LazyItemIterator.prefetchNext(LazyItemIterator.java:120)
	  at org.apache.jackrabbit.core.LazyItemIterator.next(LazyItemIterator.java:257)
	  at info.magnolia.jcr.iterator.DelegatingNodeIterator.next(DelegatingNodeIterator.java:79)
{code}

This is the offending thread:
{code}
Timer-1@2545 daemon, prio=5, in group 'main', status: 'MONITOR'
	 blocks main@1
	 waiting for main@1 to release lock on {1}
	  at org.apache.commons.dbcp.AbandonedTrace.addTrace(AbandonedTrace.java:176)
	  at org.apache.commons.dbcp.AbandonedTrace.init(AbandonedTrace.java:92)
	  at org.apache.commons.dbcp.AbandonedTrace.<init>(AbandonedTrace.java:82)
	  at org.apache.commons.dbcp.DelegatingStatement.<init>(DelegatingStatement.java:61)
	  at org.apache.commons.dbcp.DelegatingConnection.createStatement(DelegatingConnection.java:224)
	  at org.apache.commons.dbcp.PoolableConnectionFactory.validateConnection(PoolableConnectionFactory.java:331)
	  at org.apache.commons.dbcp.PoolableConnectionFactory.validateObject(PoolableConnectionFactory.java:312)
	  at org.apache.commons.pool.impl.GenericObjectPool.evict(GenericObjectPool.java:1217)
	  at org.apache.commons.pool.impl.GenericObjectPool$Evictor.run(GenericObjectPool.java:1341)
	  at java.util.TimerThread.mainLoop(Timer.java:512)
	  at java.util.TimerThread.run(Timer.java:462)
{code}"
1,"Cookie.parse exception when parsing expiry date in single quotesA Netscape-Enterprise/3.6 SP3 server sends a cookie where the parameter expires='Thu, 05-Dec-
2002 12:07:45 GMT'. 
Cookie.parse throws an exception because none of the four built-in formats 
matches - I have tested that the parse code works OK if the single quotes are omitted from the value 
being parsed.

Resolution: If the value of the 'expires' parameter starts and ends with a 
single quote then strip the first and last character before parsing."
0,Create a sample search pageThe web application should have a search page that shows how to use the query features in jackrabbit.
1,"getAllLinearVersions does not return the base versionIt appears that for a given linear version history, getAllLinearVersions returns less versions than getAllVersions -- the root version seems to be missing."
1,"WorkspaceImpl.dispose() might cause ClassNotFoundExceptionWenn using Jackrabbit in an environment, where ClassLoaders may get inactivated in the sense, the loading new classes is not possible anymore, shutting down the repository may result in a ClassNotFoundException during WorkspaceImpl.dispose().

Reason for this is, that in the dispose() method, the ObservationManager is asked for all registered event listeners for them to be removed from the ObservationManager one-by-one. Asking for the listeners results in a new EventListenerIteratorImpl object being created.

If now, this class has never been used during the live time of the repository, this would cause a ClassNotFoundException because the class loader is not laoding classes anymore in the specific environment.

The specific environment is Eclipse, where one plugin is managing different Repository instances provided by separate plugins. When now the Jackrabbit provider plugin has already been stopped while the managing plugin tries to shutdown the Jackrabbit repository, the EventListenerIteratorImpl class cannot be loaded anymore and disposing the WorkspaceImpl in a controlled way fails.

I suggest adding an ObservationManagerImpl.dispose() method, which is called by the WorkspaceImpl like :
    WorkspaceImpl.dispose() {
       if (obsMgr != null) {
         obsMgr.dispose();
         obsMgr = null;
        }
    }

As a side effect of not calling getObservationManager[Impl]() the observation manager would also not be created if not existing yet.

As a side effect to having the dispose method is, that the ObservationManagerImpl class could also do other cleanup work in addition to clearing the listener lists."
0,"Modify confusing javadoc for queryNormSee http://markmail.org/message/arai6silfiktwcer

The javadoc confuses me as well."
1,"Cluster revision entries should be retrieved in orderThe selectRevisionStmtSQL (DatabaseJournal#buildSqlStatements) returns a result set which may not be ordered by REVISION_ID. This has the effect that cluster instances that want to synchronize to the latest revision do not update their local revision appropriately since they assume that the revision result set is ordered (see code in AbstractJournal#doSync). This might cause a lot of unnecessary CPU cycles on these machines with degraded performance as a result. Furthermore, it causes functional issues as well as events may be fired multiple times and in the wrong order."
0,"Access to version history results in reading all versions of versionable nodeInternalVersionHistoryImpl loads all versions at once during initialization. Because of that all versioning operations (incl. checkin, label, restore) are significantly slower when node has many versions.

"
1,"Request/Response race condition when doing multiple requests on the same connection.If one tries to do multiple request over the same socket connection a race 
condition occurs in the input/output streams.
eg. 
-- Some request -->
<- HTTP/1.1 200 OK
<- Some: Headers
<- 
<- The body.

-- Next request -->
<- HTTP/1.1 200 OK
<- More: Headers
<- 
<- Some data.

If the second request is sent, but the second response isn't yet received 
before the client starts to try to read it, it'll get 
a ""org.apache.commons.httpclient.HttpRecoverableException: Error in parsing the 
status  line from the response: unable to find line starting with ""HTTP/"""" 
exception (it will think ""The body."" is part of the second response).

The following code will reproduce the problem:

import java.io.*;
import java.net.*;
import java.util.*;
import org.apache.commons.httpclient.*;
import org.apache.commons.httpclient.methods.*;

public class HttpClientRaceBug {
    public static void main(String[] args) {
        try {
            SimpleHttpServer.listen(8987);
            HttpClient client = new HttpClient();
            client.startSession(""localhost"", 8987);
            client.getState().setCredentials(""Test Realm"",  
                new UsernamePasswordCredentials(""foo"", ""bar""));
            
            for (int i = 0; i < 100; i++) {
                GetMethod meth = new GetMethod();
                client.executeMethod(meth);
            }
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
    
    private static final class SimpleHttpServer implements Runnable {
        private Socket socket;
        public SimpleHttpServer(Socket socket) {
            this.socket = socket;
        }
        public static void listen(final int port) {
            Thread server = new Thread() {
                public void run() {
                    try {
                        ServerSocket ss = new ServerSocket(port);
                        while (true) {
                            new Thread(new 
                                SimpleHttpServer(ss.accept())).start();
                        }
                    } catch (Exception e) {
                        e.printStackTrace();
                    }
                }
            };
            
            server.setDaemon(true);
            server.start();
        }
        public void run() {
            try {
                BufferedReader in = new BufferedReader(new 
                    InputStreamReader(this.socket.getInputStream()));
                
                int len = 0;
                boolean auth = false;
                String line;
                while ((line = in.readLine()) != null) {
                    System.out.println(""> "" + line);
                    
                    if (line.trim().equals("""")) {
                        in.read(new char[len]);
                        doOutput(auth);
                        auth = false;
                        len = 0;
                        
                    } else if (line.indexOf(':') > -1) {
                        StringTokenizer tok = new StringTokenizer(line, "":"");
                        String key = tok.nextToken().toLowerCase();
                        if (key.equals(""content-length"")) {
                            len = Integer.parseInt(tok.nextToken().trim());
                        } else if (key.equals(""authorization"")) {
                            auth = true;
                        }
                    }
                }
            } catch (Exception e) {}
        }
        private static int count = 0;
        public void doOutput(boolean authorized) throws IOException {
            Writer out = new OutputStreamWriter(this.socket.getOutputStream());
            count++;
            
            String id = (count < 100) ? 
                ((count < 10) ? ""00"" + count : ""0"" + count) : """" + count;
            if (authorized) {
                write(out, ""HTTP/1.1 200 OK\r\n"");
            } else {
                write(out, ""HTTP/1.1 401 Unauthorized\r\n"");
            }
            write(out, ""WWW-Authenticate: Basic realm=\""Test Realm\""\r\n"");
            write(out, ""Response-Id: "" + id + ""\r\n"");
            write(out, ""Content-Type: text/html; charset=iso-8859-1\r\n"");
            write(out, ""Content-Length: 17\r\n\r\n"");
            write(out, ""My Response ("" + id + "")"");
            out.close();
        }
        private void write(Writer out, String text) throws IOException {
            System.out.print(""< "" + text);
            out.write(text);
        }
    }
}"
0,"Allow overriding the specification version in MANIFEST.MFThe specification version in MANIFEST.MF should only consist of
digits. When we e. g. build a release candidate with a version like
2.3-rc1 then we have to specify a different specification version.

See related discussion:
http://www.gossamer-threads.com/lists/lucene/java-dev/56611

"
1,"DataStore.close() is never calledI've searched through the jackrabbit-core code and never found a call to DataStore.close(), although the method exists on the DataStore interface"
1,"If you hit the ""max term prefix"" warning when indexing, it never goes awaySilly bug.

If IW's infoStream is on, we warn whenever we hit a ridiculously long term (> 16 KB in length).  The problem is, we never reset this warning, so, once one doc contains such a massive term, we then keep warning over and over about that same term for future docs."
0,"XSLT pretty-printer for JCR document view export filesThe attached XSLT pretty-prints Jackrabbit XML document view export files.

I'm uploading it here so others can use it or improve it.

For now I'm using it standalone to document content structures, later I might create a servlet that applies it live to repository content."
0,"[OCM] rename o.a.j.ocm.persistence.PersistenceManager to avoid confusion with core componentPersistenceManager is a well known and established interface in jackrabbit's architecture. the same-named class in the
jcr-mapping contrib project should IMO be renamed in order to avoid confusion in mailing list threads and jira issues,"
0,Adding a custom location header extractor method for RedirectStrategy.Sometimes Web Servers respond to http requests with non-standard location response headers during a server side redirect. (302)  ADding a convenience method to over come this.
0,"Fix pom.xml in jackrabbit core (small fix, big return)Change the following dependency in pom.xml to match what is really available at the maven repos:
FROM:
    <dependency>
      <groupId>jsr170</groupId>
      <artifactId>jcr</artifactId>
      <version>1.0</version>
    </dependency>


TO:
    <dependency>
      <groupId>javax.jcr</groupId>
      <artifactId>jcr</artifactId>
      <version>1.0</version>
    </dependency>
"
1,JCR2SPI: VersionManagerImpl.getVersionableNodeEntry uses toString() rather than getString() to obtain property valueVersionManagerImpl.getVersionableNodeEntry uses toString() rather than getString() to obtain property value.
1,"TestNRTManager hangdidn't check 3.x yet, just encountered this one running the tests"
0,"Make ""ant -projecthelp"" show the javadocs and docs targets as wellAdded a description to the targets ""javadocs"" and ""docs"".
This makes ant show them when the executes ""ant -projecthelp""
"
0,"Ensure the features.html and index.html adequately give httpclient enough creditSee the email thread started by Eric Johnson.
http://archives.apache.org/eyebrowse/BrowseList?listId=128&by=thread&from=316092

Initial post:
Based on the recent URI discussion, and some other points, it strikes me that we
could take a little more credit for the work that has gone into HttpClient.

On the HttpClient home page
(http://jakarta.apache.org/commons/httpclient/index.html) four RFCs are listed.

Given all the discussion about URIs being thrown around, I think it might be
reasonable to add RFC 2396 - for URI compliance.  Then there is RFC 1867, for
multipart/form-data POST requests (I think I got the right number there).  Are
there RFCs corresponding to our ""cookie"" compliance? Any other RFCs we can claim
credit for conforming to?

With the recent ""Protocol"" changes, I think we've made it relatively
straightforward for clients of HttpClient to plug in their own secure sockets
implementations, making it easier to use third party, non-Sun solutions."
0,"GData Server IndexComponentNew Feature added:

-> Indexcomponent.
-> Content extraction from entries.
-> Custom content ext. strategies added.
-> user defined index schema.
-> extended gdata-config.xml schema (xsd)
-> Indexcomponent UnitTests
-> Spellchecking on some JavaDoc.

##############
New jars included:

nekoHTML.jar 
xercesImpl.jar

@yonik: don't miss the '+' button to add directories :)"
0,"SPI POM improvementsWhile the SPI components were upgraded from the sandbox I didn't pay too much attention to the POM details and thus there still are a number of configuration entries that duplicate stuff from the Jackrabbit parent POM, etc.

I plan to get rid of any such duplication, remove some unneeded dependencies (spi-commons has a compile scope dependency on junit) and generally update the POMs to be in line with the other release components."
0,"Add Japanese filter to replace term attribute with readingsKoji and Robert are working on LUCENE-3888 that allows spell-checkers to do their similarity matching using a different word than its surface form.

This approach is very useful for languages such as Japanese where the surface form and the form we'd like to use for similarity matching is very different.  For Japanese, it's useful to use readings for this -- probably with some normalization."
0,"Change all multi-term querys so that they extend MultiTermQuery and allow for a constant score modeCleans up a bunch of code duplication, closer to how things should be - design wise, gives us constant score for all the multi term queries, and allows us at least the option of highlighting the constant score queries without much further work."
0,"Speedup Startupjackrabbit startup gets slower the more items are in the repository.

possible reasons:
- versioning
- search index"
1,"TestAddIndexes reproducible test failure on turnktrunk: r1133385

{code}
    [junit] Testsuite: org.apache.lucene.index.TestAddIndexes
    [junit] Tests run: 2843, Failures: 1, Errors: 0, Time elapsed: 137.121 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] java.io.FileNotFoundException: _cy.fdx
    [junit]     at org.apache.lucene.store.RAMDirectory.fileLength(RAMDirectory.java:121)
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.fileLength(MockDirectoryWrapper.java:606)
    [junit]     at org.apache.lucene.index.SegmentInfo.sizeInBytes(SegmentInfo.java:294)
    [junit]     at org.apache.lucene.index.TieredMergePolicy.size(TieredMergePolicy.java:633)
    [junit]     at org.apache.lucene.index.TieredMergePolicy.useCompoundFile(TieredMergePolicy.java:611)
    [junit]     at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:2459)
    [junit]     at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes3.doBody(TestAddIndexes.java:847)
    [junit]     at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:675)
    [junit] java.io.FileNotFoundException: _cx.fdx
    [junit]     at org.apache.lucene.store.RAMDirectory.fileLength(RAMDirectory.java:121)
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.fileLength(MockDirectoryWrapper.java:606)
    [junit]     at org.apache.lucene.index.SegmentInfo.sizeInBytes(SegmentInfo.java:294)
    [junit]     at org.apache.lucene.index.TieredMergePolicy.size(TieredMergePolicy.java:633)
    [junit]     at org.apache.lucene.index.TieredMergePolicy.useCompoundFile(TieredMergePolicy.java:611)
    [junit]     at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:2459)
    [junit]     at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes3.doBody(TestAddIndexes.java:847)
    [junit]     at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:675)
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithRollback -Dtests.seed=9026722750295014952:2645762923088581043 -Dtests.multiplier=3
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=SimpleText, content=SimpleText, d=MockRandom, c=SimpleText}, locale=fr, timezone=Africa/Kigali
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestAddIndexes]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=68050392,total=446234624
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAddIndexesWithRollback(org.apache.lucene.index.TestAddIndexes):       FAILED
    [junit]
    [junit] junit.framework.AssertionFailedError:
    [junit]     at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithRollback(TestAddIndexes.java:932)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1362)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1280)
    [junit]
    [junit]
    [junit] Test org.apache.lucene.index.TestAddIndexes FAILED
{code}


Fails randomly in my while(1) test run, and Fails after a few min of running: 
{code}
ant test -Dtestcase=TestAddIndexes -Dtests.seed=9026722750295014952:2645762923088581043 -Dtests.multiplier=3 -Dtests.iter=200 -Dtests.iter.min=1
{code}"
0,"Allow similarity to encode norms other than a single byteLUCENE-3628 cut over norms to docvalues. This removes the long standing limitation that norms are a single byte. Yet, we still need to expose this functionality to Similarity to write / encode norms in a different format. "
1,"ArrayStoreException while reregistering existing node types
class: NodeTypeManagerImpl
method: public NodeType[] registerNodeTypes(InputStream in, String contentType, boolean reregisterExisting)

                ...
                return (NodeType[]) nodeTypes.toArray(new NodeTypeDef[nodeTypes.size()]);
                ...

=> should be (I suppose !)

                return (NodeType[]) nodeTypes.toArray(new NodeType[nodeTypes.size()]);
"
1,"There are a few binary search implmentations in lucene that suffer from a now well known overflow bughttp://googleresearch.blogspot.com/2006/06/extra-extra-read-all-about-it-nearly.html

The places I see it are:

MultiSearcher.subSearcher(int)
TermInfosReader.getIndexOffset(Term)
MultiSegmentReader.readerIndex(int, int[], int)
MergeDocIDRemapper.remap(int)

I havn't taken much time to consider how likely any of these are to overflow. The values being averaged would have to be very large. That would rule out possible problems for at least a couple of these, but how about something like the MergeDocIDRemapper? Is there a document number that could be reached that has a chance of triggering this bug? If not we can close this and have a record of looking into it."
1,"corrupted paths after moving nodeswe just found a bug which corrupts the results of Node.getPath() - it seems to be related to older Jackrabbit bugs (e.g. JCR-768) but still happens in jackrabbit 1.3 and jackrabbit-1.4-SNAPSHOT

Basically we have a node with 3 subnodes (a, b, c), we move all of them to index 1 - this works fine, unless we call getPath() of the third Node before moving it.

The expected paths after moving would be:
a: /pages[37]/page/element[3]
b: /pages[37]/page/element[2]
c: /pages[37]/page/element

But we get these paths:

a: /pages[37]/page/element[3]
b: /pages[37]/page/element
c: /pages[37]/page/element"
0,"Improve reliability of canAddMixinThe current implementation of canAddMixin in JCR2SPI lacks flexibility. It only consults the (SPI) node type registry, checking for (1) whether the mixin exists, and (2) whether it is already present and (3) whether it's consistent with the node's type.

This is fine for stores where any legal mixin can be added anywhere. It doesn't work well for stores that are limited in what they can do; for instance when nt:file nodes can be made mix:versionable, but nt:folder nodes can't.

Proposal: enhance QNodeTypeDefinition with

  public Name[] getSupportedMixins();

where the return value is either null (no constraints or no constraints known), or a list of mixin types that are supported for this node type."
0,Missing log4j.properties fileThe log4j.properties file is missing in the test resources.
0,"Replace deprecated TermAttribute by new CharTermAttributeAfter LUCENE-2302 is merged to trunk with flex, we need to carry over all tokenizers and consumers of the TokenStreams to the new CharTermAttribute.

We should also think about adding a AttributeFactory that creates a subclass of CharTermAttributeImpl that returns collation keys in toBytesRef() accessor. CollationKeyFilter is then obsolete, instead you can simply convert every TokenStream to indexing only CollationKeys by changing the attribute implementation."
0,"Use NativeFSLockFactory as default for new API (direct ctors & FSDir.open)A user requested we add a note in IndexWriter alerting the availability of NativeFSLockFactory (allowing you to avoid retaining locks on abnormal jvm exit). Seems reasonable to me - we want users to be able to easily stumble upon this class. The below code looks like a good spot to add a note - could also improve whats there a bit - opening an IndexWriter does not necessarily create a lock file - that would depend on the LockFactory used.


{code}  <p>Opening an <code>IndexWriter</code> creates a lock file for the directory in use. Trying to open
  another <code>IndexWriter</code> on the same directory will lead to a
  {@link LockObtainFailedException}. The {@link LockObtainFailedException}
  is also thrown if an IndexReader on the same directory is used to delete documents
  from the index.</p>{code}

Anyone remember why NativeFSLockFactory is not the default over SimpleFSLockFactory?"
0,"Configurable actions upon authorizable creation and removali would like to add the possibility to configure custom actions that are executed upon user (and group) creation before 
the operation is persisted. this would allow applications to run custom code without the need of subclassing the
usermanager implementation. e.g.: creating additional mandatory properties, setting up permissions, calculating default 
group membership. the same applies for user/group removal."
0,"Speedup CharArraySet if set is emptyCharArraySet#contains(...) always creates a HashCode of the String, Char[] or CharSequence even if the set is empty. 
contains should return false if set it empty"
0,"Add Bundle Persistence Managerswe (day software) offer our set of bundle persistence managers to the jackrabbit project. those pms combine the node and property states into a single bundle and store them together. this improves performance and reduces storage-memory overhead (no exact numbers available). The bundle pms also have a ""bundle-cache"" that does a memory sensitive caching of the bundles and a negative cache for non-existent bundles. small binary properties are inlined into the bundle rather than stored in the blobstore."
0,"Include generated website in the distributionA user should be able to build the non-api docs as well.

So it would be nice, to include xdocs in the source packages as well ..."
0,"Each TransactionContext creates new threadThe rollback threads are not stopped when the transaction commits, but only when the timeout occurs. This has the effect that lots of threads are created and sleeping when many transactions are committed in a short time frame. The rollback thread should be signaled when the transaction is committed or even better a Timer should be used with a single thread for all transaction contexts."
1,"Deadlocks in ConcurrentVersioningWithTransactionsTestPatch follows for a ConcurrentVersioningWithTransactionsTest, based on the existing ConcurrentVersioningTest but using transactions around the versioning operations.

On my macbook, running the test with CONCURRENCY = 100 and NUM_OPERATIONS = 100 causes a deadlock after a few seconds, thread dumps follow.

Note that I had to ignore StaleItemStateException (which is probably justified, due to not locking stuff IIUC) to let the threads run long enough to show the problem.

Running the test a few times showed the same locking pattern several times: some threads are locked at line 87 (session.save(), no transaction) while others are at line 93 (transaction.commit()), in testConcurrentCheckinInTransaction():

    80    public void testConcurrentCheckinInTransaction() throws RepositoryException {
    81      runTask(new Task() {
    82        public void execute(Session session, Node test) throws RepositoryException {
    83          int i = 0;
    84          try {
    85            Node n = test.addNode(""test"");
    86            n.addMixin(mixVersionable);
    87            session.save();
    88            for (i = 0; i < NUM_OPERATIONS / CONCURRENCY; i++) {
    89              final UserTransaction utx = new UserTransactionImpl(test.getSession());
    90              utx.begin();
    91              n.checkout();
    92              n.checkin();
    93              utx.commit();
    94            }
    95            n.checkout();
    96          } catch (Exception e) {
    97            final String threadName = Thread.currentThread().getName();
    98            final Throwable deepCause = getLevel2Cause(e);
    99            if(deepCause!=null && deepCause instanceof StaleItemStateException) {
   100              // ignore 
   101            } else {
   102              throw new RepositoryException(threadName + "", i="" + i + "":"" + e.getClass().getName(), e);
   103            }
   104          }
   105        }
   106      }, CONCURRENCY);
   107    }"
1,"SpanOrQuery skipTo() doesn't always move forwardsIn SpanOrQuery the skipTo() method is improperly implemented if the target doc is less than or equal to the current doc, since skipTo() may not be called for any of the clauses' spans:

    public boolean skipTo(int target) throws IOException {
          if (queue == null) {
            return initSpanQueue(target);
          }

          while (queue.size() != 0 && top().doc() < target) {
            if (top().skipTo(target)) {
              queue.adjustTop();
            } else {
              queue.pop();
            }
          }
          
        	return queue.size() != 0;
        }

This violates the correct behavior (as described in the Spans interface documentation), that skipTo() should always move forwards, in other words the correct implementation would be:

    public boolean skipTo(int target) throws IOException {
          if (queue == null) {
            return initSpanQueue(target);
          }

          boolean skipCalled = false;
          while (queue.size() != 0 && top().doc() < target) {
            if (top().skipTo(target)) {
              queue.adjustTop();
            } else {
              queue.pop();
            }
            skipCalled = true;
          }
          
          if (skipCalled) {
        	return queue.size() != 0;
          }
          return next();
        }"
1,"TestIndexWriterOnDiskFull.testAddIndexOnDiskFull fails with java.lang.IllegalStateException: CFS has pending open files {noformat}
 Testsuite: org.apache.lucene.index.TestIndexWriterOnDiskFull
    [junit] Testcase: testAddIndexOnDiskFull(org.apache.lucene.index.TestIndexWriterOnDiskFull):	Caused an ERROR
    [junit] CFS has pending open files
    [junit] java.lang.IllegalStateException: CFS has pending open files
    [junit] 	at org.apache.lucene.store.CompoundFileWriter.close(CompoundFileWriter.java:162)
    [junit] 	at org.apache.lucene.store.CompoundFileDirectory.close(CompoundFileDirectory.java:206)
    [junit] 	at org.apache.lucene.index.IndexWriter.createCompoundFile(IndexWriter.java:4099)
    [junit] 	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3661)
    [junit] 	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3260)
    [junit] 	at org.apache.lucene.index.SerialMergeScheduler.merge(SerialMergeScheduler.java:37)
    [junit] 	at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1902)
    [junit] 	at org.apache.lucene.index.IndexWriter.forceMerge(IndexWriter.java:1716)
    [junit] 	at org.apache.lucene.index.IndexWriter.forceMerge(IndexWriter.java:1670)
    [junit] 	at org.apache.lucene.index.TestIndexWriterOnDiskFull.testAddIndexOnDiskFull(TestIndexWriterOnDiskFull.java:304)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:529)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit] 
    [junit] 
    [junit] Tests run: 4, Failures: 0, Errors: 1, Time elapsed: 31.96 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterOnDiskFull -Dtestmethod=testAddIndexOnDiskFull -Dtests.seed=-7dd066d256827211:127c018cbf5b0975:20481cd18a7d8b6e -Dtests.multiplier=3 -Dtests.nightly=true -Dargs=""-Dfile.encoding=ISO8859-1""
    [junit] NOTE: test params are: codec=SimpleText, sim=RandomSimilarityProvider(queryNorm=true,coord=false): {field=DFR GB1, id=DFR I(F)L1, content=IB SPL-D3(800.0), f=DFR G2}, locale=de_AT, timezone=America/Cambridge_Bay
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestAssertions, TestSearchForDuplicates, TestMockAnalyzer, TestDocValues, TestPerFieldPostingsFormat, TestDocument, TestAddIndexes, TestConcurrentMergeScheduler, TestCrashCausesCorruptIndex, TestDocCount, TestDocumentsWriterDeleteQueue, TestFieldInfos, TestFilterIndexReader, TestFlex, TestIndexInput, TestIndexWriter, TestIndexWriterMergePolicy, TestIndexWriterMerging, TestIndexWriterNRTIsCurrent, TestIndexWriterOnDiskFull]
    [junit] NOTE: FreeBSD 8.2-RELEASE amd64/Sun Microsystems Inc. 1.6.0 (64-bit)/cpus=16,threads=1,free=39156976,total=180748288
{noformat}"
0,Some Workspace tests require a second workspaceSome workspace test require a second workspace even though it is not used in the test cases.
1,"Security issue - DigestScheme uses constant nonce count valueThe nonce count value in DigestScheme is static (set to 00000001) and never changes.  (also seen as comment in said file).

This means that it fails against servers that correctly detect man-in-the-middle or replay attacks, leading to additional 401 requests (every second time), or such servers must be configured to turn such checks off (which is either poor security or poor for performance).

I suggest that at minimum, this count is incremented for every call to DigestScheme#createDigest.  It should also be an instance variable instead of a static, as it really relates to the challenge (assuming cases where instances are cached for reuse).  AtomicInteger is a good choice for implementing this counter.

See RFC 2617 chapters 3.2.2 and 3.2.3"
1,"ISOLatin1AccentFilter discards position increments of filtered termsNot sure if this is a bug, but looks like one to me..."
1,"KeywordTokenizer does not set start/end offset of the Token it producesI think just adding these two lines in the next(Token) method is the right fix:

           reusableToken.setStartOffset(0);
           reusableToken.setEndOffset(upto);

I don't think this is a back compat issue because the start/end offset are now meaningless since they will inherit whatever the reusable token had previously been used for."
0,"Update slf4jPlease update slf4j from 1.3.0 to 1.5.2.

jcl104-over-slf4j has been renamed as jcl-over-slf4j, so if one uses a recent version, he has to exclude jcl104-over-slf4j for every jackrabbit dependency, which is quite a pain...

No impact observed.

Best regards,

Stephane Landelle"
1,"JCR-SQL2 : no count when WHERE clause is providedwhenever you provide a where-clause to a sql2 select, jcr/jackrabbit does not provide the hit count.

E.g.:
   select * from [nt:unstructured]
   order by [jcr:score]
returns the hit count (query.execute().getRows().getSize()), 
whereas
  select * from [nt:unstructured]
  where entity = ""customer""
  order by [jcr:score]
doesn't.
"
0,"Factor merge policy out of IndexWriterIf we factor the merge policy out of IndexWriter, we can make it pluggable, making it possible for apps to choose a custom merge policy and for easier experimenting with merge policy variants."
0,"Implementation of Delete methodThe HTTP request method, Delete, had not been implemented. I needed it and created an HttpDelete class modeled after HttpGet."
1,"spi2dav: Accessing moved referenceble nodes results in PathNotFoundExceptionthe following code fragment causes a PathNotFoundException on an existing path
and there seems to be no way to recover the session from this incorrect state:

	// assuming an existing nt:file node at path /apps/foo/bar.txt
	Node n1 = session.getNode(""/apps/foo/bar.txt"");
	Node n2 = n1.getNode(""jcr:content"");
	n2.setProperty(""jcr:data"", new java.io.ByteArrayInputStream(((String)(""blahblah"")).getBytes()));
	n2.save();
	Workspace ws0 = session.getWorkspace();
	ws0.move(""/apps/foo"", ""/apps/foo1"");
	Node n3 = session.getNode(""/apps/foo1/bar.txt"");
	Node n4 = n3.getNode(""jcr:content"");
	n4.refresh(false);
	Node n5 = n3.getNode(""jcr:content"");     // => PathNotFoundException

Please note that the preceeding Node.refresh() call seems to cause the inconsistency.
the problem doesn't occur when omitting this call."
1,"XPath QueryFormat may produce malformed XPath statementWhen the query tree contains select properties *and* an order by clause, then the XPath QueryFormat will produce a malformed XPath statement.

E.g.:

//element(*, foo)/(@a|@b) order by @bar

round trips to:

//element(*, foo) order by @bar/(@a|@b)

"
1,"Setting Query.setOffset() passed the results total returns negative getSize() instead of zero1. Have a query that returns 3 results
2. Now set Query.setOffset(10) (passed the total of 3)
3. Row/NodeIterator.getSize() returns -7 (incorrect)

Expected: getSize() should return 0"
0,"broken test in AddEventListenerHere's the test code, comments inline prefixed with ""reschke""

    /**
     * Tests if {@link javax.jcr.observation.Event#NODE_ADDED} is created only
     * for the specified path if <code>isDeep</code> is <code>false</code>.
     */
    public void testIsDeepFalseNodeAdded() throws RepositoryException {
        EventResult listener = new EventResult(log);

        // reschke: we are listening for changes at testRoot/nodeName1, with isDeep==false 
        obsMgr.addEventListener(listener, Event.NODE_ADDED, testRoot + ""/"" + nodeName1, false, null, null, false);

        // reschke; node at ""testRoot/nodeName1"" being created, the associated parent node for this event is ""testRoot""
        Node n = testRootNode.addNode(nodeName1, testNodeType);

        // reschke: node at ""testRoot/nodeName1/nodeName2"" being created, the associated parent node for this event is ""testRoot/nodeName1""
        n.addNode(nodeName2);
        testRootNode.save();

        Event[] events = listener.getEvents(DEFAULT_WAIT_TIMEOUT);
        obsMgr.removeEventListener(listener);

        // reschke: test case expects event with path ""testRoot/nodeName1""
        checkNodeAdded(events, new String[]{nodeName1});
    }

So, in plain english:

- test case listens for events where the associated parent node equals ""testRoot/nodeName1"", but
- it expects a single event where the Event.getPath() returns ""testRoot/nodeName1"".

This is incorrect (IMHO), because the associated parent node for *that* event is ""testRoot"". 

So the correct test would be to check for:

        checkNodeAdded(events, new String[]{nodeName1 + ""/"" + nodeName2});

Making this change of course leads to a test failure reported against the RI.

Feedback appreciated.
"
0,"[PATCH] Extension to binary Fields that allows fixed byte bufferThis is a very simple patch that supports storing binary values in the index
more efficiently.  A new Field constructor accepts a length argument, allowing a
fixed byte[] to be reused acrossed multiple calls with arguments of different
sizes.  A companion change to FieldsWriter uses this length when storing and/or
compressing the field.

There is one remaining case in Document.  Intentionally, no direct accessor to
the length of a binary field is provided from Document, only from Field.  This
is because Field's created by FieldReader will never have a specified length and
this is usual case for Field's read from Document.  It seems less confusing for
most users.

I don't believe any upward incompatibility is introduced here (e.g., from the
possibility of getting a larger byte[] than actually holds the value from
Document), since no such byte[] values are possible without this patch anyway.

The compression case is still inefficient (much copying), but it is hard to see
how Lucene can do too much better.  However, the application can do the
compression externally and pass in the reused compression-output buffer as a
binary value (which is what I'm doing).  This represents a substantialy
allocation savings for storing large documents bodies (compressed) into the
Lucene index.

Two patch files are attached, both created by svn on 3/17/05."
0,"Ant contrib test can fail if there is a space in path to lucene projectA couple contrib ant tests get the path to test files through a URL object, and so the path is URL encoded. Normally fine, but if you have a space in your path (/svn stuff/lucene/contrib/ant) then it will have %20 for the space and (at least on my Ubuntu system) the test will fail with filenotfound. This patch simply replaces all %20 with "" "". Not sure if we want/need to take it any further."
0,"New feature rich higlighter for Lucene.Well, I refactored (took) some code from two previous highlighters.
This highlighter:
+ use TermPositionVector where available
+ use Analyzer if no TermPositionVector found or is forced to use it.
+ support for all lucene queries (Term, Phrase with slops, Prefix, Wildcard, Range) except Fuzzy Query (can be implemented easly)

- has no support for scoring (yet)
- use same prefix,postfix for accepted terms (yet)

? It's written in Java5

In next release I'd like to add support for Fuzzy, ""coloring"" f.e. diffrent color for terms btw. phrase terms (slops), scoring of fragments

It's apache licensed - I hope so :-) I put licene statement in every file
"
1,"Deprecated Serializer does not properly delegate method calls.The deprecated org.apache.jackrabbit.core.state.util.Serializer class does not actually forward method calls to its replacement. Instead it calls itself repeatedly, leading to infinite recursion. The attached test demonstrates this and yields the following trace:

<<
java.lang.StackOverflowError
	at org.apache.jackrabbit.core.state.util.Serializer.serialize(Serializer.java:39)
>>"
1,"The ""jackrabbit-pool-"" thread prevents the process from stoppingIf the repository is not closed, and a session is still logged in, then the process doesn't terminate because of a non-daemon thread named ""jackrabbit-pool-<n>"". Test case:

public class TestThreadPreventsExit {
    public static void main(String... a) throws Exception {
        new TransientRepository().login(
                new SimpleCredentials("""", new char[0]));
    }
}

This program doesn't stop.

The non-daemon thread was introduces as part of https://issues.apache.org/jira/browse/JCR-2465

The fix is to use a daemon thread."
0,Configurable SimilarityThe similarity implementation for indexing and searching should be configurable.
1,"IllegalStateException: Authentication state already initializedHi,

I am running HttpClient 3.0 RC2 in my application and a user send me a logfile
telling ""IllegalStateException: Authentication state already initialized"". 

He wanted to access a site on SUN.com and is behind a proxy. The site seems to
redirect to a different domain.

I have attached a Debug+Trace HttpClient log.

Ben"
1,"The getOutputStream of the MemoryFileSystem class can replace a folder with a newly created fileIt seems that if the filePath parameter passed to the getOutputStream method of the MemoryFileSystem class points to an  existing folder and not to a file - the folder will be replaced with a newly created file.
The function should probably check whether the passed path points to a file and throw an exception if it points to a folder."
1,"PropertyValue constraint fails with implicit selectorName using JCR-SQL2Compiling a JCR-SQL2 query involving a PropertyValue constraint using a qualified property name fails if selectorName is not explicitly defined.

The following query works:

SELECT * FROM [my:thing] AS thing WHERE thing.[my:property] = 'abc'

the following doesn't:

SELECT * FROM [my:thing] AS thing WHERE [my:property] = 'abc'

(the ""AS thing"" is unecessary here, I can leave it out with the same result).

The second query results in an:
javax.jcr.query.InvalidQueryException: Query:
SELECT * FROM [my:thing] AS thing WHERE [(*)my:property] = 'abc';
expected: NOT, (

The spec final draft however states:

PropertyValue ::= [selectorName'.'] propertyName
   /* If only one selector exists in this query,
      explicit specification of the selectorName is
      optional */"
0,Add simple benchmarking tools for jcr2spi read performance
1,"NullPointerException when using HttpHead and Request/Response interceptorsWhen you try to execute a HttpHead object instead of a HttpGet object while using the add request/response interceptors, you get a nullpointerexception.

I can replicate the exception when using the ClientGZipContentCompression example that can be found at the HttpClient examples. But instead of using the HttpGet object I execute a HttpHead object. When I comment the interceptor parts out, I don't get the exception. 

This is the error stack trace I get when executing the code in netbeans:

Exception in thread ""main"" java.lang.NullPointerException
	at testhttphead.ClientGZipContentCompression$2.process(ClientGZipContentCompression.java:74)
	at org.apache.http.protocol.ImmutableHttpProcessor.process(ImmutableHttpProcessor.java:116)
	at org.apache.http.protocol.HttpRequestExecutor.postProcess(HttpRequestExecutor.java:342)
	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:472)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:820)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:754)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:732)
	at testhttphead.ClientGZipContentCompression.main(ClientGZipContentCompression.java:92)
Java Result: 1

Here is the code that gives me the error:

package testhttphead;

import java.io.IOException;
import java.io.InputStream;
import java.util.zip.GZIPInputStream;
import org.apache.http.*;
import org.apache.http.client.methods.HttpHead;
import org.apache.http.entity.HttpEntityWrapper;
import org.apache.http.impl.client.DefaultHttpClient;
import org.apache.http.protocol.HttpContext;
import org.apache.http.util.EntityUtils;

/**
 * Demonstration of the use of protocol interceptors to transparently modify
 * properties of HTTP messages sent / received by the HTTP client.
 * <p/>
 * In this particular case HTTP client is made capable of transparent content
 * GZIP compression by adding two protocol interceptors: a request interceptor
 * that adds 'Accept-Encoding: gzip' header to all outgoing requests and a
 * response interceptor that automatically expands compressed response entities
 * by wrapping them with a uncompressing decorator class. The use of protocol
 * interceptors makes content compression completely transparent to the consumer
 * of the {@link org.apache.http.client.HttpClient HttpClient} interface.
 */
public class ClientGZipContentCompression {

    public final static void main(String[] args) throws Exception {
        DefaultHttpClient httpclient = new DefaultHttpClient();

        try {
            httpclient.addRequestInterceptor(new HttpRequestInterceptor() {

                public void process(
                        final HttpRequest request,
                        final HttpContext context) throws HttpException, IOException {
                    if (!request.containsHeader(""Accept-Encoding"")) {
                        request.addHeader(""Accept-Encoding"", ""gzip"");
                    }
                }
            });

            httpclient.addResponseInterceptor(new HttpResponseInterceptor() {

                public void process(
                        final HttpResponse response,
                        final HttpContext context) throws HttpException, IOException {
                    HttpEntity entity = response.getEntity();
                    Header ceheader = entity.getContentEncoding();
                    if (ceheader != null) {
                        HeaderElement[] codecs = ceheader.getElements();
                        for (int i = 0; i < codecs.length; i++) {
                            if (codecs[i].getName().equalsIgnoreCase(""gzip"")) {
                                response.setEntity(
                                        new GzipDecompressingEntity(response.getEntity()));
                                return;
                            }
                        }
                    }
                }
            });

            HttpHead httpHead = new HttpHead(""http://www.howest.be"");

            // Execute HTTP request
            System.out.println(""executing request "" + httpHead.getURI());
            HttpResponse response = httpclient.execute(httpHead);

            System.out.println(""----------------------------------------"");
            System.out.println(response.getStatusLine());
            System.out.println(response.getLastHeader(""Content-Encoding""));
            System.out.println(response.getLastHeader(""Content-Length""));
            System.out.println(""----------------------------------------"");

            HttpEntity entity = response.getEntity();

            if (entity != null) {
                String content = EntityUtils.toString(entity);
                System.out.println(content);
                System.out.println(""----------------------------------------"");
                System.out.println(""Uncompressed size: "" + content.length());
            }

        } finally {
            // When HttpClient instance is no longer needed,
            // shut down the connection manager to ensure
            // immediate deallocation of all system resources
            httpclient.getConnectionManager().shutdown();
        }
    }

    static class GzipDecompressingEntity extends HttpEntityWrapper {

        public GzipDecompressingEntity(final HttpEntity entity) {
            super(entity);
        }

        @Override
        public InputStream getContent()
                throws IOException, IllegalStateException {

            // the wrapped entity's getContent() decides about repeatability
            InputStream wrappedin = wrappedEntity.getContent();

            return new GZIPInputStream(wrappedin);
        }

        @Override
        public long getContentLength() {
            // length of ungzipped content is not known
            return -1;
        }
    }
}

With kind regards,

Peter"
1,Incorrect excerpt for index aggregatesIncorrect excerpts may be created when the relevant node has an index aggregate configured and the nodes have properties configured for the node scope index with some of them excluded for use in excerpts.
1,"Two or more writers over NFS can cause index corruptionWhen an index is used over NFS, and, more than one machine can be a
writer such that they swap roles quickly, it's possible for the index
to become corrupt if the NFS client directory cache is stale.

Not all NFS clients will show this.  Very recent versions of Linux's
NFS client do not seem to show the issue, yet, slightly older ones do,
and the latest Mac OS X one does as well.

I've been working with Patrick Kimber, who provided a standalone test
showing the problem (thank you Patrick!).  This came out of this
thread:

  http://www.gossamer-threads.com/lists/engine?do=post_view_flat;post=50680;page=1;sb=post_latest_reply;so=ASC;mh=25;list=lucene

Note that the first issue in that discussion has been resolved
(LUCENE-948).  This is a new issue.
"
0,"Similarity can only be set per index, but I may want to adjust scoring behaviour at a field levelSimilarity can only be set per index, but I may want to adjust scoring behaviour at a field level, to faciliate this could we pass make field name available to all score methods.
Currently it is only passed to some such as lengthNorm() but not others such as tf()"
1,"OutOfMemory problem: HandleMonitor does not release closed input streamsThe class o.a.j.core.fs.local.HandleMonitor does not release closed MonitoredInputStream. There is a close method, but it is never called. The input streams are kept in a hash set / map of HandleMonitor. Eventually, this leads to an OutOfMemory exception after opening / closing many files."
0,"AccessControlImporter does not import repo level ac contentthe implementation of the ProtectedNodeImporter responsible for dealing with access control content should be
adjusted such that it can properly cope with repository level access control that may be stored together with
the root node (by using access control API with null path)."
0,"Avoid ${project.version} in dependenciesAnother one for Jackrabbit 1.5, we should avoid using ${project.version} for our dependencies and override the versions of any transitive dependencies that use ${project.version} (notably the Jetty dependencies in jackrabbit-standalone) to avoid problems with Maven < 2.0.9 caused by MNG-2339 [1].

[1] http://jira.codehaus.org/browse/MNG-2339
"
0,"Replace license headers with new policy textWe need to replace all of the license headers with a new template
that replaces the Copyright and license lines with

---BEGIN PROPOSED SOURCE FILE HEADER---
  Licensed to the Apache Software Foundation (ASF) under one or more
  contributor license agreements.  The ASF licenses this file to You
  under the Apache License, Version 2.0 (the ""License""); you may not
  use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an ""AS IS"" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
---END PROPOSED SOURCE FILE HEADER---

The copyright line is being removed from files due to legal advice from ASF attorneys.
It is replaced with a statement that the copyright owners have licensed it to the ASF.
"
0,"Optimize copies between IndexInput and OutputWe've created an optimized copy of files from Directory to Directory. We've also optimized copyBytes recently. However, we're missing the opposite side of the copy - from IndexInput to Output. I'd like to mimic the FileChannel API by having copyTo on IndexInput and copyFrom on IndexOutput. That way, both sides can optimize the copy process, depending on the type of the IndexInput/Output that they need to copy to/from.

FSIndexInput/Output can use FileChannel if the two are FS types. RAMInput/OutputStream can copy to/from the buffers directly, w/o going through intermediate ones. Actually, for RAMIn/Out this might be a big win, because it doesn't care about the type of IndexInput/Output given - it just needs to copy to its buffer directly.

If we do this, I think we can consolidate all Dir.copy() impls down to one (in Directory), and rely on the In/Out ones to do the optimized copy. Plus, it will enable someone to do optimized copies between In/Out outside the scope of Directory.

If this somehow turns out to be impossible, or won't make sense, then I'd like to optimize RAMDirectory.copy(Dir, src, dest) to not use an intermediate buffer."
0,"FST apis out of sync between trunk/3.xLooks like the offender is LUCENE-3030 :)

Not sure if everything is generally useful but it does change the public API (e.g. you can specify FreezeTail to the super-scary Builder ctor among other things).

Maybe we should sync up for 3.x? "
1,"Range queries fail on large repositoriesAs discussed on the user mailing list, queries on large repositories with date constraints like ""field > constant"" treat the constraint as always true, returning results that should not be returned."
0,"Use POIExtractor wherever possiblePOI scratchpad comes with a couple of text extractor utilities, which makes it easier to extract text. We should rather use those utilities than writing our own extractor code. This helps avoid issues like JCR-1530."
1,"NPE in ObservationManagerImpl.getRegisteredEventListeners() during shutdown after broken startupSee JCR-2378. The variable ""dispatcher"" is passed as null in the constructor."
0,"Performance improvement for TermInfosReaderCurrently we have a bottleneck for multi-term queries: the dictionary lookup is being done
twice for each term. The first time in Similarity.idf(), where searcher.docFreq() is called.
The second time when the posting list is opened (TermDocs or TermPositions).

The dictionary lookup is not cheap, that's why a significant performance improvement is
possible here if we avoid the second lookup. An easy way to do this is to add a small LRU 
cache to TermInfosReader. 

I ran some performance experiments with an LRU cache size of 20, and an mid-size index of
500,000 documents from wikipedia. Here are some test results:

50,000 AND queries with 3 terms each:
old:                  152 secs
new (with LRU cache): 112 secs (26% faster)

50,000 OR queries with 3 terms each:
old:                  175 secs
new (with LRU cache): 133 secs (24% faster)

For bigger indexes this patch will probably have less impact, for smaller once more.

I will attach a patch soon."
1,"Missing sync in IndexWriter.addIndexes(IndexReader[])The 3.x build just hit this:

{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestAddIndexes
    [junit] Testcase: testAddIndexesWithThreads(org.apache.lucene.index.TestAddIndexes):	FAILED
    [junit] expected:<3160> but was:<2701>
    [junit] junit.framework.AssertionFailedError: expected:<3160> but was:<2701>
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:779)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:745)
    [junit] 	at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithThreads(TestAddIndexes.java:708)
    [junit] 
    [junit] 
    [junit] Tests run: 15, Failures: 1, Errors: 0, Time elapsed: 9.28 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] java.lang.AssertionError: RefCount is 0 pre-decrement for file ""_8a.tvf""
    [junit] 	at org.apache.lucene.index.IndexFileDeleter$RefCount.DecRef(IndexFileDeleter.java:608)
    [junit] 	at org.apache.lucene.index.IndexFileDeleter.decRef(IndexFileDeleter.java:505)
    [junit] 	at org.apache.lucene.index.IndexFileDeleter.decRef(IndexFileDeleter.java:496)
    [junit] 	at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:2972)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes.doBody(TestAddIndexes.java:681)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:624)
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithThreads -Dtests.seed=-6912763261803132408:-5575674032550262483 -Dtests.multiplier=3
    [junit] NOTE: test params are: locale=en_AU, timezone=America/Atka
{noformat}

It looks like it's caused by a long-standing missing sync (since at least 2.9.x).  I think likely we hit it just now thanks to adding random Thread.yield()'s in MockDirWrapper!"
1,"Hop 0 sample app doesn't exit because of on-daemon thread pool-1-thread-1When starting the sample app Hop 0 (or any other Hop sample app) if there is no ""repository"" directory, then the application doesn't exit because there is a non-daemon thread named ""pool-1-thread-1""."
0,"Grouping module should allow subclasses to set the group key per documentThe new grouping module can only group by a single-valued indexed field.

But, if we make the 'getGroupKey' a method that a subclass could override, then I think we could refactor Solr over to the module, because it could do function queries and normal queries via subclass (I think).

This also makes the impl more extensible to apps that might have their own interesting group values per document."
1,FALSE predicate always returns trueorg.apache.jackrabbit.spi.commons.iterator.Predicates..FALSE always returns true instead of false.
0,"Nuke SpanFilters and CachingSpanFilter (maybe move to sandbox)SpanFilters are inefficient and OOM easily (they don't scale at all: Create large Lists of Objects for every match, also filtering deleted docs is a pain). Some talks with Grant on Eurocon and also the fact that caching of them is still broken in 3.x (but fixed on trunk) - I assume nobody uses them, so let's nuke them. They are also in wrong package, so standard statement: ""Die, SpanFilters, die!"""
0,"SimilarityDelegator is missing a delegating scorePayload() methodThe handy SimilarityDelegator method is missing a scoreDelegator() delegating method.
The fix is trivial, add the code below at the end of the class:

  public float scorePayload(String fieldName, byte [] payload, int offset, int length)
  {
      return delegee.scorePayload(fieldName, payload, offset, length);
  }
"
0,Return bind variable names on RepositoryService.checkQueryStatement()To properly support JSR 283 bind variables the SPI layer needs to return the names of the bind variables. Otherwise the jcr2spi implementation cannot check for unknown names.
0,"Data store garbage collection: ScanEventListener not workingThe ScanEventListener is currently only called when using the 'scan all nodes recursively' strategy. It is not called when all persistence managers implement IterablePersistenceManager (GarbageCollector.scanPersistenceManagers). The ScanEventListener should be called in every case, otherwise it is not possible to see the progress of the garbage collection.

However there is a problem: IterablePersistenceManager.getAllNodeIds() doesn't return Node objects, and it would make little sense to create real node objects (the performance advantage of scanPersistenceManagers would be lost).

Therefore, I propose a workaround: the ScanEventListener is called using a 'PseudoNode'. This is a class that implements Node but only has meaningful getUUID() and toString() methods. This allows to create a meaningful progress bar (as the UUIDs are returned in order)."
1,"PulsingTermState.clone leaks memoryI looked at the heap dump from the OOME this morning (thank you Uwe
for turning this on!), and I think it's a real memory leak.

Well, not really a leak; rather, the cloned PulsingTermState, which we
cache in the terms dict cache, is hanging onto large byte[]
unnecessarily.
"
0,Automatic upgrade to 2.0Jackrabbit 2.0 contains some changes that are not compatible with repositories created with earlier versions. It would be nice if Jackrabbit would automatically detect and upgrade repositories created with 1.x versions.
1,"Deadlock  on concurrent read & transactional write operationsIsuue has been introduced by resolving JCR-1755 (Transaction-safe versioning). This fixed changed sequence of commits, but at the same time order of acquiring locks has been disturbed.


"
0,"Remove JDOM dependencyProposed by Sylvain Wallez on the dev mailing list.

Replace the JDOM code in the config, nodetype, and xml persistence manager code with equivalent standard DOM code. This change introduces some extra lines of code, but would remove an external dependency and avoid unnecessary deployment problems."
0,Add n-gram tokenizers to contrib/analyzersIt would be nice to have some n-gram-capable tokenizers in contrib/analyzers.  Patch coming shortly.
0,"Add a method public boolean hasNodeType(String name) in NodeTypeManagerImplAs seen in the ML, we plan to add this method and update this class and the interface JackrabbitNodeTypeManager"
0,"'ant javacc' in root project should also properly create contrib/queryparser Java files'ant javacc' in the project root doesn't run javacc in contrib/queryparser
'ant javacc' in contrib/queryparser does not properly create the Java files. What still needs to be done by hand is (partly!) described in contrib/queryparser/README.javacc. I think this process should be automated. Patch provided."
0,Improve Javadoc
0,"Upgrade to Maven 2If you are interested in migrating to maven2 (or adding optional maven 2 build scripts) this is a full maven 2 pom.xml for the main jackrabbit jar.

All the xpath/javacc stuff, previously done in maven.xml, was pretty painfull to reproduce in maven2... the attached pom exactly reproduces the m1 build by using the maven2 javacc plugin + a couple of antrun executions.
Test configuration is not yet complete, I think it will be a lot better to reproduce the previous behaviour (init tests run first) without any customization (maybe using a single junit test suite with setUp tasks). Also custom packaging goals added to maven.xml (that can be esily done in m2 by using the assembly plugin) are not yet reproduced too.

If there is interest, I can also provide poms for the contribution projects (that will be easy, the only complex pom is the main one).
"
1,"IOUtils - getCreated(...) - SimpleDateFormat is not threadsafeSimpleDateFormat is not threadsafe (http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4146524)
See exception in attachment.
IMHO it will be enough to synchronize 'format' method in HttpDateFormat class."
0,RAMDirectory implements SerializableRAMDirectory is for some reason not serializable.
0,"add spanquery support for all multitermqueriesI set fix version: 4.0, but possibly we could do this for 3.x too

Currently, we have a special SpanRegexQuery in contrib, and issues like LUCENE-522 open for SpanFuzzyQuery.
The SpanRegexQuery in contrib is a little messy additionally.

For any arbitrary MultiTermQueries to work as a SpanQuery, there are only 3 requirements:
# The un-rewritten query must extend SpanQuery so it can be included in Span clauses
# The rewritten query should be SpanOrQuery instead of BooleanQuery
# The rewritten term clauses should be SpanTermQueries.

Instead of having logic like this for each query, i suggest adding two rewrite methods:
* ScoringSpanBoolean rewrite
* TopTermsSpanBoolean rewrite

as a start i wrote these up, and added a SpanMultiTermQueryWrapper that can be used to wrap any multitermquery this way.
there are a few kinks, but I think the MTQ policeman can probably help get through them.
"
1,"SnowballFilter loses token position offsetSnowballFilter doesn't set the token position increment (and thus it defaults to 1).
This also affetcs SnowballAnalyzer since it uses SnowballFilter."
0,"Split up IndexInput and IndexOutput into DataInput and DataOutputI'd like to introduce the two new classes DataInput and DataOutput
that contain all methods from IndexInput and IndexOutput that actually
decode or encode data, such as readByte()/writeByte(),
readVInt()/writeVInt().

Methods like getFilePointer(), seek(), close(), etc., which are not
related to data encoding, but to files as input/output source stay in
IndexInput/IndexOutput.

This patch also changes ByteSliceReader/ByteSliceWriter to extend
DataInput/DataOutput. Previously ByteSliceReader implemented the
methods that stay in IndexInput by throwing RuntimeExceptions.

See also LUCENE-2125.

All tests pass."
1,"Cookie.java hashCode method violates contractorg.apache.commons.httpclient.Cookie hashCode() does not meet object.hashCode
() contract.  Cookie.hashCode() returns different values even though data used 
in equals() comparison is the same.

Contract:**Whenever it is invoked on the same object more than once during an 
execution of a Java application, the hashCode method must consistently return 
the same integer, provided no information used in equals comparisons on the 
object is modified.**

Breaks use of cookie within collections such as when using contains().

Traced problem back to parent class NameValuePair.  Cookie.hashCode() calls 
NameValuePair.hashCode() which relies on name/value hashes.  Cookie does not 
rely on value to determine equality."
0,"Implement a way to override or resolve DNS entries defined in the OSWhen working with HttpClient in restrictive environments, where the user doesn't have the permissisions to edit the local /etc/hosts file or the DNS configuration, can be eased with an DNS Overrider capability. 

This can be useful with JMeter which can follow redirects automatically and resolve some of the redirected hosts against its configuration. Another example is a custom forward proxy, written in Java and based on httpclient, which can be deployed is such a restricted environment that would ease the development of various web solutions for some developers. "
0,"test cases relying on Node.equals()Several test cases rely on Node.equals to compare nodes, where instead isSame() should be used:

org.apache.jackrabbit.test.api.NodeTest.testNodeIdentity(NodeTest.java:751)
org.apache.jackrabbit.test.api.NodeTest.testNodeIdentity(NodeTest.java:753)
org.apache.jackrabbit.test.api.version.VersionHistoryTest.testInitallyGetAllVersionsContainsTheRootVersion(VersionHistoryTest.java:126)
org.apache.jackrabbit.test.api.version.VersionHistoryTest.testGetVersion(VersionHistoryTest.java:180)
org.apache.jackrabbit.test.api.version.CheckinTest.testMultipleCheckinHasNoEffect(CheckinTest.java:93)
org.apache.jackrabbit.test.api.version.VersionGraphTest.testInitialBaseVersionPointsToRootVersion(VersionGraphTest.java:47)
org.apache.jackrabbit.test.api.version.RemoveVersionTest.testRemoveVersionAdjustPredecessorSet(RemoveVersionTest.java:120)
org.apache.jackrabbit.test.api.version.RemoveVersionTest.testRemoveVersionAdjustSucessorSet(RemoveVersionTest.java:144)

 "
1,"Concurrent add/remove child node operations in a cluster may corrupt repository.Concurrent add/remove child node operations in a cluster may store an inconsistent list of child node entries, i.e. an entry in the list may appear that has no associated node. This eventually results in an ItemNotFoundException, the next time one of these bogus entries is accessed."
0,"Litmus prophighunicode test failure on JRE 1.5The WebDAV Litmus test suite contains a test case for writing and reading the Unicode character &#x10000; which can't be represented as a single 16-bit char in Java. Instead the character is stored as a surrogate pair of two 16-bit chars. Unfortunately the Xalan XML serializer used by Sun JRE 1.5 incorrectly encodes these as two separate characters in UTF-8, which leads to the following Litmus test failure:

-> running `props':
[...]
17. prophighunicode....... pass
18. propget............... FAIL (PROPFIND on `/default/litmus/prop2': XML parse error at line 1: not well-formed (invalid token))
"
1,"CachingWrapperFilter throws NPE when Filter.getDocIdSet() returns nullFollowup for [http://www.lucidimagination.com/search/document/1014ea92f15677bd/filter_getdocidset_returning_null_and_what_this_means_for_cachingwrapperfilter]:

Daniel Noll is seeing an exception like this:

{noformat}
java.lang.NullPointerException
    at org.apache.lucene.search.CachingWrapperFilter.docIdSetToCache(CachingWrapperFilter.java:84)
    at org.apache.lucene.search.CachingWrapperFilter.getDocIdSet(CachingWrapperFilter.java:112)
    at com.nuix.storage.search.LazyConstantScoreQuery$LazyFilterWrapper.getDocIdSet(SourceFile:91)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantScorer.<init>(ConstantScoreQuery.java:116)
    at org.apache.lucene.search.ConstantScoreQuery$ConstantWeight.scorer(ConstantScoreQuery.java:81)
    at org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:297)
    at org.apache.lucene.search.BooleanQuery$BooleanWeight.scorer(BooleanQuery.java:297)
    at org.apache.lucene.search.QueryWrapperFilter$2.iterator(QueryWrapperFilter.java:75)
{noformat}

The class of our own is just an intermediary which delays creating the Filter object...

{code}
@Override
public DocIdSet getDocIdSet(IndexReader reader) throws IOException {
            if (delegate == null) {
                delegate = factory.createFilter();
            }
            return delegate.getDocIdSet(reader);
}
{code}

Tracing through the code in CachingWrapperFilter, I can see that this NPE would occur if getDocIdSet() were to return null.

The Javadoc on Filter says that null will be returned if no documents will be accepted by the filter, but it doesn't seem that Lucene itself is handling null return values correctly, so which is correct?  The code or the Javadoc?  Supposing that null really is OK, does this cause any problems with how CachingWrapperFilter is implementing the caching?  I notice it's calling get() and then comparing against null so it wouldn't appear that it can distinguish ""the entry isn't in the cache"" from ""the entry is in the cache but it's null""."
0,"Improvement to MultiValueCollectionConverterImpl to Map collections with element class Object.classCurrently MultiValueCollectionConverterImpl  does not support elements of type Object.class.  The type of the contained class has to be specified either through the mapping file or through the Bean annotation.  Even with that flexibility Object.class is specifically excluded (For good reasons.).  

My view is that by definition MultiValueCollectionConverterImpl   should make a best effort to convert and that best effort should include using Undefined UndefinedTypeConverterImpl to convert an object when all the other conversion strategies run out.  To this resolve I have patched the OCM source.  I have test cases also.  I will upload the patch files right after."
1,"TestAddIndexes#testAddIndexesWithThreads fails on RealtimeSelckin reported two failures on LUCENE-3023 which I can unfortunately not reproduce at all. here are the traces

{noformat}
  [junit] Testsuite: org.apache.lucene.index.TestAddIndexes
    [junit] Testcase: testAddIndexesWithThreads(org.apache.lucene.index.TestAddIndexes):	FAILED
    [junit] expected:<3160> but was:<3060>
    [junit] junit.framework.AssertionFailedError: expected:<3160> but was:<3060>
    [junit] 	at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithThreads(TestAddIndexes.java:783)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1226)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1154)
    [junit] 
    [junit] 
    [junit] Tests run: 18, Failures: 1, Errors: 0, Time elapsed: 14.272 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithThreads -Dtests.seed=6128854208955988865:2552774338676281184
    [junit] NOTE: test params are: codec=PreFlex, locale=no_NO_NY, timezone=America/Edmonton
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestToken, TestDateTools, Test2BTerms, TestAddIndexes]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=84731792,total=258080768
    [junit] ------------- ---------------- ---------------
{noformat}
and 
{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestAddIndexes
    [junit] Testcase: testAddIndexesWithThreads(org.apache.lucene.index.TestAddIndexes):	FAILED
    [junit] expected:<3160> but was:<3060>
    [junit] junit.framework.AssertionFailedError: expected:<3160> but was:<3060>
    [junit] 	at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithThreads(TestAddIndexes.java:783)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1226)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1154)
    [junit] 
    [junit] 
    [junit] Tests run: 18, Failures: 1, Errors: 0, Time elapsed: 14.841 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestAddIndexes -Dtestmethod=testAddIndexesWithThreads -Dtests.seed=4502815121171887759:-6764285049309266272
    [junit] NOTE: test params are: codec=PreFlex, locale=tr_TR, timezone=Mexico/BajaNorte
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestToken, TestDateTools, Test2BTerms, TestAddIndexes]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=163663416,total=243335168
    [junit] ------------- ---------------- ---------------
{noformat}"
0,"Explicit management of public APII'd like to start using the Clirr Maven plugin [1] to make sure that we don't accidentally break backwards compatibility in our public APIs, most notably in jackrabbit-api and jackrabbit-jcr-commons.

Also, we should start explicitly managing the API versions exposed as a part of the OSGi package metadata. Currently all our public packages simply get the latest project version as their version number, but it would be better if the version was explicitly managed and only updated if the API actually changes. To do this I propose we use @Version annotations from the bnd tool on the package-info.java files in all packages considered a part of our public API.

The Clirr plugin should flag all changes made in the API, so we have an easy way to tell which packages need to have their version numbers updated.

[1] http://mojo.codehaus.org/clirr-maven-plugin/"
0,"[patch] Support for digest auth MD5-sessI was attempting to access a device that requires Digest authentication using
MD5-sess, which does not seem to be supported."
0,"Add a new TestBackwardsCompatibility index for flex backwards (a 3.0 one with also numeric fields)In flex we change also the encode/decoder for numeric fields (NumericTokenSteam) using BytesRef and also the collation filters. We should add a test index from 3.0 that contains these fields and do some validation, that field contents did not change when read with flex."
0,"hashCode improvementsIt would be nice for all Query classes to implement hashCode and equals to enable them to be used as keys when caching.
"
1,"[httpclient] Incorrect credentials loop infinitelyIf incorrect credentials are assigned to the request, HttpClient will loop 
forever.  It should only try once, and fail with an HttpException if a request 
with credentials set fails.

In org.apache.commons.httpclient.HttpMethodBase.execute(), a check is needed to 
track if credentials have been sent before."
0,"[JCR-RMI] Have ClientItem.isSame throw RepositoryExceptionCurrently the ClientItem.isSame(Item) method wraps a RepositoryException thrown from the path comparison into a RuntimeException and omits an exception declaration on the method. This contrasts with the API specification which allows for a RepositoryExcption to be thrown.

I suggest, to modify ClientItem.isSame(Item) such that the RepositoryException is declared and thrown."
0,JSR 283 Query
1,"WeightedSpanTermExtractor incorrectly treats the same terms occurring in different query typesGiven a BooleanQuery with multiple clauses, if a term occurs both in a Span / Phrase query, and in a TermQuery, the results of term extraction are unpredictable and depend on the order of clauses. Concequently, the result of highlighting are incorrect.

Example text: t1 t2 t3 t4 t2
Example query: t2 t3 ""t1 t2""
Current highlighting: [t1 t2] [t3] t4 t2
Correct highlighting: [t1 t2] [t3] t4 [t2]

The problem comes from the fact that we keep a Map<termText, WeightedSpanTerm>, and if the same term occurs in a Phrase or Span query the resulting WeightedSpanTerm will have a positionSensitive=true, whereas terms added from TermQuery have positionSensitive=false. The end result for this particular term will depend on the order in which the clauses are processed.

My fix is to use a subclass of Map, which on put() always sets the result to the most lax setting, i.e. if we already have a term with positionSensitive=true, and we try to put() a term with positionSensitive=false, we set the result positionSensitive=false, as it will match both cases."
1,upgrade icu jar to 4.8.1.1 / remove lucenetestcase hackThis bug fix release fixes problems with icu under java7: http://bugs.icu-project.org/trac/ticket/8734
1,"When reopen returns a new IndexReader, both IndexReaders may now control the lifecycle of the underlying Directory which is managed by reference countingRough summary. Basically, FSDirectory tracks references to FSDirectory and when IndexReader.reopen shares a Directory with a created IndexReader and closeDirectory is true, FSDirectory's ref management will see two decrements for one increment. You can end up getting an AlreadyClosed exception on the Directory when the IndexReader is open.

I have a test I'll put up. A solution seems fairly straightforward (at least in what needs to be accomplished)."
0,"Add support to provide custom classloader for class instantiation from configurationThe configuration framework is based around a BaseConfig class, which provides functionality to instantiate a class whose name is configured in the repository configuration file. Examples of such classes are the FileSystem or the PersistenceManager elements.

The current implementation of the BeanConfig.newInstance() method is to use the ""default classloader"" to load configured classes. That is, the class loader of the BeanConfig class is actually used. This is - generally - the class loader which loads the repository. In certain environments, classes may be provided from outside the core repository class loader. An example fo such an environment is an OSGi setup where each bundle gets its own class laoder, which is separate from all other class loaders except declared by configuration.

I propose to enhance the BeanConfig class as follows:

public class BeanConfig {
 ...
 // Current default class loader, default is BeanConfig's class loader
 private static ClassLoader defaultClassLoader =
BeanConfig.class.getClassLoader();
 // Current instance class loader
 private ClassLoader classLoader;
 ...
 // Sets the default class loader for new BeanConfig instances
 public static void setDefaultClassLoader(ClassLoader loader);
 // Returns the default class loader for new BeanConfig instances
 public static ClassLoader getClassLoader();
 // Sets the class loader of this BeanConfig instance
 public void setClassLoader(ClassLoader loader);
 // Returns the class loader of this BeanConfig instance
 public ClassLoader getClassLoader();
}

The BeanConfig.newInstance method would then use the following to use the class:

public Object newInstance() throws ConfigurationException {
 Class clazz = Class.forName(getClassName(), true, getClassLoader());
 ...
}


This has also been discussed on the dev list: http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/200607.mbox/%3cae03024e0607272341l52aff9b2h3957131411790bc9@mail.gmail.com%3e"
0,"Reusable Repository access and bind servletsAs discussed in http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/200705.mbox/%3C510143ac0705151453t7a0eb4cam859a40fb106e81f5@mail.gmail.com%3E and JCR-955, it would be useful to have a reusable set of servlet components for accessing and exposing repositories in various configurable ways.

My plan is to refactor the current RepositoryAccessServlet from jackrabbit-webapp and place the resulting servlet components in jackrabbit-jcr-commons, with servlet-api as a new optional (or provided) dependency."
1,"Wrong implementation of DocIdSetIterator.advance Implementations of {{DocIdSetIterator}} behave differently when advanced is called. Taking the following test for {{OpenBitSet}}, {{DocIdBitSet}} and {{SortedVIntList}} only {{SortedVIntList}} passes the test:
{code:title=org.apache.lucene.search.TestDocIdSet.java|borderStyle=solid}
...
	public void testAdvanceWithOpenBitSet() throws IOException {
		DocIdSet idSet = new OpenBitSet( new long[] { 1121 }, 1 );  // bits 0, 5, 6, 10
		assertAdvance( idSet );
	}

	public void testAdvanceDocIdBitSet() throws IOException {
		BitSet bitSet = new BitSet();
		bitSet.set( 0 );
		bitSet.set( 5 );
		bitSet.set( 6 );
		bitSet.set( 10 );
		DocIdSet idSet = new DocIdBitSet(bitSet);
		assertAdvance( idSet );
	}

	public void testAdvanceWithSortedVIntList() throws IOException {
		DocIdSet idSet = new SortedVIntList( 0, 5, 6, 10 );
		assertAdvance( idSet );
	}	

	private void assertAdvance(DocIdSet idSet) throws IOException {
		DocIdSetIterator iter = idSet.iterator();
		int docId = iter.nextDoc();
		assertEquals( ""First doc id should be 0"", 0, docId );

		docId = iter.nextDoc();
		assertEquals( ""Second doc id should be 5"", 5, docId );

		docId = iter.advance( 5 );
		assertEquals( ""Advancing iterator should return the next doc id"", 6, docId );
	}
{code}

The javadoc for {{advance}} says:
{quote}
Advances to the first *beyond* the current whose document number is greater than or equal to _target_.
{quote}
This seems to indicate that {{SortedVIntList}} behaves correctly, whereas the other two don't. 
Just looking at the {{DocIdBitSet}} implementation advance is implemented as:
{code}
bitSet.nextSetBit(target);
{code}
where the docs of {{nextSetBit}} say:
{quote}
Returns the index of the first bit that is set to true that occurs *on or after* the specified starting index
{quote}
"
0,Add parser callback to GQLThe parsing of GQL is currently hidden in the implementation. It would be nice to have a mechanism that allows client code to get callbacks whenever a field/value pair is parsed.
0,"provide a (relatively) simple way to disable anonymous access to the security workspaceAs discussed in this thread: http://sling.markmail.org/thread/st52jejjuxykfxtj, the security workspace is, by default, configured with an AccessControlProvider which provides a fixed access control policy (i.e. o.a.j.core.security.user.UserAccessControlProvider). In order to prevent anonymous access to security-related nodes requires the use of an alternate AccessControlProvider.

The attached patch provides a simpler mechanism. By adding

<param name=""anonymousAccessToSecurityWorkspace"" value=""false"" />

to the configuration of the DefaultSecurityManager, anonymous access to the security workspace is forbidden.
"
0,AbstractRepositoryService should be able to handle GuestCredentialsAbstractRepositoryService.createSessionInfo should handle GuestCredentials. Currently it only handle SimpleCredentials
0,IndexReader.listCommits should return a List and not an abstract CollectionSpinoff from here: http://www.mail-archive.com/dev@lucene.apache.org/msg07509.html
1,toString() causes StackOverflowErrorfurther regressions of JCR-2763...
0,"reorganize xdocs and websitei would like to propose a change to the website and xdocs structure.

the current structure below seems like it could use a brush up.

---
Overview
Architecture Doc
.Overview
..JSR-170 Levels
.Deployment Models
..Application HOWTO
..Model 1 HOWTO
..Model 2 HOWTO
..Model 3 HOWTO
.Core Operations
..Start-up, Initialize
..QueryManager Implementation
First Steps
JCR API Documentation
Layout
Downloads
FAQ
---

i would propose the following, instead:
---
About 
Documentation
.First Steps
.JCR
.API Documentation
.Jackrabbit Architecture
..Start-up, Initialize
..QueryManager Implementation
.Deployment Models
..Application HOWTO
..Model 1 HOWTO
..Model 2 HOWTO
..Model 3 HOWTO
.Nodetype Modelling *new*
Layout
Downloads
FAQ
---

also i would like to introduce a new ""homepage"" with a little bit more attractive content like jackrabbit
news, maybe featured jackrabbit applications, schedules and events.

ideally i would like to re-organize the file structure according to the navigation, which may break
bookmarks and search indexes.

thoughts?"
1,"Indexing configuration not refreshed after node type registrationThe indexing configuration has internal caches that speed up node type matches. Those caches are not updated on new node type registration and newly registered node types are not properly resolved when index-rules are checked.

See also test case in attached patch."
1,"Invalid behavior of StandardTokenizerImplThe following code prints the output of StandardAnalyzer:

        Analyzer analyzer = new StandardAnalyzer();
        TokenStream ts = analyzer.tokenStream(""content"", new StringReader(""<some text>""));
        Token t;
        while ((t = ts.next()) != null) {
            System.out.println(t);
        }

If you pass ""www.abc.com"", the output is (www.abc.com,0,11,type=<HOST>) (which is correct in my opinion).
However, if you pass ""www.abc.com."" (notice the extra '.' at the end), the output is (wwwabccom,0,12,type=<ACRONYM>).

I think the behavior in the second case is incorrect for several reasons:
1. It recognizes the string incorrectly (no argue on that).
2. It kind of prevents you from putting URLs at the end of a sentence, which is perfectly legal.
3. An ACRONYM, at least to the best of my understanding, is of the form A.B.C. and not ABC.DEF.

I looked at StandardTokenizerImpl.jflex and I think the problem comes from this definition:
// acronyms: U.S.A., I.B.M., etc.
// use a post-filter to remove dots
ACRONYM    =  {ALPHA} ""."" ({ALPHA} ""."")+

Notice how the comment relates to acronym as U.S.A., I.B.M. and not something else. I changed the definition to
ACRONYM    =  {LETTER} ""."" ({LETTER} ""."")+
and it solved the problem.

This was also reported here:
http://www.nabble.com/Inconsistent-StandardTokenizer-behaviour-tf596059.html#a1593383
http://www.nabble.com/Standard-Analyzer---Host-and-Acronym-tf3620533.html#a10109926
"
0,"TestExcetions never runIn one of the testcases of HttpClient, TestExcetions, it reads:

    // ------------------------------------------------------------------- Main
    public static void main(String args[]) {
        String[] testCaseName = { TestChallengeParser.class.getName() };
        junit.textui.TestRunner.main(testCaseName);
    }

    // ------------------------------------------------------- TestCase Methods

    public static Test suite() {
        return new TestSuite(TestChallengeParser.class);
    }

Where ""TestChallengeParser"" should be ""TestExcetions""."
1,"Static variables need to be final (or access should be synchronised):Static variables need to be final (or access should be synchronised):

Index: module-client/src/main/java/org/apache/http/conn/params/HttpConnectionManagerParams.java
===================================================================
--- module-client/src/main/java/org/apache/http/conn/params/HttpConnectionManagerParams.java	(revision 652021)
+++ module-client/src/main/java/org/apache/http/conn/params/HttpConnectionManagerParams.java	(working copy)
@@ -53,7 +53,7 @@
     public static final int DEFAULT_MAX_TOTAL_CONNECTIONS = 20;
 
     /** The default maximum number of connections allowed per host */
-    private static ConnPerRoute DEFAULT_CONN_PER_ROUTE = new ConnPerRoute() {
+    private static final ConnPerRoute DEFAULT_CONN_PER_ROUTE = new ConnPerRoute() {
         
         public int getMaxForRoute(HttpRoute route) {
             return ConnPerRouteBean.DEFAULT_MAX_CONNECTIONS_PER_ROUTE;
"
1,"IndexReader overwrites future commits when you open it on a past commitHit this on trying to build up a test index for perf testing...

IndexReader (and Writer) accept an IndexCommit on open.

This is quite powerful, because, if you use a deletion policy that keeps multiple commits around, you can open a not-current commit, make some changes, write a new commit, all without altering the ""future"" commits.

I use this to first build up a big wikipedia index, including one commit w/ multiple segments, then another commit after optimize(), and then I open an writable IR to perform deletions off of both those commits.  This gives me a single test index that has all four combinations (single vs multi segment; deletions vs no deletions).

But IndexReader has a bug whereby it overwrites the segments_N file.  (IndexWriter works correctly)."
0,"add db connection autoConnect for BundleDbPersistenceManager.Since bundled db pm doesn't inherited from database pm, it can't reconnect once database is bounced. it would be nice to add this feature. "
0,"Node type documentation tool (NTDoc)This is the first time I post a contrib here on Jira. Hope I do this the right way :-) 

Some weeks ago I postet a message on the forum about a node type documentation tool I had made. Now, finally I have cleaned up the code and fixed some bugs. It is now useful for the majority out there. I do not guarantee it to be bug-free, but will do my best to fix any bugs that is reported. 

A readme file is included in the distribution. Must build using maven 2. Have not done it maven 1 compliant."
0,Replace TrackingInpuStream with Commons IOThe TrackingInputStream class in jackrabbit-core implements essentially the same functionality as the Commons IO class CountingInputStream.
1,Background text extraction not possible when supportHighlighting is set trueThere is an IndexingQueue that holds nodes that are indexed with a background thread when text extraction takes more time than a configurable limit. When supportHighlighting is set to true the IndexingQueue is never used because the text extract is immediately requested in NodeIndexer. Instead the text extract should be retrieved only when the node is added to the index. 
1,"Typo in the DeltaVConstants class in constant XML_CHECKOUT_CHECKIN valueJust spotted a typo in the http://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-webdav/src/main/java/org/apache/jackrabbit/webdav/version/DeltaVConstants.java
(same is in released 1.4 version)

There's line
    public static final String XML_CHECKOUT_CHECKIN = ""checkin-checkout"";
Probably should be
    public static final String XML_CHECKOUT_CHECKIN = ""checkout-checkin"";
"
0,"Add a variable-sized int block codecWe already have support for fixed block size int block codecs, making it very simple to create a codec from a int encoder algorithms like FOR/PFOR.

But algorithms like Simple9/16 are not fixed -- they encode a variable number of adjacent ints at once, depending on the specific values of those ints."
0,"investigate solr test failures using flexWe have a branch of Solr located here: https://svn.apache.org/repos/asf/lucene/solr/branches/solr

Currently all the tests pass with lucene trunk jars.

I plopped in the flex jars and they do not, so I thought these might be interesting to look at.
"
0,"BaseTestRangeFilter can be extremely slowThe tests that extend BaseTestRangeFilter can sometimes be very slow:
TestFieldCacheRangeFilter, TestMultiTermConstantScore, TestTermRangeFilter

for example, TestFieldCacheRangeFilter just ran for 10 minutes on my computer before I killed it,
but i noticed these tests frequently run for over a minute.

I think at the least we should change these to junit4 so the index is built once in @beforeClass"
1,"Repository lock file is not removed on shutdownThe repository lock file is not removed when Jackrabbit runs on a windows platform:

*ERROR* [Thread-4] RepositoryImpl: Unable to release repository lock (RepositoryImpl.java, line 283)

I assume this problem does not occur on unix based platforms, because they allow to delete a file while another process still uses it."
1,"Duplicate key in DatabasePersistenceManagerHi,

I ran into the exception pasted below. We had 2 threads that both were saving. Maybe it is a race condition?  

Regards,

Martijn Hendriks
<GX> creative online development B.V.
 
t: 024 - 3888 261
f: 024 - 3888 621
e: martijnh@gx.nl
 
Wijchenseweg 111
6538 SW Nijmegen
http://www.gx.nl/ 


Jan 26, 2007 2:23:36 PM org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager store
SEVERE: failed to write property state: e3847bad-f1ee-4adb-a109-e134900935b7/{http://gx.nl}edit_language
ERROR 23505: The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique in dex identified by 'DEFAULT_PROP_IDX' defined on 'DEFAULT_PROP'.
        at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
        at org.apache.derby.impl.sql.execute.IndexChanger.insertAndCheckDups(Unknown Source)
        at org.apache.derby.impl.sql.execute.IndexChanger.doInsert(Unknown Source)
        at org.apache.derby.impl.sql.execute.IndexChanger.insert(Unknown Source)
        at org.apache.derby.impl.sql.execute.IndexSetChanger.insert(Unknown Source)
        at org.apache.derby.impl.sql.execute.RowChangerImpl.insertRow(Unknown Source)
        at org.apache.derby.impl.sql.execute.InsertResultSet.normalInsertCore(Unknown Source)
        at org.apache.derby.impl.sql.execute.InsertResultSet.open(Unknown Source)
        at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
        at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
        at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(Unknown Source)
        at org.apache.derby.impl.jdbc.EmbedPreparedStatement.execute(Unknown Source)
        at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.executeStmt(DatabasePersistenceManager.java:835)
        at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:466)
        at org.apache.jackrabbit.core.persistence.AbstractPersistenceManager.store(AbstractPersistenceManager.java:75)
        at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.store(DatabasePersistenceManager.java:274)
        at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:675)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:808)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
        at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:302)
        at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:295)
        at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1210)
"
1,"Unable to create repository using jackrabbit-webapp because a directory called ""jackrabbit"" already existsI mount the jackrabbit-webapp.war in a Jetty installation
* at startup i have the following exception:
ERROR RepositoryStartupServlet: Either create thejackrabbit/bootstrap.properties file or
ERROR RepositoryStartupServlet: use the '/config/index.jsp' for easy configuration.
ERROR RepositoryStartupServlet: RepositoryStartupServlet initializing failed: javax.servlet.ServletException: Repository startup configuration is not valid.
* then when i access http://localhost:8080/ i am forwarded to the page:
 http://localhost:8080/bootstrap/missing.jsp
* creating the repository by clicking on ""Create Content Repository"" button fails complaining that the jackrabbit directory already exists

Indeed, i find a jackrabbit directory in my JETTY_HOME (from where is started Jetty).

A workaround is to delete this ""jackrabbit"" directory and then i can create the repository by clicking on the previous button and therefore access the newly created repository."
0,"Minor typo in org.apache.commons.httpclient.Wire 2.0-rc1Minor typo ""...may noy be null"" in 
public static final void output(final String s) and
public static final void input(final String s)
of org.apache.commons.httpclient.Wire."
0,"jcr-commons: add cnd writer functionalitycurrently jcr-commons only provides an cnd-reader while the writer functionality is only present in spi-commons.
for JCR-2948 a implementation independent cnd-writer would be useful and i would therefore suggest to
add this to jcr-commons based on the code present in spi-commons and let the implementation in spi-commons
extend from the general functionality."
0,"The Field ctors that take byte[] shouldn't take Store, since it must be YESAPI silliness.  Makes you think you can set Store.NO for binary fields.  This used to be meaningful when we also accepted COMPRESS, but now it's an orphan."
0,"increase maxmemory for unit testsWe have some unit tests that require a fair amount of RAM.  But, sometimes the JRE does not give itself a very large max heap size, by default.  EG on a Mac Pro with 6 GB physical RAM, I see JRE 1.6.0 defaulting to max 80 GB and it always then hits this exception during testing:

    [junit] Testcase: testHugeFile(org.apache.lucene.store.TestHugeRamFile):	Caused an ERROR
    [junit] Java heap space
    [junit] java.lang.OutOfMemoryError: Java heap space
    [junit] 	at java.util.Arrays.copyOf(Arrays.java:2760)
    [junit] 	at java.util.Arrays.copyOf(Arrays.java:2734)
    [junit] 	at java.util.ArrayList.ensureCapacity(ArrayList.java:167)
    [junit] 	at java.util.ArrayList.add(ArrayList.java:351)
    [junit] 	at org.apache.lucene.store.RAMFile.addBuffer(RAMFile.java:69)
    [junit] 	at org.apache.lucene.store.RAMOutputStream.switchCurrentBuffer(RAMOutputStream.java:129)
    [junit] 	at org.apache.lucene.store.RAMOutputStream.writeBytes(RAMOutputStream.java:115)
    [junit] 	at org.apache.lucene.store.TestHugeRamFile.testHugeFile(TestHugeRamFile.java:68)

The fix is simple: add maxmemory=512M into common-build.xml.  I'll commit shortly."
1,"[PATCH] Error in GermanStemmer.java,v 1.11GermanStemmer.java,v 1.11 in  lucene-1.4-final
Ã at the end of a word is not replaced by ss"
0,dists include analyzer contrib in src dist but not binary distdists include analyzer contrib in src dist but not binary dist
0,"BlockJoinQuery/CollectorI created a single-pass Query + Collector to implement nested docs.
The approach is similar to LUCENE-2454, in that the app must index
documents in ""join order"", as a block (IW.add/updateDocuments), with
the parent doc at the end of the block, except that this impl is one
pass.

Once you join at indexing time, you can take any query that matches
child docs and join it up to the parent docID space, using
BlockJoinQuery.  You then use BlockJoinCollector, which sorts parent
docs by provided Sort, to gather results, grouped by parent; this
collector finds any BlockJoinQuerys (using Scorer.visitScorers) and
retains the child docs corresponding to each collected parent doc.

After searching is done, you retrieve the TopGroups from a provided
BlockJoinQuery.

Like LUCENE-2454, this is less general than the arbitrary joins in
Solr (SOLR-2272) or parent/child from ElasticSearch
(https://github.com/elasticsearch/elasticsearch/issues/553), since you
must do the join at indexing time as a doc block, but it should be
able to handle nested joins as well as joins to multiple tables,
though I don't yet have test cases for these.

I put this in a new Join module (modules/join); I think as we
refactor join impls we should put them here.
"
1,"SpellChecker file descriptor leak - no way to close the IndexSearcher used by SpellChecker internallyI can't find any way to close the IndexSearcher (and IndexReader) that
is being used by SpellChecker internally.

I've worked around this issue by keeping a single SpellChecker open
for each index, but I'd really like to be able to close it and
reopen it on demand without leaking file descriptors.

Could we add a close() method to SpellChecker that will close the
IndexSearcher and null the reference to it? And perhaps add some code
that reopens the searcher if the reference to it is null? Or would
that break thread safety of SpellChecker?

The attached patch adds a close method but leaves it to the user to
call setSpellIndex to reopen the searcher if desired."
0,"Implement RepositoryFactory in jcr2davIt's currently a bit cumbersome to set up a spi2dav instance because of the two levels of factories (RepositoryFactory & RepositoryServiceFactory) involved in the process. It would be easier if spi2dav implemented RepositoryFactory directly, so downstream users would only need to provide the server URI parameter instead of specifying also the RepositoryServiceFactory classname.

To do this, spi2dav would need to depend also on jcr2spi. This change would actually simplify downstream projects, that then wouldn't need to depend also to jcr2spi to get JCR -> DAV connectivity."
0,"URI class constructors need revision, optimization1. Currently there's not way to pass an escaped string as a parameter to URI
class. As a result the url parameter in HttpMethodBase#HttpMethodBase(String)
constructor gets converted into an array of char just to be converted back to
string in URI contructor called in that method. 

2. The overall design of URI class contructors does not appear very coherent (at
least to me)"
0,"Changes.html generation improvementsBug fixes for and improvements to changes2html.pl, which generates Changes.html from CHANGES.txt:

# When the current location has a fragment identifier, expand parent sections, so that the linked-to section is visible.
# Properly handle beginning-of-release comments that don't fall under a section heading (previously: some content in release ""1.9 final"" was invisible).
# Auto-linkify SOLR-XXX and INFRA-XXX JIRA issues (previously: only LUCENE-XXX issues).
# Auto-linkify Bugzilla bugs prefaced with ""Issue"" (previously: only ""Bug"" and ""Patch"").
# Auto-linkify Bugzilla bugs in the form ""bugs XXXXX and YYYYY"".
# Auto-linkify issues that follow attributions.
"
0,"XML text extraction in Jackrabbit 1.x accesses external resourcesAs discussed on users@, we should add the following code to the ExtractorHandler class:

   public InputSource resolveEntity(String publicId, String systemId) {
       return new InputSource(new ByteArrayInputStream(new byte[0]));
   }
"
1,"Intermittent thread safety issue with EnwikiDocMakerIntermittent thread safety issue with EnwikiDocMaker

When I run the conf/wikipediaOneRound.alg, sometimes it gets started
OK, other times (about 1/3rd the time) I see this:

     Exception in thread ""Thread-0"" java.lang.RuntimeException: java.io.IOException: Bad file descriptor
     	at org.apache.lucene.benchmark.byTask.feeds.EnwikiDocMaker$Parser.run(EnwikiDocMaker.java:76)
     	at java.lang.Thread.run(Thread.java:595)
     Caused by: java.io.IOException: Bad file descriptor
     	at java.io.FileInputStream.readBytes(Native Method)
     	at java.io.FileInputStream.read(FileInputStream.java:194)
     	at org.apache.xerces.impl.XMLEntityManager$RewindableInputStream.read(Unknown Source)
     	at org.apache.xerces.impl.io.UTF8Reader.read(Unknown Source)
     	at org.apache.xerces.impl.XMLEntityScanner.load(Unknown Source)
     	at org.apache.xerces.impl.XMLEntityScanner.scanQName(Unknown Source)
     	at org.apache.xerces.impl.XMLNSDocumentScannerImpl.scanStartElement(Unknown Source)
     	at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl$FragmentContentDispatcher.dispatch(Unknown Source)
     	at org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source)
     	at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
     	at org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)
     	at org.apache.xerces.parsers.XMLParser.parse(Unknown Source)
     	at org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)
     	at org.apache.lucene.benchmark.byTask.feeds.EnwikiDocMaker$Parser.run(EnwikiDocMaker.java:60)
     	... 1 more

The problem is that the thread that pulls the XML docs is started as
soon as EnwikiDocMaker class is instantiated.  When it's started, it
uses the fileIS (FileInputStream) to feed the XML Parser.  But,
openFile is actually called twice on starting the alg, if you use any
task deriving from ResetInputsTask, which closes the original fileIS
that the XML parser may be using.

I changed the thread to instead start on-demand the first time next()
is called.  I also removed a redundant resetInputs() call (which was
opening the file more frequently than needed).  Finally, I added logic
in the thread to detect that the input stream was closed (because
LineDocMaker.resetInputs() was called, eg, if we are not running the
doc maker to exhaustion).

"
0,"Some improvements to contrib/benchmarkI've made some small improvements to the contrib/benchmark, mostly
merging in the ad-hoc benchmarking code I've been using in LUCENE-843:

  - Fixed thread safety of DirDocMaker's usage of SimpleDateFormat

  - Print the props in sorted order

  - Added new config ""autocommit=true|false"" to CreateIndexTask

  - Added new config ""ram.flush.mb=int"" to AddDocTask

  - Added new configs ""doc.term.vector.positions=true|false"" and
    ""doc.term.vector.offsets=true|false"" to BasicDocMaker

  - Added WriteLineDocTask.java, so you can make an alg that uses this
    to build up a single file containing one document per line in a
    single file.  EG this alg converts the reuters-out tree into a
    single file that has ~1000 bytes per body field, saved to
    work/reuters.1000.txt:

      docs.dir=reuters-out
      doc.maker=org.apache.lucene.benchmark.byTask.feeds.DirDocMaker
      line.file.out=work/reuters.1000.txt
      doc.maker.forever=false
      {WriteLineDoc(1000)}: *

    Each line has tab-separted TITLE, DATE, BODY fields.

  - Created feeds/LineDocMaker.java that creates documents read from
    the file created by WriteLineDocTask.java.  EG this alg indexes
    all documents created above:

      analyzer=org.apache.lucene.analysis.SimpleAnalyzer
      directory=FSDirectory
      doc.add.log.step=500

      docs.file=work/reuters.1000.txt
      doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker
      doc.tokenized=true
      doc.maker.forever=false

      ResetSystemErase
      CreateIndex
      {AddDoc}: *
      CloseIndex

      RepSumByPref AddDoc

I'll attach initial patch shortly.
"
0,"Upgrade to Tika 0.6 and PDFBox 1.0.0Tika version 0.6 uses POI 3.6 that's notably smaller (-10MB!) than previous versions. There are also a number of other improvements in Tika 0.6 since the 0.5 release.

While doing the upgrade we should also force the PDFBox version to 1.0.0 from the 0.8.0-incubating version that Tika 0.6 uses. PDFBox 1.0.0 has some nice performance gains (around 30% faster) to text extraction along with other improvements."
0,"Remove unused (and untested) methods from ReaderUtil that are also veeeeery ineffectiveReaderUtil contains two methods that are nowhere used and not even tested. Additionally those are implemented with useless List->array copying; ineffective docStart calculation for a binary search later instead directly returning the reader while scanning -- and I am not sure if they really work as expected. As ReaderUtil is @lucene.internal we should remove them in 3.x and trunk, alternatively the useless array copy / docStarts handling should be removed and tests added:

{code:java}
public static IndexReader subReader(int doc, IndexReader reader)
public static IndexReader subReader(IndexReader reader, int subIndex)
{code}
"
0,"[PATCH] Allow RepositoryAccessServlet to get the Repository from a ServletContext attributeThe attached patch adds a repository.context.attribute.name init parameter to the RepositoryAccessServlet:

        <init-param>
          <param-name>repository.context.attribute.name</param-name>
          <param-value>javax.jcr.Repository</param-value>
          <description>
            If this is set, the RepositoryAccessServlet expects a Repository in the ServletContext 
            attribute having this name. This allows servlets of this module to be used with repositories
            intialized by the jackrabbit-jcr-servlet module utilities.
          </description>
        </init-param>"
0,Add customizable filtering to GQLCurrently GQL is not very flexible because it does not have any hooks that  allows you to modify the query that gets generated from the GQL syntax. As a first step I'd like to introduce a filtering mechanism that can be used to post process the result set and exclude certain rows. This is useful when you cannot express an application constraint in GQL.
0,"outdated information in Analyzer javadocI'm sure you find more ways to improve the javadoc, so feel free to change and extend my patch."
1,"CheckIndex incorrectly sees deletes as index corruptionThere is a silly bug in CheckIndex whereby any segment with deletes is
considered corrupt.

Thanks to Bogdan Ghidireac for reporting this."
1,"TestFSTs.testRandomWords throws AIOBE when ""verbose""=trueSeems like invalid utf-8 sometimes gets passed to Bytesref.utf8ToString() in the verbose ""println""s."
1,ArrayIndexOutOfBoundsException during indexinghttp://search.lucidimagination.com/search/document/f29fc52348ab9b63/arrayindexoutofboundsexception_during_indexing
1,"Persistence data of versioning not cleaned up correctlywhen deleting a version or removing its label, the respective persistence data is not always properly removed."
1,"ClassCastException when updating properties using WebDAVWhen issuing PROPPATCH commands, a ClassCastException is raised.

e.g. 

PROPPATCH /jackrabbit-webapp-1.4/repository/default/test/test_file_v.txt HTTP/1.1
Host: localhost:9000
Connection: TE
TE: trailers, deflate, gzip, compress
User-Agent: UCI DAV Explorer/0.91 RPT-HTTPClient/0.3-3E
Translate: f
Authorization: Basic Y3Jvc3NqYTp0ZXN0
Accept-Encoding: deflate, gzip, x-gzip, compress, x-compress
Content-type: text/xml
Content-length: 170

<A:propertyupdate xmlns:A=""DAV:"">
<A:set>
<A:prop>
<A:auto-version>checkout-checkin</A:auto-version>
</A:prop>
</A:set>
</A:propertyupdate>


results in



24.01.2008 15:38:34 *ERROR* [Webdav]: Servlet.service() for servlet Webdav threw
 exception (StandardWrapperValve.java, line 257)
java.lang.ClassCastException: org.apache.jackrabbit.webdav.property.DefaultDavPr
operty
        at org.apache.jackrabbit.webdav.simple.DavResourceImpl.alterProperties(D
avResourceImpl.java:456)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doPropPatch
(AbstractWebdavServlet.java:457)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(Abs
tractWebdavServlet.java:234)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(Abs
tractWebdavServlet.java:192)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:803)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(Appl
icationFilterChain.java:269)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationF
ilterChain.java:188)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperV
alve.java:210)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextV
alve.java:174)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.j
ava:127)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.j
ava:117)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineVal
ve.java:108)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.jav
a:151)
        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java
:870)
        at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.p
rocessConnection(Http11BaseProtocol.java:665)
        at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpo
int.java:528)
        at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFol
lowerWorkerThread.java:81)
        at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadP
ool.java:685)
        at java.lang.Thread.run(Thread.java:595)
24.01.2008 15:38:34 *ERROR* [Webdav]: Servlet.service() for servlet Webdav threw
 exception (SLF4JLocationAwareLog.java, line 174)
java.lang.ClassCastException: org.apache.jackrabbit.webdav.property.DefaultDavPr
operty
        at org.apache.jackrabbit.webdav.simple.DavResourceImpl.alterProperties(D
avResourceImpl.java:456)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doPropPatch
(AbstractWebdavServlet.java:457)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(Abs
tractWebdavServlet.java:234)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(Abs
tractWebdavServlet.java:192)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:803)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(Appl
icationFilterChain.java:269)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationF
ilterChain.java:188)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperV
alve.java:210)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextV
alve.java:174)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.j
ava:127)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.j
ava:117)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineVal
ve.java:108)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.jav
a:151)
        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java
:870)
        at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.p
rocessConnection(Http11BaseProtocol.java:665)
        at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpo
int.java:528)
        at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFol
lowerWorkerThread.java:81)
        at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadP
ool.java:685)
        at java.lang.Thread.run(Thread.java:595)
24.01.2008 15:53:54 *ERROR* [Webdav]: Servlet.service() for servlet Webdav threw
 exception (StandardWrapperValve.java, line 257)
java.lang.ClassCastException: org.apache.jackrabbit.webdav.property.DefaultDavPr
operty
        at org.apache.jackrabbit.webdav.simple.DavResourceImpl.alterProperties(D
avResourceImpl.java:456)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doPropPatch
(AbstractWebdavServlet.java:457)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(Abs
tractWebdavServlet.java:234)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(Abs
tractWebdavServlet.java:192)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:803)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(Appl
icationFilterChain.java:269)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationF
ilterChain.java:188)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperV
alve.java:210)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextV
alve.java:174)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.j
ava:127)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.j
ava:117)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineVal
ve.java:108)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.jav
a:151)
        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java
:870)
        at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.p
rocessConnection(Http11BaseProtocol.java:665)
        at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpo
int.java:528)
        at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFol
lowerWorkerThread.java:81)
        at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadP
ool.java:685)
        at java.lang.Thread.run(Thread.java:595)
24.01.2008 15:53:54 *ERROR* [Webdav]: Servlet.service() for servlet Webdav threw
 exception (SLF4JLocationAwareLog.java, line 174)
java.lang.ClassCastException: org.apache.jackrabbit.webdav.property.DefaultDavPr
operty
        at org.apache.jackrabbit.webdav.simple.DavResourceImpl.alterProperties(D
avResourceImpl.java:456)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doPropPatch
(AbstractWebdavServlet.java:457)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(Abs
tractWebdavServlet.java:234)
        at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(Abs
tractWebdavServlet.java:192)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:803)
        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(Appl
icationFilterChain.java:269)
        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationF
ilterChain.java:188)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperV
alve.java:210)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextV
alve.java:174)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.j
ava:127)
        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.j
ava:117)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineVal
ve.java:108)
        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.jav
a:151)
        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java
:870)
        at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.p
rocessConnection(Http11BaseProtocol.java:665)
        at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpo
int.java:528)
        at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFol
lowerWorkerThread.java:81)
        at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadP
ool.java:685)
        at java.lang.Thread.run(Thread.java:595) "
0,"Improve how ConcurrentMergeScheduler handles too-many-merges caseCMS now lets you set ""maxMergeThreads"" to control max # simultaneous
merges.

However, when CMS hits that max, it still allows further merges to
run, by running them in the foreground thread.  So if you set this max
to 1, and use 1 thread to add docs, you can get 2 merges running at
once (which I think is broken).

I think, instead, CMS should pause the foreground thread, waiting
until the number of merge threads drops below the limit.  Then, kick
off the backlog merge in a thread and return control back to primary
thread.
"
0,Publish the jackrabbit-ocm DTDThe jackrabbit-ocm DTD from jackrabbit-ocm/src/dtd should be made available for reference on the Jackrabbit web site.
1,webapp doesn't compile (use of enum keyword)AbstractConfig.java and JNDIConfig.java have local variables named 'enum' that aren't allowed when using JDK5 or later compilers.
0,"HttpClient OSGi Export-Package doesn't specify versionThe ""Export-Package"" manifest entry doesn't specify the version of the package being exported.  This means that packages importing it can't specify a version to import."
0,"waitForResponse is using busy waitIn HttpConnection, the method waitForResponse is using busywait, instead of 
blocking until the response is arriving.

Is this on purpose, or shouldn't it handle this by blocking instead ??"
1,"jcr2spi: Unprocessed ItemInfos call to RepositoryService#getItemInfosstefan reported the following problem:

- batchread config reads with depths infinity
- invalidate tree by calling Node.refresh(false)
- force loading of the tree (e.g. Node.getPath())

afterwards, there may still be invalidated item states indicating that not all ItemInfos were processed.
consequently, there are additional calls to getItemInfos that should have been covered by the loading of the tree.
the problem occuring is not related to limitation of the item-cache size.

problem analysis:

there is a bug in WorkspaceItemStateFactory#createItemStates.
there is a wrapper built around the ItemInfo-Iterator but later on the ItemInfo-Iterator is used instead of the wrapper, which pre-fetches items from the underlying iterator and process them upon hasNext()/next()."
0,"SPI: Get rid of unused method ItemInfo.getParentId()Looking at the various SPI impls in the trunk and in the sandbox reveals that ItemInfo.getParentId is not used at all.
I'd like to suggest to get rid of that method.

Any objections/concerns?
angela

"
0,"Improve reading of cached UUID for given document numberCachingIndexReader.document(int n, FieldSelector fieldSelector) creates a new
Field from the cached UUID. The lucene Field implementation always does a
String.intern() on the field name, which is quite slow. We should probably have
our own implementation for that specific use case where we know that the name
is already interned. e.g. UUIDField implements Fieldable."
0,"Parallelize TestsThe Lucene tests can be parallelized to make for a faster testing system.  

This task from ANT can be used: http://ant.apache.org/manual/CoreTasks/parallel.html

Previous discussion: http://www.gossamer-threads.com/lists/lucene/java-dev/69669

Notes from Mike M.:
{quote}
I'd love to see a clean solution here (the tests are embarrassingly
parallelizable, and we all have machines with good concurrency these
days)... I have a rather hacked up solution now, that uses
""-Dtestpackage=XXX"" to split the tests up.

Ideally I would be able to say ""use N threads"" and it'd do the right
thing... like the -j flag to make.
{quote}"
0,"review TSCCM for spurious wakeupsReview the code of the TSCCM/ConnPoolByRoute for places where spurious wakeups may happen.
Verify that this case is dealt with correctly. Unit test by giving invalid wakeup signals?"
1,"Creating and saving a mix:versionable node creates two VersionHistory nodesSteps:
   - Create a new mix:versionable node
      [ This creates a new VersionHistory node below jcr:persistentVersionStore
        and sets the new node's versionHistory property to the UUID of this
        VersionHistory node. ]
   - Save the session (or alternatively save the parent of the new node)
      [ This creates a new VersionHistory node below jcr:persistentVersionStore
        and sets the node's versionHistory property to the UUID of this
        VersionHistory node. ]

As you can see, you end up with two VersionHistory nodes for the same node, of which the first VersionHistory node is never used again, because the second VersionHistory node is used from now on."
0,Improve FileRevision extensibilityIt'd be nice to make FileRevision more extensible by chaning some of its private variables to protected so it can be extended easier when needed.
1,"CJKTokenizer convert   HALFWIDTH_AND_FULLWIDTH_FORMS wrongCJKTokenizer have these lines..
                if (ub == Character.UnicodeBlock.HALFWIDTH_AND_FULLWIDTH_FORMS) {
                    /** convert  HALFWIDTH_AND_FULLWIDTH_FORMS to BASIC_LATIN */
                    int i = (int) c;
                    i = i - 65248;
                    c = (char) i;
                }

This is wrong. Some character in the block (e.g. U+ff68) have no BASIC_LATIN counterparts.
Only 65281-65374 can be converted this way.

The fix is

             if (ub == Character.UnicodeBlock.HALFWIDTH_AND_FULLWIDTH_FORMS && i <= 65474 && i> 65281) {
                    /** convert  HALFWIDTH_AND_FULLWIDTH_FORMS to BASIC_LATIN */
                    int i = (int) c;
                    i = i - 65248;
                    c = (char) i;
                }"
1,"UserManager: concurrent user creation using same intermediate path failsconcurrently creating users using same intermediate path fails with ""node ... has been modified externally"".

the problem is the intermediate path. if it doesn't exist multiple threads try to create it concurrently: 

o.a.jackrabbit.core.security.user.UserManagerImpl, line 1310ff:


            String[] segmts = defaultPath.split(""/"");
            NodeImpl folder = (NodeImpl) session.getRootNode();
            String authRoot = (isGroup) ? groupsPath : usersPath;

            for (String segment : segmts) {
                if (segment.length() < 1) {
                    continue;
                }
                if (folder.hasNode(segment)) {
                    folder = (NodeImpl) folder.getNode(segment);
                    if (Text.isDescendantOrEqual(authRoot, folder.getPath()) &&
                            !folder.isNodeType(NT_REP_AUTHORIZABLE_FOLDER)) {
                        throw new ConstraintViolationException(""Invalid intermediate path. Must be of type rep:AuthorizableFolder."");
                    }
                } else {
                    Node parent = folder;
                    folder = addNode(folder, session.getQName(segment), NT_REP_AUTHORIZABLE_FOLDER);
                }
            }

the attached test case illustrates this issue/"
0,"Add WaitForMergesTaskWhen building an index, if you just .close the IW, you may leave merges still needing to be done... so a WaitForMerges task lets algs fix this."
0,"Set source and output encoding in POMsModification to POM files to explicitly set the source and report encoding. I've set everything to UTF-8, but this may not be appropriate. However, the encoding properties should be set to ensure that source files are compiled correctly, resources are filtered appropriately and that the reports are using a consistent encoding.

Related info
http://docs.codehaus.org/display/MAVENUSER/POM+Element+for+Source+File+Encoding
http://docs.codehaus.org/display/MAVEN/Reporting+Encoding+Configuration"
0,Bundle cache is not cleared when *BundlePersistenceManager is closedClose method of persistence managers is responsible for releasing all acquired resources. In case of BundlePersistenceManager it should also free memory by clearing the bundle cache.
0,"Setting CONNECTION_TIMEOUT and SO_TIMEOUT on a per-method basisThe capability of setting connection timeout and socket timeout on a per-method
basis should be provided. This would enable different threads, sharing the same
HttpClient, to set different timeouts for their methods executions."
0,tck doesn't compile (use of enum keyword)Use if enum keyword in TestFinder (patch will be attached)
1,"Workspace.clone throws ItemNotFoundException on a referenceable node with childrenAn ItemNotFoundException is thrown when a referenceable node with children is cloned, this happens after the first time the node is cloned.
            
Example:

            Node root = session.getRootNode();   
            Node parent = root.addNode(""parent"");
            parent.addMixin(""mix:referenceable"");
            session.save();
            
// clone parent
            WS2.clone(""default"", ""/parent"", ""/parent"", true);
            
            Node child = parent.addNode(""child"");
// add child
            child.addMixin(""mix:referenceable"");
            session.save();

// clone parent with child            
            WS2.clone(""default"", ""/parent"", ""/parent"", true); 

// clone parent again,   ItemNotFoundException - from now on can't clone parent node.
            WS2.clone(""default"", ""/parent"", ""/parent"", true);


Stacktrace:
javax.jcr.ItemNotFoundException: failed to build path of 229083e5-5f24-4102-b007-785f43be983a: cafebabe-cafe-babe-cafe-babecafebabe has no child entry for 229083e5-5f24-4102-b007-785f43be983a
	at org.apache.jackrabbit.core.HierarchyManagerImpl.buildPath(HierarchyManagerImpl.java:308)
	at org.apache.jackrabbit.core.CachingHierarchyManager.buildPath(CachingHierarchyManager.java:159)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:357)
	at org.apache.jackrabbit.core.CachingHierarchyManager.getPath(CachingHierarchyManager.java:221)
	at org.apache.jackrabbit.core.BatchedItemOperations.checkRemoveNode(BatchedItemOperations.java:700)
	at org.apache.jackrabbit.core.BatchedItemOperations.recursiveRemoveNodeState(BatchedItemOperations.java:1514)
	at org.apache.jackrabbit.core.BatchedItemOperations.removeNodeState(BatchedItemOperations.java:1216)
	at org.apache.jackrabbit.core.BatchedItemOperations.copyNodeState(BatchedItemOperations.java:1642)
	at org.apache.jackrabbit.core.BatchedItemOperations.copy(BatchedItemOperations.java:311)
	at org.apache.jackrabbit.core.WorkspaceImpl.internalCopy(WorkspaceImpl.java:294)
	at org.apache.jackrabbit.core.WorkspaceImpl.clone(WorkspaceImpl.java:401)
	at test.CloneTest.main(CloneTest.java:64)

            "
0,Remove (deprecated) ExtendedFieldCache and Auto/Custom caches and lot's of deprecated sort logicRemove (deprecated) ExtendedFieldCache and Auto/Custom caches and sort
1,"Select * does not return declared properties of node type in FROM clauseThe query only returns the default columns: jcr:primaryType, jcr:score and jcr:path"
0,"the demo application does not work as of 3.0the demo application does not work. QueryParser needs a Version argument.

While I am here, remove @author too"
0,"Better name and path factory exception messagesI've ran across a few cases where the name and path factories throw an exception about an invalid path or name, but fail to include the actual path or name in the exception message. It would be very helpful to have that extra bit of information included."
1,"NPE in OpenOfficeTextExtractorI try to load some Open Office Writer document (see attachment) and receive such exception. 

2008-06-10 17:19:59 <WARN > [btpool0-1] CompositeTextExtractor: Failed to extract text content(92)
java.lang.NullPointerException
    at org.apache.jackrabbit.extractor.OpenOfficeTextExtractor.extractText(OpenOfficeTextExtractor.java:7
8)
    at org.apache.jackrabbit.extractor.CompositeTextExtractor.extractText(CompositeTextExtractor.java:90)
    at org.apache.jackrabbit.core.query.lucene.JackrabbitTextExtractor.extractText(JackrabbitTextExtracto
r.java:195)
    at org.apache.jackrabbit.core.query.lucene.NodeIndexer.addBinaryValue(NodeIndexer.java:393)
    at org.apache.jackrabbit.core.query.lucene.NodeIndexer.addValue(NodeIndexer.java:282)
    at org.apache.jackrabbit.core.query.lucene.NodeIndexer.createDoc(NodeIndexer.java:221)
    at org.apache.jackrabbit.core.query.lucene.SearchIndex.createDocument(SearchIndex.java:892)
    at org.apache.jackrabbit.core.query.lucene.SearchIndex$2.next(SearchIndex.java:543)
    at org.apache.jackrabbit.core.query.lucene.MultiIndex.update(MultiIndex.java:428)
    at org.apache.jackrabbit.core.query.lucene.SearchIndex.updateNodes(SearchIndex.java:527)
    at org.apache.jackrabbit.core.SearchManager.onEvent(SearchManager.java:504)
    at org.apache.jackrabbit.core.observation.EventConsumer.consumeEvents(EventConsumer.java:231)
    at org.apache.jackrabbit.core.observation.ObservationDispatcher.dispatchEvents(ObservationDispatcher.
java:201)
    at org.apache.jackrabbit.core.observation.EventStateCollection.dispatch(EventStateCollection.java:425
)
    at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:737
)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:873)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:334)
    at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:337)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:310)
    at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:317)
    at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1247)
    at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:897)
    at org.apache.jackrabbit.jca.JCASessionHandle.save(JCASessionHandle.java:178)"
0,"DocValues cleanup: constructor & getInnerArray()DocValues constructor taking a numDocs parameter is not very clean.
Get rid of this.

Also, it's optional getInnerArray() method is not very clean.
This is necessary for better testing, but currently tests will fail if it is not implemented.
Modify it to throw UnSupportedOp exception (rather than returning an empty array).
Modify tests to not fail but just warn if the tested iml does not override it.

These changes should make it easier to implement DocValues for other ValueSource's, e.g. above payloads, with or without caching.
"
0,"Add Payload retrieval to SpansIt will be nice to have access to payloads when doing SpanQuerys.

See http://www.gossamer-threads.com/lists/lucene/java-dev/52270 and http://www.gossamer-threads.com/lists/lucene/java-dev/51134

Current API, added to Spans.java is below.  I will try to post a patch as soon as I can figure out how to make it work for unordered spans (I believe I have all the other cases working).

{noformat}
 /**
   * Returns the payload data for the current span.
   * This is invalid until {@link #next()} is called for
   * the first time.
   * This method must not be called more than once after each call
   * of {@link #next()}. However, payloads are loaded lazily,
   * so if the payload data for the current position is not needed,
   * this method may not be called at all for performance reasons.<br>
   * <br>
   * <p><font color=""#FF0000"">
   * WARNING: The status of the <b>Payloads</b> feature is experimental.
   * The APIs introduced here might change in the future and will not be
   * supported anymore in such a case.</font>
   *
   * @return a List of byte arrays containing the data of this payload
   * @throws IOException
   */
  // TODO: Remove warning after API has been finalized
  List/*<byte[]>*/ getPayload() throws IOException;

  /**
   * Checks if a payload can be loaded at this position.
   * <p/>
   * Payloads can only be loaded once per call to
   * {@link #next()}.
   * <p/>
   * <p><font color=""#FF0000"">
   * WARNING: The status of the <b>Payloads</b> feature is experimental.
   * The APIs introduced here might change in the future and will not be
   * supported anymore in such a case.</font>
   *
   * @return true if there is a payload available at this position that can be loaded
   */
  // TODO: Remove warning after API has been finalized
  public boolean isPayloadAvailable();
{noformat}"
0,"AccessControlManager#getApplicablePolicy should check for colliding rep:policy nodewhile AccessControlManager#getApplicablePolicy returns an empty iterator if the target node cannot get the accesscontrollable-mixin set, it does not test if there is a colliding child node that would prevent the policy to be applied calling AccessControlManager#setPolicy. consequently, the setPolicy call fails with ItemExistsException. A simple test upfront could prevent this unexpected failure."
1,CheckIndex overstates how many fields have norms enabledIt simply tells you how many unique fields there are... it should instead only say how many have norms.
0,"HostConfiguration handling requires cleanupAs discussed on the mailing list, the host configuration handling currently
appears faulty:

http://marc.theaimsgroup.com/?t=109644952000001&r=1&w=2

Oleg"
0,"Demo and contrib jars should contain NOTICE.TXT and LICENSE.TXTWe should include NOTICE.TXT and LICENSE.TXT not only in the core jar but also
in the demo and contrib jars."
1,CustomScoreQuery calls weight() where it should call createWeight()Thanks to Uwe for helping me track down this bug after I pulled my hair out for hours on LUCENE-3174.
0,"Backport FilteredQuery/IndexSearcher changes to 3.x: Remove filter logic from IndexSearcher and delegate to FilteredQuerySpinoff from LUCENE-1536: We simplified the code in IndexSearcher to no longer do the filtering there, instead wrap all Query with FilteredQuery, if a non-null filter is given. The conjunction code would then only exist in FilteredQuery which makes it easier to maintain. Currently both implementations differ in 3.x, in trunk we used the more optimized IndexSearcher variant with addition of a simplified in-order conjunction code.

This issue will backport those changes (without random access bits)."
0,"[PATCH] Remove Stutter in NodeStateCode duplicates code for no reason

Index: src/main/java/org/apache/jackrabbit/jcr2spi/state/NodeState.java
===================================================================
--- src/main/java/org/apache/jackrabbit/jcr2spi/state/NodeState.java	(revision 740824)
+++ src/main/java/org/apache/jackrabbit/jcr2spi/state/NodeState.java	(working copy)
@@ -449,7 +449,7 @@
              */
             NodeState parent = getParent();
             NodeId wspId = (NodeId) getWorkspaceId();
-            def = definitionProvider.getQNodeDefinition(getParent().getNodeTypeNames(), getName(), getNodeTypeName(), wspId);
+            def = definitionProvider.getQNodeDefinition(parent.getNodeTypeNames(), getName(), getNodeTypeName(), wspId);
         }
         return def;
     }
"
1,"BitVector.isSparse is sometimes wrongIn working on LUCENE-3246, I found a few problems with
BitVector.isSparse:

  * Its math can overflow int, such that if there are enough deleted
    docs and maxDoc() is largish, isSparse may incorrectly return true

  * It over-estimates the size of the sparse file, since when
    estimating number of bytes for the vInt dgaps it uses bits.length
    instead of bits.length divided by number of set bits (ie, the
    ""average"" gap between set bits)

This is relatively harmless (just affects performance / size of .del
file on disk, not correctness).
"
0,"Calls to SegmentInfos.message should be wrapped w/ infoStream != null checksTo avoid the expensive message creation (which involves the '+' operator on strings, calls to message should be wrapped w/ infoStream != null check, rather than inside message(). I'll attach a patch which does that."
0,"move o.a.l.index.codecs.* -> o.a.l.codecs.*These package names are getting pretty long, e.g.:

org.apache.lucene.index.codecs.lucene40.values.XXXXYYYY

I think we should move it to just the codecs package now while it won't cause anyone any trouble."
0,"SegmentInfos shouldn't blindly increment version on commitSegmentInfos currently increments version on the assumption that there are always changes.

But, both DirReader and IW are more careful about tracking whether there are changes.  DirReader has hasChanges and IW has changeCount.  I think these classes should notify the SIS when there are in fact changes; this will fix the case Simon hit on fixing LUCENE-2082 when the NRT reader thought there were changes, but in fact there weren't because IW simply committed the exact SIS it already had.
"
0,"Build fails on system without XThe failing test is: testFileContains(org.apache.jackrabbit.core.query.FulltextQueryTest)

caused by:

java.lang.InternalError: Can't connect to X11 window server using ':0.0' as the value of the DISPLAY variable.
	at sun.awt.X11GraphicsEnvironment.initDisplay(Native Method)
	at sun.awt.X11GraphicsEnvironment.access$000(X11GraphicsEnvironment.java:53)
	at sun.awt.X11GraphicsEnvironment$1.run(X11GraphicsEnvironment.java:142)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.awt.X11GraphicsEnvironment.<clinit>(X11GraphicsEnvironment.java:131)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:164)
	at java.awt.GraphicsEnvironment.getLocalGraphicsEnvironment(GraphicsEnvironment.java:68)
	at sun.awt.X11.XToolkit.<clinit>(XToolkit.java:96)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:164)
	at java.awt.Toolkit$2.run(Toolkit.java:821)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.awt.Toolkit.getDefaultToolkit(Toolkit.java:804)
	at java.awt.Toolkit.getEventQueue(Toolkit.java:1592)
	at java.awt.EventQueue.isDispatchThread(EventQueue.java:666)
	at javax.swing.SwingUtilities.isEventDispatchThread(SwingUtilities.java:1270)
	at javax.swing.text.StyleContext.reclaim(StyleContext.java:437)
	at javax.swing.text.StyleContext.addAttribute(StyleContext.java:294)
	at javax.swing.text.StyleContext$NamedStyle.addAttribute(StyleContext.java:1486)
	at javax.swing.text.StyleContext$NamedStyle.setName(StyleContext.java:1296)
	at javax.swing.text.StyleContext$NamedStyle.<init>(StyleContext.java:1244)
	at javax.swing.text.StyleContext.addStyle(StyleContext.java:90)
	at javax.swing.text.StyleContext.<init>(StyleContext.java:70)
	at javax.swing.text.DefaultStyledDocument.<init>(DefaultStyledDocument.java:88)
	at org.apache.tika.parser.rtf.RTFParser.parse(RTFParser.java:42)
"
0,Review test cases and cross check with 1.0 specificationThis jira task is meant to collect issues with the TCK test cases.
1,"GData-server storage fix activation depthFixed nullpointer exception while rendering feeds with big amount of extensions. DB4O context.

"
1,"Analysis back compat breakOld and new style token streams don't mix well.
"
0,"Lucene requires ant 1.6?The latest version in CVS as of April 3rd only builds with ant 1.6.   If this is intentional, BUILD.txt should 
be updated.

Here's the error I get with ant 1.5:

BUILD FAILED
file:/Users/skybrian/remote-cvs/jakarta-lucene/build.xml:11: Unexpected element ""tstamp"""
0,Javadoc mistake in SegmentMerger
1,"3.x indexes have the wrong normType set in fieldinfos3.x codec claims the single byte norms are BYTES_VAR_STRAIGHT in FieldInfos,
but the norms implementation itself then has the type as FIXED_INTS_8."
0,"Deprecate NamespaceListener and AbstractNamespaceResolverThe NamespaceListener interface is no longer used with the JSR 283 style namespace handling that avoids lots of the synchronization that was previously to keep the local namespace mappings up to date.

Also, the only (remaining) purpose of the AbstractNamespaceResolver class is to add support for managing NamespaceListeners. Since that functionality is nowhere used anymore, we can make all subclasses use the NamespaceResolver interface directly.

Since NamespaceListener and AbstractNamespaceResolver are public in jackrabbit-spi-commons, I will for now only mark them as deprecated. We can get rid of them in Jackrabbit 2.0."
0,"Text Search Syntax Deviates from SpecOriginal JSR 170 EG Email by David B Victor 2005/03/23:

For Query test XPathQueryLevel2Test.java (src\java\org\apache\jackrabbit\test\api\query) in the TCK, method getFullTextStatement() (used by testFullTextSearch()) uses the word ""AND"" in the syntax in its test that is not in the spec (/*[jcrfnContains(""'quick brown' AND -cat"")]...).  Section ""6.6.4.2 contains function"" of v0.16.3, page 100, outlines the EBNF, which does not include the word ""AND"".  Additionally, the paragraphs here go out of their way to explain that AND is implicit.

At this point, I think it would be best to omit ""AND"" from the TCK method and let it test the implicit AND.

------------------------------------------------------------
David Neuscheler Reply 2005/03/24:

thanks for pointing that out.

i think we should probably track all the tck bugs in jackrabbit jira.
http://issues.apache.org/jira/browse/JCR

could you open a bug for that?

this actually is because we used an non-spec compliant query
parser in the RI, so it actually is even a bug in the RI and the TCK.

thanks again.

regards,
david"
1,"SessionItemStateManager.getIdOfRootTransientNodeState() may cause NPEregression of JCR-2425

in certain scenarios, calling SessionItemStateManager.getIdOfRootTransientNodeState() may cause a NPE.

Test case: 

        Repository repository = new TransientRepository(); 
        Session session = repository.login( 
                new SimpleCredentials(""admin"", ""admin"".toCharArray())); 
        Session session2 = repository.login( 
                new SimpleCredentials(""admin"", ""admin"".toCharArray())); 

        try { 
            while (session.getRootNode().hasNode(""test"")) { 
                session.getRootNode().getNode(""test"").remove(); 
            } 
            Node test = session.getRootNode().addNode(""test""); 
            session.save(); 
            Node x = test.addNode(""x""); 
            session.save(); 

            Node x2 = session2.getRootNode().getNode(""test"").getNode(""x""); 
            x2.remove(); 
            x.addNode(""b""); 
            session2.save(); 
            session.save(); // throws NPE 
        } finally { 
            session.logout(); 
            session2.logout(); 
        }"
1,"fix some more locale problems in lucene/solrset ANT_ARGS=""-Dargs=-Duser.language=tr -Duser.country=TR""
ant clean test

We should make sure this works across all of lucene/solr"
0,"Remove rest of analysis deprecations (Token, CharacterCache)These removes the rest of the deprecations in the analysis package:
- -Token's termText field-- (DONE)
- -eventually un-deprecate ctors of Token taking Strings (they are still useful) -> if yes remove deprec in 2.9.1- (DONE)
- -remove CharacterCache and use Character.valueOf() from Java5- (DONE)
- Stopwords lists
- Remove the backwards settings from analyzers (acronym, posIncr,...). They are deprecated, but we still have the VERSION constants. Do not know, how to proceed. Keep the settings alive for index compatibility? Or remove it together with the version constants (which were undeprecated)."
1,"JCR2SPI: several broken equals() comparisonsDetected by FindBugs:

H C EC: Call to equals() comparing unrelated class and interface in org.apache.jackrabbit.jcr2spi.nodetype.NodeTypeManagerImpl.nodeTypeReRegistered(QName)	xythos-jcr/src/main/java/org/apache/jackrabbit/jcr2spi/nodetype	NodeTypeManagerImpl.java	line 218	1190978573312	1664752
H C EC: Call to equals() comparing unrelated class and interface in org.apache.jackrabbit.jcr2spi.nodetype.NodeTypeManagerImpl.nodeTypeReRegistered(QName)	xythos-jcr/src/main/java/org/apache/jackrabbit/jcr2spi/nodetype	NodeTypeManagerImpl.java	line 227	1190978573312	1664753
H C EC: Call to equals() comparing unrelated class and interface in org.apache.jackrabbit.jcr2spi.nodetype.NodeTypeManagerImpl.nodeTypeUnregistered(QName)	xythos-jcr/src/main/java/org/apache/jackrabbit/jcr2spi/nodetype	NodeTypeManagerImpl.java	line 255	1190978573312	1664754
H C EC: Call to equals() comparing unrelated class and interface in org.apache.jackrabbit.jcr2spi.nodetype.NodeTypeManagerImpl.nodeTypeUnregistered(QName)	xythos-jcr/src/main/java/org/apache/jackrabbit/jcr2spi/nodetype	NodeTypeManagerImpl.java	line 264	1190978573312	1664755
H C EC: org.apache.jackrabbit.jcr2spi.WorkspaceManager.canAccess(String) uses equals to compare an array and nonarray	
"
0,"Explore streaming Viterbi search in KuromojiI've been playing with the idea of changing the Kuromoji viterbi
search to be 2 passes (intersect, backtrace) instead of 4 passes
(break into sentences, intersect, score, backtrace)... this is very
much a work in progress, so I'm just getting my current state up.
It's got tons of nocommits, doesn't properly handle the user dict nor
extended modes yet, etc.

One thing I'm playing with is to add a double backtrace for the long
compound tokens, ie, instead of penalizing these tokens so that
shorter tokens are picked, leave the scores unchanged but on backtrace
take that penalty and use it as a threshold for a 2nd best
segmentation...
"
0,"Cleanup some unused and unnecessary codeSeveral classes in trunk have some unused and unnecessary code. This includes unused fields, unused automatic variables, unused imports and unnecessary assignments. Attached it a patch to clean these up."
1,"TrecContentSource should use a fixed encoding, rather than system dependentTrecContentSource opens InputStreamReader w/o a fixed encoding. On Windows, this means CP1252 (at least on my machine) which is ok. However, when I opened it on a Linux machine w/ a default of UTF-8, it failed to read the files. The patch changes it to use ISO-8859-1, which seems to be the right one (and http://mg4j.dsi.unimi.it/man/manual/ch01s04.html mentions this encoding in its example of a script which reads the data).

Patch to follow shortly."
1,"Versioned node importXML failsWhen importing system-view XML previously exported for a repository, any nodes with a version history cannot be reimported. This appears to be due to the version manager attempting to create a new version history for the node, which fails due to a previous history existing for the same UUID. The behavior occurs with ImportUUIDBehavior.IMPORT_UUID_COLLISION_REPLACE_EXISTING and  ImportUUIDBehavior.IMPORT_UUID_COLLISION_REMOVE_EXISTING, with the following stack trace:

javax.jcr.version.VersionException: History already exists for node a892651d-1688-46cd-bb12-14f2f0b3d886
	at org.apache.jackrabbit.core.version.VersionManagerImpl.createVersionHistory(VersionManagerImpl.java:194)
	at org.apache.jackrabbit.core.ItemImpl.initVersionHistories(ItemImpl.java:900)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1313)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:766)

I am using the 1.0-dev version, revision 209290 obtained on 05 Jul, 2005 at 9:18:02 EST. Attached please find my repository configuration and the test code. Thanks!


"
0,"Current implementation of fuzzy and wildcard queries inappropriately implemented as Boolean query rewritesThe implementation of MultiTermQuery in terms of BooleanQuery introduces several problems:

1) Collisions with maximum clause limit on boolean queries which throws an exception.  This is most problematic because it is difficult to ascertain in advance how many terms a fuzzy query or wildcard query might involve.

2) The boolean disjunctive scoring is not appropriate for either fuzzy or wildcard queries.  In effect the score is divided by the number of terms in the query which has nothing to do with the relevancy of the results.

3) Performance of disjunctive boolean queries for large term sets is quite sub-optimal"
0,"File Formats Documentation is not correct for Term VectorsFrom Samir Abdou on the dev mailing list:

Hi, 

There is an inconsistency between the files format page (from Lucene
website) and the source code. It concerns the positions and offsets of term
vectors. It seems that documentation (website) is not up to date. According
to the file format page, offsets and positions are not stored! Is that
correct?

Many thanks,

Samir
-----
Indeed, in the file formats term vectors section it doesn't talk about the storing of position and offset info.
"
1,"EasyX509TrustManager no longer checks cert expiryEasyX509TrustManager was made even ""easier"" by the last commit:  a socket will
now be created when talking to a server with an expired certificate.

2 commits ago it looked like this (notice ""return false"" on line 107):

102             try {
103                 certificate.checkValidity();
104             }
105             catch (CertificateException e) {
106                 LOG.error(e.toString());
107                 return false;
108             }


Now it looks like this:

102             try {
103                 certificate.checkValidity();
104             }
105             catch (CertificateException e) {
106                 LOG.error(e.toString());
107             }


I'm proposing we just do:

102             certificate.checkValidity();

Now that we're using Java 1.4 in the contrib code, we'll just let the
CertificateException fly up the stack."
0,"Build with JDK 1.4, get many javadoc warningsBuilding httpclient ""dist"" ant target, I get lots of ""warning - The first
sentence is interpreted to be:"".

As the summary says, I get these warnings when I build using JDK 1.4.  Using JDK
1.3.1 yields far fewer problems.  I see this with the latest sources as of this
posting."
0,Reduce calls to RepositoryService.getRepositoryDescriptors()Descriptors do not change and should not be requested for each session.
1,custom sort broken if IS uses executorservice
0,"configurable MultiTermQuery TopTermsScoringBooleanRewrite pq sizeMultiTermQuery has a TopTermsScoringBooleanRewrite, that uses a priority queue to expand the query to the top-N terms.

currently N is hardcoded at BooleanQuery.getMaxClauseCount(), but it would be nice to be able to set this for top-N MultiTermQueries: e.g. expand a fuzzy query to at most only the 50 closest terms.

at a glance it seems one way would be to expose TopTermsScoringBooleanRewrite (it is private right now) and add a ctor to it, so a MultiTermQuery can instantiate one with its own limit."
1,"DEFAULT_HEADERS not added to subsequent requestsDEFAULT_HEADERS are added to the original request only, not to subsequent requests for redirects or authentication."
1,"Search with Filter does not work!See attached JUnitTest, self-explanatory


"
0,"Make MMapDirectory.MAX_BBUF user configureable to support chunking the index files in smaller partsThis is a followup for java-user thred: http://www.lucidimagination.com/search/document/9ba9137bb5d8cb78/oom_with_2_9#9bf3b5b8f3b1fb9b

It is easy to implement, just add a setter method for this parameter to MMapDir."
0,"Consolidate compare behaviour for Value(s) and Comparable(s)There are 2 different implementations of Value comparison (ValueComparator and Util). With the introduction of JCR-2906 which introduces arrays into the mix, I'd like to refactor all of them into one place, namely o.a.j.core.query.lucene.Util.

This will also allow for a wider scope of comparison for Value[], marked as TODO in the ValueComparator class.

Will attach patch shortly"
0,"exceptions from other threads in beforeclass/etc do not fail the testLots of tests create indexes in beforeClass methods, but if an exception is thrown from another thread
it won't fail the test... e.g. this test passes:
{code}
public class TestExc extends LuceneTestCase {
  @BeforeClass
  public static void beforeClass() {
    new Thread() {
      public void run() {
        throw new RuntimeException(""boo!"");
      }  
    }.start();
  }
  
  public void test() { }
}
{code}

this is because the uncaught exception handler is in setup/teardown"
0,"Avoid INFINITE RECURSION when Object Model has cycles.The default ObjectConverterImpl is restricted to acyclic graphs in the object model.

Many Java object models are NOT acyclic.   For instance, I am on your Friends list.   Yoar are on my Friends list.     Java encourages such structures.   Almost any large object model in Java will have hidden cycles.

Saving an Object Model that contains cycles using Graffito causes an infinite recursion.

Clearly, it is important to maintain a 1-to-1 correspondence between Nodes and Objects to prevent this.   In the absence of Multiple Parent Nodes, it will be necessary to use REFERENCE or UNDEFINED Items in place of the 2nd (or greater) Node representing a given Object.   My preference si that the default ObjectConverterImpl should support REFERENCE.,    Failing this, use of UNDEFINED also solves this problem and would  acceptable (as default).  Whether or not REFERENCE is used, both insertion and retrieval must provide a reasonable result.   A custom ojbect converter should be available to switch UNDEFINED to REFERENCE, or vice versa.

Also, it is probably best to keep the targeted, well-defined Nodes close to the Root Node.    This implies that the default ObjectConverterImpl should implement a Breadth-First, rather than a Depth-First, traversal of the Object Model on both insertion and retrieval.   Again, if the default is Depth-First, a custom object converter should be available that implements Breadth-First.

Admittedly, support for (2 representations) X (2 traversals) implies a drastic refactoring and/or rewriting of the ObjectConverterImpl class."
0,"Provide query support  for WEAKREFERENCE reverse lookupthe current implementation of Node.getWeakReferences() and getWeakReferences(String) uses a fulltext query in order to find weak references to a particular node.

this requires the PlainTextExtractor to be enabled in the Search config, e.g. :

    <param name=""textFilterClasses"" value=""org.apache.jackrabbit.extractor.PlainTextExtractor""/>

providing 'native' WEAKREFERNCE reverse lookup in Jackrabbit's QOM implementation would be certainly more efficient.

"
1,"Cluster sync not always done when calling session.refresh(..)Session.refresh(..) is supposed to synchronize cluster changes, but this doesn't always happen, specially if the syncDelay is low. The reason is a wrong assumption in ClusterNode.sync: The code there to avoid duplicate sync calls doesn't always work as expected. The following algorithm is used:

        int count = syncCount;
        syncLock.acquire();
        if (count == syncCount) {
            journalSync();
            syncCount++;
        }
        syncLock.release();

The problem is that the background thread might be at the line ""syncCount++"" when Session.refresh(..) is called, so that the main thread believes journalSync was already called and thus doesn't call it."
0,"Separate NOTICEs and LICENSEs for binary and source packagesBased on recent discussions on sling-dev@ (see [1]) and on legal-discuss@  (see [2]), I'd like to rearrange our NOTICE and LICENSE files so that the root level files refer only to bits included in source releases and that the (in some cases different) files to be included in the binary artifacts would be placed in src/main/resources/META-INF.

See also JCR-1630 for related work.

[1] http://markmail.org/message/2enw6ktxhc4ixmrk
[2] http://markmail.org/message/bttmkavpicxxg7gl
"
0,most tests should use MockRAMDirectory not RAMDirectory
0,"TransientRepository: application doesn't exit quicklyWhen using the TransientRepository, the repository should be closed when the last session logs out. This works, but in some cases there is a very long (60 seconds) delay between closing the last session and closing the repository.

Test case:

    public static void main(String[] args) throws Exception {
        Repository repository = new TransientRepository();
        Session session = repository.login(new SimpleCredentials("""", new char[0]));
        session.getRootNode().setProperty(""a"", ""0"");
        session.save(); // very quick logout without this line
        session.logout();
        System.out.println(""Logout..."");
        final long time = System.currentTimeMillis();
        Runtime.getRuntime().addShutdownHook(new Thread() {
            public void run() {
                System.out.println(""End after: "" + (System.currentTimeMillis() - time));
            }
        });
    }

"
0,"spi2dav Improve performance for large binary propertiesSending large binary properties over spi2dav is slow and requires a lot of heap space in both client and server.
One problematic part is base64 conversion of the property value.

On the contrary, using 'normal' webdav interface (/repository/default/ instead of /server) for uploading a file (through traditional webdav client) it is pretty fast and don't have such impact on heap space.

Some suggestions from the previous discussion:
 - avoid temporary copies of the data, and persist large objects as early as possible. 
 - transfer large objects in blocks from the Jackrabbit SPI client to the server (and back).
 - make usage of the global data store (JCR-926). 
 - straight forward PUT for single-valued properties

Link to discussion: http://www.mail-archive.com/dev@jackrabbit.apache.org/msg09481.html
"
0,Expose directory on IndexReaderIt would be really useful to expose the index directory on the IndexReader class.
0,"Basic refactoring of DocumentsWriterAs a starting point for making DocumentsWriter more understandable,
I've fixed its inner classes to be static, and then broke the classes
out into separate sources, all in org.apache.lucene.index package.

"
1,"Can not instantiate lucene Analyzer in SearchIndexIn the Lucene 3, the there is no default constructor anymore in Analyzer classes


11:46:45.946 [main] WARN  o.a.j.core.query.lucene.SearchIndex - Invalid Analyzer class: org.apache.lucene.analysis.standard.StandardAnalyzer
java.lang.InstantiationException: org.apache.lucene.analysis.standard.StandardAnalyzer
        at java.lang.Class.newInstance0(Class.java:340) ~[na:1.6.0_26]
        at java.lang.Class.newInstance(Class.java:308) ~[na:1.6.0_26]
        at org.apache.jackrabbit.core.query.lucene.SearchIndex.setAnalyzer(SearchIndex.java:1892) ~[jackrabbit-core-2.4.0.jar:2.4.0]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.6.0_26]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) ~[na:1.6.0_26]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) ~[na:1.6.0_26]
        at java.lang.reflect.Method.invoke(Method.java:597) ~[na:1.6.0_26]
        at org.apache.jackrabbit.core.config.BeanConfig.setProperty(BeanConfig.java:255) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.config.BeanConfig.newInstance(BeanConfig.java:203) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.config.RepositoryConfigurationParser$1.getQueryHandler(RepositoryConfigurationParser.java:652) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.config.WorkspaceConfig.getQueryHandler(WorkspaceConfig.java:251) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.SearchManager.<init>(SearchManager.java:171) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getSearchManager(RepositoryImpl.java:1855) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.doPostInitialize(RepositoryImpl.java:2092) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.initialize(RepositoryImpl.java:1997) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.RepositoryImpl.initStartupWorkspaces(RepositoryImpl.java:510) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:318) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:582) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.jndi.BindableRepository.createRepository(BindableRepository.java:141) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.jndi.BindableRepository.init(BindableRepository.java:117) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.jndi.BindableRepository.<init>(BindableRepository.java:106) [jackrabbit-core-2.4.0.jar:2.4.0]
        at org.apache.jackrabbit.core.jndi.BindableRepositoryFactory.getObjectInstance(BindableRepositoryFactory.java:52) [jackrabbit-core-2.4.0.jar:2.4.0]
"
0,Add accessor for parent to NodeInfoBuilder/PropertyInfoBuilderNodeInfoBuilder and PropertyInfoBuilder should allow access to its respective parents. I suggest to add a getParent() method to both classes. 
0,"Spatial uses java util logging that causes needless minor work (multiple string concat, a method call) due to not checking log levelNot sure there should be logging here - just used in two spots and looks more for debug - but if its going to be there, should check for isFineEnabled."
0,"Move *.log files to target/The jackrabbit-core component already puts the derby.log file in target/ along with other build  and test artifacts, but many other components don't do that yet. Having all generated files in target/ is good as it makes it very easy to clean things up. Also things like the RAT checks (JCR-1937) are easier when there's no need to worry about such extra files.
"
0,"Change visibility of getComparator method in SortField from protected to publicHi,

Currently I'm using SortField for the creation of FieldComparators, but I ran into an issue.
I cannot invoke SortField.getComparator(...) directly from my code, which forces me to use a  workaround. (subclass SortField and override the getComparator method with visiblity public)
I'm proposing to make this method public. Currently I do not see any problems changing the visibility to public, I do not know if there are any (and the reason why this method is currently protected)
I think that this is a cleaner solution then the workaround I used and also other developers can benefit from it. I will also attach a patch to this issue based on the code in the trunk (26th of May). place). 
Please let me know your thoughts about this.

Cheers,

Martijn

 "
1,"Token of  """" returns in CJKTokenizer + new TestCJKTokenizerThe """" string returns as Token in the boundary of two byte character and one byte character. 

There is no problem in CJKAnalyzer. 
When CJKTokenizer is used with the unit, it becomes a problem. (Use it with 
Solr etc.)"
1,"always apply position increment gap between valuesI'm doing some fancy stuff with span queries that is very sensitive to term positions.  I discovered that the position increment gap on indexing is only applied between values when there are existing terms indexed for the document.  I suspect this logic wasn't deliberate, it's just how its always been for no particular reason.  I think it should always apply the gap between fields.  Reference DocInverterPerField.java line 82:

if (fieldState.length > 0)
          fieldState.position += docState.analyzer.getPositionIncrementGap(fieldInfo.name);

This is checking fieldState.length.  I think the condition should simply be:  if (i > 0).
I don't think this change will affect anyone at all but it will certainly help me.  Presently, I can either change this line in Lucene, or I can put in a hack so that the first value for the document is some dummy value which is wasteful."
1,"Possible concurrency bug with Workspace.copy() Hi,

Enclosed below is a test case that can be used to reproduce a
concurrency bug. This test case uses two con-current threads to
execute Workspace.copy() to copy a node to same destination. The
parent node has set its allowSameNameSiblings to false. According to
the javadoc of Workspace.copy(String srcAbsPath, String destAbsPath) :
""This method copies the node at srcAbsPath to the new location at
destAbsPath. If successful, the change is persisted immediately, there
is no need to call save."".  ""An ItemExistException is thrown if a
property already exists at destAbsPath or a node already exist there,
and same name siblings are not allowed. ""

However in reality this is not the case.  The test case can end up
with two child nodes with same names. Please note, not every run can
reproduce the problem, but generally I can get the problem within 3 to
10 iterations. I also got an InvalidItemStateException once (only
once).  Can someone kindly help to confirm if this is a bug in
Jackrabbit or maybe I am using JackRabbit in a wrong way? The test
case has been tested on Jackrabbit 1.6 branch
(http://svn.apache.org/repos/asf/jackrabbit/tags/1.6.0), Windows
Vista, JDK 1.5.0_14.

The test case is also attached for your convenience.

Thanks,
Jervis Liu

package org.apache.jackrabbit.core;

import org.apache.jackrabbit.test.AbstractJCRTest;
import javax.jcr.ItemExistsException;
import javax.jcr.Node;
import javax.jcr.Session;
import javax.jcr.Value;
import javax.jcr.NodeIterator;
import java.util.Random;
import java.util.ArrayList;
import java.util.Iterator;
import javax.jcr.nodetype.NodeType;

import org.apache.jackrabbit.test.NotExecutableException;
import javax.jcr.RepositoryException;
import javax.jcr.nodetype.NodeTypeManager;


public class ConcurrentCopyTest extends AbstractJCRTest {

    private static final int NUM_ITERATIONS = 40;
    private static final int NUM_SESSIONS = 2;

    String sourcePath;
    String destPath;

    public void testConcurrentCopy() throws Exception {
        for (int n = 0; n < NUM_ITERATIONS; n++) {
            System.out.println(""---Iteration---- "" + n);

            // clean up testRoot first
            if (testRootNode.hasNode(""ConcurrentCopyTestNode"")) {
                Node testNode = testRootNode.getNode(""ConcurrentCopyTestNode"");
                testNode.remove();
                testRootNode.save();
                System.out.println(""---old node removed---"");
            }

            // create a parent node where allowSameNameSiblings is set to false
            Node snsfNode = testRootNode.addNode(""ConcurrentCopyTestNode"",
                    ""nt:folder"");
            testRootNode.save();
            sourcePath = snsfNode.getPath();
            destPath = sourcePath + ""/"" + ""CopiedFromConcurrentCopyTestNode"";
            System.out.println(""---sourcePath-----------------"" + sourcePath);
            System.out.println(""---destPath-----------------"" + destPath);

            // firstly we verify it works with single thread.
            Session rootSession = helper.getSuperuserSession();
            rootSession.getWorkspace().copy(sourcePath, destPath + ""test"");

            // copy again to same destPath, expect an ItemExistsException
            try {
                rootSession.getWorkspace().copy(sourcePath, destPath + ""test"");
                fail(""Node exists below '"" + destPath + ""'. Test should fail."");
            } catch (ItemExistsException e) {
            }

            Thread[] threads = new Thread[NUM_SESSIONS];
            for (int i = 0; i < threads.length; i++) {
                // create new session
                Session session = helper.getSuperuserSession();
                TestSession ts = new TestSession(""s"" + i, session);
                Thread t = new Thread(ts);
                t.setName((NUM_ITERATIONS - n) + ""-s"" + i);
                t.start();
                log.println(""Thread#"" + i + "" started"");
                threads[i] = t;
                // Thread.yield();
                // Thread.sleep(100);
            }
            for (int i = 0; i < threads.length; i++) {
                threads[i].join();
            }

            NodeIterator results = testRootNode.getNode(
                    ""ConcurrentCopyTestNode"").getNodes(
                    ""CopiedFromConcurrentCopyTestNode"");
            while (results.hasNext()) {
                Node node = results.nextNode();
                System.out.println(""--result node- "" + node.getName());
            }

            assertEquals(1, results.getSize());
        }
    }

    // --------------------------------------------------------< inner classes >
    class TestSession implements Runnable {

        Session session;
        String identity;
        Random r;

        TestSession(String identity, Session s) {
            session = s;
            this.identity = identity;
            r = new Random();
        }

        private void randomSleep() {
            long l = r.nextInt(90) + 20;
            try {
                Thread.sleep(l);
            } catch (InterruptedException ie) {
            }
        }

        public void run() {

            log.println(""started."");
            String state = """";
            try {
                this.session.getWorkspace().copy(sourcePath, destPath);
                session.save();
                Node newNode =
testRootNode.getNode(""ConcurrentCopyTestNode/CopiedFromConcurrentCopyTestNode"");
                System.out.println(""--Added node- "" + newNode.getName());

                session.save();
                randomSleep();
            } catch (Exception e) {
                log.println(""Exception while "" + state + "": "" + e.getMessage());
                e.printStackTrace();
            } finally {
                session.logout();
            }

            log.println(""ended."");
        }
    }

}

"
0,"Determination of property state difference should skip binary valueso.a.j.jcr2spi.state.PropertyState.diffPropertyData, PropertyData) should alway consider two binary values to be different. The current implementation compares two binary values with equals(). An implementation will in general have to do a byte by byte comparison of both values. This is most likely always more expensive than considering the values different right from the start. 

"
0,Provide fail-over for multi-home remote servers (if one server in a farm goes down)The HTTP Client does not provide automatic fail-over for multi-home remote servers (web-farm) if one server in a farm goes down
1,"automaton termsenum bug when running with multithreaded searchThis one popped in hudson (with a test that runs the same query against fieldcache, and with a filter rewrite, and compares results)

However, its actually worse and unrelated to the fieldcache: you can set both to filter rewrite and it will still fail.
"
0,"Upgrade benchmark from commons-compress-1.0 to commons-compress-1.1 for 15 times faster gzip decompressionIn LUCENE-1540 TrecContentSource moved from Java's GZipInputStream to common-compress 1.0. 
This slowed down gzip decompression by a factor of 15. 
Upgrading to 1.1 solves this problem.
I verified that the problem is only in GZIP, not in BZIP.
On the way, as 1.1 introduced constants for the compression methods, the code can be made a bit nicer."
0,"replace text from an online collection (used in few test cases) with text that is surely 100% free.Text from an online firstaid collection (firstaid . ie . eu . org)  is used as arbitrary text for test documents creation, in:
   o.a.l.analysis.Analyzer.FunctionTestSetup.DOC_TEXT_LINES
   o.a.l.benchmark.byTask.feeds.SimpleDocMaker.DOC_TEXT

I once got this text from Project Gutenberg and was sure that it is free. But now the referred Web site does not seem to respond, and I can no more find that firstaid eBook in the Project Gutenberg site.

Since it doesn't matter what text we use there, I will just replace that with some of my own words..."
0,"MemcachedHttpCacheStorage should throw IOExceptions instead of Runtime ExceptionsThe MemcachedHttpCacheStorage class implements HttpCacheStorage which defines that methods will throw IOExceptions, but the underlying net.spy.memcached.MemcachedClientIF throws runtime exceptions. These exceptions are not caught in the code where IOExceptions are expected causing these exception bubble up to the calling code. It seems like the MemcachedHttpCacheStorage class should treat at least some of these runtime exceptions as IOExceptions so that normal code execution paths can be followed.  

I'm proposing that MemcachedHttpCacheStorage treat a OperationTimeoutException from the memcached client as an IOException. This would allow the existing CachingHttpClient code to catch and log the exception as a warning, instead of bubbling the exception up the calling code.
"
1,"Node merge method doesnt seems to recurse thru childs of the right source nodeI checked the NodeImpl.merge(...)

it seems the way it process the childs nodes is wrong
as it calls the merge on the childs of the src node that come from the source workspace.
plus in the case srcNode is null it would end on a NullPointerException as

it does  NodeIterator ni = srcNode.getNodes(); in the second statment of the if condition
"
1,"exception during writeRequest leaves the connection un-releasedThe execute method has the following (simplified) flow:
1) get connection
2) write request
3) read result
4) release connection.
The release in step 4 happens when the input is completely read, which works fine.
If an exception occurs between steps 1 and 2, the connection is also released
properly.
However, if an exception occurs during step 2, the connection is never released
back and the connection manager eventually runs out of connections.

The easiest way to test this is to make a simple subclass of PostMethod that
overrides the writeRequest method:

public class TestConnectionReleaseMethod extends PostMethod
{
    protected void writeRequest(HttpState state, HttpConnection conn) throws
IOException, HttpException
    {
         throw new IOException(""for testing"");
    }
}"
0,"Create new method optimize(int maxNumSegments) in IndexWriterSpinning this out from the discussion in LUCENE-847.

I think having a way to ""slightly optimize"" your index would be useful
for many applications.

The current optimize() call is very expensive for large indices
because it always optimizes fully down to 1 segment.  If we add a new
method which instead is allowed to stop optimizing once it has <=
maxNumSegments segments in the index, this would allow applications to
eg optimize down to say <= 10 segments after doing a bunch of updates.
This should be a nice compromise of gaining good speedups of searching
while not spending the full (and typically very high) cost of
optimizing down to a single segment.

Since LUCENE-847 is now formalizing an API for decoupling merge policy
from IndexWriter, if we want to add this new optimize method we need
to take it into account in LUCENE-847.
"
0,"Automatic license header checking using the Apache Rat PluginTo avoid problems with incorrect license headers, we should include some automated header check in the Maven build and have Hudson run the check whenever changes are committed."
1,"Host configuration properties not updated when the method is redirectedthe above uri:

http://www.adobe.com/cgi-bin/redirect?http://lists.w3.org/Archives/Public/www-xsl-fo

generates two 302 responses:

from the original to http://lists.w3.org/Archives/Public/www-xsl-fo
and from that to http://lists.w3.org/Archives/Public/www-xsl-fo/

the client accepts and follows these redirects (a trace of the process shows it's working well) but when 
you ask the getmethod what uri we ended up at using the getURI() method it returns the bastardised 
result:

http://www.adobe.com/Archives/Public/www-xsl-fo/

instead of the correct 

http://lists.w3.org/Archives/Public/www-xsl-fo/

that the client has actually downloaded.

using cvsup'd copy showing version string "" Jakarta Commons-HttpClient/2.1m1"""
0,"contrib/memory: PatternAnalyzerTest is a very, very, VERY, bad unit testwhile working on something else i was started getting consistent IllegalStateExceptions from PatternAnalyzerTest -- but only when running the test from the top level.

Digging into the test, i've found numerous things that are very scary...
* instead of using assertions to test that tokens streams match, it throws an IllegalStateExceptions when they don't, and then logs a bunch of info about the token streams to System.out -- having assertion messages that tell you *exactly* what doens't match would make a lot more sense.
* it builds up a list of files to analyze using patsh thta it evaluates relative to the current working directory -- which means you get different files depending on wether you run the tests fro mthe contrib level, or from the top level build file
* the list of files it looks for include: ""../../*.txt"", ""../../*.html"", ""../../*.xml"" ... so not only do you get different results when you run the tests in the contrib vs at the top level, but different people runing the tests via the top level build file will get different results depending on what types of text, html, and xml files they happen to have two directories above where they checked out lucene.
* the test comments indicates that it's purpose is to show that PatternAnalyzer produces the same tokens as other analyzers - but points out this will fail for WhitespaceAnalyzer because of the 255 character token limit WhitespaceTokenizer imposes -- the test then proceeds to compare PaternAnalyzer to WhitespaceTokenizer, garunteeing a test failure for anyone who happens to have a text file containing more then 255 characters of non-whitespace in a row somewhere in ""../../"" (in my case: my bookmarks.html file, and the hex encoded favicon.gif images)
"
0,"In 3.x branch (starting with 3.4) the IndexFormatTooOldException was backported, but the error message was not modified for 3.xIn 3.x branch (starting with 3.4) the IndexFormatTooOldException was backported, but the error message was not modified for 3.x:

bq. This version of Lucene only supports indexes created with release 3.0 and later.

In 3.x it must be:

bq. This version of Lucene only supports indexes created with release 1.9 and later.

Indexes before 1.9 will throw this exception on reading SegmentInfos (LUCENE-3255)."
0,Add test case support for shard searchingNew test case that helps stress test the APIs to support sharding....
0,"cleanup contrib/demoI don't think we should include optimize in the demo; many people start from the demo and may think you must optimize to do searching, and that's clearly not the case.

I think we should also use a buffered reader in FileDocument?

And... I'm tempted to remove IndexHTML (and the html parser) entirely.  It's ancient, and we now have Tika to extract text from many doc formats."
0,"MMapDirectory speedupsMMapDirectory has some performance problems:
# When the file is larger than Integer.MAX_VALUE, we use MultiMMapIndexInput, 
which does a lot of unnecessary bounds-checks for its buffer-switching etc. 
Instead, like MMapIndexInput, it should rely upon the contract of these operations
in ByteBuffer (which will do a bounds check always and throw BufferUnderflowException).
Our 'buffer' is so large (Integer.MAX_VALUE) that its rare this happens and doing
our own bounds checks just slows things down.
# the readInt()/readLong()/readShort() are slow and should just defer to ByteBuffer.readInt(), etc
This isn't very important since we don't much use these, but I think there's no reason
users (e.g. codec writers) should have to readBytes() + wrap as bytebuffer + get an 
IntBuffer view when readInt() can be almost as fast..."
1,"DefaultHttpRequestRetryHandler must not retry non-idempotent http methods (violates RFC 2616)In DefaultHttpRequestRetryHandler, in case of NoHttpResponseException, the request is retried, without taking into account whether the http method is idempotent or not. This violates RFC 2616 section 8.1.4 which states :
{quote}
This means that clients, servers, and proxies MUST be able to recover
   from asynchronous close events. Client software SHOULD reopen the
   transport connection and retransmit the aborted sequence of requests
   without user interaction so long as the request sequence is
   idempotent (see section 9.1.2). Non-idempotent methods or sequences
   MUST NOT be automatically retried, although user agents MAY offer a
   human operator the choice of retrying the request(s).
{quote}

The fix is simple : at line 94, just remove the {{if (exception instanceof NoHttpResponseException) }} block. This way the idempotency of the method will be taken into account a bit further in the same method."
0,"Jackrabbit web page scroll is slow with FirefoxWhen I visit http://jackrabbit.apache.org/ from my Firefox in Ubuntu  the browser scroll is very slow and make CPU go to 100%. 

The problem seems to be the ""fixed"" attribute in the css background definition, so it should be removed.

body {
  background:white url(bg.png) repeat-x fixed center bottom;
  font-family:Verdana,Helvetica,Arial,sans-serif;
  font-size:small;
  margin:0pt;
  padding:0pt;
}"
1,"Custom similarity is ignored when using MultiSearcherSymptoms:
I am using Searcher.setSimilarity() to provide a custom similarity that turns off tf() factor. However, somewhere along the way the custom similarity is ignored and the DefaultSimilarity is used. I am using MultiSearcher and BooleanQuery.

Problem analysis:
The problem seems to be in MultiSearcher.createWeight(Query) method. It creates an instance of CachedDfSource but does not set the similarity. As the result CachedDfSource provides DefaultSimilarity to queries that use it.

Potential solution:
Adding the following line:
    cacheSim.setSimilarity(getSimilarity());
after creating an instance of CacheDfSource (line 312) seems to fix the problem. However, I don't understand enough of the inner workings of this class to be absolutely sure that this is the right thing to do.

"
0,"Create jackrabbit-parentCurrently the Jackrabbit components use the top-level multi-module POM as their parent POM for sharing many of the common build settings. However, with the planned mixed component release model for 1.5.x we need to be able to increase the version number of the top-level POM without affecting individual components. Thus it is better if we move the shared settings to an explicit jackrabbit-parent component and keep the top-level POM simply as the multi-module container."
0,"WebDAV: pack AbstractWebdavServlet with the jackrabbit-webdav projectsuggestion posted by alan cabrera on the dev list:

""Quite a handy servlet.  Too bad it's in jackrabbit-server.  Would this not be better placed in jackrabbit-webdav?  I'm writing my own server bits under WEBDAV and would prefer not to have JCR/Jackrabbit stuff.  I realize that this is a fussy preference.""

"
1,"SegmentReader.getFieldNames ignores FieldOption.DOC_VALUESwe use this getFieldNames api in segmentmerger if we merge something that isn't a SegmentReader (e.g. FilterIndexReader)

it looks to me that if you use a FilterIndexReader, call addIndexes(Reader...) the docvalues will be simply dropped.

I dont think its enough to just note that the field has docvalues either right? We need to also set the type 
correctly in the merged field infos? This would imply that instead of FieldOption.DOCVALUES, we need to have a 
FieldOption for each ValueType so that we correctly update the type.

But looking at FI.update/setDocValues, it doesn't look like we 'type-promote' here anyway?
"
0,"Use Apache Codec 1.41.4 fixes many bugs and added some nice features: http://commons.apache.org/codec/changes-report.html

It took me a whiel to find out, that this was the main reason my tests failed (MethodNotFoundException).
"
0,Make inspection of BooleanQuery more efficientJust attempting to inspect a BooleanQuery allocates two new arrays.  This could be cheaper.
0,"Change contrib QP API that uses CharSequence as string identifierThere are some API methods on contrib queryparser that expects CharSequence as identifier. This is wrong, since it may lead to incorrect or mislead behavior, as shown on LUCENE-2855. To avoid this problem, these APIs will be changed and enforce the use of String instead of CharSequence on version 4. This patch already deprecate the old API methods and add new substitute methods that uses only String."
0,"Reduce usage of String.intern()String.intern() is used for interning the namespace URI in NameImpl. For some trivial cases the intern() method shouldn't be called but a constant should be
used. E.g. I'm thinking about the empty namespace URI, where calling String.intern() is way more expensive than checking if the length of the URI string is zero. "
0,"Connection pool uses Thread.interrupt()The connection pool for TSCCM uses Thread.interrupt() to wake up waiting threads.
This interferes with application interrupts.

- expose InterruptedException in interface
- change pool implementation to use wait/notify
"
1,"KeywordMarkerFilter resets keyword attribute state to false for tokens not in protwords.txtKeywordMarkerFilter sets true or false for the KeywordAttribute on all tokens. This erases previous state established further up the filter chain, for example in the case where a custom filter wants to prevent a token from being stemmed. 

If a token is already marked as a keyword (KeywordAttribute.isKeyword() == true), perhaps the KeywordMarkerFilterFactory should not re-set the state to false."
0,"Incorrect support for java interfaces in typed collection fieldsIf a typed collection field is defined with an Interface as the type, the following exception is thrown when the main object is inserted : 

org.apache.jackrabbit.ocm.exception.JcrMappingException: Cannot load class interface [name of the interface];

Here is a example : 

@Node
public class EntityA {
       @Field(path=true) String path;
       @Collection List<MyInterface> entityB;
       ....
}

When inserting a new instance of EntityA with a not null entityB, the exception is thrown. 
A workaround is to add the elementClassName on the annotation @Collection. ex. : 

@Collection (elementClassName=MyInterface.class) List<MyInterface> entityB;

elementClassName is used only for untyped collections but if you specify it for a typed collection, the ObjectContentManager will not use reflexion to check the collection class name. 
 
This should be nice to avoid the usage of elementClassName for typed collections. 

"
0,"Replace Maven POM templates with full POMs, and change documentation accordinglyThe current Maven POM templates only contain dependency information, the bare bones necessary for uploading artifacts to the Maven repository.

The full Maven POMs in the attached patch include the information necessary to run a multi-module Maven build, in addition to serving the same purpose as the current POM templates.

Several dependencies are not available through public maven repositories.  A profile in the top-level POM can be activated to install these dependencies from the various {{lib/}} directories into your local repository.  From the top-level directory:

{code}
mvn -N -Pbootstrap install
{code}

Once these non-Maven dependencies have been installed, to run all Lucene/Solr tests via Maven's surefire plugin, and populate your local repository with all artifacts, from the top level directory, run:

{code}
mvn install
{code}

When one Lucene/Solr module depends on another, the dependency is declared on the *artifact(s)* produced by the other module and deposited in your local repository, rather than on the other module's un-jarred compiler output in the {{build/}} directory, so you must run {{mvn install}} on the other module before its changes are visible to the module that depends on it.

To create all the artifacts without running tests:

{code}
mvn -DskipTests install
{code}

I almost always include the {{clean}} phase when I do a build, e.g.:

{code}
mvn -DskipTests clean install
{code}
"
0,"explore using automaton for fuzzyquerywe can optimize fuzzyquery by using AutomatonTermsEnum. The idea is to speed up the core FuzzyQuery in similar fashion to Wildcard and Regex speedups, maintaining all backwards compatibility.

The advantages are:
* we can seek to terms that are useful, instead of brute-forcing the entire terms dict
* we can determine matches faster, as true/false from a DFA is array lookup, don't even need to run levenshtein.

We build Levenshtein DFAs in linear time with respect to the length of the word: http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.16.652

To implement support for 'prefix' length, we simply concatenate two DFAs, which doesn't require us to do NFA->DFA conversion, as the prefix portion is a singleton. the concatenation is also constant time with respect to the size of the fuzzy DFA, it only need examine its start state.

with this algorithm, parametric tables are precomputed so that DFAs can be constructed very quickly.
if the required number of edits is too large (we don't have a table for it), we use ""dumb mode"" at first (no seeking, no DFA, just brute force like now).

As the priority queue fills up during enumeration, the similarity score required to be a competitive term increases, so, the enum gets faster and faster as this happens. This is because terms in core FuzzyQuery are sorted by boost value, then by term (in lexicographic order).

For a large term dictionary with a low minimal similarity, you will fill the pq very quickly since you will match many terms. 
This not only provides a mechanism to switch to more efficient DFAs (edit distance of 2 -> edit distance of 1 -> edit distance of 0) during enumeration, but also to switch from ""dumb mode"" to ""smart mode"".

With this design, we can add more DFAs at any time by adding additional tables. The tradeoff is the tables get rather large, so for very high K, we would start to increase the size of Lucene's jar file. The idea is we don't have include large tables for very high K, by using the 'competitive boost' attribute of the priority queue.

For more information, see http://en.wikipedia.org/wiki/Levenshtein_automaton"
1,"BundleDbPersistenceManager does not work with MySQLIt seems that the bundle persistence manager base does not work with MySQL. A SQLException is thrown on the line ""con.commit();"" in BundleDbPersistenceManager.checkSchema() because autoCommit is set to true in the init method. For some reason, this is ignored by the Oracle and MSSQL drivers. Anyway, commenting out the line fixes the issue, I think."
0,"codec postings api (finishDoc) is inconsistentfinishDoc says:

{noformat}
  /** Called when we are done adding positions & payloads
   *  for each doc.  Not called  when the field omits term
   *  freq and positions. */
   public abstract void finishDoc() throws IOException;
{noformat}

But this is confusing (because a field can omit just positions, is it called then?!),
and wrong (because merging calls it always, even if freq+positions is omitted).

I think we should fix the javadoc and fix FreqProxTermsWriter to always call finish()
"
0,"if the build fails to download JARs for contrib/db, just skip its testsEvery so often our nightly build fails because contrib/db is unable to download the necessary BDB JARs from http://downloads.osafoundation.org.  I think in such cases we should simply skip contrib/db's tests, if it's the nightly build that's running, since it's a false positive failure."
1,"search vs explain - score discrepanciesI'm on a mission to demonstrate (and then hopefully fix) any inconsistencies between the score you get for a doc when executing a search, and the score you get when asking for an explanation of the query for that doc."
0,"Eliminate synchronization contention on initial index reading in TermInfosReader ensureIndexIsRead synchronized method ensureIndexIsRead in TermInfosReader causes contention under heavy load

Simple to reproduce: e.g. Under Solr, with all caches turned off, do a simple range search e.g. id:[0 TO 999999] on even a small index (in my case 28K docs) and under a load/stress test application, and later, examining the Thread dump (kill -3) , many threads are blocked on 'waiting for monitor entry' to this method.

Rather than using Double-Checked Locking which is known to have issues, this implementation uses a state pattern, where only one thread can move the object from IndexNotRead state to IndexRead, and in doing so alters the objects behavior, i.e. once the index is loaded, the index nolonger needs a synchronized method. 

In my particular test, this uncreased throughput at least 30 times.

"
0,"Override method MultipartEntity.addPart so that applications may use FormBodyPartFormBodyPart is similar to Part in HttpClient 3.x in that it couples the form name with the value.  Some applications may find this useful, but cannot really utilize these objects since there is only MultipartEntity.addPart(String name,ContentBody) and FormBodyPart does not have a getContent method:

  entity.addPart(part.getName(), part.getContent()); // Almost but there is no getContent method

How about overriding addPart to take a FormBodyPart object:

  entity.addPart(part);"
1,"BLOB Store: only open a stream when really necessaryCurrently, PropertyImpl.getValue() opens a FileInputStream if the BLOBStore is used.
If the application doesn't use the value, this stream is never closed. 

See also JCR-2067 (FileDataStore)"
0,Move & copy objectsAdd new methods in the persistence manager to move and copy objects
0,"LockInfo.logginOut(SessionImpl): javadoc does not correspond to executed code/**
     * {@inheritDoc}
     * <p/>
     * When the owning session is logging out, we have to perform some
     * operations depending on the lock type.
     * (1) If the lock was session-scoped, we unlock the node.
     * (2) If the lock was open-scoped, we remove the lock token
     *     from the session.
     */
    public void loggingOut(SessionImpl session) {
        if (live) {
            if (sessionScoped) {
                lockMgr.unlock(this);
            } else {
                if (session.equals(lockHolder)) {
                    lockHolder = null;
                }
            }
        }
    } 


if (2) is true, the lockToken is not removed from the session (at least not within the method).

regards
angela"
0,"getPayloadSpans on org.apache.lucene.search.spans.SpanQuery should be abstractI just spent a long time tracking down a bug resulting from upgrading to Lucene 2.4.1 on a project that implements some SpanQuerys of its own and was written against 2.3.  Since the project's SpanQuerys didn't implement getPayloadSpans, the call to that method went to SpanQuery.getPayloadSpans which returned null and caused a NullPointerException in the Lucene code, far away from the actual source of the problem.  

It would be much better for this kind of thing to show up at compile time, I think.

Thanks!"
0,"Make IndexReader.open() always return MSR to simplify (re-)opens.As per discussion in mailing list, I'm making DirectoryIndexReader.open() always return MSR, even for single-segment indexes.
While theoretically valid in the past (if you make sure to keep your index constantly optimized) this feature is made practically obsolete by per-segment collection.

The patch somewhat de-hairies (re-)open logic for MSR/SR.
SR no longer needs an ability to pose as toplevel directory-owning IR.
All related logic is moved from DIR to MSR.
DIR becomes almost empty, and copying two or three remaining fields over to MSR/SR, I remove it.
Lots of tests fail, as they rely on SR returned from IR.open(), I fix by introducing SR.getOnlySegmentReader static package-private method.
Some previous bugs are uncovered, one is fixed in LUCENE-1645, another (partially fixed in LUCENE-1648) is fixed in this patch. "
0,"Issue LUCENE-352 was closed, but the patch there is not applied in the current trunkSee here:
http://issues.apache.org/jira/browse/LUCENE-352

And thanks for making JIRA easier, I noticed the Lucene Java project
was preselected for me.

Regards,
Paul Elschot"
0,"Add TrieRangeFilter to contribAccording to the thread in java-dev (http://www.gossamer-threads.com/lists/lucene/java-dev/67807 and http://www.gossamer-threads.com/lists/lucene/java-dev/67839), I want to include my fast numerical range query implementation into lucene contrib-queries.

I implemented (based on RangeFilter) another approach for faster
RangeQueries, based on longs stored in index in a special format.

The idea behind this is to store the longs in different precision in index
and partition the query range in such a way, that the outer boundaries are
search using terms from the highest precision, but the center of the search
Range with lower precision. The implementation stores the longs in 8
different precisions (using a class called TrieUtils). It also has support
for Doubles, using the IEEE 754 floating-point ""double format"" bit layout
with some bit mappings to make them binary sortable. The approach is used in
rather big indexes, query times are even on low performance desktop
computers <<100 ms (!) for very big ranges on indexes with 500000 docs.

I called this RangeQuery variant and format ""TrieRangeRange"" query because
the idea looks like the well-known Trie structures (but it is not identical
to real tries, but algorithms are related to it).

"
1,Basic Authentification fails with non-ASCII username/password charactershttp://marc.theaimsgroup.com/?t=106866959500001&r=1&w=2
0,"multilingual analyzer based on icuThe standard analyzer in lucene is not exactly unicode-friendly with regards to breaking text into words, especially with respect to non-alphabetic scripts.  This is because it is unaware of unicode bounds properties.

I actually couldn't figure out how the Thai analyzer could possibly be working until i looked at the jflex rules and saw that codepoint range for most of the Thai block was added to the alphanum specification. defining the exact codepoint ranges like this for every language could help with the problem but you'd basically be reimplementing the bounds properties already stated in the unicode standard. 

in general it looks like this kind of behavior is bad in lucene for even latin, for instance, the analyzer will break words around accent marks in decomposed form. While most latin letter + accent combinations have composed forms in unicode, some do not. (this is also an issue for asciifoldingfilter i suppose). 

I've got a partially tested standardanalyzer that uses icu Rule-based BreakIterator instead of jflex. Using this method you can define word boundaries according to the unicode bounds properties. After getting it into some good shape i'd be happy to contribute it for contrib but I wonder if theres a better solution so that out of box lucene will be more friendly to non-ASCII text. Unfortunately it seems jflex does not support use of these properties such as [\p{Word_Break = Extend}] so this is probably the major barrier.

Thanks,
Robert



"
1,"MOVE method returns error 412 Precondition FailedHi, I was trying MacOS X 10.5 Finder's WebDAV client to do testing on Jackrabbit 1.4 which is hosted on Tomcat 5.5.25 on a Windows XP SP2 computer on a LAN. I encounter an error while doing remote editing, I was able to open the text document, but the problem is I couldn't save it.

I tried to find some log on Tomcat but sadly Jackrabbit didn't produces any log files regarding of my problem. So I used Ethereal 0.99.0 to check the packets from the Windows XP computer. The below trace is a summary from the exported text file of the packet analyzer where the problem occur:-

line 11818:-
No.     Time        Source                Destination           Protocol Info
4352 27.629257   10.60.1.90            10.60.1.187           HTTP     MOVE /jackrabbit-webapp-1.4/repository/default/.TemporaryItems/folders.501/TemporaryItems/(A%20Document%20Being%20Saved%20By%20TextEdit)/Copy%20of%20Request_for_GAMS_User_Account.rtf HTTP/1.1

Frame 4352 (592 bytes on wire, 592 bytes captured)
Ethernet II, Src: AppleCom_72:c3:5e (00:0d:93:72:c3:5e), Dst: 00:19:d1:a0:34:f7 (00:19:d1:a0:34:f7)
Internet Protocol, Src: 10.60.1.90 (10.60.1.90), Dst: 10.60.1.187 (10.60.1.187)
Transmission Control Protocol, Src Port: 64970 (64970), Dst Port: 8080 (8080), Seq: 69060, Ack: 90475, Len: 526
    Source port: 64970 (64970)
    Destination port: 8080 (8080)
    Sequence number: 69060    (relative sequence number)
    Next sequence number: 69586    (relative sequence number)
    Acknowledgement number: 90475    (relative ack number)
    Header length: 32 bytes
    Flags: 0x0018 (PSH, ACK)
    Window size: 524280 (scaled)
    Checksum: 0xd4f9 [correct]
    Options: (12 bytes)
Hypertext Transfer Protocol
    MOVE /jackrabbit-webapp-1.4/repository/default/.TemporaryItems/folders.501/TemporaryItems/(A%20Document%20Being%20Saved%20By%20TextEdit)/Copy%20of%20Request_for_GAMS_User_Account.rtf HTTP/1.1\r\n
        Request Method: MOVE
        Request URI: /jackrabbit-webapp-1.4/repository/default/.TemporaryItems/folders.501/TemporaryItems/(A%20Document%20Being%20Saved%20By%20TextEdit)/Copy%20of%20Request_for_GAMS_User_Account.rtf
        Request Version: HTTP/1.1
    User-Agent: WebDAVFS/1.5 (01508000) Darwin/9.1.0 (Power Macintosh)\r\n
    Accept: */*\r\n
    Destination: http://10.60.1.187:8080/jackrabbit-webapp-1.4/repository/default/au/gov/arc/www/rtf/Copy%20of%20Request_for_GAMS_User_Account.rtf\r\n
    Authorization: Basic YWRtaW46YWRtaW4=\r\n
        Credentials: admin:admin
    Content-Length: 0\r\n
    Connection: keep-alive\r\n
    Host: 10.60.1.187:8080\r\n
    \r\n

line 11850 -
No.     Time        Source                Destination           Protocol Info
4353 27.630345   10.60.1.187           10.60.1.90            HTTP     HTTP/1.1 412 Precondition Failed (text/html)

Frame 4353 (1191 bytes on wire, 1191 bytes captured)
Ethernet II, Src: 00:19:d1:a0:34:f7 (00:19:d1:a0:34:f7), Dst: AppleCom_72:c3:5e (00:0d:93:72:c3:5e)
Internet Protocol, Src: 10.60.1.187 (10.60.1.187), Dst: 10.60.1.90 (10.60.1.90)
Transmission Control Protocol, Src Port: 8080 (8080), Dst Port: 64970 (64970), Seq: 90475, Ack: 69586, Len: 1125
    Source port: 8080 (8080)
    Destination port: 64970 (64970)
    Sequence number: 90475    (relative sequence number)
    Next sequence number: 91600    (relative sequence number)
    Acknowledgement number: 69586    (relative ack number)
    Header length: 32 bytes
    Flags: 0x0018 (PSH, ACK)
    Window size: 65535
    Checksum: 0x1c18 [incorrect, should be 0xa2f0]
    Options: (12 bytes)
Hypertext Transfer Protocol
    HTTP/1.1 412 Precondition Failed\r\n
        Request Version: HTTP/1.1
        Response Code: 412
    Server: Apache-Coyote/1.1\r\n
    Content-Type: text/html;charset=utf-8\r\n
    Content-Length: 965\r\n
    Date: Fri, 29 Feb 2008 02:31:01 GMT\r\n
    \r\n
Line-based text data: text/html
    <html><head><title>Apache Tomcat/5.5.25 - Error report</title><style><!--H1 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#525D76;font-size:22px;} H2 {font-family:Tahoma,Arial,sans-serif;color:white;background-color:#52
"
1,"Large file download over webdav causes exceptionDownloading a large file (>2GB) from webdav causes an exception.

(Note: uploading the file works ok, when jackrabbit is configured to use the filesystem DataStore.)

When trying to retrieve the file with e.g. ""wget"", we get the following error:

Gozer:Desktop greg$ wget --http-user=xxx --http-passwd=xxx http://localhost:8080/jackrabbit/repository/workbench/pkgs/demo/zip/zips/largetest-1.zip
--08:59:50--  http://localhost:8080/jackrabbit/repository/workbench/pkgs/demo/zip/zips/largetest-1.zip
           => `largetest-1.zip'
Resolving localhost... done.
Connecting to localhost[127.0.0.1]:8080... connected.
HTTP request sent, awaiting response... 500 For input string: ""3156213760""
09:04:53 ERROR 500: For input string: ""3156213760"".

In the server log we see this:

06.03.2009 08:59:50 *INFO * RepositoryImpl: SecurityManager = class org.apache.jackrabbit.core.security.simple.SimpleSecurityManager (RepositoryImpl.java, line 432)
2009-03-06 09:04:53.822::WARN:  /jackrabbit/repository/workbench/pkgs/demo/zip/zips/largetest-1.zip
java.lang.NumberFormatException: For input string: ""3156213760""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
	at java.lang.Integer.parseInt(Integer.java:459)
	at java.lang.Integer.parseInt(Integer.java:497)
	at org.apache.jackrabbit.webdav.io.OutputContextImpl.setContentLength(OutputContextImpl.java:60)
	at org.apache.jackrabbit.server.io.ExportContextImpl.informCompleted(ExportContextImpl.java:192)
	at org.apache.jackrabbit.server.io.IOManagerImpl.exportContent(IOManagerImpl.java:157)
	at org.apache.jackrabbit.webdav.simple.DavResourceImpl.spool(DavResourceImpl.java:332)
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.spoolResource(AbstractWebdavServlet.java:422)
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.doGet(AbstractWebdavServlet.java:388)
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.execute(AbstractWebdavServlet.java:229)
	at org.apache.jackrabbit.webdav.server.AbstractWebdavServlet.service(AbstractWebdavServlet.java:196)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:363)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:324)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:864)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:533)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:207)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:403)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)
	at org.mortbay.thread.BoundedThreadPool$PoolThread.run(BoundedThreadPool.java:451)


The problem seems to lie in OutputContextImpl.java it makes the mistake of potentially trying to parse a Long as an Integer, here: http://svn.apache.org/repos/asf/jackrabbit/trunk/jackrabbit-webdav/src/main/java/org/apache/jackrabbit/webdav/io/OutputContextImpl.java

in the method setContentLength(long contentLength):

public void setContentLength(long contentLength) {
       int length = Integer.parseInt(contentLength + """");
       if (length >= 0) {
           response.setContentLength(length);
       }
   }

I'm not sure, but a fix might be like this:

public void setContentLength(long contentLength) {
       if(contentLength <= Integer.MAX_VALUE && contentLength >= 0) {
           response.setContentLength((int) contentLength);
       }else if (contentLength >  Integer.MAX_VALUE) {
            response.addHeader(""Content-Length"", Long.toString(contentLength));
       }
   }

This would at least set the Content-Length header, and in some preliminary tests does seem to allow downloading the files."
0,"Upgrade to PDFBox 0.7.3while trying to upload a PDF document (which I can view fine with Acrobat Reader once it is loaded) I get the following exception: 

01.05.2008 12:24:44 *WARN * PdfTextExtractor: Failed to extract PDF text content (PdfTextExtractor.java, line 91)
java.io.IOException: Error: Expected an integer type, actual='%%EOF'
        at org.pdfbox.pdfparser.BaseParser.readInt(BaseParser.java:1159)
        at org.pdfbox.pdfparser.PDFParser.parseObject(PDFParser.java:349)
        at org.pdfbox.pdfparser.PDFParser.parse(PDFParser.java:132)
        at org.apache.jackrabbit.extractor.PdfTextExtractor.extractText(PdfTextExtractor.java:69)
        at org.apache.jackrabbit.extractor.CompositeTextExtractor.extractText(CompositeTextExtractor.java:90)
        at org.apache.jackrabbit.core.query.lucene.JackrabbitTextExtractor.extractText(JackrabbitTextExtractor.java:195)
        at org.apache.jackrabbit.core.query.lucene.NodeIndexer.addBinaryValue(NodeIndexer.java:393)
 ....

I replaced the version of pdfbox (0.6.4) that is bundled with the jackrabbit war file with a more recent version (0.7.3 and fontbox 01.) and it worked fine. The bundled versions should be upgraded.

On the other hand, this software appears to be inactive. Probably a different package should be selected in the long run, but for now, a simple upgrade will do the trick."
0,"Add ability to open prior commits to IndexReaderIf you use a customized DeletionPolicy, which keeps multiple commits
around (instead of the default which is to only preserve the most
recent commit), it's useful to be able to list all such commits and
then open a reader against one of these commits.

I've added this API to list commits:

  public static Collection IndexReader.listCommits(Directory)

and these two new open methods to IndexReader to open a specific commit:

  public static IndexReader open(IndexCommit)
  public static IndexReader open(IndexCommit, IndexDeletionPolicy)

Spinoff from here:

  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200806.mbox/%3c85d3c3b60806161735o207a3238sa2e6c415171a8019@mail.gmail.com%3e

"
0,"Remove circular dependency between VersionManagerImpl and VersionItemStateProviderFrom a architectural perspective the VersionManagerImpl (VMgr) is at a higher level as the VersionItemStateProvider (VISP). While the VMgr deals with Items the VISP deals with ItemState object. Nonetheless the VISP has a reference to the VMgr and also calls the method setNodeReferences(), which violates the rule of a strictly layered system. E.g. one negative effect of this was a deadlock as described in JCR-672. It also makes it hard to solve JCR-962.

The attached patch includes the following changes:

- Move method VersionManagerImpl.setNodeReferences() VersionItemStateManager. The method can operate on ItemStates only and does not need to be in VersionManagerImpl. As can be seen in the current method it directly calls the PeristenceManager, which indicates it should be located in a lower layer.
- Promote the class VersionItemStateManager to a top level class
- Change method VersionManagerImpl.createSharedStateManager to return a VersionItemStateManager
- Remove VersionManagerImpl instance variable from VersionItemStateProvider
- In VersionItemStateProvider.setNodeReferences() call VersionItemStateManager.setNodeReferences()
- Instead of using the PersistenceManager in VersionManagerImpl.getItemReferences() use the ItemStateManager. It also seems that locking is not necessary for this method."
0,"Add javadoc notes about ICUCollationKeyFilter's advantages over CollationKeyFiltercontrib/collation's ICUCollationKeyFilter, which uses ICU4J collation, is faster than CollationKeyFilter, the JVM-provided java.text.Collator implementation in the same package.  The javadocs of these classes should be modified to add a note to this effect.

My curiosity was piqued by [Robert Muir's comment|https://issues.apache.org/jira/browse/LUCENE-1581?focusedCommentId=12720300#action_12720300] on LUCENE-1581, in which he states that ICUCollationKeyFilter is up to 30x faster than CollationKeyFilter.

I timed the operation of these two classes, with Sun JVM versions 1.4.2/32-bit, 1.5.0/32- and 64-bit, and 1.6.0/64-bit, using 90k word lists of 4 languages (taken from the corresponding Debian wordlist packages and truncated to the first 90k words after a fixed random shuffling), using Collators at the default strength, on a Windows Vista 64-bit machine.  I used an analysis pipeline consisting of WhitespaceTokenizer chained to the collation key filter, so to isolate the time taken by the collation key filters, I also timed WhitespaceTokenizer operating alone for each combination.  The rightmost column represents the performance advantage of the ICU4J implemtation (ICU) over the java.text.Collator implementation (JVM), after discounting the WhitespaceTokenizer time (WST): (JVM-ICU) / (ICU-WST). The best times out of 5 runs for each combination, in milliseconds, are as follows:

||Sun JVM||Language||java.text||ICU4J||WhitespaceTokenizer||ICU4J Improvement||
|1.4.2_17 (32 bit)|English|522|212|13|156%|
|1.4.2_17 (32 bit)|French|716|243|14|207%|
|1.4.2_17 (32 bit)|German|669|264|16|163%|
|1.4.2_17 (32 bit)|Ukranian|931|474|25|102%|
|1.5.0_15 (32 bit)|English|604|176|16|268%|
|1.5.0_15 (32 bit)|French|817|209|17|317%|
|1.5.0_15 (32 bit)|German|799|225|20|280%|
|1.5.0_15 (32 bit)|Ukranian|1029|436|26|145%|
|1.5.0_15 (64 bit)|English|431|89|10|433%|
|1.5.0_15 (64 bit)|French|562|112|11|446%|
|1.5.0_15 (64 bit)|German|567|116|13|438%|
|1.5.0_15 (64 bit)|Ukranian|734|281|21|174%|
|1.6.0_13 (64 bit)|English|162|81|9|113%|
|1.6.0_13 (64 bit)|French|192|92|10|122%|
|1.6.0_13 (64 bit)|German|204|99|14|124%|
|1.6.0_13 (64 bit)|Ukranian|273|202|21|39%|
"
0,"TermOrdVal/DocValuesComparator does too much work in compareBottomWe now have logic to fall back to by-value comparison, when the bottom
slot is not from the current reader.

But this is silly, because if the bottom slot is from a different
reader, it means the tie-break case is not possible (since the current
reader didn't have the bottom value), so when the incoming ord equals
the bottom ord we should always return x > 0.

I added a new random string sort test case to TestSort...

I also renamed DocValues.SortedSource.getByValue -> getOrdByValue and
cleaned up some whitespace.
"
1,"Error logged when repository is shut downThis only happens with the bundle DerbyPersistenceManager.

In DerbyPersistenceManager.close() the embedded derby database is shut down and then super.close() is called. There the ConnectionRecoveryManager is closed, which tries to operate on a connection to the already shut down derby database. The log contains entries like:

25.03.2008 13:49:29 *ERROR* [Thread-5] ConnectionRecoveryManager: failed to close connection, reason: No current connection., state/code: 08003/40000 (ConnectionRecoveryManager.java, line 453)"
1,"Problems mapping custom collectionsWhen using a custom list that extends from java.util.AbstractList, ManageableCollectionUtil.getManageableCollection raises a JcrMappingException because it does not consider the custom list to be a java.util.List. This is because it uses ""if (object.getClass().equals(List.class))"" instead of ""if (object instanceof List)"". The same thing will probably happen when using a custom Collection, a custom ArrayList, etc. This is the stack trace:

org.apache.jackrabbit.ocm.exception.JcrMappingException: Unsupported collection 
type : *********** (MyCustomList class) 
        at org.apache.jackrabbit.ocm.manager.collectionconverter.ManageableColle 
ctionUtil.getManageableCollection(ManageableCollectionUtil.java:153) 
        at org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverte 
rImpl.insertCollectionFields(ObjectConverterImpl.java:780) 
        at org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverte 
rImpl.insert(ObjectConverterImpl.java:221) 
        at org.apache.jackrabbit.ocm.manager.objectconverter.impl.ObjectConverte 
rImpl.insert(ObjectConverterImpl.java:146) 
        at org.apache.jackrabbit.ocm.manager.impl.ObjectContentManagerImpl.inser 
t(ObjectContentManagerImpl.java:407) 

I have come up to this bug using a MyCustomList<MyClass>, with MyCustomList extending java.util.AbstractList<MyClass>."
1,"SharedFieldCache can cause a memory leakThe SharedFieldCache has some problems with the way it builds the cache:
 - as key is has the IndexReader
 - as value it has a inner cache (another map) that has as a key a static inner class called 'Key'.

This 'Key' holds a reference to the comparator used for in the queries ran.
Assuming this comparator is of any type that extends from AbstractFieldComparator (I think all of the custom JR comparators), then it keeps a reference to all the InderReader instances in order to be able to load the values as Comparable(s).

So the circle is complete and the SharedFieldCache entries never get GC'ed.

One option would have been to implement a 'purge' method on the cache, similar to the lucene mechanism, and when an InderReader gets closed is could call 'purge'. But that is both ugly AND is doesn't seem to work that well :)

A more radical option is to remove the cache completely. Each instance of SimpleFieldComparator (the only client of this cache) already builds an array of the available values, so the cache would only help other instances of the same type. We'll not analyze this further.

The proposed solution (patch will follow shortly) is to remove the Comparator reference from the Key class. 
It looks like it has no real purpose there, just to impact the 'equals' of the key, which makes no sense in the first place as the lucene query does not use the Comparator info at all.
If anything, using the same field and 2 different Comparators we'll get 2 different cache entries based on the same values from the lucene index.

Feedback is appreciated!








"
0,"Deprecate Spatial ContribThe spatial contrib is blighted by bugs.  The latest series, found by Grant and discussed [here|http://search.lucidimagination.com/search/document/c32e81783642df47/spatial_rethinking_cartesian_tiers_implementation] shows that we need to re-think the cartesian tier implementation.

Given the need to create a spatial module containing code taken from both lucene and Solr, it makes sense to deprecate the spatial contrib, and start from scratch in the new module.


"
0,"Small imprecision in Search package JavadocsSearch package Javadocs states that Scorer#score(Collector) will be abstract in Lucene 3.0, which is not accurate anymore."
1,"QueryObjectModel does not generate the corresponding SQL2 Query when dealing with spaces in the pathThis is the original issue:
----------
I tried to get the childnodes of a node names ""/a b"" using the following code
  QueryManager queryManager=session.getWorkspace().getQueryManager();
  QueryObjectModelFactory qomf=queryManager.getQOMFactory();
  Source source1=qomf.selector(NodeType.NT_BASE, ""selector_0"");
  Column[] columns = new Column[]{qomf.column(""selector_0"", null, null)};
  Constraint constraint2 = qomf.childNode(""selector_0"", ""/a b"");
  QueryObjectModel qom = qomf.createQuery(source1, constraint2 , null, columns);

This is not giving any result when the session is acquired through webdav. But when connected using JNDI it is giving the child nodes. 

The sql statement getting created is 
SELECT selector_0.* FROM [nt:base] AS selector_0 WHERE ISCHILDNODE(selector_0, 
[/a b]).

When using webdav If i give this SQL2 query directly along with quotes around 
the path i.e. ['/a b'] then it is working as expected.
----------

this doesn't have anything to do with webdav. the problem is the QueryObjectModel generates an SQL2 query that is not 100% equivalent, it fails to escape paths that have spaces in them.
this way, in the case of davex remoting, the jr client will use the statement generated instead, which is not escaped, and will fail to return the expected nodes. 

This can be seen easily if we do a System.out.println(qom.getStatement())


"
1,"Filters need hashCode() and equals()Filters need to implement hashCode() and equals(), esp since certain query types can contain a filter (FilteredQuery, ConstantScoreQuery)"
0,"HttpMethodBase.getResponseBodyAsString(long limit)Currently HttpMethodBase.getResponseBodyAsString() prints warning in log, and suggests using getResponseStream(). However getResponseBodyAsString() is extremely useful (as it is easy to use). So my wish is to have method

getResponseBodyAsString(long limit)

that should throw HttpException if response size exceeds specified limit.

Same things with getResponseBody(long limit) .

Original methods should be deprecated because of danger, explained in javadoc."
1,"JCR2SPI: NPE when parentId returned by NodeInfo.getParentId does not show up in parent's child node listIn this custom SPI implementation, version history nodes appear as children of jcr:versionStorage, but jcr:versionStorage does not return them as children (which would be impractical for performance reasons - I expect similar approaches used by others...).

getParentId of a NodeInfo of a VersionHistory return the NodeId for jcr:versionStorage. In this case, I get the NPE below:

java.lang.NullPointerException
	at org.apache.jackrabbit.jcr2spi.state.WorkspaceItemStateFactory.createNodeState(WorkspaceItemStateFactory.java:99)
	at org.apache.jackrabbit.jcr2spi.state.CachingItemStateManager.resolve(CachingItemStateManager.java:168)
	at org.apache.jackrabbit.jcr2spi.state.CachingItemStateManager.getItemState(CachingItemStateManager.java:94)
	at org.apache.jackrabbit.jcr2spi.WorkspaceManager.getItemState(WorkspaceManager.java:328)
	at org.apache.jackrabbit.jcr2spi.state.TransientISFactory.createNodeState(TransientISFactory.java:120)
	at org.apache.jackrabbit.jcr2spi.state.CachingItemStateManager.resolve(CachingItemStateManager.java:168)
	at org.apache.jackrabbit.jcr2spi.state.CachingItemStateManager.getItemState(CachingItemStateManager.java:94)
	at org.apache.jackrabbit.jcr2spi.state.TransientItemStateManager.getItemState(TransientItemStateManager.java:209)
	at org.apache.jackrabbit.jcr2spi.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:155)
	at org.apache.jackrabbit.jcr2spi.SessionImpl.getNodeById(SessionImpl.java:271)
	at org.apache.jackrabbit.jcr2spi.SessionImpl.getNodeByUUID(SessionImpl.java:239)

Returning null in this special case fixes the problem over here, but seems to create new problems elsewhere.

Need to clarify the SPI itself, and potentially fix JCR2CPI.
"
0,Add example test case for surround query language
0,"Omit positions but keep termFreqit would be useful to have an option to discard positional information but still keep the term frequency - currently setOmitTermFreqAndPositions discards both. Even though position-dependent queries wouldn't work in such case, still any other queries would work fine and we would get the right scoring."
1,"IndexOutOfBoundsException from FieldsReader after problem reading the indexThere is a situation where there is an IOException reading from Hits, and then the next time you get a NullPointerException instead of an IOException.

Example stack traces:

java.io.IOException: The specified network name is no longer available
	at java.io.RandomAccessFile.readBytes(Native Method)
	at java.io.RandomAccessFile.read(RandomAccessFile.java:322)
	at org.apache.lucene.store.FSIndexInput.readInternal(FSDirectory.java:536)
	at org.apache.lucene.store.BufferedIndexInput.readBytes(BufferedIndexInput.java:74)
	at org.apache.lucene.index.CompoundFileReader$CSIndexInput.readInternal(CompoundFileReader.java:220)
	at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:93)
	at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:34)
	at org.apache.lucene.store.IndexInput.readVInt(IndexInput.java:57)
	at org.apache.lucene.index.FieldsReader.doc(FieldsReader.java:88)
	at org.apache.lucene.index.SegmentReader.document(SegmentReader.java:344)
	at org.apache.lucene.index.IndexReader.document(IndexReader.java:368)
	at org.apache.lucene.search.IndexSearcher.doc(IndexSearcher.java:84)
	at org.apache.lucene.search.Hits.doc(Hits.java:104)

That error is fine.  The problem is the next call to doc generates:

java.lang.NullPointerException
	at org.apache.lucene.index.FieldsReader.getIndexType(FieldsReader.java:280)
	at org.apache.lucene.index.FieldsReader.addField(FieldsReader.java:216)
	at org.apache.lucene.index.FieldsReader.doc(FieldsReader.java:101)
	at org.apache.lucene.index.SegmentReader.document(SegmentReader.java:344)
	at org.apache.lucene.index.IndexReader.document(IndexReader.java:368)
	at org.apache.lucene.search.IndexSearcher.doc(IndexSearcher.java:84)
	at org.apache.lucene.search.Hits.doc(Hits.java:104)

Presumably FieldsReader is caching partially-initialised data somewhere.  I would normally expect the exact same IOException to be thrown for subsequent calls to the method.
"
0,"clean up obselete information on the websiteWhen searching for information on 'lucene indexing speed' I get back some really out of date stuff:
1. on the features page it proudly proclaims 20MB/minute, on some really old hardware. I think we should
change this to 95GB/hour: http://blog.mikemccandless.com/2010/09/lucenes-indexing-is-fast.html
2. there are ancient benchmarks results from versioned data we link to the website. We list versioned
websites for ancient versions going back to 1.4.3. Also i noticed when just casually googling for
API documentation I tend to get results going to these ancient versions. I think we should remove
stuff for all versions prior to 2.9"
1,"TestSort testParallelMultiSort reproducible seed failuretrunk r1202157
{code}
    [junit] Testsuite: org.apache.lucene.search.TestSort
    [junit] Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.978 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestSort -Dtestmethod=testParallelMultiSort -Dtests.seed=-2996f3e0f5d118c2:32c8e62dd9611f63:7a90f44586ae8263 -Dargs=""-Dfile.encoding=UTF-8""
    [junit] WARNING: test method: 'testParallelMultiSort' left thread running: Thread[pool-1-thread-1,5,main]
    [junit] WARNING: test method: 'testParallelMultiSort' left thread running: Thread[pool-1-thread-2,5,main]
    [junit] WARNING: test method: 'testParallelMultiSort' left thread running: Thread[pool-1-thread-3,5,main]
    [junit] NOTE: test params are: codec=Lucene40: {short=Lucene40(minBlockSize=98 maxBlockSize=214), contents=PostingsFormat(name=MockSep), byte=PostingsFormat(name=SimpleText), int=Pulsing40(freqCutoff=4 minBlockSize=58 maxBlockSize=186), string=PostingsFormat(name=NestedPulsing), i18n=Lucene40(minBlockSize=98 maxBlockSize=214), long=PostingsFormat(name=Memory), double=Pulsing40(freqCutoff=4 minBlockSize=58 maxBlockSize=186), parser=MockVariableIntBlock(baseBlockSize=88), float=Lucene40(minBlockSize=98 maxBlockSize=214), custom=PostingsFormat(name=MockRandom)}, sim=RandomSimilarityProvider(queryNorm=false,coord=false): {short=BM25(k1=1.2,b=0.75), tracer=DFR I(ne)B2, byte=DFR I(ne)B3(800.0), contents=IB LL-LZ(0.3), int=DFR I(n)BZ(0.3), string=IB LL-D3(800.0), i18n=DFR GB2, double=DFR I(ne)B2, long=DFR GB1, parser=DFR GL2, float=BM25(k1=1.2,b=0.75), custom=DFR I(ne)Z(0.3)}, locale=ga_IE, timezone=America/Louisville
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestSort]
    [junit] NOTE: Linux 3.0.6-gentoo amd64/Sun Microsystems Inc. 1.6.0_29 (64-bit)/cpus=8,threads=4,free=78022136,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testParallelMultiSort(org.apache.lucene.search.TestSort): FAILED
    [junit] expected:<[ZJ]I> but was:<[JZ]I>
    [junit] junit.framework.AssertionFailedError: expected:<[ZJ]I> but was:<[JZ]I>
    [junit]     at org.apache.lucene.search.TestSort.assertMatches(TestSort.java:1245)
    [junit]     at org.apache.lucene.search.TestSort.assertMatches(TestSort.java:1216)
    [junit]     at org.apache.lucene.search.TestSort.runMultiSorts(TestSort.java:1202)
    [junit]     at org.apache.lucene.search.TestSort.testParallelMultiSort(TestSort.java:855)
    [junit]     at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:523)
    [junit]     at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:149)
    [junit]     at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:51)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.search.TestSort FAILED
{code}"
1,LuceneQueryFactory should call QueryHits.close() after running a queryLuceneQueryFactory which is responsible for the JCR_SQL2 implementation does not close QueryHits after running a query.
1,"Locking bugIn org.apache.lucene.store.Lock, line 57 (lucene_1_4_final branch):

if (++sleepCount == maxSleepCount)

is incorrect, the sleepCount is incremented before the compare causing it
throwing the exception with out waiting for at least 1 interation.

Should be changed instead to:
if (sleepCount++ == maxSleepCount)

As this is a self-contained simple fix, I am not submitting a patch.

Thanks

-John"
1,Search results not orderedThe query statements in search.jsp do not have an order by.
0,"Upgrade to Java 5 as the base platformAs discussed on the mailing list, Jackrabbit 2.0 will use Java 5 as the base platform.

Now that 1.x has been branched, we can update the build settings in trunk to use Java 5."
0,"Optimize ReadOnlyIndexReader.read(int[] docs, int[] freqs)This method is currently implemented trivially using next(), doc() and freq(). It should read in blocks and filter out deleted docs."
0,JSR 283 NodeType Management
0,"Use only the standard Maven repository for dependenciesThe JCR API jars are now available in the standard Maven repository, see http://jira.codehaus.org/browse/MAVENUPLOAD-1050. We could thus remove the dependency on the Day repository, as requested in http://jira.codehaus.org/browse/MEV-453.
"
0,"Move multipart request to a new RequestEntity typeMultipart posts are currently handled via a separate post method, the MultipartPostMethod.  This 
separate method is unnecessary given the new RequestEntity mechanism."
1,"Prefix fulltext queries with Japanese or Chinese characters fail to matchPrefix fulltext queries with Japanese or Chinese characters do not match because the prefix part is not tokenized. This means, when the prefix length is >1 the sequence of characters is taken as one term to do the index lookup. This will not match anything because on indexing time such characters are always broken into individual tokens."
0,"port url+email tokenizer to standardtokenizerinterface (or similar)We should do this so that we can fix the LUCENE-3358 bug there, and preserve backwards.
We also want this mechanism anyway, for upgrading to new unicode versions in the future.

We can regenerate the new TLD list for 3.4 but, we should ensure the existing one is used for the urlemail33 or whatever,
so that its exactly the same."
0,"JCR2SPI: remove duplicate item statesthe original approach with duplicate item state objects connected to each is not required any more 
and can be simplified."
1,"DefaultRequestDirector converts redirects of PUT/POST to GET for status codes 301, 302, 307The DefaultRequestDirector treats redirect requests created by all redirect status codes (HttpStatus.SC_MOVED_TEMPORARILY: , HttpStatus.SC_MOVED_PERMANENTLY, HttpStatus.SC_SEE_OTHER, HttpStatus.SC_TEMPORARY_REDIRECT) the same, converting PUT/POST methods to GET.  The HttpClient Tutorial even documents this as being in accordance with the specification, but I don't believe that's true.

Per the RFC (http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html), conversion of PUT/POST to GET is appropriate only for 303 (See Other).  The others do not suggest this behavior.  In fact, the following notes attached to them call it out as incorrect.

301 (Moved Permanently) has this note:

      Note: When automatically redirecting a POST request after
      receiving a 301 status code, some existing HTTP/1.0 user agents
      will erroneously change it into a GET request.

And 302 (Found) say this:

      Note: RFC 1945 and RFC 2068 specify that the client is not allowed
      to change the method on the redirected request.  However, most
      existing user agent implementations treat 302 as if it were a 303
      response, performing a GET on the Location field-value regardless
      of the original request method. The status codes 303 and 307 have
      been added for servers that wish to make unambiguously clear which
      kind of reaction is expected of the client.

The currently implemented behavior is causing problems with interacting with Central Authentication Service protected resources, among other things."
0,"Move some *TermsEnum.java from oal.search to oal.indexI think FilteredTermsEnum, SingleTermsEnum should move?

I left TermRangeTermsEnum and FuzzyTermsEnum and PrefixTermsEnum since they seemed search specific."
1,"Preemptive auth flags disregarded during ssl tunnel creationUsing a Squid2.4 proxy, the connection is dropped when trying to connect to a 
ssl site. In order for the connection to remain open, preemptive authorization 
is needed for the proxy. The preemptive authorization flags are not propagated 
down to where the ssl tunnel is created in HttpMethodDirectors executeConnect 
method. A new ConnectMethod object is created for the tunnel but the preemptive 
flags set as parameters are not being set on the new ConnectMethod object.

Here is the code that would replicate the problem using a Squid(2.4) proxy :

HttpClient client = new HttpClient();
client.getHostConfiguration().setProxyHost(new ProxyHost(""someproxy"", 3128));
client.getParams().setAuthenticationPreemptive(true);
client.getState().setProxyCredentials(AuthScope.ANY, new 
UsernamePasswordCredentials(""user"", ""password""));
GetMethod httpget = new GetMethod(""https://www.verisign.com/"");
httpget.getProxyAuthState().setPreemptive();
client.executeMethod(httpget);
httpget.releaseConnection();"
1,"TestParser.testSpanTermXML fails with some simshere is why this test sometimes fails (my explanation in the test i wrote):

{noformat}
  /** make sure all sims work with spanOR(termX, termY) where termY does not exist */
  public void testCrazySpans() throws Exception {
    // The problem: ""normal"" lucene queries create scorers, returning null if terms dont exist
    // This means they never score a term that does not exist.
    // however with spans, there is only one scorer for the whole hierarchy:
    // inner queries are not real queries, their boosts are ignored, etc.
{noformat}

Basically, SpanQueries aren't really queries, you just get one scorer. it calls extractTerms on the whole hierarchy and computes weights (e.g. IDF) on
the whole bag of terms, even if they don't exist.

This is fine, we already have tests that sim's won't bug-out in computeStats() here: however they don't expect to actually score documents based on
these terms that don't exist... however this is exactly what happens in Spans because it doesn't use sub-scorers.

Lucene's sim avoids this with the (docFreq + 1)
"
1,"benchmark cannot parse highlight-vs-vector-highlight.alg, but only on 3.x?!A new test (TestPerfTasksParse.testParseExamples) was added in LUCENE-3768 that 
guarantees all .alg files in the conf/ directory can actually be parsed...

But highlight-vs-vector-highlight.alg cannot be parsed on 3.x (NumberFormatException), 
however it works fine on trunk... and the .alg is exactly the same in both cases.

{noformat}
    [junit] ------------- Standard Error -----------------
    [junit] java.lang.NumberFormatException: For input string: ""maxFrags[3.0],fields[body]""
    [junit] 	at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1222)
    [junit] 	at java.lang.Float.parseFloat(Float.java:422)
    [junit] 	at org.apache.lucene.benchmark.byTask.tasks.SearchTravTask.setParams(SearchTravTask.java:76)
    [junit] 	at org.apache.lucene.benchmark.byTask.tasks.SearchTravRetVectorHighlightTask.setParams(SearchTravRetVectorHighlightTask.java:124)
    [junit] 	at org.apache.lucene.benchmark.byTask.utils.Algorithm.<init>(Algorithm.java:112)
    [junit] 	at org.apache.lucene.benchmark.byTask.TestPerfTasksParse.testParseExamples(TestPerfTasksParse.java:132)
{noformat}
"
0,"Occur incompletely implemented for remote use.Occur does not implement readResolve() creating problems for
ParallelMultiSearcher y."
1,"trectopicsreader doesn't properly read descriptions or narrativesTrecTopicsReader does not read these fields correctly, as demonstrated by the test case.
"
1,"Rare thread hazard in IndexWriter.commit()The nightly build 2 nights ago hit this:

{code}
 NOTE: random seed of testcase 'testAtomicUpdates' was: -5065675995121791051
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAtomicUpdates(org.apache.lucene.index.TestAtomicUpdate):	FAILED
    [junit] expected:<100> but was:<91>
    [junit] junit.framework.AssertionFailedError: expected:<100> but was:<91>
    [junit] 	at org.apache.lucene.index.TestAtomicUpdate.runTest(TestAtomicUpdate.java:142)
    [junit] 	at org.apache.lucene.index.TestAtomicUpdate.testAtomicUpdates(TestAtomicUpdate.java:194)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)
{code}

It's an intermittant failure that only happens when multiple threads
are calling commit() at once.  With autoComit=true and
ConcurrentMergeScheduler, this can happen more often because each
merge thread calls commit after it's done.

The problem happens when one thread has already begun the commit
process, but another two or more threads then come along wanting to
also commit after further changes have happened.  Those two or more
threads would wait until the currently committing thread finished, and
then they'd wake up and do their commit.  The problem was, after
waking up they would fail to check whether they had been superseded,
ie whether another thread had already committed more up-to-date
changes.

The fix is simple -- after waking up, check again if your commit has
been superseded, and skip your commit if so.
"
1,"IndexWriter.optimize(boolean doWait) ignores doWait parameter{{IndexWriter.optimize(boolean doWait)}} ignores the doWait parameter and always calls {{optimize(1, true)}}.

That does not seem to be the intended behavior, based on the doc comment."
1,"import must not ignore xml prefixed attributesXML import currently ignores attributes that are in the xml namespace.
e.g., DocViewImportHandler's startElement():

                if (atts.getQName(i).startsWith(""xml:"")) {
                    // skipping xml:space, xml:lang, etc.
                    log.debug(""skipping reserved/system attribute "" + atts.getQName(i));
                    continue;
                }

That is a significant loss of information, since xml:base, xml:lang, and xml:id attributes are critical to the content.  We should register the xml prefix as a reserved namespace (not needing an xmlns declaration) and then treat it like any other attribute.

Here are some useful XML examples:

http://xformsinstitute.com/essentials/browse/ch03s02.php
http://www.zvon.org/HowTo/Output/
http://www.w3.org/Math/testsuite/testsuite/TortureTests/Complexity/complex1.xml
http://intertwingly.net/wiki/pie/EchoExample
http://support.sas.com/onlinedoc/913/getDoc/en/engxml.hlp/a002973381.htm

"
0,Jcr-Server Module: Remove Dependency from Jackrabbit-Core
0,Generate jar containing test classes.The test classes are useful for writing unit tests for code external to the Lucene project. It would be helpful to build a jar of these classes and publish them as a maven dependency.
0,"Create resource sensitive cache for item statesthere is currently a lru-caching strategy for the itemstates in the shared ism, with a hardcoded limit of 1000 entries. the problem is that the size of the states is not respected in the caching strategy; this poses a problem, if the
states are large (i.e. large values in property states, or large number of childnode entries)."
0,"Make collecting group membership information lazyJCR-2710 added a more scalable content model for storing group membership information. To further leverage the new model it would be preferable when group membership collecting where lazy. (i.e. Group#getDeclaredMembers() and Group#getMembers() should not construct the list of all members up front). 
"
0,"Remove code duplication from Token class, just extend TermAttributeImplThis issue removes the code duplication from Token, as it shares the whole char[] buffer handling code with TermAttributeImpl. This issue removes this duplication by just extending TermAttributeImpl.

When the parent issue LUCENE-2302 will extend TermAttribute to support CharSequence and Appendable and also the new BytesRefAttribute gets added, Token will automatically provide this too, so no further code duplication.

This code should also be committed to trunk, as it has nothing to do with flex."
1,Session#move doesn't trigger rebuild of parent node aggregationThe summary says it all.
1,"XercesImpl is missing in WebDav contrib project$ /usr/local/maven/bin/maven
 __  __
|  \/  |__ _Apache__ ___
| |\/| / _` \ V / -_) ' \  ~ intelligent projects ~
|_|  |_\__,_|\_/\___|_||_|  v. 1.0.2

build:start:

multiproject:install:
multiproject:projects-init:
    [echo] Gathering project list
Starting the reactor...
Our processing order:
JCRWebdavServer Webdav Library
JCRWebdavServer Server Library
JCRWebdavServer Client Library
JCRWebdavServer WebApplication
+----------------------------------------
| Gathering project list JCRWebdavServer Webdav Library
| Memory: 3M/4M
+----------------------------------------
+----------------------------------------
| Gathering project list JCRWebdavServer Server Library
| Memory: 3M/4M
+----------------------------------------
+----------------------------------------
| Gathering project list JCRWebdavServer Client Library
| Memory: 3M/4M
+----------------------------------------
+----------------------------------------
| Gathering project list JCRWebdavServer WebApplication
| Memory: 3M/4M
+----------------------------------------
Starting the reactor...
Our processing order:
JCRWebdavServer Webdav Library
JCRWebdavServer Server Library
JCRWebdavServer Client Library
JCRWebdavServer WebApplication
+----------------------------------------
| Executing multiproject:install-callback JCRWebdavServer Webdav Library
| Memory: 3M/4M
+----------------------------------------
Attempting to download jackrabbit-commons-1.0-SNAPSHOT.jar.
Response content length is not known
Artifact /org.apache.jackrabbit/jars/jackrabbit-commons-1.0-SNAPSHOT.jar doesn't exists in remote repository, but it exists locally

multiproject:goal:
build:start:

multiproject:install-callback:
    [echo] Running jar:install for JCRWebdavServer Webdav Library
java:prepare-filesystem:

java:compile:
    [echo] Compiling to /home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/target/classes
    [javac] Compiling 109 source files to /home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/target/classes
/home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/src/java/org/apache/jackrabbit/webdav/WebdavResponseImpl.java:26: package org.apache.xml.serialize does not exist
import org.apache.xml.serialize.OutputFormat;
                                ^
/home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/src/java/org/apache/jackrabbit/webdav/WebdavResponseImpl.java:27: package org.apache.xml.serialize does not exist
import org.apache.xml.serialize.XMLSerializer;
                                ^
/home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/src/java/org/apache/jackrabbit/webdav/WebdavResponseImpl.java:149: cannot resolve symbol
symbol  : class OutputFormat 
location: class org.apache.jackrabbit.webdav.WebdavResponseImpl
                OutputFormat format = new OutputFormat(""xml"", ""UTF-8"", true);
                ^
/home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/src/java/org/apache/jackrabbit/webdav/WebdavResponseImpl.java:149: cannot resolve symbol
symbol  : class OutputFormat 
location: class org.apache.jackrabbit.webdav.WebdavResponseImpl
                OutputFormat format = new OutputFormat(""xml"", ""UTF-8"", true);
                                          ^
/home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/src/java/org/apache/jackrabbit/webdav/WebdavResponseImpl.java:150: cannot resolve symbol
symbol  : class XMLSerializer 
location: class org.apache.jackrabbit.webdav.WebdavResponseImpl
                XMLSerializer serializer = new XMLSerializer(out, format);
                ^
/home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/src/java/org/apache/jackrabbit/webdav/WebdavResponseImpl.java:150: cannot resolve symbol
symbol  : class XMLSerializer 
location: class org.apache.jackrabbit.webdav.WebdavResponseImpl
                XMLSerializer serializer = new XMLSerializer(out, format);
                                               ^
Note: Some input files use or override a deprecated API.
Note: Recompile with -deprecation for details.
6 errors

BUILD FAILED
File...... /home/jeremi/.maven/cache/maven-multiproject-plugin-1.3.1/plugin.jelly
Element... maven:reactor
Line...... 217
Column.... -1
Unable to obtain goal [multiproject:install-callback] -- /home/jeremi/.maven/cache/maven-java-plugin-1.5/plugin.jelly:63:-1: <ant:javac> Compile failed; see the compiler error output for details.
Total time: 8 seconds
Finished at: Fri Jan 27 07:14:37 CET 2006

$"
0,"Flex on non-flex emulation of TermsEnum incorrectly seeks/nexts beyond current fieldSpinoff of LUCENE-2111, where Uwe found this issue with the flex on non-flex emulation."
1,"suggest.fst.Sort.BufferSize should not automatically fail just because of freeMemory()Follow up op dev thread: [FSTCompletionTest failure ""At least 0.5MB RAM buffer is needed"" | http://markmail.org/message/d7ugfo5xof4h5jeh]"
0,"Add Thread-Safety note to IndexWriter JavaDocIndexWriter Javadocs should contain a note about thread-safety. This is already mentioned on the wiki FAQ page but such an essential information should be part of the module documentation too.
"
1,"spi2davex: session-scoped lock tokens not included in if-headerdetected while running API lock tests.
org.apache.jackrabbit.test.api.lock.DeepLockTest#testParentChildDeepLock failed though it used to work with spi2dav.

fix is simple: SessionInfoImpl.getAllLockTokens must be used to populate the if-header as it is done in spi2dav."
0,"add a test for PorterStemFilterThere are no tests for PorterStemFilter, yet svn history reveals some (very minor) cleanups, etc.
The only thing executing its code in tests is a test or two in SmartChinese tests.

This patch runs the StemFilter against Martin Porter's test data set for this stemmer, checking for expected output.

The zip file is 100KB added to src/test, if this is too large I can change it to download the data instead.
"
0,Javadoc improvements for Payload classSome methods in org.apache.lucene.index.Payload don't have javadocs
0,"NTLM implementation lacks support for NTLMv1, NTLMv2, and NTLM2 Session forms of NTLMThe current HttpClient implementation lacks support for all enhancements to NTLM after Windows 95.  That includes NTLMv1, NTLMv2, and NTLM2 Session Response varieties of the protocol.

This seriously impacts the usability of HttpClient in enterprise situations, which has required the Lucene Connector Framework team to extend HttpClient to address the issue.

I've attached a patch which contains the implementation used by LCF.
"
0,"GData-Server - Website sandbox partAdded GData-Server to the sandbox part of the website -- xdocs/sandbox/

Build of website is fine."
1,"jcr:frozenUuid does not contain jcr:contentWhen I store versionable files, I get problems retrieving the jcr:data from a custom node type.

I am storing a node type:

xrc:learningContent
        pd: xrc:Keywords
        pd: xrc:MimeType
        pd: jcr:mixinTypes
        pd: xrc:Description
        pd: xrc:Language
        pd: xrc:Creator
        pd: jcr:created
        pd: xrc:Title
        pd: jcr:primaryType
Extends: nt:resource
        pd: jcr:uuid
        pd: jcr:mixinTypes
        pd: jcr:data
        pd: jcr:encoding
        pd: jcr:mimeType
        pd: jcr:lastModified
        pd: jcr:primaryType

So I commit the changes, then later pull up the version and get it's frozenNode.

Node frozenNode = v.getNode(JcrConstants.JCR_FROZENNODE);

And then I return all of the properties contained within:

PropertyIterator pi = frozenNode.getProperties();
                while (pi.hasNext()) {
                    System.out.println(pi.nextProperty().getName());
}


All that are returned are:

jcr:frozenUuid
jcr:uuid
jcr:frozenPrimaryType
jcr:frozenMixinTypes
jcr:primaryType

Here is the frozen node type:

nt:frozenNode
        pd: *
        pd: *
        pd: jcr:frozenUuid
        pd: jcr:uuid
        pd: jcr:mixinTypes
        pd: jcr:frozenPrimaryType
        pd: jcr:frozenMixinTypes
        pd: jcr:primaryType



So basically it would seem that the recursive copy inside the InternalFrozenNodeImpl is not working. But it seems that is not the case from the code trace I did. Add this to line 368 of InternalFrozenNodeImpl.java

System.out.println(""New node created. Props: "");
        try {
            PropertyState [] ps = node.getProperties();
            for (PropertyState p : ps) {
                System.out.println(p.getName());
                System.out.println(p.toString());
            }
            NodeStateEx [] ns = node.getChildNodes();
            for (NodeStateEx n : ns) {
                System.out.println(n.getName());
                System.out.println(n.toString());
            }
        } catch (ItemStateException e) {
            // TODO Auto-generated catch block
            e.printStackTrace();
        }


And you will get the result:


New node created. Props:
{http://www.jcp.org/jcr/1.0}uuid
org.apache.jackrabbit.core.state.PropertyState@10dd791
{http://www.jcp.org/jcr/1.0}frozenPrimaryType
org.apache.jackrabbit.core.state.PropertyState@1c38291
{http://www.jcp.org/jcr/1.0}frozenMixinTypes
org.apache.jackrabbit.core.state.PropertyState@b12fbb
{http://www.jcp.org/jcr/1.0}baseVersion
org.apache.jackrabbit.core.state.PropertyState@b4d4b6
{http://www.jcp.org/jcr/1.0}primaryType
org.apache.jackrabbit.core.state.PropertyState@1f9045f
{http://www.jcp.org/jcr/1.0}isCheckedOut
org.apache.jackrabbit.core.state.PropertyState@18e16b5
{http://www.jcp.org/jcr/1.0}frozenUuid
org.apache.jackrabbit.core.state.PropertyState@174cb00
{http://www.jcp.org/jcr/1.0}predecessors
org.apache.jackrabbit.core.state.PropertyState@bb7c1b
{http://www.jcp.org/jcr/1.0}data
org.apache.jackrabbit.core.state.PropertyState@d10133
{http://www.jcp.org/jcr/1.0}versionHistory
org.apache.jackrabbit.core.state.PropertyState@1a5f001
{http://www.jcp.org/jcr/1.0}encoding
org.apache.jackrabbit.core.state.PropertyState@12fe3ef
{http://www.jcp.org/jcr/1.0}mimeType
org.apache.jackrabbit.core.state.PropertyState@11d92c8
{http://www.jcp.org/jcr/1.0}lastModified
org.apache.jackrabbit.core.state.PropertyState@8fb83a
New node created. Props:
{http://www.xerceo.com/learn/jcr-1.0}Keywords
org.apache.jackrabbit.core.state.PropertyState@18808f3
{http://www.jcp.org/jcr/1.0}uuid
org.apache.jackrabbit.core.state.PropertyState@397a4
{http://www.jcp.org/jcr/1.0}frozenPrimaryType
org.apache.jackrabbit.core.state.PropertyState@1d88ffd
{http://www.xerceo.com/learn/jcr-1.0}Creator
org.apache.jackrabbit.core.state.PropertyState@d5625b
{http://www.xerceo.com/learn/jcr-1.0}Language
org.apache.jackrabbit.core.state.PropertyState@12c70e6
{http://www.xerceo.com/learn/jcr-1.0}Title
org.apache.jackrabbit.core.state.PropertyState@a836b3
{http://www.jcp.org/jcr/1.0}frozenMixinTypes
org.apache.jackrabbit.core.state.PropertyState@19f273c
{http://www.jcp.org/jcr/1.0}primaryType
org.apache.jackrabbit.core.state.PropertyState@1c8e97d
{http://www.jcp.org/jcr/1.0}frozenUuid
org.apache.jackrabbit.core.state.PropertyState@15915a3
{http://www.jcp.org/jcr/1.0}predecessors
org.apache.jackrabbit.core.state.PropertyState@19ba907
{http://www.xerceo.com/learn/jcr-1.0}MimeType
org.apache.jackrabbit.core.state.PropertyState@763ca1
{http://www.xerceo.com/learn/jcr-1.0}Description
org.apache.jackrabbit.core.state.PropertyState@8687e8
{http://www.jcp.org/jcr/1.0}versionHistory
org.apache.jackrabbit.core.state.PropertyState@44ca0f
{http://www.jcp.org/jcr/1.0}content
org.apache.jackrabbit.core.version.NodeStateEx@2da721

So the new Node definately has these new properties.

Do I have to somehow extend my frozenNode to work with this? Can anyone help me?"
0,"Allow readOnly OpenReader taskI'd like to change OpenReader in contrib/benchmark to open a readOnly reader by default, and take readOnly optional param if for some reason a ""writable IndexReader"" becomes necessary in the future."
0,"Fix incorrect IndexingQueueTest logicThe IndexingQueueTest class assumes that a Session.save() call will push all pending text extraction tasks to the indexing queue, when in fact those can still be kept waiting in the VolatileIndex."
1,"ConcurrentModificationException in QueryStatImplRunning with qurystats enabled the Query#execute can throw ConcurrentModificationException

caused by the iterator which backing collection is changed from another thread

see logQuery method
        Iterator<QueryStatDtoImpl> iterator = popularQueries.iterator();
        while (iterator.hasNext()) {
-->            QueryStatDtoImpl qsdi = iterator.next();
            if (qsdi.equals(qs)) {
                qs.setOccurrenceCount(qsdi.getOccurrenceCount() + 1);
                iterator.remove();
                break;
            }
        }
        popularQueries.offer(qs);
"
1,"NullPointerException in NegotiateScheme- server is configured to allow client to authenticate with kerberos with principal foobar
- client, using httpclient with a registered authscheme SPNEGO set as a NegotiateSchemeFactory

- when the client authenticate with the (correct) principal foobar, it works !
- when the client authenticate with the (wrong) principal fooba, it fails with a NPE below.


Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.commons.codec.binary.Base64.encodeBase64(Base64.java:233)
	at org.apache.commons.codec.binary.Base64.encode(Base64.java:521)
	at org.apache.http.impl.auth.NegotiateScheme.authenticate(NegotiateScheme.java:240)
	at org.apache.http.client.protocol.RequestTargetAuthentication.process(RequestTargetAuthentication.java:99)
	at org.apache.http.protocol.ImmutableHttpProcessor.process(ImmutableHttpProcessor.java:108)
	at org.apache.http.protocol.HttpRequestExecutor.preProcess(HttpRequestExecutor.java:167)
	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:460)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:689)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:624)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:602)
"
0,Codec is not consistently passed in internal APIWhile working on SOLR-1942 I ran into a couple of glitches with codec which is not consistently passed to SegmentsInfo and friends. Codecs should really be consistently passed though. I have fixed the pieces which lead to errors in Solr but I  guess there might be others too. Patch is coming up... 
0,"Add ""tokenize documents only"" task to contrib/benchmarkI've been looking at performance improvements to tokenization by
re-using Tokens, and to help benchmark my changes I've added a new
task called ReadTokens that just steps through all fields in a
document, gets a TokenStream, and reads all the tokens out of it.

EG this alg just reads all Tokens for all docs in Reuters collection:

  doc.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersDocMaker
  doc.maker.forever=false
  {ReadTokens > : *
"
1,"Database Data Store: close result setsThe database data store doesn't close one result sets. This is not a problem for most databases, but anyway should be fixed."
1,"Data Store: UTFDataFormatException when using large minRecordLengthIf using a value larger than 33000 for minRecordLength, and then trying to store a value with 33000 bytes, the following exception is thrown: UTFDataFormatException. The reason is that values are serialized using DataOutputStream.writeUTF. There is size limitation of 65 K when using this method. Small entries are hex encoded, and there is a prefix, so the limitation for minRecordLength should be 32000.

This is a problem for both FileDataStore and DbDataStore.
"
0,"Misleading exception message for jcr:deref()If the type of the second argument in a jcr:deref() function is not a String an InvalidQueryException is thrown with a misleading message: ""Wrong second argument type for jcr:like""

It should be rather something like: ""Second argument for jcr:deref must be a String"""
0,Remove @author tags in jackrabbit-jcr-rmiIt is a recommendation within Apache not to use @author tags or other means to identify source code with individual developers.  The @author tags in jackrabbit-jcr-rmi should therefore be removed.
1,"Core: WEAKREFERENCE properties object have type REFERENCE when being read from the persistent layerit seems to me that WEAKREFERENCE properties are properly created and stored as such but are read as REFERENCE 
properties when built again from the persistent layer.

how to reproduce:

- create a new WEAKREFERENCE property and save the changes
- force reading from the persistent layer  (in my case I used Day's CRX and restartet the server)
- the former WEAKREFERENCE will now be displayed as REFERENCE.

"
0,"NodeTypeRegistry could auto-subtype from nt:basethis is basically a copy of JCR-433, which was fixed but somehow sneaked in again:

when tying to register a (primary) nodetype that does not extend from nt:base the following error is
thrown:

""all primary node types except nt:base itself must be (directly or indirectly) derived from nt:base""

since the registry is able to detect this error, it would be easy to auto-subtype all nodetypes from nt:base. 
imo it's pointless to explicitly add the nt:base to every superclass set. as an analogy, you don't need to 
'extend from java.lang.Object' explicitly - the compiler does that automatically for your."
1,"Jcr2Spi: ExportSysViewTest#testExportSysView_handler_session_saveBinary_* occasionally failingfrom time to time i saw ExportSysViewTest#testExportSysView_handler_session_saveBinary_* test failing. this doesn't occur consistently and i never managed to reproduce it when running the tests in the idea.
"
1,"Workspace operations (copy/clone) do not handle references correctlyREFERENCE properties created through Workspace.copy() or Workspace.clone() are not reflected by 
Node.getReferences() and are as a consequence not enforced."
1,"NPE if you open IW with CREATE on an index with no segments fileI have a simple test case that hits this NPE:

{noformat}
    [junit] java.lang.NullPointerException
    [junit] 	at java.io.File.<init>(File.java:305)
    [junit] 	at org.apache.lucene.store.NIOFSDirectory.openInput(NIOFSDirectory.java:67)
    [junit] 	at org.apache.lucene.store.FSDirectory.openInput(FSDirectory.java:333)
    [junit] 	at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:213)
    [junit] 	at org.apache.lucene.index.IndexFileDeleter.<init>(IndexFileDeleter.java:218)
    [junit] 	at org.apache.lucene.index.IndexWriter.<init>(IndexWriter.java:1113)
    [junit] 	at org.apache.lucene.index.TestIndexWriter.testNoSegmentFile(TestIndexWriter.java:4975)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:277)
{noformat}

It happens if you have an aborted index, ie, there are segment files in there (*.frq, *.tis, etc.) but no segments_N file, and then you try to open an IW with CREATE on that index."
0,"support for DB2 in BundleDbPersistenceManagerBundleDbPersistenceManager doesn't work with DB2, db2.ddl file is missing.I've created the database scheme for DB2."
1,"LuceneTaxonomyReader .decRef() may close the inner IR, renderring the LTR in a limbo.TaxonomyReader which supports ref-counting, has a decRef() method which delegates to an inner IndexReader and calls its .decRef(). The latter may close the reader (if the ref is zeroes) but the taxonomy would remain 'open' which will fail many of its method calls.

Also, the LTR's .close() method does not work in the same manner as IndexReader's - which calls decRef(), and leaves the real closing logic to the decRef(). I believe this should be the right approach for the fix."
0,"Improve the documentation of VersionIn my opinion, we should elaborate more on the effects of changing the Version parameter.
Particularly, changing this value, even if you recompile your code, likely involves reindexing your data.
I do not think this is adequately clear from the current javadocs.
"
0,"Restructure the Jackrabbit source treeReintroduce some of the changes in JCR-157 as a more general restructuring to simplify the Jackrabbit project structure. See http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/9170/ for the rationale and discussion. The main parts of this restructuring would be:

1. Create a Jackrabbit ""super-project"" (artifactId: jackrabbit) in trunk/

2. Use the super-project POM as the parent of all Jackrabbit component POMs

3. Move the contents of trunk/jackrabbit/src/site directly to trunk/src/site, and use the super-project to generate the web site

4. Create independent subprojects for the the jackrabbit-api and jackrabbit-commons components, moving the the corresponding parts of the source tree

5. Move the jcr-server subprojects on level up

6. Rename the subproject directories to match their artifactIds

Note that this restructuring depends on JCR-611 and JCR-332, since the best way to implement this by utilizing a snapshot repository for the component dependencies."
1,TestNRTManager test failurereproduces for me
0,Consolidate ItemDef/QItemDefinitionThere is a great deal of duplicate code in ItemDef (jackrabbit-core) and QItemDefinition (jackrabbit-spi) and their implementing classes.
0,"Database connection poolingJackrabbit should use database connection pools instead of a single connection per persistence manager, cluster journal, or database data store."
1,"Garbage collection deletes temporary files in FileDataStoreIn FileDataStore.addRecord(InputStream), a temporary file is created. The data is written to the file and then it is moved to its final location (based on the contents hash).

If the garbage collector runs whilst this temp file is present, it deletes it (on Solaris 10 at least), and the addRecord fails at the attempt to rename the now non-existent temp file.

I am attaching a minimal patch that prevents these temp files being deleted by deleteOlderRecursive(..), regardless of their lastModified() value.

I have made this a Minor priority, since there is the obvious workaround of disabling the GC.
"
1,"NPE in EventStateCollectionWhen removing a Version with a versionlabel and restoring an other Version from the same containing history within 1 transaction, a NPE occured. When debugging I noticed the method createEventStates was entered with an UUID from a versionLabel. The ChangeLog.get(id) returned null.

Caused by: java.lang.NullPointerException
	at org.apache.jackrabbit.core.observation.EventStateCollection.getNodeType(EventStateCollection.java:614)
	at org.apache.jackrabbit.core.observation.EventStateCollection.createEventStates(EventStateCollection.java:381)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:697)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:1085)
	at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:163)
	at org.apache.jackrabbit.core.version.XAVersionManager.prepare(XAVersionManager.java:509)
	at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:154)
	at org.apache.jackrabbit.core.XASessionImpl.prepare(XASessionImpl.java:331)
	at org.springmodules.jcr.jackrabbit.support.JackRabbitUserTransaction.commit(JackRabbitUserTransaction.java:100)
	at org.springmodules.jcr.jackrabbit.LocalTransactionManager.doCommit(LocalTransactionManager.java:192)"
0,"Allow SnapshotDeletionPolicy to be reused across writer close/openIf you re-use the same instance of SnapshotDeletionPolicy across a
close/open of your writer, and you had a snapshot open, it can still
be removed when the 2nd writer is opened.  This is because SDP is
comparing IndexCommitPoint instances.

The fix is to instead compare segments file names.

I've also changed the inner class IndexFileDeleter.CommitPoint to be
static so an instance of SnapshotDeletionPolicy does not hold
references to IndexFileDeleter, DocumentsWriter, etc.

Spinoff from here:

  http://markmail.org/message/bojgqfgyxkkv4fyb
"
0,"Move Query.weight() to IndexSearcher as protected methodWe had this issue several times, latest in LUCENE-3207.

The method Query.weight() was left in Query for backwards reasons in Lucene 2.9 when we changed Weight class. This method is only to be called on top-level queries - and this is done by IndexSearcher. This method is just a utility method, that has nothing to do with the query itsself (it just combines the createWeight method and calls the normalization afterwards). 

The problem we have is that any query that wraps other queries (like CustomScore, ConstantScore, Boolean) calls Query.weight() instead of Query.createWeight(), it will do normalization two times, leading to strange bugs.

For 3.3 I will make Query.weight() simply delegate to IndexSearcher's replacement method with a big deprecation warning, so user sees this. In IndexSearcher itsself the method will be protected to only be called by itsself or subclasses of IndexSearcher. Delegation for backwards is no problem, as protected is accessible by classes in same package.

I would suggest the method name to be IndexSearcher.createNormalizedWeight(Query q)"
1,"Text.unescape() should should preserve 'unicode' charactersWhen an input to Text.unescape() contains characters > \u00ff, the most significant byte is lost resulting in garbled output. The unescape() function should preserve such characters in order to be useful to decode Internationalized Resource Identifiers (RFC 3987). "

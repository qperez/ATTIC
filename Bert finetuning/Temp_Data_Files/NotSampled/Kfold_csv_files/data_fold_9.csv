label,summmarydescription
0,Remove ItemInfo.getName() since it is redundantI propose to remove the method getName() from org.apache.jackrabbit.spi.ItemInfo since it is redundant. The name is always the last element of the path which is available via the getPath() method.
0,"Move backwards compatibility tests to trunkAs discussed on dev@, I'd like to move the backwards compatibility tests from the Jackrabbit sandbox to trunk."
0,"CompactNodeTypeReader fails to explain why valid JCR names cause errorsfor example, you cannot use underscores in node type definitions:
[my:example_breaks2]

In fact only A-Z, a-z, 0-9, : are allowed, unless you quote the name. The error message you see when you make this mistake doesn't give any hint:
Missing ']' delimiter for end of node type name (nodetypes.cnd, line 8)

and the documentation on the website and the javadoc for CompactNodeTypeDefReader both just say:

 * unquoted_string ::= ...a string...

... not helpful. If you made this mistake, you end up needing to look at the source to figure out what you've done wrong. 

A few suggested solutions:
- change the documentation to say unquoted string is '[A-Za-z0-9:]+'
- change the error message to mention the token causing the problem, eg:
if (!currentTokenEquals(Lexer.END_NODE_TYPE_NAME)) {
            lexer.fail(""Missing '"" + Lexer.END_NODE_TYPE_NAME + ""' delimiter for end of node type name, found "" + currentToken);
}
- add ""st.wordChars('_','_');"" to the lexer, its probably going to be the most common cause, and doesnt conflict with other rules.
"
1,"webdav's PropertyDefinitionImpl's toXML doesn't seem to attach query operators element to the returned domPropertyDefinitionImpl.toXML does

        // JCR 2.0 extension
        Element qopElem = document.createElement(AVAILABLE_QUERY_OPERATORS_ELEMENT);
        String[] qops = getAvailableQueryOperators();
        for (int i = 0; i < qops.length; i++) {
            Element opElem = document.createElement(AVAILABLE_QUERY_OPERATOR_ELEMENT);
            DomUtil.setText(opElem, qops[i]);
            qopElem.appendChild(opElem);
        }

        return elem;

which doesn't attach the qopElem to the returned dom."
0,Keep WebDAV exception causesThe DavMethodBase and ExceptionConverter classes in jackrabbit-webdav and jackrabbit-spi2dav don't include the cause when throwing an exception based on some caught cause. This makes it harder to identify what is causing a  particular problem. The attached patch fixes that.
0,"Extend the client's redirect handling interface to allow control of the content of the redirectThe existing RedirectHandler interface provides the ability influence which situations cause redirects, but gives you no control over the content of the redirect itself.  For example, if you want the client follow the redirect of a POST request with a POST request to the new location, you can't do it.  DefaultRequestDirector decides what method will be used on the redirect request and as of the most recent patch, it's always either a HEAD or a GET.

One option for resolving this might be extending the RedirectHandler interface to be a factory for creating the redirect request object.  The the DefaultRequestDirector could then be changed to ask the RedirectHandler to create the appropriate request for the situation.

Thanks,
Ben"
1,"ConnectException not handled in DefaultHttpMethodRetryHandlerCopied from my mailing list post, Oleg suggested I post it to JIRA for 4.0 fix.

i am using commons-httpclient.3.0.1 and I am sending some requests
through https protocol. I have a problem with a long creation of
connection if ip address of remote service is not existing. I think
problem is in the situation when https connection is not created and
ConnectException is thrown after connection timeout. This exception is
catched in HttpMethodDirector.java in method executeWithRetry. Then
the DefaultHttpMethodRetryHandler is called to recognize whether
connection creation will be repeated or not.
I think, that special handling for ConnectException is missing in
retryMethod of DefaultHttpMethodRetryHandler, because exception is not
recognized and connetions are created again.
On the other hand, ConnectTimeoutException is thrown after connection
timeout for HTTP. This exception is handled in
DefaultHttpMethodRetryHandler and call is stopped.

These lines of code handle ConnectTimeoutException in retryMethod of
DefaultHttpMethodRetryHandler:
if (exception instanceof InterruptedIOException) {
            // Timeout
            return false;
        }

Probably this is missing for ConnectException:
if (exception instanceof InterruptedIOException || exception
instanceof ConnectException) {
            // Timeout
            return false;
        }

"
0,Add missing license headersThe RAT tool (http://code.google.com/p/arat/) points out a few files within Jackrabbit trunk that are currently missing the correct license header. We should fix those.
1,"JNDIDatabaseJournal doesn't work with ""oracle"" schema (or: unable to use OracleDatabaseJournal with a jndi datasource)Database journal works fine on oracle when using the OracleDatabaseJournal implementation; but when you need to use a jndi datasource you actually need to use org.apache.jackrabbit.core.journal.JNDIDatabaseJournal which doesn't work fine with the ""oracle"" schema.

With the following configuration:
<Cluster id=""node1"" syncDelay=""10"">
    <Journal class=""org.apache.jackrabbit.core.journal.JNDIDatabaseJournal"">
      <param name=""schema"" value=""oracle"" />

jackrabbit crashes at startup with a not well defined sql error. Investigating on the problem I see that the ""oracle.ddl"" file contains a ""tablespace"" variable that is replaced only by the OracleDatabaseJournal implementation.

As a workaround users can create a different ddl without a tablespace variable, but this should probably work better out of the box.

WDYT about one of the following solutions?
- make the base DatabaseJournal implementation support jndi datasource just like PersistenceManagers do (without a specific configuration property but specifying a jndi location in the url property)
- move the replacement of the tablespace variable (and maybe: add a generic replacement of *any* parameter found in the databaseJournal configuration) to the main DatabaseJournal implementation. This could be handy and it will make the OracleDatabaseJournal extension useless, but I see that at the moment there can be a problem with the MsSql implementation, since it adds ""on "" to the tablespace name only when it's not set to an empty string.





"
0,"remove DocsAndPositionsEnum.getPayloadLengthThis was an accidental leftover; now that getPayload returns a BytesRef, this method is not needed."
1,"leading wildcard's don't work with trailing wildcardAs reported by Antony Bowesman, leading wildcards don't work when there is a trailing wildcard character -- instead a PrefixQuery is constructed.


http://www.nabble.com/QueryParser-bug--tf3270956.html"
0,"customize handling of 302 redirectsI tried this with both the beta2 2.0 release, and the nightly build.  The
following code snippet describes what I am trying to do:

httpClient.getHostConfiguration().setHost(sHost, 80, ""http"");
HttpMethod method=null;
if (sMethod.indexOf(""POST"")!=-1) {
     method=new PostMethod(sURLInfo);
} else {
     method=new GetMethod(sURLInfo);
}
method.setFollowRedirects(true);
httpClient.executeMethod(method);

After this code executes, the ""getFollowRedirects"" method still returns false,
and any redirects which are sent by the webserver are not followed.  As a
temporary workaround, since I want all redirects followed, I commented out the
following code in the HttpMethodBase class in the ""processRedirectResponse"" method:

/*if (!getFollowRedirects()) {
     LOG.info(""Redirect requested but followRedirects is ""
     + ""disabled"");
     return false;
}*/

If this bug has already been reported, I apologize...I searched for and found
nothing related to this issue."
0,"Implement Connection TimeoutsI was writing test code to use the setSoTimeout(int millis) method to set a
timeout value when connecting to a URL.  It appears to me that no matter what I
set the timeout to be a HttpConnection will try to connect but uses some other
timeout value(I'm guessing the OS's default value).  I looked at the code for
HttpConnection and it uses the Socket(host,port) constructor which tries to
connect write away.  I'd like to suggest the following code below so the timeout
is set before first the connection is even made.

/* Compile the code as is and it should timeout within a sec.  If you
 * uncomment the first two lines after the try statement and comment
 * out the other socket connect statements and run the code again you will
 * notice write away that the timeout is something else because it connects
 * right away in the constructor.  Its like the timeout is worthless at this
 * point.  As a matter a fact the code should never get there.
 * This all assumes that 192.168.168.50 is not on your network.
 */

import java.io.*;
import java.net.*;

public class SocketTest {
    public static void main(String[] args) {
        long start = System.currentTimeMillis();

        try {
            //Socket socket = new Socket(""192.168.168.50"",80);
            //socket.setSoTimeout(1000);

            //Setting timeout before the connection is made.
            Socket socket = new Socket();
            InetSocketAddress sAddress =
                new InetSocketAddress(""192.168.168.50"",80);
            socket.connect(sAddress,1000);

        } catch (UnknownHostException e) {
            System.out.println(e);
        } catch (SocketException e) {
            System.out.println(e);
        } catch (IOException e) {
            System.out.println(e);
        }

        System.out.println(System.currentTimeMillis() - start);
    }
}"
0,"Changes.html formatting improvementsSome improvements to the Changes.html generated by the changes2html.pl script via the 'changes-to-html' ant task:

# Simplified the Simple stylesheet (removed monospace font specification) and made it the default.  The Fancy stylesheet is really hard for me to look at (yellow text on light blue background may provide high contrast with low eye strain, but IMHO it's ugly).
# Moved the monospace style from the Simple stylesheet to a new stylesheet named ""Fixed Width""
# Fixed syntax errors in the Fancy stylesheet, so that it displays as intended.
# Added <span style=""attrib"">  to change attributions.
# In the Fancy and Simple stylesheets, change attributions are colored dark green.
# Now properly handling change attributions in CHANGES.txt that have trailing periods.
# Clicking on an anchor to expand its children now changes the document location to show the children.
# Hovering over anchors now causes a tooltip to be displayed - either ""Click to expand"" or ""Click to collapse"" - the tooltip changes appropriately after a collapse or expansion."
1,"[PATCH] Loosing first matching document in BooleanQueryThis patch fixes loosing of first matching document when BooleanQuery
with BooleanClause.Occur.SHOULD is added to another BooleanQuery."
1,ChainedTermEnum omits initial termsThis is a regression caused by JCR-2393.
0,"TCK: NodeReadMethodsTest#testGetPrimaryItemItemNotFoundException selects wrong test dataMethod locateNodeWithoutPrimaryItem is used to locate recursively node which does not define a primary item, but this method calls internally locateNodeWithPrimaryItem instead of locateNodeWithoutPrimaryItem."
1,"primaryItemName is not inheritedif no primaryItemName is defined for a nodetype definition, it should be inherited from one of the supertypes. the spec is unclear about this, though it seems to be the natural behaviour.

for example when extending nt:resource, the subtype should not be force to redefine the jcr:data as primaryItemName.

"
0,"[PATCH] Decouple locking implementation from Directory implementationThis is a spinoff of http://issues.apache.org/jira/browse/LUCENE-305.

I've opened this new issue to capture that it's wider scope than
LUCENE-305.

This is a patch originally created by Jeff Patterson (see above link)
and then modified as described here:

  http://issues.apache.org/jira/browse/LUCENE-305#action_12418493

with some small additional changes:

  * For each FSDirectory.getDirectory(), I made a corresponding
    version that also accepts a LockFactory instance.  So, you can
    construct an FSDirectory with your own LockFactory.

  * Cascaded defaulting for FSDirectory's LockFactory implementation:
    if you pass in a LockFactory instance, it's used; else if
    setDisableLocks was called, we use NoLockFactory; else, if the
    system property ""org.apache.lucene.store.FSDirectoryLockFactoryClass""
    is defined, we use that; finally, we'll use the original locking
    implementation (SimpleFSLockFactory).

The gist is that all locking code has been moved out of *Directory and
into subclasses of a new abstract LockFactory class.  You can now set
the LockFactory of a Directory to change how it does locking.  For
example, you can create an FSDirectory but set its locking to
SingleInstanceLockFactory (if you know all writing/reading will take
place a single JVM).

The changes pass all unit tests (on Ubuntu Linux Sun Java 1.5 and
Windows XP Sun Java 1.4), and I added another TestCase to test the
LockFactory code.

Note that LockFactory defaults are not changed: FSDirectory defaults
to SimpleFSLockFactory and RAMDirectory defaults to
SingleInstanceLockFactory.

Next step (separate issue) is to create a LockFactory that uses the OS
native locks (through java.nio).
"
1,"Intermittent failure in TestIndexWriterMergePolicy.testMaxBufferedDocsChangeLast night's build failed from it: http://hudson.zones.apache.org/hudson/job/Lucene-trunk/1019/changes

Here's the exc:

{code}
    [junit] Testcase: testMaxBufferedDocsChange(org.apache.lucene.index.TestIndexWriterMergePolicy):	FAILED
    [junit] maxMergeDocs=2147483647; numSegments=11; upperBound=10; mergeFactor=10
    [junit] junit.framework.AssertionFailedError: maxMergeDocs=2147483647; numSegments=11; upperBound=10; mergeFactor=10
    [junit] 	at org.apache.lucene.index.TestIndexWriterMergePolicy.checkInvariants(TestIndexWriterMergePolicy.java:234)
    [junit] 	at org.apache.lucene.index.TestIndexWriterMergePolicy.testMaxBufferedDocsChange(TestIndexWriterMergePolicy.java:164)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:208)
{code}

Test doesn't fail if I run on opensolaris nor os X machines..."
0,"SystemSession#createSession should return SessionImpl againa long with the fix of  JCR-2890 (revision 1089436) the behavior of SystemSession#createSession has changed to
return a SystemSession instead of SessionImpl as it used to be.

while i basically consider this move to be correct and the better way of dealing with that session-cloning
mechanism as it prevents the user of this method to convert a SystemSession into a regular session
for extra writing operations (such as e.g. access control editing that is not supported with the
system session to prevent chicken-egg-problems on repo startup).

therefore i would like to revert that change for the 2.4 release in order to prevent regressions.

for the time after 2.4 i would however suggest that we finally take the time to clearly define the
usages, abilities and responsibilities of the system session and also review how and where we
expose them to the individual 'modules' of jackrabbit core..  i started working on this but decided
that this is definitely too risky for 2.4 whereas reverting the change mentioned above should
imo impose very limited risk as all usages of those sessions i am aware of use them as ""Session""
or ""SesssionImpl"", most of them not even having access to the SystemSession class."
0,Jackrabbit concurrency review and invariantsI've been working on reviewing and verifying the internal concurrency model of Jackrabbit in an attempt to proactively prevent the kinds of deadlock issues we've seen before. Overall things seem pretty good nowadays. I'll be updating the web site with some resulting design and review docs that should help guide future work in this area. I also have created some global invariant checks that I'll be adding to the codebase.
1,"Property.getValue() throws RepositoryException with internal errorRunning ConcurrentReadWriteTest (NUM_NODES=5, NUM_THREADS=3, RUN_NUM_SECONDS=120) resulted in a RepositoryException calling Property.getValue():

javax.jcr.RepositoryException: Internal error while retrieving value of b3fc1ea8-3364-4236-bcc7-dea0baf90640/{}test: null: null

Debugging shows that it is a NullPointerException:

java.lang.NullPointerException
	at org.apache.jackrabbit.core.PropertyImpl.getValue(PropertyImpl.java:481)
	at org.apache.jackrabbit.core.ConcurrentReadWriteTest$1$1.execute(ConcurrentReadWriteTest.java:68)
	at org.apache.jackrabbit.core.AbstractConcurrencyTest$Executor.run(AbstractConcurrencyTest.java:110)
	at java.lang.Thread.run(Thread.java:619)

It's probably the state which has been discarded after the sanityCheck()."
0,"Improve error messages for index aggregatesIn the case where an index aggregate fails because of a node that doesn't exist the logged warn messages contain a full stack-trace.
Besides the fact that this can be misleading (you may think that there's something wrong that you need to fix right away) it is also borderline useless.

The desired behavior would be to just log an ""info"" message mentioning that a certain node was skipped, similar to what the SeachManager does."
0,"Remove redundant RepositoryService.executeQuery() method There are currently two executeQuery() methods on RepositoryService. For simplicity we should remove the one that assumes default values for limit, offset and bind variable values."
0,"Add narrow API for loading stored fields, to replace FieldSelectorI think we should ""invert"" the FieldSelector API, with a ""push"" API
whereby FieldsReader invokes this API once per field in the document
being visited.

Implementations of the API can then do arbitrary things like save away
the field's size, load the field, clone the IndexInput for later lazy
loading, etc.

This very thin API would be a mirror image of the very thin index time
API we now have (IndexableField) and, importantly, it would have no
dependence on our ""user space"" Document/Field/FieldType impl, so apps
are free to do something totally custom.

After we have this, we should build the ""sugar"" API that rebuilds a
Document instance (ie IR.document(int docID)) on top of this new thin
API.  This'll also be a good test that the API is sufficient.

Relevant discussions from IRC this morning at
http://colabti.org/irclogger/irclogger_log/lucene-dev?date=2011-07-13#l76
"
0,"Introduce daily integration test suiteSome time ago we discussed integration tests that would be run on a daily basis. See also comments in issue JCR-1452. It seems we reached consensus that running a daily integration test suite is desirable.

Here's my proposal:

- Introduce a test suite org.apache.jackrabbit.core.integration.daily.DailyIntegrationTest which includes all tests that should be run on a daily basis.
- Configure our continuous integration system to run the test suite on a daily basis. e.g. mvn -Dtest=DailyIntegrationTest package

With this approach we don't need to introduce maven profiles or any other pom magic, yet it's easy for a developer to run the daily tests when needed."
0,IOContext should be part of the SegmentReader cache key Once IOContext (LUCENE-2793) is landed the IOContext should be part of the key used to cache that reader in the pool
0,EntryCollector may log warning for inexistent itemCurrently the EntryCollector may log a warning when the node reported in the event does not exist. This may happen when the repository runs in a cluster and a node is created and immediately removed again. This issue is related to JCR-3014. The call to Session.nodeExists() should actually return false when the identifier path cannot be resolved. Currently it throws a RepositoryException.
0,"Correct 2 minor javadoc mistakes in core, javadoc.access=privatePatches Token.java and TermVectorsReader.java"
0,"Lazy initialize ItemDefinitionThe item definition is currently set immediately when an ItemData is instantiated. Accessing nodes usually does not require reading the item definition, thus it is not necessary to load/set it that early.

Lazy initialization also has the benefit that content migration in an upgrade scenario becomes easier. Instead of throwing an exception early, jackrabbit could allow access to the item until an item definition is really required for the operation."
1,"JCR2SPI: remove node operation missing in submitted SPI batchIn JCR2SPI, the following sequence of operations seems to lead to an incorrect SPI batch being submitted:

1) remove ""/a""
2) add ""/a""
3) add ""/a/b""
4) session.save()

This seems to create an SPI batch where the first remove operation is missing.

Note that the problem only seems to occur when step 3 is part of the sequence.

Full Java source for test:

    try {
      if (session.getRepository().getDescriptor(Repository.LEVEL_2_SUPPORTED).equals(""true"")) {
        Node testnode;
        String name = ""delete-test"";
          
        Node root = session.getRootNode();
        
        // make sure it's there
        if (! root.hasNode(name)) {
          root.addNode(name, ""nt:folder"");
          session.save();
        }
        
        // now test remove/add in one batch
        if (root.hasNode(name)) {
          testnode = root.getNode(name);
          testnode.remove();
          // session.save(); // un-commenting this makes the test pass
        }
        
        testnode = root.addNode(name, ""nt:folder"");
        // add one child
        testnode.addNode(name, ""nt:folder""); // commenting this out makes the test pass
        
        session.save();
      }
    } finally {
      session.logout();
    }
    
    "
1,"Creating AccessControlEntryImpl from a base entry results in wrong restrictionsduring creation of a new AccessControlEntryImpl using a base entry the restrictions of the base entry are
not copied to the new instance."
0,"If tests fail, don't report about unclosed resourcesLuceneTestCase ensures in afterClass() if you closed all your directories, which in turn will check if you have closed any open files.

This is good, as a test will fail if we have resource leaks.

But if a test truly fails, this is just confusing, because its usually not going to make it to the part of its code where it would call .close()

So, if any tests fail, I think we should omit this check in afterClass()"
0,"JSR 283 Repository Descriptors- new methods returning Value objects (jcr2spi)
- check descriptor-report in jcr-server and corresponding handling on the client side
- etc.etc."
0,"Support system properties in ${...} vars in XML config filesThe variable replacement (${...}) in config files like repository.xml currently only allows for the special variables introduced by Jackrabbit, eg. ${wsp.name} or ${rep.home}. But it would be useful to support all java system properties here as it is some kind of a standard in Java XML config files (see Spring for an example).

This makes it easier to inject variables from outside the config file, eg. by setting them on the command line or injecting them programmatically in test cases. Typical parameters for that include database connection credentials, which one wants to avoid to put into repository.xml files that are often checked into SVN.

This is especially true for test cases, eg. I currently work on a persistence manager component and I want to include the repository.xml in the source tree (under applications/test) but without my specific credentials. These are applied by loading a user-specific properties file through the test case before the repository is started and the config is read.
"
0,"SPI-commons:  QValueTest.testDateValueEquality2 fails due to changes made with JCR-1018with the introduction of QValue.getCalendar() the internal value for DATE-properties is now a Calendar (was
String). however, the equals() method has not been adjusted."
0,"Add optional packing to FST buildingThe FSTs produced by Builder can be further shrunk if you are willing
to spend highish transient RAM to do so... our Builder today tries
hard not to use much RAM (and has options to tweak down the RAM usage,
in exchange for somewhat lager FST), even when building immense FSTs.

But for apps that can afford highish transient RAM to get a smaller
net FST, I think we should offer packing.
"
0,"TestConstantScoreRangeQuery does not compile with ecjTestConstantScoreRangeQuery has an assertEquals(String, Float, Float)
but most of the calls to assertEquals are (String, int, int).

ecj complains with the following error:
The method assertEquals(String, float, float) is ambiguous for the type TestConstantScoreRangeQuery

The simple solution is to supply an assertEquals(String, int, int) which calls Assert.assertEquals(String, int, int)

Patch to follow.
"
1,"recovery tool does not recover when version history can be instantiated, but root version can notJCR-2551 introduced a recovery mode which tries to instantiate the version history, and if this fails, disconnects the VH (version history) and makes the node unversioned.

However, it appears it can happen that the persistence is damaged such as getting the VH does indeed work, but subsequent operations fail due to other problems. One problem that has been seen is a missing frozenNode property of the root version (or a missing frozenNode itself).

As a quick fix, we may want to change the checker so that it actually also tries to get the rootVersion and it's frozenNode. Long term, depending on how frequent this problem is, we may have to think about a less drastic recovery than disconnecting the VH."
0,"wire logger skips empty lineWhen logging with 
org.apache.commons.logging.simplelog.log.httpclient.wire=debug, HttpConnection 
skips one line of server output in logs -- CRLF line between headers and body."
0,EnwikiQueryMaker
0,Data store garbage collection: log deleted files and total sizeThe data store garbage collection should list the names and the total size of all deleted files.
0,"NRTCachingDirectory, to buffer small segments in a RAMDirI created this simply Directory impl, whose goal is reduce IO
contention in a frequent reopen NRT use case.

The idea is, when reopening quickly, but not indexing that much
content, you wind up with many small files created with time, that can
possibly stress the IO system eg if merges, searching are also
fighting for IO.

So, NRTCachingDirectory puts these newly created files into a RAMDir,
and only when they are merged into a too-large segment, does it then
write-through to the real (delegate) directory.

This lets you spend some RAM to reduce I0.
"
1,"DOMException: NAMESPACE_ERR thrown when exporting document viewWhen I try to export some nodes with ExportDocumentView I get a DOMException with Jackrabbit 1.5.2. Version 1.4.6 works fine. Xerces version was 2.8.1.

Code:

Document document = documentBuilder.newDocument();
Element exportElement = (Element) document.appendChild(document.createElement(""Export""));
Result result = new DOMResult(exportElement);
TransformerHandler transformerHandler = saxTransformerFactory.newTransformerHandler();
transformerHandler.setResult(result);
session.exportDocumentView(workflowNode.getPath(), transformerHandler, true, false);

Exception:

org.w3c.dom.DOMException: NAMESPACE_ERR: An attempt is made to create or change an object in a way which is incorrect with regard to namespaces.
	at org.apache.xerces.dom.CoreDocumentImpl.checkDOMNSErr(Unknown Source)
	at org.apache.xerces.dom.AttrNSImpl.setName(Unknown Source)
	at org.apache.xerces.dom.AttrNSImpl.<init>(Unknown Source)
	at org.apache.xerces.dom.CoreDocumentImpl.createAttributeNS(Unknown Source)
	at org.apache.xerces.dom.ElementImpl.setAttributeNS(Unknown Source)
	at com.sun.org.apache.xalan.internal.xsltc.trax.SAX2DOM.startElement(SAX2DOM.java:194)
	at com.sun.org.apache.xml.internal.serializer.ToXMLSAXHandler.closeStartTag(ToXMLSAXHandler.java:204)
	at com.sun.org.apache.xml.internal.serializer.ToSAXHandler.flushPending(ToSAXHandler.java:277)
	at com.sun.org.apache.xml.internal.serializer.ToXMLSAXHandler.startElement(ToXMLSAXHandler.java:646)
	at com.sun.org.apache.xalan.internal.xsltc.trax.TransformerHandlerImpl.startElement(TransformerHandlerImpl.java:263)
	at org.apache.jackrabbit.commons.xml.Exporter.startElement(Exporter.java:438)
	at org.apache.jackrabbit.commons.xml.DocumentViewExporter.exportNode(DocumentViewExporter.java:76)
	at org.apache.jackrabbit.commons.xml.Exporter.exportNode(Exporter.java:298)
	at org.apache.jackrabbit.commons.xml.Exporter.exportNodes(Exporter.java:214)
	at org.apache.jackrabbit.commons.xml.DocumentViewExporter.exportNode(DocumentViewExporter.java:77)
	at org.apache.jackrabbit.commons.xml.Exporter.exportNode(Exporter.java:295)
	at org.apache.jackrabbit.commons.xml.Exporter.export(Exporter.java:144)
	at org.apache.jackrabbit.commons.AbstractSession.export(AbstractSession.java:461)
	at org.apache.jackrabbit.commons.AbstractSession.exportDocumentView(AbstractSession.java:241)"
0,"remove support for event bundle IDsEvent bundle IDs currently are not used. We can re-add them later in case we need them.
"
0,"QueryParser throws new exceptions even if custom parsing logic threw a better oneWe have subclassed QueryParser and have various custom fields.  When these fields contain invalid values, we throw a subclass of ParseException which has a more useful message (and also a localised message.)

Problem is, Lucene's QueryParser is doing this:

{code}
    catch (ParseException tme) {
        // rethrow to include the original query:
        throw new ParseException(""Cannot parse '"" +query+ ""': "" + tme.getMessage());
    }
{code}

Thus, our nice and useful ParseException is thrown away, replaced by one with no information about what's actually wrong with the query (it does append getMessage() but that isn't localised.  And it also throws away the underlying cause for the exception.)

I am about to patch our copy to simply remove these four lines; the caller knows what the query string was (they have to have a copy of it because they are passing it in!) so having it in the error message itself is not useful.  Furthermore, when the query string is very big, what the user wants to know is not that the whole query was bad, but which part of it was bad.

"
0,"remove TermVectorsWriter (it's no longer used)We should remove TermVectorsWriter: it's no longer used now that
DocumentsWriter writes the term vectors directly to the index."
0,"FieldCacheImpl's getCacheEntries() is buggy as it uses WeakHashMap incorrectly and leads to ConcurrentModExceptionsThe way how WeakHashMap works internally leads to the fact that it is not allowed to iterate over a WHM.keySet() and then get() the value. As each get() operation inspects the ReferenceQueue of the weak keys, they may suddenly disappear. If you use the entrySet() iterator you get key and value and no need to call get(), contains(),... that inspects the ReferenceQueue."
0,"contrib/benchmark - few improvements and a bug fixBenchmark byTask was slightly improved:

1. fixed a bug in the ""child-should-not-report"" mechanism. If a task sequence contained only simple tasks it worked as expected (i.e. child tasks did not report times/memory) but if a child was a task sequence, then its children would report - they should not - this was fixed, so this property is now ""penetrating/inherited"" all the way down.

2. doc size control now possible also for the Reuters doc maker. (allowing to index N docs of size C characters each.)

3. TrecDocMaker was added - it reads as input the .gz files used in Trec - e.g. .gov data - this can be handy to benchmark Lucene on these large collections.  Similar to the Reuters collection, the doc-maker scans the input directory for all the files and extracts documents from the files.  Here there are multiple documents in each input file. Unlike the Reuters collection, we cannot provide a 'loader' for these collections - they are available from http://trec.nist.gov - for research purposes.

4. a new BasicDocMaker abstract class handles most of doc-maker tasks, including creating docs with specific size, so adding new doc-makers for other data is now much simpler."
0,"make similarities/term/collectionstats take long (for > 2B docs)As noted by Yonik and Andrzej on SOLR-1632, this would be useful for distributed scoring.

we can also add a sugar method add() to both of these to make it easier to sum."
1,"Clustering: race condition may cause duplicate entries in search indexThere seems to be a race condition that may cause duplicate search index entries. It is reproducible as follows (Jackrabbit 1.3):
1) Start clusternode 1 that just adds a single node of node type clustering:test.
2) Shutdown clusternode 1.
3) Start clusternode 2 with an empty search index.
4) Execute the query  //element(*, clustering:test).
4) Print the result of the query (UUIDs of nodes in the result set).

When I just run clusternode 2, then there is one node in the resultset, as expected. However, when I debug clusternode 2 and have a breakpoint (i.e., a pause of a few seconds at line 306 of RepositoryImpl.java - just before the clusternode is started), then the resultset contains two results, both with the same UUID.
"
0,"Unreferenced VersionHistory should be deleted automatically.since the creation of a VersionHistory is triggered by the creation of a mix:versionable node, the removal should happen automatically, as soon as no references to that version histroy exist anymore. this is the case, when all mix:versionable nodes (in all workspaces) belonging to that VH are deleted, and all the versions in the VH are removed i.e. only the jcr:rootVersion is left. At this point, the VH should be deleted aswell."
0,"avoid converting property values to stringsQValues currently can not expose properties of types LONG and DOUBLE in a parsed format. Thus, setting/retrieving properties of these types requires roundtripping through Strings, which we should avoid.

Proposal:

1) Add ""long getLong()"" and ""double getDouble()"" to QValue.

2) Add matching create methods to QValueFactory.

3) Take advantage of the new methods in JCR2SPI, for instance by allowing it's own Value implementation to internally just hold the QValue.

"
0,"Merge UUID to NodeIdThe current NodeId class is mostly just a wrapper around UUID, which causes two objects to be instantiated for each node identifier that the system uses. The memory and processing overhead is quite small, but given that there are tons of NodeId instances it would be good to eliminate that overhead.

There is also lots of code that just converts UUIDs to NodeIds and vice versa. We could simplify such code if we just used NodeId everywhere.

Also, we might want to open up the possibility of using non-UUID node identifiers at some point in future, so it would make a lot of sense to remove the NodeId.getUUID method and rely directly on NodeId and it's equals(), hashCode(), and toString() methods in many places where we currently use UUIDs."
0,"Change default write lock file location to index directory (not java.io.tmpdir)Now that readers are read-only, we no longer need to store lock files
in a different global lock directory than the index directory.  This
has been a source of confusion and caused problems to users in the
past.

Furthermore, once the write lock is stored in the index directory, it
no longer needs the big digest prefix that was previously required
to make sure lock files in the global lock directory, from different
indexes, did not conflict.

This way, all files related to an index will appear in a single
directory.  And you can easily list that directory to see if a
""write.lock"" is present to check whether a writer is open on the
index.

Note that this change just affects how FSDirectory creates its default
lockFactory if no lockFactory was specified.  It is still possible
(just no longer the default) to pick a different directory to store
your lock files by pre-instantiating your own LockFactory.

As part of this I would like to remove LOCK_DIR and the no-argument
constructor, in SimpleFSLockFactory and NativeFSLockFactory.  I don't
think we should have the notion of a global default lock directory
anymore.  This is actually an API change.  However, neither
SimpleFSLockFactory nor NativeFSLockFactory haver been released yet,
so I think this API removal is allowed?

Finally I want to deprecate (but not yet remove, because this has been
in the API for many releases) the static LOCK_DIR that's in
FSDirectory.  But it's now entirely unused.

See here for discussion leading to this:

  http://www.gossamer-threads.com/lists/lucene/java-dev/43940
"
1,"ParallelReader crashes when trying to merge into a new indexParallelReader causes a NullPointerException in
org.apache.lucene.index.ParallelReader$ParallelTermPositions.seek(ParallelReader.java:318)
when trying to merge into a new index.

See test case and sample output:

$ svn diff
Index: src/test/org/apache/lucene/index/TestParallelReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestParallelReader.java    (revision 179785)
+++ src/test/org/apache/lucene/index/TestParallelReader.java    (working copy)
@@ -57,6 +57,13 @@
 
   }
  
+  public void testMerge() throws Exception {
+    Directory dir = new RAMDirectory();
+    IndexWriter w = new IndexWriter(dir, new StandardAnalyzer(), true);
+    w.addIndexes(new IndexReader[] { ((IndexSearcher)
parallel).getIndexReader() });
+    w.close();
+  }
+
   private void queryTest(Query query) throws IOException {
     Hits parallelHits = parallel.search(query);
     Hits singleHits = single.search(query);
$ ant -Dtestcase=TestParallelReader test
Buildfile: build.xml
[...]
test:
    [mkdir] Created dir:
/Users/skirsch/text/lectures/da/thirdparty/lucene-trunk/build/test
    [junit] Testsuite: org.apache.lucene.index.TestParallelReader
    [junit] Tests run: 2, Failures: 0, Errors: 1, Time elapsed: 1.993 sec

    [junit] Testcase: testMerge(org.apache.lucene.index.TestParallelReader):  
Caused an ERROR
    [junit] null
    [junit] java.lang.NullPointerException
    [junit]     at
org.apache.lucene.index.ParallelReader$ParallelTermPositions.seek(ParallelReader.java:318)
    [junit]     at
org.apache.lucene.index.ParallelReader$ParallelTermDocs.seek(ParallelReader.java:294)
    [junit]     at
org.apache.lucene.index.SegmentMerger.appendPostings(SegmentMerger.java:325)
    [junit]     at
org.apache.lucene.index.SegmentMerger.mergeTermInfo(SegmentMerger.java:296)
    [junit]     at
org.apache.lucene.index.SegmentMerger.mergeTermInfos(SegmentMerger.java:270)
    [junit]     at
org.apache.lucene.index.SegmentMerger.mergeTerms(SegmentMerger.java:234)
    [junit]     at
org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:96)
    [junit]     at
org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:596)
    [junit]     at
org.apache.lucene.index.TestParallelReader.testMerge(TestParallelReader.java:63)
    [junit]     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    [junit]     at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    [junit]     at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)


    [junit] Test org.apache.lucene.index.TestParallelReader FAILED

BUILD FAILED
/Users/skirsch/text/lectures/da/thirdparty/lucene-trunk/common-build.xml:188:
Tests failed!

Total time: 16 seconds
$"
0,"Invert IR.getDelDocs -> IR.getLiveDocsSpinoff from LUCENE-1536, where we need to fix the low level filtering
we do for deleted docs to ""match"" Filters (ie, a set bit means the doc
is accepted) so that filters can be pushed all the way down to the
enums when possible/appropriate.

This change also inverts the meaning first arg to
TermsEnum.docs/AndPositions (renames from skipDocs to liveDocs).
"
0,"Handle Null Arguments consistantlyConsider throwing a NullPointerException or InvalidArgumentException for null
argument when they are not allowed.  Be consistant and document behaviour."
0,"sort missing string fields lastA SortComparatorSource for string fields that orders documents with the sort
field missing after documents with the field.  This is the reverse of the
default Lucene implementation.

The concept and first-pass implementation was done by Chris Hostetter."
1,checkindex fails if docfreq >= skipInterval and term is indexed more than once at same positionThis is a bad check in the skipping verification logic
0,"Better integration of the TestWebApp-HowTo into the documentationThe excellent webapp howto written by Olegolas needs to be integrated better
into httpclient documentation.  Currently it is in the docs directory as a html
file, but it would be better if it was in the xdocs directory as an xml file."
0,"Handle Returning Null consistantlyConsider returning empty arrays instead of null consistantly.  eg:
getResponseBody().  There may be good reason for both null and empty array
depending on the circumstannces."
0,"Allow customizing/subclassing of DirectoryReaderDirectoryReader is final and has only static factory methods. It is not possible to subclass it in any way.

The problem is mainly Solr, as Solr accesses directory(), IndexCommits,... and therefore cannot work on abstract IndexReader anymore. This should be changed, by e.g. handling reopening in the IRFactory, also versions, commits,... Currently its not possible to implement any other IRFactory that returns something else.

On the other hand, it should be possible to ""wrap"" a DirectoryReader / CompositeReader to handle filtering of collection based information (subreaders, reopening hooks,...). This can be done by making DirectoryReader abstract and let DirectoryReader.open return a internal hidden class ""StandardDirectoryReader"". This is similar to the relatinship between IndexReader and hidden DirectoryReader in the past.

DirectoryReader will have final implementations of most methods like getting document stored fields, global docFreq and other statistics, but allows hooking into doOpenIfChanged. Also it should not be limited to SegmentReaders as childs - any AtomicReader is fine. This allows users to create e.g. a Directory-based ParallelReader (see LUCENE-3736) that supports reopen and (partially commits)."
0,"Lower log level in o.a.j.jcr2spi.query.NodeIteratorImplNodeIteratorImpl.fetchNext() logs an error when it cannot load a node and skips that node. Since this is not an error condition (the node could have been deleted by another session), logging should occur at the warn level."
1,"TransientFileFactory may throw ConcurrentModificationException on shutdownWhen Jackrabbit is stopped the shutdown hook of the TransientFileFactory iterates over all tracked temp files and deletes them. At the same time the reaper thread may still remove file references from the list of tracked temp files. This may lead to a ConcurrentModificationException in the shutdown hook:

java.util.ConcurrentModificationException
	at java.util.AbstractList$Itr.checkForComodification(Unknown Source)
	at java.util.AbstractList$Itr.next(Unknown Source)
	at org.apache.jackrabbit.util.TransientFileFactory$1.run(TransientFileFactory.java:86)
"
0,"In IndexSearcher class, make subReader and docCount arrays protected so sub classes can access themPlease make these two member variables protected so subclasses can access them, e.g.:

  protected IndexReader[] subReaders;
  protected int[] docStarts;

Thanks"
1,"JCR2SPI: Workspace.getImportHandler creates a handler which doesn't work properly under JDK 1.4.JCR2SPI returns an import handler which delegates work to a SAXTransformerHandler. In JDK, that one has a known issue not processing namespace prefix mappings properly (will attach a separate test case).

Proposals:

- drop JDK 1.4 support
- tune the JCR2SPI handler to create namespace attributes when needed
- use an entirely different serializer

My personal preference would be just to drop JDK 1.4 support, but that may not be acceptable for everyone.
"
0,"tests should run checkIndex on indexes they createI think we should add a boolean checkIndexesOnClose (default=true) to MockDirectoryWrapper.

Only a very few tests need to disable this.
"
0,"Remove GData from trunk GData doesn't seem to be maintained anymore. We're going to remove it before we cut the 2.3 release unless there are negative votes.

In case someones jumps in in the future and starts to maintain it, we can re-add it to the trunk.

If anyone is using GData and needs it to be in 2.3 please let us know soon!"
0,"Eliminate class HostConfigurationRemove the target host attribute from the HostConfiguration class. This will allow one HostConfiguration object to be used for different targets.
The problem is that currently MultiThreadedHttpConnectionManager uses HostConfiguration objects as cache keys, which needs to be changed.

This is a followup to HTTPCLIENT-615.

cheers,
  Roland
"
0,"Fix pulsingcodec to reuse its enumsPulsingCodec currently doesnt always reuse its enums, which could lead to behavior like LUCENE-3515.

The problem is sometimes it returns the 'wrapped' enum, but other times it returns its 'pulsingenum' depending upon
whether terms are pulsed...

we can use the fact that these enums allow attributes to keep the reuse information for both so it can reuse when stepping through terms.
"
0,Add option to make sorting in user/group query case insensitiveSorting on string properties is currently case sensitive in the user/group search. There should be a way to specify whether sorting shuld be case (in)sensitive.
0,"Improvement to UndefinedTypeConverterImpl to map super types effectivelyImprovement to org.apache.jackrabbit.ocm.manager.atomictypeconverter.impl.UndefinedTypeConverterImpl's implementation of 
public Value getValue(ValueFactory valueFactory, Object propValue) , used equality check of class names to decide whether Object propValue is worthy of any attempt to map to an apropriate property.  Since the purpose of the class is to provide a 'best effort' attempt to map an Object of type java.lang.Object it will be better to use 'instanceof'.  This approach will convert the specific class as well as any inherited objects.  For example using instanceof will let us map a BufferedInputStream, and any other sub classes of InputStream to a Binary Property."
1,"Weird BooleanQuery behaviorHere's a simple OR-connected query.

T:files T:deleting C:thanks C:exists

The query above hits 1 document. But following *same* query only
with parenthesis results nothing.

(T:files T:deleting) (C:thanks C:exists)

Another combinations of MUST and SHOULD.

""T:files T:deleting +C:production +C:optimize"" hits 1 document.
""(T:files T:deleting) (+C:production +C:optimize)"" hits 1 document."
0,"ConstantScoreQuery should directly support wrapping Query and simply strip off scoresEspecially in MultiTermQuery rewrite modes we often simply need to strip off scores from Queries and make them constant score. Currently the code to do this looks quite ugly: new ConstantScoreQuery(new QueryWrapperFilter(query))

As the name says, QueryWrapperFilter should make any other Query constant score, so why does it not take a Query as ctor param? This question was aldso asked quite often by my customers and is simply correct, if you think about it.

Looking closer into the code, it is clear that this would also speed up MTQs:
- One additional wrapping and method calls can be removed
- Maybe we can even deprecate QueryWrapperFilter in 3.1 now (it's now only used in tests and the use-case for this class is not really available) and LUCENE-2831 does not need the stupid hack to make Simon's assertions pass
- CSQ now supports out-of-order scoring and topLevel scoring, so a CSQ on top-level now directly feeds the Collector. For that a small trick is used: The score(Collector) calls are directly delegated and the scores are stripped by wrapping the setScorer() method in Collector

During that I found a visibility bug in Scorer (LUCENE-2839): The method ""boolean score(Collector collector, int max, int firstDocID)"" should be public not protected, as its not solely intended to be overridden by subclasses and is called from other classes, too! This leads to no compiler bugs as the other classes that calls it is mainly BooleanScorer(2) and thats in same package, but visibility is wrong. I will open an issue for that and fix it at least in trunk where we have no backwards-requirement."
0,"Set default precisionStep for NumericField and NumericRangeFilterThis is a spinoff from LUCENE-1701.

A user using Numeric* should not need to understand what's
""under the hood"" in order to do their indexing & searching.

They should be able to simply:
{code}
doc.add(new NumericField(""price"", 15.50);
{code}

And have a decent default precisionStep selected for them.

Actually, if we add ctors to NumericField for each of the supported
types (so the above code works), we can set the default per-type.  I
think we should do that?

4 for int and 6 for long was proposed as good defaults.

The default need not be ""perfect"", as advanced users can always
optimize their precisionStep, and for users experiencing slow
RangeQuery performance, NumericRangeQuery with any of the defaults we
are discussing will be much faster.
"
0,"Extend the consistency check in BundleDbPersistenceManager's to fix child-parent relationsIt could happen that a child node is not in the ChildNodeEntries of its parent node.
You will get something like (javax.jcr.ItemNotFoundException: failed to build path of node1: parentNode has no child entry for node1) if you try to retrieve the path from node1.
We should handle such cases and fix it on consistency check"
0,"jcr2spi: use jcr names and path for log and exception messagein a couple of places jcr2spi adds the string representation of Path, Name and ItemId to the exception/error message. it would be convenient to convert them to jcr names and jcr path where ever possible."
0,"Add more unit on collection fieldscollection fields based on List are  supported not yet tested correctly.
Check if other kind collection are well tested"
1,CharsRef#append broken on trunk & 3.xCurrent impl. for append on CharsRef is broken - it overrides the actual content rather than append. its used in many places especially in solr so we might have some broken 
1,"DefaultRedirectHandler does not access correct HttpParamsIn the getLocationURI(HttpResponse, HttpContext) method, the HttpParams for determining REJECT_RELATIVE_REDIRECT and ALLOW_CIRCULAR_REDIRECTS are retrieved with:

HttpParams params = response.getParams();

The response HttpParams do not contain these values, however the request HttpParams do. The correct implementation is:

HttpRequest request = (HttpRequest) context.getAttribute(HttpExecutionContext.HTTP_REQUEST);
HttpParams params = request.getParams();

"
1,"jackrabbit-server.war is missing the slf4j-log4j12 libraryReported by Martin Perez:

But I found a bug on the .war file. It is missing the slf4j-log4j12-1.0.jar. It's in someway tricky to detect it because if you do not include it a ClassNotFoundException will be thrown but poiting to the JCR class with the log statement. Anyways, if you include the .jar file on the WEB-INF/lib directory, then the exception goes to exception's hell.

"
0,"Internal Timeout Handling in the TransactionContext is not XA Spec. conformThe problem here is that in a 2 phase transaction the xa spec does not  
permit a RB* return code in response to xa_commit().  The xa spec says  
the following about RB* return codes in the xa_commit() section:        
                                                                        
""The resource manager did not commit the work done on behalf of the     
transaction branch.  Upon return, the resource manager has rolled back  
the branch?s work and has released all held resources.  These values may
be returned only if TMONEPHASE is set in flags""                         
                                                                        
Essentially, the only two return codes from xa_commit that J2EE Containers can     
handle sensibly are XA_OK (normal case) and XA_RMFAIL.  RMFAIL will     
cause the containers to retry to commit the  transaction.  Any other return code will result in a heuristic          
transaction outcome (non-atomic).  

In a xa environment the TMONEPHASE is not set on the flags and so XA_RBTIMEOUT is 
not a permitted return code. A Container  transaction service cannot do anything to ensure an atomic     
outcome if an XAResource fails to honour its promise to be able to commit it made when it answer XA_OK in response to xa_prepare(). 

The internal timeout handling will rollback the Jackrabbit XAResource if the time exceeds between prepare and commit.
and in the commit Method will always throw a XA_RBTIMEOUT.

We should not handle the timeout internal because this should make the container in a 2 Phase transaction."
0,"Auto method retrial brokenFolks,
While working on the exception handling guide for the 3.0-alpha2 release I
stumbled upon a problem with HttpTimeoutException and its subclasses. In 3.0a1
HttpTimeoutException subclasses HttpRecoverableException which causes HTTP
methods failed due to a connect or socket timeout to be automatically retried. 

[INFO] HttpMethodDirector - -Recoverable exception caught when processing request
[INFO] HttpMethodDirector - -Recoverable exception caught when processing request
[INFO] HttpMethodDirector - -Recoverable exception caught when processing request
[INFO] HttpMethodDirector - -Recoverable exception caught when processing request
[WARN] HttpMethodDirector - -Recoverable exception caught but
MethodRetryHandler.retryMethod() returned false, rethrowing exception
org.apache.commons.httpclient.IOTimeoutException: Read timed out
	at
org.apache.commons.httpclient.HttpConnection$WrappedInputStream.handleException(HttpConnection.java:1350)
	at
org.apache.commons.httpclient.HttpConnection$WrappedInputStream.read(HttpConnection.java:1360)
	at java.io.FilterInputStream.read(FilterInputStream.java:66)
	at java.io.PushbackInputStream.read(PushbackInputStream.java:120)
	at org.apache.commons.httpclient.HttpParser.readRawLine(HttpParser.java:76)
	at org.apache.commons.httpclient.HttpParser.readLine(HttpParser.java:104)
	at org.apache.commons.httpclient.HttpConnection.readLine(HttpConnection.java:1054)
	at
org.apache.commons.httpclient.HttpMethodBase.readStatusLine(HttpMethodBase.java:1785)
	at
org.apache.commons.httpclient.HttpMethodBase.readResponse(HttpMethodBase.java:1546)
	at org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java:977)
	at
org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:383)
	at
org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:164)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:437)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:324)
	at Test.main(Test.java:13)
Caused by: java.net.SocketTimeoutException: Read timed out
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.read(SocketInputStream.java:129)
	at java.net.SocketInputStream.read(SocketInputStream.java:182)
	at
org.apache.commons.httpclient.HttpConnection$WrappedInputStream.read(HttpConnection.java:1358)
	... 13 more
Exception in thread ""main"" 

This probably is not what we want. Besides, for non-idempotent methods this may
simply be fatal and result in all sorts of unpleasant side-effects.

One possibilty that I personally favour is to make HttpTimeoutException class
extend IOException instead of HttpRecoverableException. There are others. The
question is whether timeout exceptions should be considered recoverable from the
conseptual standpoint. What do you think?

Oleg"
0," RAMDirectory(Directory dir, boolean closeDir)  constructor uses memory inefficiently.recently I found that  RAMDirectory(Directory dir, boolean closeDir)  constructor uses memory inefficiently.
files from source index are read entirely intro memory as single byte array which is after all is thrown away. And if I want to load my 200M optimized, compound format index to memory for faster search I should give JVM at least 400Mb memory limit. For larger indexes this can be an issue.

I've attached patch how to solve this problem."
1,"StandardQueryParser ignores AND operator for tokenized query termsThe standard query parser uses the default query operator for query clauses that are created from tokenization in the query parser instead of the actual operator for the source term.

here is an example:
{code}
StandardQueryParser parser = new StandardQueryParser(new StandardAnalyzer(Version.LUCENE_34));
parser.setDefaultOperator(Operator.OR);
System.out.println(((BooleanQuery)parser.parse(""_deleted:true AND title:東京"", ""f"")));
{code}

this should yield:
+_deleted:true +(title:東 title:京)

as our former core query parser does but actually yields:
+_deleted:true title:東 title:京

seems like a bug to me, looking at the tests seems we don't test for this kind of queries in the standard query parser tests too.
"
1,"SearchIndex class contains garbled StringSomehow during the switch to SL4J also a String literal in the SearchIndex class got garbled.

See:
http://svn.apache.org/viewcvs.cgi/incubator/jackrabbit/trunk/jackrabbit/src/main/java/org/apache/jackrabbit/core/query/lucene/SearchIndex.java?rev=385280&r1=378221&r2=385280

Since this is a low risk change I would like to get this included into the 1.0 branch."
0,"Extend apache parent pom for Apache wide configurationApache wide config is published in the apache parent pom, please use"
0,"UserManagement: membership cache default size too smallThe membership cache that has been introduced in JCR-2703 is making use of an LRUMap to cache group memberships (authorizable nodeId -> group nodeIds). In environments where users belong to more than 100 groups, the cache quickly becomes ineffective due to the default maximum size of the LRUMap.

Once the cache limit is hit, the rather expensive Node#getWeakReferences API calls resulting in search queries are executed again, leading to quite noticeable performance drops. Thus I'd suggest to either make the membership cache configurable or introduce some logic to let the cache grow dynamically as needed (still having some kind of hard limit to avoid memory issues)."
1,"Concurrent locking operations failI prepared simple test which tries to lock/unlock single node from many
threads. I expected only LockExceptions thrown by some threads which can
occur if node is already locked by other thread.

But I get incorrect effect sporadically. It looks like some thread
managed to acquire lock, but then can't release it.

Following exception is thrown then :

javax.jcr.InvalidItemStateException:
7c198c7b-76c8-47c8-96a8-d9dfefd4b387 has been modified externally
    at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1193)
    at org.apache.jackrabbit.core.NodeImpl.unlock(NodeImpl.java:3790)
    at JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:95)

additionally warning appears in log

org.apache.jackrabbit.core.lock.LockManagerImpl$LockInfo.loggingOut(LockManagerImpl.java:892)
- Unable to unlock session-scoped lock on node
'7c198c7b-76c8-47c8-96a8-d9dfefd4b387-W': Unable to unlock node. Node
has pending changes: /folder

In consequence node is left in locked state. It looks like a bug.
If one thread locked node successfully, then none other can modify it,
and the same thread should release lock without any problems.

Shouldn't be lock operation atomic itself ?

Przemo


package com.oyster.mom.contentserver.jcr.transaction;

import javax.jcr.Node;
import javax.jcr.RepositoryException;
import javax.jcr.Session;
import javax.jcr.SimpleCredentials;
import javax.jcr.lock.LockException;

import org.apache.jackrabbit.core.RepositoryImpl;
import org.apache.jackrabbit.core.config.RepositoryConfig;

public class JrTestConcurrentLocks extends Thread {

   private static final org.apache.commons.logging.Log log = org.apache.commons.logging.LogFactory.getLog(JrTestConcurrentLocks.class);

   public static String REPOSITORY_HOME = ""d:/repo/jackrabbit/"";

   public static String REPOSITORY_CONFIG = REPOSITORY_HOME + ""repository.xml"";

   public static void main(String[] args) throws Exception {

       JrTestConcurrentLocks test = new JrTestConcurrentLocks(-1);
       test.startup();

       JrTestConcurrentLocks tests[] = new JrTestConcurrentLocks[3];

       for (int i = 0; i < tests.length; i++) {
           JrTestConcurrentLocks x = new JrTestConcurrentLocks(i);
           x.setSession(repository.login(new SimpleCredentials(""admin"", ""admin"".toCharArray())));
           x.start();
           tests[i] = x;
       }

       for (int i = 0; i < tests.length; i++) {
           tests[i].join();
           tests[i].getSession().logout();
       }

       test.shutdown();
   }

   private static RepositoryImpl repository;

   private int id;

   private Session session;

   public void setSession(Session session) {
       this.session = session;
   }

   public Session getSession() {
       return this.session;
   }

   public JrTestConcurrentLocks(int i) {
       this.id = i;
   }

   public void startup() throws Exception {
       System.setProperty(""java.security.auth.login.config"", ""c:/jaas.config"");

       RepositoryConfig config = RepositoryConfig.create(REPOSITORY_CONFIG, REPOSITORY_HOME);
       repository = RepositoryImpl.create(config);

       Session session = repository.login(new SimpleCredentials(""admin"", ""admin"".toCharArray()));
       Node rootNode = session.getRootNode();
       if (!rootNode.hasNode(""folder"")) {
           Node folder = rootNode.addNode(""folder"");
           folder.addMixin(""mix:versionable"");
           folder.addMixin(""mix:lockable"");
           rootNode.save();
       }
       session.logout();
   }

   public void shutdown() throws RepositoryException {
       repository.shutdown();
   }

   public Node getFolder(Session session) throws RepositoryException {
       return session.getRootNode().getNode(""folder"");
   }

   public void run() {

       for (int i = 0; i < 10; i++) {
           log.info(""START id:"" + id + "", i="" + i);

           try {
               session.refresh(false);

               Node folder = getFolder(session);
               folder.lock(false, true);
               folder.unlock();

               log.info(""SUCCESS id:"" + id + "", i="" + i);
           }
           catch (LockException e) {
               log.info(""FAIL:"" + id + "", i="" + i);
           }
           catch (Exception e) {
               log.warn(""ERROR:"" + id + "", i="" + i, e);
           }


       }

   }
}


15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=0
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=0
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=0
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=0
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=1
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=1
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=0
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=1
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=1
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=2
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=2
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=3
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=3
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=4
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=2
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=2
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=3
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=3
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=4
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=4
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=5
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:97) - SUCCESS id:1, i=4
15:46:17 WARN  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:103) - ERROR:0, i=5
javax.jcr.ItemNotFoundException: 7c198c7b-76c8-47c8-96a8-d9dfefd4b387/{http://www.jcp.org/jcr/1.0}lockOwner
       at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:463)
       at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:319)
       at org.apache.jackrabbit.core.NodeImpl.getProperty(NodeImpl.java:1436)
       at org.apache.jackrabbit.core.NodeImpl.getOrCreateProperty(NodeImpl.java:428)
       at org.apache.jackrabbit.core.NodeImpl.internalSetProperty(NodeImpl.java:1267)
       at org.apache.jackrabbit.core.NodeImpl.lock(NodeImpl.java:3740)
       at JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:94)
15:46:17 WARN  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:103) - ERROR:2, i=0
javax.jcr.InvalidItemStateException: 7c198c7b-76c8-47c8-96a8-d9dfefd4b387 has been modified externally
       at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1193)
       at org.apache.jackrabbit.core.NodeImpl.unlock(NodeImpl.java:3790)
       at JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:95)
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=5
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=6
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=1
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=5
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=6
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=1
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=6
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=7
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=2
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=6
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=7
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=2
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=7
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=8
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=3
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=7
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=8
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=3
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=8
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=9
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=4
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=8
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=9
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=4
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=9
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=5
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=5
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=6
15:46:18 WARN  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:103) - ERROR:1, i=9
javax.jcr.InvalidItemStateException: /folder: the node cannot be saved because it has been modified externally.
       at org.apache.jackrabbit.core.NodeImpl.makePersistent(NodeImpl.java:908)
       at org.apache.jackrabbit.core.ItemImpl.persistTransientItems(ItemImpl.java:682)
       at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1173)
       at org.apache.jackrabbit.core.NodeImpl.lock(NodeImpl.java:3744)
       at JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:94)
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=6
15:46:18 WARN  org.apache.jackrabbit.core.lock.LockManagerImpl$LockInfo.loggingOut(LockManagerImpl.java:892) - Unable to unlock session-scoped lock on node '7c198c7b-76c8-47c8-96a8-d9dfefd4b387-W': Unable to unlock node. Node has pending changes: /folder
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=7
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=7
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=8
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=8
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=9
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=9

"
0,"Support for passing an SSLContext to the SSLSocketFactory of HttpClientWould it be possible to use an existing instance of SSLContext to initialise an SSLSocketFactory? This would allow using SSLContexts configured with more options, such as CRLs.

(This follows the thread of the httpclient-commons-dev list: http://marc.info/?l=httpclient-commons-dev&m=121737017814116&w=2 )."
0,"Change Term to use bytesin LUCENE-2426, the sort order was changed to codepoint order.

unfortunately, Term is still using string internally, and more importantly its compareTo() uses the wrong order [utf-16].
So MultiTermQuery, etc (especially its priority queues) are currently wrong.

By changing Term to use bytes, we can also support terms encoded as bytes such as numerics, instead of using
strange string encodings.
"
0,Some implementations require a save() after a mixin has been assignedSome test cases do not call save() after a mixin has been added.
1,"Deadlock in DefaultISMLockingThere seems to be a bug in DefaultISMLocking which was detected as part of JCR-2746.

1) The main thread gets a read lock.

2) The ObservationManager thread tries to lock for writing, which is blocked because there is still a read lock.

3) Then the main thread tries to get a second read lock, which is blocked because there is a waiting write lock.

The bug was introduced as part of JCR-2089 (Use java.util.concurrent), revisions 995411 and 995412. I think the safe solution is to revert those to commits, and add a test case. If the DefaultISMLocking is changed later on, more test cases are required. An efficient solution is relatively complicated.
"
0,"Collapse Searcher/Searchable/IndexSearcher; remove contrib/remote; merge PMS into IndexSearcherWe've discussed cleaning up our *Searcher stack for some time... I
think we should try to do this before releasing 4.0.

So I'm attaching an initial patch which:

  * Removes Searcher, Searchable, absorbing all their methods into IndexSearcher

  * Removes contrib/remote

  * Removes MultiSearcher

  * Absorbs ParallelMultiSearcher into IndexSearcher (ie you can now
    pass useThreads=true, or a custom ES to the ctor)

The patch is rough -- I just ripped stuff out, did search/replace to
IndexSearcher, etc.  EG nothing is directly testing using threads with
IndexSearcher, but before committing I think we should add a
newSearcher to LuceneTestCase, which randomly chooses whether the
searcher uses threads, and cutover tests to use this instead of making
their own IndexSearcher.

I think MultiSearcher has a useful purpose, but as it is today it's
too low-level, eg it shouldn't be involved in rewriting queries: the
Query.combine method is scary.  Maybe in its place we make a higher
level class, with limited API, that's able to federate search across
multiple IndexSearchers?  It'd also be able to optionally use thread
per IndexSearcher.
"
0,Enable DocValues by default for every CodecCurrently DocValues are enable with a wrapper Codec so each codec which needs DocValues must be wrapped by DocValuesCodec. The DocValues writer and reader should be moved to Codec to be enabled by default.
1,"A failure to connect to a MySQL database when JackRabbit starts a session leaves a .lock file in the repository. Subsequent sessions cannot be created by the same thread.I investigating the robustness of JackRabbit in the face of unexpected database errors, such as the database being unavailable. In my particular case, I am attempting to start a JackRabbit session using a TransientRepository while the database is not yet running. This correctly fails. However, if I attempt to create another session within the same thread after a short while, an exception occurs saying that the repository has already been locked. I would expect the repository folder not to be locked. Maybe the code meant to remove the .lock file was not triggered because of an uncaught exception.

Please see the attached files:
-a test class to reproduce the problem
-my repository.xml config
-the log file quantel.txt with details about the stack trace.
"
0,"Tests not executable for already present mixinsorg.apache.jackrabbit.test.api.NodeRemoveMixinTest.testCheckedIn() and 
org.apache.jackrabbit.test.api.NodeAddMixinTest.testCheckedIn() 

fail when the mixin being added is already present on the node. The tests should check for this and trow a NotExecutableException."
0,"toplevel exception cleanupHttpClient.execute should throw only one exception, for easier general use.
HttpMethod constructors (HttpGet, HttpPut, etc..) should throw IllegalArgumentException in the string constructor (imply the string is pre-checked).  People wanting to see a URIException can use 'new HttpGet(new URI(uri))' and trigger the exception from the explicit URI creation."
0,"o.a.jackrabbit.spi.commons.conversion.NameParser should not assume that namespace URI's are registeredaccording to JCR 2.0, ""3.4.3.4 Parsing Lexical Paths"":

<quote>
An otherwise valid path containing an expanded name with an unregistered 
namespace URI will always resolve into a valid internal representation of a path 
</quote>

the current implementation assumes that namespace URIs encountered in 
expanded form names are registered, otherwise the name is treated as
qualified name. "
1,"DbDatastore: Problems indexing pdf fileAs reported by Claus Köll:

When importing a pdf file into a repository configured with a DbDataStore the following exception occurs. This happens only when using the DbDataStore with copyWhenReading=true

java.io.IOException: Stream closed
       at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:156)
       at java.io.BufferedInputStream.read(BufferedInputStream.java:315)
       at org.apache.jackrabbit.core.data.db.TempFileInputStream.read(TempFileInputStream.java:107)
       at java.io.BufferedInputStream.read1(BufferedInputStream.java:265)
       at java.io.BufferedInputStream.read(BufferedInputStream.java:324)
       at java.io.BufferedInputStream.fill(BufferedInputStream.java:229)
       at java.io.BufferedInputStream.read(BufferedInputStream.java:246)
       at java.io.FilterInputStream.read(FilterInputStream.java:89)
       at java.io.PushbackInputStream.read(PushbackInputStream.java:141)
       at org.pdfbox.io.PushBackInputStream.peek(PushBackInputStream.java:71)
       at org.pdfbox.io.PushBackInputStream.isEOF(PushBackInputStream.java:88)
       at org.pdfbox.pdfparser.PDFParser.parseObject(PDFParser.java:370)
       at org.pdfbox.pdfparser.PDFParser.parse(PDFParser.java:176)"
1,"o.a.j.spi.commons.query.sql2.ParserTest uses platform encoding with non-ASCII charactersThe ParserTest class loads a series of test SQL statements from test.sql2.txt, which contains a few non-ASCII characters (good to test those!). Unfortunately the file is read using the default platform encoding, which breaks the Linux-based test builds.

I'll recode the file to UTF-8 and explicitly specify the encoding when the file is read."
0,"Fix for small syntax omission in TermQuery documentationA coding example, which could be cut'n'paste by a user, has unbalanced parenthesis.

This fix corrects the documentation, making no changes to functionality, only readability."
1,JCA build failure with J2EE 1.3The fix to JCR-736 introduced a similar problem as was previously reported in JCR-413. The fix in JCR-413 should apply also to this case.
1,"Benchmark does not close its Reader when OpenReader/CloseReader are not usedOnly the Searcher is closed, but because the reader is passed to the Searcher, the Searcher does not close the Reader, causing a resource leak."
0,"Deprecate IndexModifierSee discussion at http://www.gossamer-threads.com/lists/lucene/java-dev/52017?search_string=deprecating%20indexmodifier;#52017

This is to deprecate IndexModifier before 3.0 and remove it in 3.0.

This patch includes:
  1 IndexModifier and TestIndexModifier are deprecated.
  2 TestIndexWriterModify is added. It is similar to TestIndexModifer but uses IndexWriter and has a few other changes. The changes are because of the difference between IndexModifier and IndexWriter.
  3 TestIndexWriterLockRelease and TestStressIndexing are switched to use IndexWriter instead of IndexModifier."
0,"Incorrect slf4j-log4j12 dependency scope in spi-commonsThe slf4j-log4j12 dependency scope in jackrabbit-spi-commons is ""runtime"", when it should be ""test"". We don't want to impose a specific logging solution to downstream projects."
0,JSR 283: New Event Types
0,"GetReferencesNodeTest test assumptionsBad test assumptions in GetReferencesNodeTest:

1) In setUp(): there is a primary node type including mixin:versionable. Proposed fix: just create the node, try to add mixin:versionable, check the node type after save.

2) The repository supports non-protected reference properties. Proposed fix: check with AbstractJCRTest's ensureCanSetProperty method, and let NotExecutableException be thrown.
"
0,"Remove commons-collections and slf4j-api dependencies from jcr-commonsAs noted in JCR-1615 and discussed on the mailing list [1] it would be good if jackrabbit-jcr-commons didn't come with extra dependencies beyond the standard Java class libraries and the JCR API.

Currently jackrabbit-jcr-commons depends on both commons-collections and slf4j-api, but both dependencies are relatively isolated and could be dropped with relatively little effort. Both dependency changes may be backwards incompatible with existing clients, but since the impact is reasonably small and easy to resolve I'd be OK doing this in 1.5.

[1] http://markmail.org/message/724ruk4l7b5rjtan"
0,"Generify PriorityQueuePriority Queue should use generics like all other Java 5 Collection API classes. This very simple, but makes code more readable."
0,"Revise internal data structures of ThreadSafeClientConnManagerThreadSafeClientConnManager internal data structures can be improved:
- keep track of issued connections with weak references
- use class derived from WeakReference instead of a lookup table for callbacks from ReferenceThread
  (or drop ReferenceThread in favor of occasionally polling the issued connections for leaks)
"
0,"Log / trace wrapper for the JCR APII have implemented the log / trace mechanism for the JCR API. A short summary:

- A wrapper for a Repository. All other objects that where created directly or indirectly (Session, Node and so on) are wrapped as well. 
- The wrappers log all JCR method calls to a file and call the underlying methods. 
- Return values and calling method / line number can be logged as well (optional). 
- The log file itself is mainly Java source code and can be compiled and run.
- Included is a player to re-play log files (for example, if the log file is too big to be compiled).
"
0,"""ant dist"" no longer generates md5's for the top-level artifactsMark hit this for 2.9.0, and I just hit it again for 2.9.1.  It used to work..."
0,"ChildNodeEntriesImpl.update logs incorrect errorsThe ChildNodeEntriesImpl logs errors on a correct update.

""ChildInfo iterator contains multiple entries with the same name|index or uniqueID -> ignore ChildNodeInfo.""  (line 186)"
1,"leak in MultiThreadedHttpConnectionManager.ConnectionPool.mapHostsOnce entries are added to MultiThreadedHttpConnectionManager.ConnectionPool.mapHosts, they are never cleaned up unless MultiThreadedHttpConnectionManager is shutdown."
0,"Add toString() or getName() method to IndexReaderIt would be very useful for debugging if IndexReader either had a getName() method, or a toString() implementation that would get a string identification for the reader.

for SegmentReader, this would return the same as getSegmentName()
for Directory readers, this would return the ""generation id""?
for MultiReader, this could return something like ""multi(sub reader name, sub reader name, sub reader name, ...)

right now, i have to check instanceof for SegmentReader, then call getSegmentName(), and for all other IndexReader types, i would have to do something like get the IndexCommit and get the generation off it (and this may throw UnsupportedOperationException, at which point i have would have to recursively walk sub readers and try again)

I could work up a patch if others like this idea"
0,"need DOAP file for LuceneCan someone please draft a DOAP file for Lucene, so that we're listed at http://projects.apache.org/?

A DOAP generator is at:

http://projects.apache.org/create.html

Please attach it to this bug report.  Thanks."
0,"MTQ rewrite + weight/scorer init should be single passSpinoff of LUCENE-2690 (see the hacked patch on that issue)...

Once we fix MTQ rewrite to be per-segment, we should take it further and make weight/scorer init also run in the same single pass as rewrite."
0,"Optimize BlockTermsReader.seekWhen we seek, we first consult the terms index to find the right block
of 32 (default) terms that may hold the target term.  Then, we scan
that block looking for an exact match.

The scanning just uses next() and then compares the full term, but
this is actually rather wasteful.  First off, since all terms in the
block share a common prefix, we should compare the target against that
common prefix once, and then only compare the new suffix of each
term.  Second, since the term suffixes have already been read up front
into a byte[], we should do a no-copy comparison (vs today, where we
first read a copy into the local BytesRef and then compare).

With this opto, I removed the ability for BlockTermsWriter/Reader to
support arbitrary term sort order -- it's now hardwired to
BytesRef.utf8SortedAsUnicode.
"
1,"InternalValue.createCopy for binary properties (jcr:data) causes problemsRunning 1.4 with no data store configured, and option org.jackrabbit.useDataStore not set (i.e true), the following code gives 0 for the property length.

Node n = root.getNode(relPath);
session.getWorkspace().copy(n.getPath(), destPath);
Node contentNode = n.getNode(JcrConstants.JCR_CONTENT);
Property p = contentNode.getProperty(JcrConstants.JCR_DATA);
System.out.println(""length = ""+p.getLength());

InternalValue.createCopy checks USE_DATA_STORE and returns the same value for the source node's state. BundleBinding.writeState() calls BLOBInMemory.discard() when persisting the new node. This has now changed the value of the existing nodes property. Setting the option org.jackrabbit.useDataStore to false works fine. Possibly the check for binary property type in InternalValue.createCopy should be done first?"
0,"Disk full during addIndexes(Directory[]) can corrupt indexThis is a spinoff of LUCENE-555

If the disk fills up during this call then the committed segments file can reference segments that were not written.  Then the whole index becomes unusable.

Does anyone know of any other cases where disk full could corrupt the index?

I think disk full should worse lose the documents that were ""in flight"" at the time.  It shouldn't corrupt the index."
0,"Via NTLM proxy to SSL Apache/BasicAuth. - worked in may 22nd, but broken in beta1Hi there,

This morning I downloaded beta 1 and tried a small piece of code to connect to 
a SSLified apache server (using basic authentication) via a MS-Proxy 2.0 with 
NTLM enabled. The sourcecode of my crashme is based on the 1st attachment for 
HTTPCLIENT-153. It differs from the original in using basic authentication for the 
webserver instead of NTLM.

It failed with this error:

--
10-jun-2003 16:39:05 org.apache.commons.httpclient.HttpMethodBase 
processAuthenticationResponse
INFO: Already tried to authenticate to ""website#"" but still receiving 407.
Status: 407 : Proxy authentication required
--

Then I downloaded a fresh night build (commons-httpclient-20030605) which also 
failed :/

Then I went back to an old build from May (commons-httpclient-20030522) which 
worked like a charm!!!

Using MSIE I can succesfully connect to the apache server. I know it's not a 
problem with typos because I have MSIE ask me for all creds.

Seems somethings got broken along the way. If I can help, please ask!

Cheers."
0,"repository-1.5.dtd: change order of main elementsCurrently the order of elements in repository.xml is:
<!ELEMENT Repository (FileSystem,Security,Workspaces,Workspace,Versioning,SearchIndex?,Cluster?,DataStore?)>

I would like to change it to
<!ELEMENT Repository (Cluster?,FileSystem,DataStore?,Security,Workspaces,Workspace,Versioning,SearchIndex?)>
because I think that makes more sense.

Currently XML validation is disabled, and therefore the order of elements in the DTD does not need to match the repository.xml file. However as soon as XML validation is enabled, repository.xml files that use the wrong order will no longer work (the repository can not be started).

There is a request to enable XML validation at http://issues.apache.org/jira/browse/JCR-1462
"
0,"Stats for QueriesRe-enable the stats for queries, as they were disabled during the refactoring phase."
0,"Collapse nested OR expressionsExecuting a query with multiple OR expressions in a predicate leads to score values that depend on the order of the operands.

For example, the following query:

//*[jcr:contains(@prop1, 'foo') or jcr:contains(@prop2, 'foo') or jcr:contains(@prop3, 'foo')] order by @jcr:score descending

will return a slightly different result compared to:

//*[jcr:contains(@prop3, 'foo') or jcr:contains(@prop1, 'foo') or jcr:contains(@prop2, 'foo')] order by @jcr:score descending

Internally jackrabbit parses the predicate of the first query into a tree:

orExpr(orExpr(contains(prop1, 'foo'), contains(prop2, 'foo')), contains(prop3, 'foo'))

Lucene will calculate the score for the inner OR expression first and then for the outer, which is not equivalent with a nested expression that has property names in a different sequence.

The query should be translated internally into a single OR expression with three operands. That way, the score value is always the same, irrespective of the order of the operands."
1,"wrong eval order of access control entries within a single node (node-based ac)it seems to me that with the node-based access control the ac entries within a given node are currently collected in the wrong order.
if i remember correctly this worked before and i removed at some point (for reasons i don't recall exactly but have the vague idea that it
was related to the allow-only for groups).

anyway:
while playing around with the permission in our CRX recently i found, that the evaluation of the following setup didn't work as I would
have expected:

- user A is member of group B and C
- for both groups an ACE exists on a given node /a/b/c
- the acl looks like  { deny for B, allow for C }

I would have expected that the allow for C would have reverted the previous deny for B since - in the GUI - I read the ace eval order from first entry to last entry... in the order I added them."
0,Use the remote-resources-plugin to add LICENSE and NOTICE files to binariesSince JCRSITE-13 the remote resources plugin is configured to automatically add LICENSE and NOTICE files to all of our binary artifacts (including -sources and -javadoc jars). We should adapt the configuration so that these files get to include all the correct licensing metadata we currently maintain in src/main/resources/META-INF.
0,"Avoid exceptions thrown in finalize handler of RepositoryImpl constructorIf an exception happens during initialization of the repository, it might be overlayed by an exception thrown in the finalize handler of the RepositoryImpl constructor (see line 382 ff in [1]). The latter exception wins and the original exception is lost (if you don't have a log). This makes it hard to figure out the real problem.

This problem is actually a bit self-enforcing: if something goes wrong during startup, the code in the shutdown() method that is called is actually very prone to fail as it might not expect such a broken-startup state. In my case the overlaying NPE happened in ObservationManagerImpl.getRegisteredEventListeners, where this.dispatcher was unexpectedly null [2].

I think both places should be fixed (NPE guard in ObservationManagerImpl constructor for ""dispatcher"") and a try/catch block in the finalizer, just logging the exception:

    } finally {
        if (!succeeded) {
            try {
                // repository startup failed, clean up...
                shutdown();
            } catch (Throwable t) {
                // shutdown() likely to fail now, as startup was broken...
                log.error(""In addition to startup fail, another problem occurred while shutting down the repository again."", e);
            }
        }
    }


[1] http://svn.apache.org/viewvc/jackrabbit/trunk/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/RepositoryImpl.java?view=markup

[2] Overlaying exception's stacktrace:
Caused by: java.lang.NullPointerException
	at org.apache.jackrabbit.core.observation.ObservationManagerImpl.getRegisteredEventListeners(ObservationManagerImpl.java:143)
	at org.apache.jackrabbit.core.SessionImpl.removeRegisteredEventListeners(SessionImpl.java:1190)
	at org.apache.jackrabbit.core.SessionImpl.logout(SessionImpl.java:1215)
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.doDispose(RepositoryImpl.java:2153)
	at com.day.crx.core.CRXRepositoryImpl$CRXWorkspaceInfo.doDispose(CRXRepositoryImpl.java:1095)
	at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.dispose(RepositoryImpl.java:2108)
	at org.apache.jackrabbit.core.RepositoryImpl.doShutdown(RepositoryImpl.java:1146)
	at com.day.crx.core.CRXRepositoryImpl.doShutdown(CRXRepositoryImpl.java:845)
	at org.apache.jackrabbit.core.RepositoryImpl.shutdown(RepositoryImpl.java:1098)
	at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:387)
	at com.day.crx.core.CRXRepositoryImpl.<init>(CRXRepositoryImpl.java:201)
	at com.day.crx.core.CRXRepositoryImpl.create(CRXRepositoryImpl.java:190)
	... 28 more
"
0,"wildcardquery rewrite improvementswildcardquery has logic to rewrite to termquery if there is no wildcard character, but
* it needs to pass along the boost if it does this
* if the user asked for a 'constant score' rewriteMethod, it should rewrite to a constant score query for consistency.

additionally, if the query is really a prefixquery, it would be nice to rewrite to prefix query.
both will enumerate the same number of terms, but prefixquery has a simpler comparison function."
0,"terms index should not store useless suffixesThis idea came up when discussing w/ Robert how to improve our terms index...

The terms dict index today simply grabs whatever term was at a 0 mod 128 index (by default).

But this is wasteful because you often don't need the suffix of the term at that point.

EG if the 127th term is aa and the 128th (indexed) term is abcd123456789, instead of storing that full term you only need to store ab.  The suffix is useless, and uses up RAM since we load the terms index into RAM.

The patch is very simple.  The optimization is particularly easy because terms are now byte[] and we sort in binary order.

I tested on first 10M 1KB Wikipedia docs, and this reduces the terms index (tii) file from 3.9 MB -> 3.3 MB = 16% smaller (using StandardAnalyzer, indexing body field tokenized but title / date fields untokenized).  I expect on noisier terms dicts, especially ones w/ bad terms accidentally indexed, that the savings will be even more.

In the future we could do crazier things.  EG there's no real reason why the indexed terms must be regular (every N terms), so, we could instead pick terms more carefully, say ""approximately"" every N, but favor terms that have a smaller net prefix.  We can also index more sparsely in regions where the net docFreq is lowish, since we can afford somewhat higher seek+scan time to these terms since enuming their docs will be much faster."
0,"Path.getAncestor and Path.isAncestor are not symmetricAlthough the method names refer to ancestors they operate on sub-paths. Consider:

PathFactory pf = PathFactoryImpl.getInstance();
Path.Element p = pf.getParentElement();

Path path = pf.create(new Path.Element[]{p, p});
Path ancestor = path.getAncestor(1);

assertFalse(ancestor.isAncestorOf(path) )  

This is not what one would expect from looking an the method signatures. 
I suggest to rename getAncestor to getSubPath, clarify the javadoc, and deprecate getAncestorCount. 

A patch follows.
"
0,"Fix 2.9 contrib builds to succeed when JDK 1.4 is used (leaving out contribs that require 1.5)When you build and test Lucene 2.9 with Java 1.4, building and testing of contrib fails. This patch fixes this to repect the current compiler version and disables all contribs that need Java 1.5 by checking their javac.source property.

This patch can be ported to 3.x or trunk, when 1.6 contribs will appear."
0,"Remove Serializable on ItemState classesItemStates are never directly serialized, which means they don't have to implement Serializable anymore.

See also: http://markmail.org/message/wsqnih2lembkcrdf"
0,"add IndexCommit.isOptimized methodSpinoff from here:

    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200807.mbox/%3C69de18140807010347s6269fea5r12c3212e0ec0a12a@mail.gmail.com%3E"
0,Add benchmark task for FastVectorHighlighter
0,"add concurrent merge policyProvide the ability to handle merges in one or more concurrent threads, i.e., concurrent with other IndexWriter operations.

I'm factoring the code from LUCENE-847 for this."
0,"JCR2SPI: error level logging when cleaning up session locks LockManagerImpl.loggingOut() tries to unlock nodes that have a session lock. If, while doing so, a RepositoryException is thrown, this gets locked on error level.

The TCK tests tearDown code removes test nodes using a separate session; thus we see RepositoryExceptions for the simple reason that the nodes have already been removed by somebody else.

Proposal: handle ItemNotFoundExc and PathNotFoundExc separately, not logging them.
"
0,"improve HttpRoute APISome of the constructors of HttpRoute have three boolean parameters.
Use enumerations to reduce the potential for confusion.

The flags for tunnelled and layered are not independent, since layered implies tunnelled.
These can be combined to a 3-valued enum.
"
0,"Add open ended range query syntax to QueryParserThe QueryParser fails to generate open ended range queries.
Parsing e.g. ""date:[1990 TO *]""  gives zero results,
but
ConstantRangeQuery(""date"",""1990"",null,true,true)
does produce the expected results.

""date:[* TO 1990]"" gives the same results as ConstantRangeQuery(""date"",null,""1990"",true,true)."
1,"RMI problems prevent proper startup of the Jackrabbit webappA trouble in binding the repository to a RMI registry will prevent the entire Jackrabbit webapp from starting properly. Since RMI is seldom the primary function of the webapp, it's more appropriate to simply log a warning in such cases."
1,"URI path resolution problems.URI does not completely conform to RFC 2396.  In particular it does not handle the following 
relative URIs correctly:

../../../g
../../../../g"
0,"Make QValueFactoryImpl extensibleThe class is currently final and other modules therefore copied code. This kind of duplication it hard to maintain and should be avoided.

If QValueFactoryImpl would be designed to be extensible then other classes could reuse much of the code."
1,"IndexWriter.numDocs doesn't take into account applied but not flushed deletesThe javadoc states that buffered deletes are not taken into account and so you must call commit first.

But, if you do that, and you're using CMS, and you're unlucky enough to have a background merge commit just after you call commit but before you call .numDocs, you can still get a wrong count back.

The fix is trivial -- numDocs should also consult any pooled readers for their current del count.

This is causing an intermittent failure in the new TestNRTThreads.
"
1,"DatabaseJournal improperly finds tables in external schemas when used on OracleThe DatabaseJournal currently calls database metadata to determine if the journal table has already been created.  It uses the following code to do so:

ResultSet rs = metaData.getTables(null, null, tableName, null);

The Oracle driver sometimes will return the table if it is in another schema on the same database.  Other DBMS code within JackRabbit has a specific Oracle version that handles this case.  In order for the journal table to be properly created, Oracle databases will need the schema name included in the getTables() call."
0,"Cookie.compare(...) uses single instance STRING_COLLATOR to do blocking comparesI am using a MultiThreadedHttpConnectionManager with a single HttpClient instance and multiple GetMethod objects.  I have a 500 thread max.  I recently noticed that all 500 threads are in the same place and seem to be blocking each other - the stack trace is below.  I dug into the Cookie.compare(...) method and saw that it is using STRING_COLLARTOR.compare(c1.getPath(), c2.getPath()).  STRING_COLLATOR is defined as a single instance object, 'private static final RuleBasedCollator STRING_COLLATOR = (RuleBasedCollator) RuleBasedCollator.getInstance(new Locale(""en"", ""US"", """"));'.  I also saw that RuleBasedCollator.compare is synchronized.  That means that every thread that is trying to make a request is getting blocked while it tries to add cookies to the request method.  I do not see a workaround because this is the same static final object in every Cookie instance.  So, the more threads, the more synchronized comparisons.  At times I am fetching URLs all from the same site so I am going through this code a lot.  I need it to be much faster than it currently is because all of my threads are getting eaten up on this call and backlogging my system.  Can a different RuleBasedCollator be used for each compare (use the RuleBasedCollator.getInstance() for every compare?  I think that would solve things.

Name: pool-1-thread-1443: 72.21.206.5
State: BLOCKED on java.text.RuleBasedCollator@190330a owned by: pool-1-thread-1867: 72.21.206.5
Total blocked: 9,598  Total waited: 381

Stack trace: 
java.text.RuleBasedCollator.compare(RuleBasedCollator.java:396)
org.apache.commons.httpclient.Cookie.compare(Cookie.java:484)
org.apache.commons.httpclient.cookie.CookieSpecBase.addInPathOrder(CookieSpecBase.java:578)
org.apache.commons.httpclient.cookie.CookieSpecBase.match(CookieSpecBase.java:557)
org.apache.commons.httpclient.HttpMethodBase.addCookieRequestHeader(HttpMethodBase.java:1179)
org.apache.commons.httpclient.HttpMethodBase.addRequestHeaders(HttpMethodBase.java:1305)
org.apache.commons.httpclient.HttpMethodBase.writeRequestHeaders(HttpMethodBase.java:2036)
org.apache.commons.httpclient.HttpMethodBase.writeRequest(HttpMethodBase.java:1919)
org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java:993)
org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:397)
org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:170)
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:396)
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:324)"
0,"Move PatternAnalyzer out of contrib/memory to contrib/analyzersin the memory index contrib there is a PatternAnalyzer.
i think this analyzer belongs in contrib/analyzers instead, it has no relation to memory index."
0,[PATCH] import cleanupThis patch just removes useless imports so you get less warnings in Eclipse.
1,"NPE in MultiReader.isCurrent() and getVersion()I'm attaching a fix for the NPE in MultiReader.isCurrent() plus a testcase. For getVersion(), we should throw a better exception that NPE. I will commit unless someone objects or has a better idea."
0,Add support for benchmarking CollectorsAs the title says.
0,"Support for OpenOffice text extractionHi, here is the patch.

>hi nicolas,
>
>thanks for your offer to contribute your openoffice textfilter, that's greatly appreciated!
>
>i suggest you post a jira 'Improvement' or ""New Feature' issue and attach your code as an svn patch. somebody will take care 
>of it (i assume marcel), i.e. review your contribution and provide feedback/further instructions.

>cheers
>stefan

>On 2/2/06, Nicolas Jouanin <nicolas.jouanin@gmail.com> wrote:
>>
>>
>>
>> Hi Stefan,
>>
>>
>>
>> I work with Martin Perez, main developper of jLibrary.
>>
>> Therefore, I developed a class which extracts metadata and text 
>> content from any openoffice file (that was not the hardest job). This 
>> class is already used into jLibrary.
>>
>> As Martin suggested me, I used this class to create a new TextFilter 
>> subclass into textfilters contrib project. I downloaded textfilters 
>> project from svn, created my class into the project tree and tested it 
>> with a test class , just like it was done with the other extractors.
>>
>> I can send you the code if you want to review it, or just tell me how 
>> I can commit it.
>>
>>
>>
>> Regards,
>>
>>
>>
>> Nicolas.
"
0,"Benchmark's ContentSource should not rely on file suffixes to be lower cased when detecting file type (gzip/bzip2/text)file.gz is correctly handled as gzip, but file.GZ handled as text which is wrong.
"
0,"HTTP Version configuration and trackingHTTP version tracking is currently oversimplified with a single http11 boolean. 
Extend this to handle any http version simply, and efficiently.

Possible suggestion:
> get rid of setHttp11() an isHttp11
> void setHttpVersion(String version)
> String getHttpVersion()
> boolean isHttpVersion(String version)"
0,"Unit and integration test cases for the new SimilaritiesWrite test cases to test the new Similarities added in [LUCENE-3220|https://issues.apache.org/jira/browse/LUCENE-3220]. Two types of test cases will be created:
 * unit tests, in which mock statistics are provided to the Similarities and the score is validated against hand calculations;
 * integration tests, in which a small collection is indexed and then searched using the Similarities.

Performance tests will be performed in a separate issue."
0,"[API Doc] Compile performance optimization guidePerformance optimization guide is long overdue and badly needed. The more people
start using HttpClient in all sorts of creative ways the more we are going to
need it.

Oleg"
0,"JSR 283: Create RepositoryFactory implementationJSR 283 specifies a RepositoryFactory to retrieve a repository instance based on a map of parameters. We should have the following implementations:

- local repository with repository home and repository configuration parameters
- repository obtained via JNDI
- repository obtained via RMI
"
0,"reorganize contrib modulesit would be nice to reorganize contrib modules, so that they are bundled together by functionality.

For example:
* the wikipedia contrib is a tokenizer, i think really belongs in contrib/analyzers
* there are two highlighters, i think could be one highlighters package.
* there are many queryparsers and queries in different places in contrib
"
1,"TransientRepository with LocalFileSystem eventually causes Repository data to be stored at path '/'I'm using a TransitoryRepository for my unit testing, with the repository's file system specified as:

    <FileSystem class=""org.apache.jackrabbit.core.fs.local.LocalFileSystem"">
        <param name=""path"" value=""${rep.home}/repository""/>
    </FileSystem>

I noticed today that when I run my unit tests Jackrabbit is creating four directories at the root of my hard drive: ""meta"", ""namespaces"", ""nodetypes"", and ""data"". I tracked the problem the fact that when a LocalFileSystem is closed, it sets the ""root"" to null - an invalid state. But when using a TransitoryRepository, the invalid state is never discovered because the LocalFileSystem object itself is not released, or re-initialized. It is simply used to create BasedFileSystem objects in RepositoryImpl. Calls to BasedFileSystem defer to the LocalFileSystem object that now has a null root. Inside the LocalFileSystem, all the calls to Java's io.File constructor have a ""null"" parent parameter, causing File to fall back to its single argument constructor which sees the path ""/meta"" and happily creates files at the root of the disk.

I'm not sure what the best solution is, but some thoughts I've had are:
- don't set the ""root"" property to null when closing a LocalFileSystem
- make RepositoryConfig re-init the FileSystem variable when it is accessed.
- don't cache the RepositoryConfig in TransitoryRepository (this might also require a new constructor that takes a class-path resource for the repository configuration file)"
0,FieldValueFitler should expose the field it usesFieldValueFitler should expose the field it uses. It currently hides this entirely.
1,"Abort Before Execute & Various Other Times FailsWith svn commit #639506, a few more scenarios become testable & can be fixed.  These are: aborting before HttpClient.execute is called, aborting between setting the connection request for aborting and setting the connection release trigger, and aborting after a redirected route uses a new connection request.  As of r639506, those three scenarios fail to abort correctly."
0,"Jcr2Spi: Avoid extra round trip to the SPI upon Node.getNode and Session.getItemUpon Session.getItem/itemExists and Node.getNode/hasNode JCR2SPI currently tries to load the Node from the persistent layer (SPI) if no corresponding entry exists in the hierarchy.

Since with JCR-1638 a flag has been introduced indicating if the child node entries are complete. In this case, the extra round trip could be omitted."
0,"improve performance when saving a node with a large number of child nodes (e.g. > 10k child node entries)JCR-307 brought a significant improvement WRT saving nodes with a large number of child nodes

unfortunately JCR-2579 broke part of the optimization (see NodeState.setChildNodeEntries(List))."
1,"jcr:successors property not persisted correctly within a transactionDuring a transaction, if you create a new version then read the version history the ""jcr:successors"" property is not updated. Note that ""jcr:predecessors"" is updated properly.

Also, the version history is sometimes not propertly read. During the transaction, it might appear empty. This behavior in not consistent from one execution to another.

After a restart of the repository, the version history and the ""jcr:successors"" property is read properly.

* Tests cases will follow shortly.

Thanks, 

Nicolas"
0,"Small speedups to DocumentsWriter's quickSortIn working on LUCENE-510 I found that DocumentsWriter's quickSort can
be further optimized to handle the common case of sorting only 2
values.

I ran with this alg:

  analyzer=org.apache.lucene.analysis.standard.StandardAnalyzer
  
  doc.maker=org.apache.lucene.benchmark.byTask.feeds.LineDocMaker
  
  docs.file=/Volumes/External/lucene/wiki.txt
  doc.stored = true
  doc.term.vector = true
  doc.add.log.step=2000
  doc.maker.forever = false
  
  directory=FSDirectory
  autocommit=false
  compound=false
  
  ram.flush.mb=64
  
  { ""Rounds""
    ResetSystemErase
    { ""BuildIndex""
      CreateIndex
      { ""AddDocs"" AddDoc > : 200000
      - CloseIndex
    }
    NewRound
  } : 5
  
  RepSumByPrefRound BuildIndex

Best of 5 was 857.3 docs/sec before the optimization and 881.6 after =
2.8% speedup, on a quad-core Mac Pro with 4-drive RAID 0 array.

The fix is trivial.  I will commit shortly.

"
0,"getDocValues should provide a MultiReader DocValues abstractionWhen scoring a ValueSourceQuery, the scoring code calls ValueSource.getValues(reader) on *each* leaf level subreader -- so DocValue instances are backed by the individual FieldCache entries of the subreaders -- but if Client code were to inadvertently  called getValues() on a MultiReader (or DirectoryReader) they would wind up using the ""outer"" FieldCache.

Since getValues(IndexReader) returns DocValues, we have an advantage here that we don't have with FieldCache API (which is required to provide direct array access). getValues(IndexReader) could be implimented so that *IF* some a caller inadvertently passes in a reader with non-null subReaders, getValues could generate a DocValues instance for each of the subReaders, and then wrap them in a composite ""MultiDocValues"".


"
1,"IndexWriter.addIndexes(IndexReader[]) fails to create compound filesEven if no exception is thrown while writing the compound file at the end of the 
addIndexes() call, the transaction is rolled back and the successfully written cfs 
file deleted. The fix is simple: There is just the 
{code:java}
success = true;
{code}
statement missing at the end of the try{} clause.

All tests pass. I'll commit this soon to trunk and 2.3.2."
0,"fileformats.xml doesn't document compound file streamsCurrent versions of Lucene generate segments in compound file stream format
files, but the fileformats documentation does not have any description of the
format for those files."
0,"Redefine HttpClient vs HttpMultiClient interface for 2.0In particular the HttpClient/HttpMultiClient issue must be resolved. 
HttpultiClient functionality should be prefered, but HttpClient is the most
suitable name.  Consider impact to other projects.  Is java1.1 compatability
really an issue anymore?"
0,"Fix typos in CHANGES.txt and contrib/CHANGES.txt prior to 2.9 releaseI noticed a few typos in CHANGES.txt and contrib/CHANGES.txt.  (Once they make it past a release, they're set in stone...)

Will attach a patch shortly."
0,"TokenWrapperAttributeFactory, CachingWrapperFilterHelper implements equals and so should also implement hashCodeits part of the contract of Object 

bq. If two objects are equal according to the equals(Object) method, then calling the hashCode method on each of the two objects must produce the same integer result."
1,"In case of SocketTimeoutException and using HttpRequestRetryHandler the execution is always +1If my request encounter a SocketTimeoutException, the HttpRequestRetryHandler#retryRequest will be called with an executionCount with a value +1."
1,"Add the org.apache.jackrabbit.rmi.jackrabbit package to the rmic generation From the UnicastRemoteObject's (ServerJackrabbitNodeTypeManager, ServerJackrabbitWorkspace) should be stubs generated.
"
0,"Content-Length & Transfer-Encoding request headers should be handled by entity enclosing methodsCurrently 'Content-Length' & 'Transfer-Encoding' request headers are handled by 
the HttpMethodBase class. This is conceptually wrong and error-prone in my 
opinion. Entity enclosing methods should control 'Content-Length' & 'Transfer-
Encoding' request headers instead, as they provide request content and 
encapsulate the requisite content transfer logic."
0,"QValueFactory improvements1) Allow all create methods to throw RepositoryException.

2) Further document that create(value,type) can throw ValueFormatException.

3) Remove special case create(File)
"
0,"Remove old byte[] norms api from IndexReaderFollowup to LUCENE-3628.

We should remove this api and just use docvalues everywhere, to allow for norms of arbitrary size in the future (not just byte[])"
0,"Populating exception message with InetSocketAddress.getHostName() can take a long timeIn the PlainSocketFactory class, when a SocketTimeoutException occurs a call is made to InetSocketAddress.getHostName() when generating the exception message. Unfortunately, this call can take a long time. In my case, the address I am specifying is an IP address, which InetSocketAddress attempts to perform a reverse-lookup on to determine the hostname; however, since  the address does not have a hostname assigned to it, the operation takes a long time to return.

I'm attaching a patch for trunk with my proposed fix. Viewing the source history, it looks like the code used to have the behavior I'm proposing, but it was changed in revision 1070943. Based on the source commits and linked issues, I cannot determine a specific reason for the change. If there is a reason the code needs to be the way it is, then I apologize for inconvenience I have caused."
0,"Query#mergeBooleanQueries argument should be of type BooleanQuery[] instead of Query[]The method #mergeBooleanQueries accepts Query[] and casts elements to BooleanQuery without checking. This will guarantee a ClassCastException if it is not a boolean query. We should enforce this by changing the signature. This won't really break back compat. as it only works with instances of BooleanQuery.

"
1,"Malformed excerpt if content contains markup and no highlights foundAny markup in content that is used in an excerpt is encoded with corresponding entity references. However, this process is broken when there are no highlights in the excerpt. In this case, the content is provided as is in the excerpt, which may lead to malformed HTML/XML."
1,"cannot PUT changes to a resource in the simple webdav serverwhen using the simple webdav server to PUT a resource, the ""versionable"" mixin node type is assigned to the new node without regard to whether the node type is already assigned to the node. this causes PUT requests that change existing resources to fail with 403 errors.

the fix is to augment AddMixinCommand to not try to add the mixin node type if the node already has it.
"
0,"Duplicate log of HTTP headerThe HTTP header line:

""HTTP/1.1 200 OK[\r][\n]"" 

is duplicated in the wire logs. Seems to be because the line is logged at:

HttpParser [line: 131] - readLine(InputStream, String)

and at:

HttpMethodBase [line: 1980] - readStatusLine(HttpState, HttpConnection)

It looks like the latter log should be removed?"
0,Add support for boolean values to QValueI suggest to add support for reading and writing boolean values to QValue and QValueFactory. I find it strange that there is such support for the other data types but booleans must be constructed via strings. 
1,ItemInfoBuilder fails to set correct path on propertiesThis only happens if the parent node's nodeId is id based (in contrast to path based). The build() method should not rely on the nodeId providing the full path. Instead it should us the parent node's getPath() method to construct the full path. 
0,"Index sorterA tool to sort index according to a float document weight. Documents with high weight are given low document numbers, which means that they will be first evaluated. When using a strategy of ""early termination"" of queries (see TimeLimitedCollector) such sorting significantly improves the quality of partial results.

(Originally this tool was created by Doug Cutting in Nutch, and used norms as document weights - thus the ordering was limited by the limited resolution of norms. This is a pure Lucene version of the tool, and it uses arbitrary floats from a specified stored field)."
0,"Source distribution packaging targets should make a tarball from ""svn export""Instead of picking and choosing which stuff to include from a local working copy, Lucene's dist-src/package-tgz-src target and Solr's package-src target should simply perform ""svn export"" with the same revision and URL as the local working copy."
1,"ContentEncodingHttpClient.execute(HttpGet, ResponseHandler<T>) throws IOException when reading compressed responseThe following snippet:

    String url = ""http://yahoo.com"";
    HttpClient httpClient = new ContentEncodingHttpClient();
    HttpGet get = new HttpGet(url);
    String content = httpClient.execute(get, new BasicResponseHandler());

throws:

java.io.IOException: Attempted read from closed stream.
	at org.apache.http.impl.io.ChunkedInputStream.read(ChunkedInputStream.java:126)
	at java.util.zip.CheckedInputStream.read(CheckedInputStream.java:42)
	at java.util.zip.GZIPInputStream.readUByte(GZIPInputStream.java:205)
	at java.util.zip.GZIPInputStream.readUShort(GZIPInputStream.java:197)
	at java.util.zip.GZIPInputStream.readHeader(GZIPInputStream.java:136)
	at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:58)
	at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:68)
	at org.apache.http.client.entity.GzipDecompressingEntity.getContent(GzipDecompressingEntity.java:63)
	at org.apache.http.conn.BasicManagedEntity.getContent(BasicManagedEntity.java:88)
	at org.apache.http.util.EntityUtils.consume(EntityUtils.java:65)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:974)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:919)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:910)
	at tv.adap.service.HttpPoolTest.testChunkedGzip(HttpPoolTest.java:41)

whereas the following snippet runs fine:

    String url = ""http://yahoo.com"";
    HttpClient httpClient = new ContentEncodingHttpClient();
    HttpGet get = new HttpGet(url);
    HttpResponse response = httpClient.execute(get);
    HttpEntity entity = response.getEntity();
    String content = EntityUtils.toString(entity);

These two snippets should be functionally the same (putting the entity body into content). Creating a JIRA per the recommendation of Oleg from httpclient-users."
0,"Allow indexingConfiguration to be loaded from the classpathThe ""indexingConfiguration"" attribute in the SearchIndex configuration (http://wiki.apache.org/jackrabbit/IndexingConfiguration) actually requires an absolute filesystem path.

It would be nice if SearchIndex would also accept a file available in the classpath... although you can use variables like ${wsp.home} or similar there are many scenarios where a classpath resource would help (for example when creating a new workspace the directory structure is automatically created by jackrabbit and doesn't need to be already available but the indexing configuration file does).

I am attaching a simple patch to SearchIndex that tries to load the file from the classpath if it has not been found. Since priority is given to the old behavior (file before classpath) so it's fully backward compatible.

Diff has been generated against trunk, it would be nice to have this patch also on the 2.0 branch.
 
 "
0,"Flexible query parser does not support open ranges and range queries with mixed inclusive and exclusive rangesFlexible query parser does not support open ranges and range queries with mixed inclusive and exclusive ranges.

These two problems were found while developing LUCENE-1768."
0,"Implement a cache to perform real request only when neededBrowsers may cache received content according to the values of different
response headers. It would be great if HttpClient could do the same."
1,"New socket timeout value wont have effect if connection is reusedReported by Teemu Tingander <Teemu.Tingander at tecnomen.fi> on The Jakarta
Commons HttpClient Developer List:

<snip>
Changing read timeout ()wont affect after successful method execution using
same connection.. 

This seems to be a bug in HttpClient class method
executeMethod(HostConfiguration ...)..

The problematic section seems to be if section checking if connection is
open
	
		method.setStrictMode(strictMode);
        		        
            if (!connection.isOpen()) {                
                connection.setConnectionTimeout(connectionTimeout);
-->		    connection.setSoTimeout(soTimeout);
                connection.open();
                if (connection.isProxied() && connection.isSecure()) {
                    method = new ConnectMethod(method);
                }
            }
 
Problem can be solved by moving the line out of if section

		method.setStrictMode(strictMode);

		connection.setSoTimeout(soTimeout);	
        		        
            if (!connection.isOpen()) {                
                connection.setConnectionTimeout(connectionTimeout);
                connection.open();
                if (connection.isProxied() && connection.isSecure()) {
                    method = new ConnectMethod(method);
                }
            }
</snip>"
0,"Add support for simple test casesAs discussed on the mailing list, I'd like to add a simple org.apache.jackrabbit.core.TestRepository helper class that could be used in a simple unit test template."
0,"Add getIndexCommit method to IndexReaderSpinoff from this thread:

  http://markmail.org/message/bojgqfgyxkkv4fyb

I think it makes sense ask an IndexReader for the commit point it has
open.  This enables the use case described in the above thread, which
is to create a deletion policy that is able to query all open readers
for what commit points they are using, and prevent deletion of them.

"
0,"Improve ArrayUtil/CollectionUtil.*Sort() methods to early-reaturn on empty or one-element lists/arraysIt might be a good idea to make CollectionUtil or ArrayUtil return early if the passed-in list or array's length <= 1 because sorting is unneeded then. This improves maybe automaton or other places, as for empty or one-element lists no SorterTermplate is created."
0,"Closing a session twice shouldn't write a warning in the logWhen closing a session twice the following warning is written to the log file as of JCR-2741:

""This session has already been closed. See the chained exception for a trace of where the session was closed.""

I think the second ""close()"" should simply be ignored, without warning.
"
0,JSR 283: Configurations and Baselines
0,Deprecated API called in o.a.l.store Directoriesjust ran into NIOFSDirectory and others still call getFile instead of getDirectory
0,Move extensions to the JSR 283 security API  from jackrabbit-core to jackrabbit-apiFor the 2.0.0 release i'd like to have the jackrabbit-specific extensions to the JSR 283 security API being part of jackrabbit-api.
0,Split the wire log into header and content parts. 
0,"contrib/javascript is not packaged into releasesthe contrib/javascript directory is (apparently) a collection of javascript utilities for lucene .. but it has not build files or any mechanism to package it, so it is excluded form releases.

"
1,"Check for correct content-type in URLEncodedUtils not working for encoding-suffixesDear DEV-Team,

i am developing an application with the httpclient. Today i found a small problem, related to URLEncodedUtils.

Our Tomcat-Server deliveres for Server-Requests, the HTTP-Header: ""Content-Type=application/x-www-form-urlencoded;charset=UTF-8"", but the httpclient only checks for: ""Content-Type=application/x-www-form-urlencoded"". This failing check results in an empty result of call to the method: URLEncodedUtils.parse(entity);

Following source-code causes the prob: 

public class URLEncodedUtils {

    /**
     * Returns true if the entity's Content-Type header is
     * <code>application/x-www-form-urlencoded</code>.
     */
    public static boolean isEncoded (final HttpEntity entity) {
        final Header contentType = entity.getContentType();
        return (contentType != null && contentType.getValue().equalsIgnoreCase(CONTENT_TYPE));
    }
}

IMO the method should be changed to:


public class URLEncodedUtils {

    /**
     * Returns true if the entity's Content-Type header is
     * <code>application/x-www-form-urlencoded</code>.
     */
    public static boolean isEncoded (final HttpEntity entity) {
        final Header contentType = entity.getContentType();
        return (contentType != null && contentType.getValue().startsWith(CONTENT_TYPE + "";""));
    }
}

Best Regards,"
0,"TestThreadSafety.testLazyLoadThreadSafety test failureTestThreadSafety.testLazyLoadThreadSafety failed with this error:

unable to create new native thread

Maybe because of SimpleText

Here is the stacktrace:
{noformat}
 [junit] Testsuite: org.apache.lucene.search.TestThreadSafe
    [junit] Testcase: testLazyLoadThreadSafety(org.apache.lucene.search.TestThreadSafe):	Caused an ERROR
    [junit] unable to create new native thread
    [junit] java.lang.OutOfMemoryError: unable to create new native thread
    [junit] 	at java.lang.Thread.start0(Native Method)
    [junit] 	at java.lang.Thread.start(Thread.java:614)
    [junit] 	at org.apache.lucene.search.TestThreadSafe.doTest(TestThreadSafe.java:129)
    [junit] 	at org.apache.lucene.search.TestThreadSafe.testLazyLoadThreadSafety(TestThreadSafe.java:148)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:795)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:768)
    [junit] 
    [junit] 
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 6.051 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestThreadSafe -Dtestmethod=testLazyLoadThreadSafety -Dtests.seed=-277698010445513699:-89599297372877779
    [junit] NOTE: test params are: codec=SimpleText, locale=zh_SG, timezone=Pacific/Tongatapu
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.search.TestThreadSafe FAILED
{noformat}"
0,"don't download/extract 20,000 files when doing the buildWhen you build lucene, it downloads and extracts some data for contrib/benchmark, especially the 20,000+ files for the reuters corpus.
this is only needed for one test, and these 20,000 files drive IDEs and such crazy.
instead of doing this by default, we should only download/extract data if you specifically ask (like wikipedia, collation do, etc)

for the qualityrun test, instead use a linedoc formatted 587-line text file, similar to reuters.first20.lines.txt already used by benchmark.
"
0,"overhaul connection manager and associated connection interfaceMultiThreadedHttpConnectionManager/HttpHostConnection needs to be overhauled to provide a layer on top of OperatedClientConnection.
Preliminary working names: ThreadSafeClientConnManager/ManagedClientConnection

This implies some work on former HttpMethodDirector and HttpClient to verify completeness of the new connection management API.
"
0,MultiPhraseQuery should allow access to terms
1,GC resources in TermInfosReader when exception occurs in its constructorI replaced IndexModifier with IndexWriter in test case TestStressIndexing and noticed the test failed from time to time because some .tis file is still open when MockRAMDirectory.close() is called. It turns out it is because .tis file is not closed if an exception occurs in TermInfosReader's constructor.
0,"upgrade icu to 4.8we should upgrade from 4.6 to 4.8.

some internal methods became public, also a package-private reflection hack can be removed."
0,"New method to add an array of parameters to PostMethodWhen posting a form a web page may have many parameters to post to the 
webserver.  Currently in PostMethod, if you wanted to add multiple parameters, 
you would have to call addParameter(name, value) for each one.

A new convinence method should be added to allow for simpler client code by 
taking an array of NameValuePair objects and adding all parameters in a single 
function call.

void addParameters(NameValuePair[] parameters) 

Also, the comments for PostMethod functions that deal with parameters 
state ""Override method of HttpMethodBase ..."" which is incorrect.  More 
informative comments should be added to this public API."
0,"Header adding in org.apache.http.client.protocol.RequestAcceptEncoding should be conditionalorg.apache.http.client.protocol.RequestAcceptEncoding adds a header in any case. Any chance to do it conditional (like in RequestClientConnControl)? The code would be something like
if (!request.containsHeader(""Accept-Encoding"")) {
    request.addHeader(""Accept-Encoding"", ""gzip,deflate"");
}

In our app this header may be added before request intercepting, so would be great if this fact is checked.
"
0,"Eliminate unnecessary uses of Hashtable and VectorLucene uses Vector, Hashtable and Enumeration when it doesn't need to. Changing to ArrayList and HashMap may provide better performance.

There are a few places Vector shows up in the API. IMHO, List should have been used for parameters and return values.

There are a few distinct usages of these classes:
# internal but with ArrayList or HashMap would do as well. These can simply be replaced.
# internal and synchronization is required. Either leave as is or use a collections synchronization wrapper.
# As a parameter to a method where List or Map would do as well. For contrib, just replace. For core, deprecate current and add new method signature.
# Generated by JavaCC. (All *.jj files.) Nothing to be done here.
# As a base class. Not sure what to do here. (Only applies to SegmentInfos extends Vector, but it is not used in a safe manner in all places. Perhaps, implements List would be better.)
# As a return value from a package protected method, but synchronization is not used. Change return type.
# As a return value to a final method. Change to List or Map.

In using a Vector the following iteration pattern is frequently used.
for (int i = 0; i < v.size(); i++) {
  Object o = v.elementAt(i);
}

This is an indication that synchronization is unimportant. The list could change during iteration.

"
1,"BundleDbPersistenceManager consistencyFix doesn't fix missing non system childnode  entries of the root nodeThe bundle check/fix mechanism completely skips the checks on the root node, but the root node can also have non system child node entries which can be broken/missing. The attached patch makes the check only check the non system child node entries of the root node. It would be nice if this patch (if/when accepted) could also be backported to the 1.5 and 1.6 branches.
"
0,"DavMethods.POST should be public, not privateDavMethods.POST is declared as a private constant, when it should be public. attached is a patch to fix this."
0,"ClientPNames.VIRTUAL_HOST is used as is; if not provided, the port should be derived from the target URLThe parameter ClientPNames.VIRTUAL_HOST allows the default Host header to be overridden.

Currently the code uses the HttpHost entry as provided, and does not automatically add the port suffix.
This means that user code has to provide the port - but only if it's not the default for the protocol.

It would be simpler for the user if the port were automatically added.

If the user does not provide the port, the code should derive it from the target URL.

If the user does provide a port number, then that should be used (as is done currently). 
This allows the user to override the port (if that should ever prove necessary)."
0,"Abstract JCR base classesImplement and use a set of abstract AbstractSession, AbstractItem, etc. classes that implement as much of the respective JCR interfaces using nothing else but calls to other JCR methods. These would be just like the AbstractMap, etc. classes in java.util.

(See the related discussion at http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/10583)"
0,"Make WordDelimiterFilter's instantiation more readableCurrently WordDelimiterFilter's constructor is:

{code}
public WordDelimiterFilter(TokenStream in,
	                             byte[] charTypeTable,
	                             int generateWordParts,
	                             int generateNumberParts,
	                             int catenateWords,
	                             int catenateNumbers,
	                             int catenateAll,
	                             int splitOnCaseChange,
	                             int preserveOriginal,
	                             int splitOnNumerics,
	                             int stemEnglishPossessive,
	                             CharArraySet protWords) {
{code}

which means its instantiation is an unreadable combination of 1s and 0s.  

We should improve this by either using a Builder, 'int flags' or an EnumSet."
0,Land DWPT on trunkWith LUCENE-2956 we have resolved the last remaining issue for LUCENE-2324 so we can proceed landing the DWPT development on trunk soon. I think one of the bigger issues here is to make sure that all JavaDocs for IW etc. are still correct though. I will start going through that first.
0,Add import-export toolWe at <GX> creative online development would like to contribute our command-line import-export tool to the Apache Jackrabbit project. This tool is capable of exporting and importing all kinds of repository content (including custom nodetypes and namespace mappings) in a persistence-layer independent way. 
1,"WebDAV server should treat non-wellformed XML in request bodies as errorThe WebDAV server should treat non-wellformed XML request bodies as errors (instead of treating the request as if the request body was missing).

(causes Litmus test suite failure in test case propfind_invalid)"
0,"Add FileBody constructor with explicit filenameFileBody does not allow the filename field in the Content-Disposition header to be overriden, the filename taken from the File object - I have software that creates temporary files and needs to assign an implicit logical filename."
1,"ConcurrentModificationException thrown in MultiThreaded codeNow seeing this error.  This is with default cookie settings.  Happening rarely, however the web sites we're talking to do not use cookies very much.


java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1010)
        at org.apache.http.client.protocol.RequestAddCookies.process(RequestAddCookies.java:152)
        at org.apache.http.protocol.BasicHttpProcessor.process(BasicHttpProcessor.java:290)
        at org.apache.http.protocol.HttpRequestExecutor.preProcess(HttpRequestExecutor.java:160)
        at org.apache.http.impl.client.DefaultClientRequestDirector.execute(DefaultClientRequestDirector.java:355)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:501)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:456)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:422)
        at com.hi5.os.Hi5RemoteContentFetcher.fetch(Hi5RemoteContentFetcher.java:279)"
0,Convenience method to Or multiple values with a single filterAdded a convenience method to add Or Filter for the same property with multiple values. This is to simulate an IN Clause in JackRabbit. 
0,"Maintain the cluster revision tableThe revision table in which cluster nodes write their changes can potentially become very large. If all cluster nodes are up to date to a certain revision number, then it seems unnecessary to keep the revisions with a lower number."
0,"improve documentation of SPI Batch addPropertyClarify that Batch.addProperty should succeed even though the property already exists.


(See mailing list thread starting with: http://mail-archives.apache.org/mod_mbox/jackrabbit-dev/200801.mbox/%3c47A1E1C1.2050107@gmx.de%3e)"
0,"Put everything in jackrabbit-spi-commons under org.apache.jackrabbit.spi.commonsTo avoid confusion and naming conflicts, we should put all classes and packages in jackrabbit-spi-commons under org.apache.jackrabbit.spi.commons."
0,"Change defaultValues format in NodeTypes XML to jcr valuecurrently, the defaultValues serialization in the nodetypes.xml is the only one that uses internal value serialization, rather than the jcr string serialization.
eg:

<propertyDef name=""jcr:requiredPrimaryTypes"" ..... >
  <defaultValues>
    <defaultValue>{http://www.jcp.org/jcr/nt/1.0}base</defaultValue>
  </defaultValues>
</propertyDef>

this in not very handy, when the custom_nodetypes.xml should be written automatically.
i suggest to change the serialization to use the jcr value one:

<propertyDef name=""jcr:requiredPrimaryTypes"" ..... >
  <defaultValues>
    <defaultValue>nt:base</defaultValue>
  </defaultValues>
</propertyDef>
"
0,"SharedFieldCache$StringIndex memory leak causing OOM's SharedFieldCache$StringIndex is not working properly. It is meant to cache the docnumbers in lucene along with the term to sort on. The issue is twofold. I have a solution for the second one, the first one is not really solvable from jr pov, because lucene index readers are already heavily caching Terms. 

Explanation of the problem:

For *each* unique property where is sorted on, a new lucene ScoreDocComparator is created (see SharedFieldComparator newComparator). This new comparator creates *per* lucene indexreader  SharedFieldCache.StringIndex which is stored in a WeakHashMap with as key, the indexreader . As this indexreader  almost *never* can be garbage collected (only if it is merged and thus unused after), the SharedFieldCache.StringIndex are there to be the rest of the jvm life (which is sometime short, as can be seen from the simple unittest attached).  Obviously, this results pretty fast in OOM.

1) issue one:  The cached terms[] in SharedFieldCache.StringIndex can become huge when you sort on a common property (date) which is present in a lot of nodes. It you sort on large properties, like 'title' this SharedFieldCache.StringIndex  will quickly use hundreds of Mb for a couple of hundred of thousand of nodes with a title. This issue is already a lucene issue, as lucene already caches the terms. OTOH, I really doubt whether we should index long string values as UNTOKENIZED in lucene at all. A half working solution might be a two-step solution, where the first sort is on the first 10 chars, and only if the comparator returns 0, take the entire string to sort on

2) issue two:  The cached terms[] in SharedFieldCache.StringIndex is frequently sparse, consuming an incredible amount of memory for string arrays containing mainly null values. For example (see attached unit test):

- add 1.000.000 nodes
- do a query and sort on a non existing property
- you'll loose 1.000.000 * 4 bytes ~ 4 Mb of memory
- sort on another non existing prop : another 4 Mb is lost
- do it 100 times --> 400 Mb is lost, and can't be reclaimed

I'll attach a solution which works really fine for me, still having the almost unavoidable memory absorption, but makes it much smaller. The solution is, that if < 10% of the String array is filled, i consider the array already sparse, and move to a HashMap solution. Performance does not decrease much (and in case of large sparsity increases because less memory consumption --> less gc, etc). 

Perhaps it does not seem to be a common issue (certainly the unit test) but our production environments memory snapshots indicate most memory being held by the SharedFieldCache$StringIndex (and the lucene Terms, which is harder to avoid)

I'd like to see this in the 1.5.1 if others are ok with it


"
0,"MultipartPostMethod does not check for error messagesIf a MultipartPost request is sent to a server which requires authentication, 
the server may respond to the request with an unauthorized header and close the 
connection before all of the data is sent.  HttpClient should monitor the 
incoming stream and cease transmitting the body if an error message is received 
(section 8.2.2 of rfc2616, see below).

At the very least HttpClient should check for a response when catching the 
HttpRecoverableException and retrying.  This probably should be done in 
HttpMethodBase so that we are in a known state when starting to retry the 
connection (ie: there isn't an existing response in the socket buffer to cause 
problems).

Ideally, HttpClient should also implement the 100 (Continue) status as 
specified in section 8.2.3 of rfc2616.

Finally, PostMethod should be tested to ensure that it does not exhibit this 
bug as well.

-------------
8.2.2 Monitoring Connections for Error Status Messages

   An HTTP/1.1 (or later) client sending a message-body SHOULD monitor
   the network connection for an error status while it is transmitting
   the request. If the client sees an error status, it SHOULD
   immediately cease transmitting the body. If the body is being sent
   using a ""chunked"" encoding (section 3.6), a zero length chunk and
   empty trailer MAY be used to prematurely mark the end of the message.
   If the body was preceded by a Content-Length header, the client MUST
   close the connection."
1,"spi2dav: Overwrite header T specified for MOVE and COPY causes failure if some API testsfailing tests are:

org.apache.jackrabbit.test.api.WorkspaceCopySameNameSibsTest#testCopyNodesNodeExistsAtDestPath
org.apache.jackrabbit.test.api.WorkspaceMoveSameNameSibsTest#testMoveNodesNodeExistsAtDestPath

those would be fixed by setting the overwrite header to F(alse)... however, this doesn't fit those cases where same-same
siblings would be allowed and the copy/move to a destination with existing item would succeed in JCR."
0,"Add a waitForMerges() method to IndexWriterIt would be very useful to have a waitForMerges() method on the IndexWriter.

Right now, the only way i can see to achieve this is to call IndexWriter.close()

ideally, there would be a method on the IndexWriter to wait for merges without actually closing the index.
This would make it so that background merges (or optimize) can be waited for without closing the IndexWriter, and then reopening a new IndexWriter

the close() reopen IndexWriter method can be problematic if the close() fails as the write lock won't be released
this could then result in the following sequence:
* close() - fails
* force unlock the write lock (per close() documentation)
* new IndexWriter() (acquires write lock)
* finalize() on old IndexWriter releases the write lock
* Index is now not locked, and another IndexWriter pointing to the same directory could be opened

If you don't force unlock the write lock, opening a new IndexWriter will fail until garbage collection calls finalize() the old IndexWriter

If the waitForMerges() method is available, i would likely never need to close() the IndexWriter until right before the process being shutdown, so this issue would not occur (worst case scenario, the waitForMerges() fails)


"
0,"HostnameVerifier shouldn't shadow simple name of implemented interfacepublic interface HostnameVerifier extends javax.net.ssl.HostnameVerifier.

As Findbugs says:

Class names shouldn't shadow simple name of implemented interface

This class/interface has a simple name that is identical to that of an implemented/extended interface, except that the interface is in a different package (e.g., alpha.Foo extends beta.Foo). This can be exceptionally confusing, create lots of situations in which you have to look at import statements to resolve references and creates many opportunities to accidently define methods that do not override methods in their superclasses. 
"
0,"dependencies for route planner implementationsThe implementations of HttpRoutePlanner that we have depend on the ConnectionManager, but use it only to look up the SchemeRegistry. Consider to depend only on the SchemeRegistry.

"
1,"Jackrabbit logs a NullPointerException on shutdown if the version manager wasn't initializedIf opening the repository fails, and the version manager was not initialized, then the shutdown method logs a NullPointerException when trying to close the version manager. This is a nuisance."
1,"MultiThreadedHttpConnectionManager setMaxTotalConnections() method doesn't workThe deprecated setMaxTotalConnections() method in the
MultiThreadedHttpConnectionManager seems like it has no effect:

Here is the source code in the current version:

    public void setMaxTotalConnections(int maxTotalConnections) {
        this.params.getMaxTotalConnections();
    }

Shouldn't it look more like this?

    public void setMaxTotalConnections(int maxTotalConnections) {
        this.params.setMaxTotalConnections(maxTotalConnections);
    }"
0,"ResponseContentEncoding should also handle x-gzip, compress and x-compressResponseContentEncoding should also handle x-gzip, compress and x-compress encodings according to specs (http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html, 3.5 Content Codings).

Also RequestAcceptEncoding should set Accept-Encoding to ""gzip,deflate,identity"". I am not sure about x-gzip, compress and x-compress here though.

Thanks"
0,"Optimization for FieldDocSortedHitQueueWhen updating core for generics,  I found the following as a optimization of FieldDocSortedHitQueue:

All FieldDoc values are Compareables (also the score or docid, if they
appear as SortField in a MultiSearcher or ParallelMultiSearcher). The code
of lessThan seems very ineffective, as it has a big switch statement on the
SortField type, then casts the value to the underlying numeric type Object,
calls Number.xxxValue() & co for it and then compares manually. As
j.l.Number is itself Comparable, I see no reason to do this. Just call
compareTo on the Comparable interface and we are happy. The big deal is that
it prevents casting and the two method calls xxxValue(), as Number.compareTo
works more efficient internally.

The only special cases are String sort, where the Locale may be used and the
score sorting which is backwards. But these are two if statements instead of
the whole switch.

I had not tested it now for performance, but in my opinion it should be
faster for MultiSearchers. All tests still pass (because they should).
"
0,"JCA project tests assume Windows pathsThe org.apache.jackrabbit.jca package in the top-level jca directory has unit tests that assume a Windows environment. It should be fixed to work in any environment. The best solution may be to use a test repository configuration file in the current directory.

The following is the start of the test case failures that I got running on MacOS X.

Testsuite: org.apache.jackrabbit.jca.test.ConnectionFactoryTest
Tests run: 3, Failures: 0, Errors: 3, Time elapsed: 0.778 sec

Testcase: testAllocation(org.apache.jackrabbit.jca.test.ConnectionFactoryTest): Caused an ERROR
org.apache.jackrabbit.core.config.ConfigurationException: Configuration file could not be read.: /Users/mark/Documents/school/whisper/jackrabbit/jackrabbit-trunk/jca/c:\dev\jcr\repository.xml (N
o such file or directory): /Users/mark/Documents/school/whisper/jackrabbit/jackrabbit-trunk/jca/c:\dev\jcr\repository.xml (No such file or directory)
org.apache.jackrabbit.core.config.ConfigurationException: Configuration file could not be read.: /Users/mark/Documents/school/whisper/jackrabbit/jackrabbit-trunk/jca/c:\dev\jcr\repository.xml (N
o such file or directory): /Users/mark/Documents/school/whisper/jackrabbit/jackrabbit-trunk/jca/c:\dev\jcr\repository.xml (No such file or directory)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createRepository(JCAManagedConnectionFactory.java:278)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createConnectionFactory(JCAManagedConnectionFactory.java:116)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createConnectionFactory(JCAManagedConnectionFactory.java:108)
        at org.apache.jackrabbit.jca.test.ConnectionFactoryTest.testAllocation(ConnectionFactoryTest.java:43)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
"
0,"Deprecating StopAnalyzer ENGLISH_STOP_WORDS - General replacement with an immutable SetStopAnalyzer and StandartAnalyzer are using the static final array ENGLISH_STOP_WORDS by default in various places. Internally this array is converted into a mutable set which looks kind of weird to me. 
I think the way to go is to deprecate all use of the static final array and replace it with an immutable implementation of CharArraySet. Inside an analyzer it does not make sense to have a mutable set anyway and we could prevent set creation each time an analyzer is created. In the case of an immutable set we won't have multithreading issues either. 
in essence we get rid of a fair bit of ""converting string array to set"" code, do not have a PUBLIC static reference to an array (which is mutable) and reduce the overhead of analyzer creation.

let me know what you think and I create a patch for it.

simon"
0,"Make FieldSortedHitQueue publicCurrently, those who utilize the ""advanced"" search API cannot sort results using
the handy FieldSortedHitQueue. I suggest making this class public to facilitate
this use, as I can't think of a reason not to."
0,"Add Apache RAT (Release Audit Tool) target to build.xml
Apache RAT is a useful tool to check for common mistakes in our source code (eg missing copyright headers):

    http://incubator.apache.org/rat/

I'm just copying the patch Grant worked out for Solr (SOLR-762).  I plan to commit to 2.4 & 2.9."
0,"Audit logJCR-2031 added the user name and path in debug logs for audit purposes. There are some problems with the fix that I had outlined in the comments for JCR-2031 and provided a patch. Additionally, it would use useful to add an update counter and size information to the debug log as well. Something like this:

17.03.2009 14:43:37 [1] 18216140 admin@/apps/acme/templates/contentpage/thumbnail.png (12343) 
17.03.2009 14:43:37 [2] 18216141 admin@/apps/acme/templates/contentpage/my.png (123) 
17.03.2009 14:43:37 [3] 18216142 admin@/apps/acme/templates/contentpage/blah.png (1423) 
17.03.2009 14:43:37 [4] 18216143 admin@/apps/acme/templates/contentpage/test.png (123423) 
17.03.2009 14:43:37 [5] 18216144 admin@/apps/acme/templates/contentpage/test2.png (123423) 

<date> <time> [<counter>] <txid> <userid>@<path> (<size>)

We should also think about whether we want this log as part of regular jackrabbit log or in a separate audit log. 
"
1,"DateUtil#formatDate uses default locale instead of USProblem reported by Yannick <yannick at meudal.net> on the httpclient-user list

==================================================================
Hello,

This is a bug report.

I'm using Commons HTTPClient (rc2) for generating HTTP requests. I put in 
headers some specific header, like the If-Modified-Since attribute. 
When I generate the date through DateUtil.formatDate method, I get a 
localized date, in french. Example: 
If-Modified-Since: dim., 10 avr. 2005 05:04:08 CEST

I get problems on my http server during parsing the received date. This is 
not a RFC 2616 compliant date format. It should be:
If-Modified-Since: Sun, 10 Apr 2005 05:04:08 CEST

A patch should be applied, by creating a new SimpleDateFormat(pattern, 
Locale.US) instead of SimpleDateFormat(pattern) (like it is done in the 
parse method, line #159).

org.apache.commons.httpclient.util.DateUtil, line #205:

    public static String formatDate(Date date, String pattern) {
        if (date == null) throw new IllegalArgumentException(""date is 
null"");
        if (pattern == null) throw new IllegalArgumentException(""pattern 
is null"");
 
        SimpleDateFormat formatter = new SimpleDateFormat(pattern, 
Locale.US);
        return formatter.format(date);
    }


Regards,

Yannick."
1,"String properties with invalid XML characters export as invalid XMLAs noted in the current JCR 1.0.1 maintenance draft, sections 6.4.1,
6.4.2.6, XML export of string properties that contain invalid XML
characters isn't well-defined currently, since those characters are
not permissible in XML.  The proposed fix is to use base64
encoding for such values in System View.

Most characters below #x20 are examples of this.  Currently, these
are escaped numerically in output (such as (amp)#0; )  but
such escape sequences can't be parsed by the XML
import methods.

The current behavior is particularly problematic, because the user
doesn't know the output is corrupt until later, when they try to import it
and get InvalidSerializedDataException.

If for some reason the base64 option is delayed, it might
make sense, as an interim solution, to fail on export
or to somehow patch import to relax its parsing and allow
these escape codes."
1,"Escape colon in statement of jcr:containsThe colon is a special character in the lucene query parser and allows to prefix query terms with an optional field name. JCR does not specify such a feature, thus a colon in the fulltext statement should be treated as a regular character. "
0,"TopDocsCollector should be abstract super class that is the real ""TopDocsCollector"" contract, a subclass should implement the priority-queue logic. e.g. PQTopDocsCollectorTopDocsCollector is both an abstract interface for producing TopDocs as well as a PriorityQueue based implementation.
Not all Collectors that could produce TopDocs must use a PriorityQueue, and it would be advantageous to allow the TopDocsCollector to be an ""interface"" type abstract class, with a PQTopDocsCollector sub-class.
While doing this, it'd be good to clean up the generics uses in these classes. As it's odd to create a TopFieldCollector and have to case the TopDocs object, when this can be fixed with generics."
0,"please log allocation of new connections to support debugging, testingI'd like to suggest that the MultiThreaded connection manager emit a trace-level log when it 
allocates a new HttpConnection to support debugging and testing.  I added one while working on 
my integration in Apache Axis (see org.apache.axis.transport.http.CommonsHTTPSender) and 
figured this would be of general use.  I'll attach a patch with the oh-so-minor addition after 
submitting this enhancement request."
1,InstantiatedIndexReader does not implement getFieldNames properlyCauses error in org.apache.lucene.index.SegmentMerger.mergeFields
0,"Make termInfosIndexDivisor configurableWorkspaces with large indexes may consume considerable heap memory. Lucene implements multi level skip lists for terms in the index. The first level of the skip list is kept in memory. This is usually not an issue, but when terms consist of long Strings the memory consumption increases drastically. Jackrabbit not just tokenizes string properties, but it also creates a single term, based on the complete string property value (needed for jcr:like function). These long terms are the reason for the increased memory consumption."
0,"The native FS lock used in test-framework's o.a.l.util.LuceneJUnitResultFormatter prohibits testing on a multi-user system{{LuceneJUnitResultFormatter}} uses a lock to buffer test suites' output, so that when run in parallel, they don't interrupt each other when they are displayed on the console.

The current implementation uses a fixed directory ({{lucene_junit_lock/}} in {{java.io.tmpdir}} (by default {{/tmp/}} on Unix/Linux systems) as the location of this lock.  This functionality was introduced on SOLR-1835.

As Shawn Heisey reported on SOLR-2739, some tests fail when run as root, but succeed when run as a non-root user.  

On #lucene IRC today, Shawn wrote:
{quote}
(2:06:07 PM) elyograg: Now that I know I can't run the tests as root, I have discovered /tmp/lucene_junit_lock.  Once you run the tests as user A, you cannot run them again as user B until that directory is deleted, and only root or the original user can do so.
{quote}
"
0,"Use Commons IO 1.4Commons IO contains a number of utility classes and methods for working with files and streams. Many of those utilities would be quite useful in Jackrabbit, so I'd like to introduce commons-io 1.4 as a dependency to jackrabbit-core.

"
0,"Lucene-core 2.9.0 missing from Maven Central RepositorySub-projects like lucene-demos, lucene-contrib, etc. exist in central, and depend on 2.9.0 of lucene-core. However, the lucene-core 2.9.0 artifact itself is missing."
0,JSR 283: Simple versioning
0,Remove duplicate code in QValueFactoryImpl (spi2dav)QValueFactoryImpl in spi2dav contains code duplicated from spi-commons QValueFactoryImpl. Once JCR-2245 has been applied the spi2dav variant can extend from the factory in spi-commons and we can simply the first.
0,"ISOLatin1AccentFilter a bit slowThe ISOLatin1AccentFilter is a bit slow giving 300+ ms responses when used in a highligher for output responses.

Patch to follow"
0,"Resolve JUnit assert deprecationsMany tests use assertEquals methods which have been deprecated.  The culprits are assertEquals(float, float), assertEquals(double, double) and assertEquals(Object[], Object[]).  Although not a big issue, they annoy me every time I see them so I'm going to fix them."
0,"ClientConnectionManager should throw InterruptedExceptionFor historical reasons, ThreadSafeClientConnectionManager throws an IllegalThreadStateException instead of an InterruptedException if the waiting thread is interrupted from outside. This design was chosen since adding InterruptedException to the HttpConnectionManager in 3.x would have broken the API. This is not a concern for HttpClient 4.0.
"
0,"OpenBitSet.prevSetBit()Find a previous set bit in an OpenBitSet.
Useful for parent testing in nested document query execution LUCENE-2454 ."
1,"BasicCookieStore treats cookies of the same name from the same host as duplicates, even if they have different pathsThe DefaultHttpClient is not handling cookies correctly when a single host returns multiple cookies of the same name but with separate paths.  For example, if a single instance of the client is used to access two different webapps on the same server, it may receive two different JSESSIONID cookies:

Cookie: [version: 0][name: JSESSIONID][value: F832C01D23F501CE5EEB296B602700C1][domain: lglom139.example.com][path: /msa-adrenalina][expiry: null]
Cookie: [version: 0][name: JSESSIONID][value: 0FC660347391B93267168F84F2B520F5][domain: lglom139.example.com][path: /maps][expiry: null]

Because the CookieIdentityComparator class does not test the cookie path when determining equality, each new JSESSIONID received replaces the previous one instead of adding a new cookie to the store.  This results in ""disconnecting"" the client from its sessions on the prior webapps.

I've confirmed that adding a path test to CookieIdentityComparator resolves this problem."
0,"DisjunctionScorerThis disjunction scorer can match a minimum nr. of docs, 
it provides skipTo() and it uses skipTo() on the subscorers. 
The score() method is abstract in DisjunctionScorer and implemented 
in DisjunctionSumScorer as an example."
0,"Extract JDBC Connection InitAn intermediate step to allowing a PM to be easily configurable through JNDI would be to extract the connection init. This will allow system integrators to subclass/wrap and dynamically configure a customized Simple PM. In org.apache.jackrabbit.core.state.db.SimpleDbPersistenceManager:

Replace lines (296-298) with

        initConnection();

Add:
	/**
	* Initialize the JDBC connection
	**/
	protected void initConnection() throws Exception {
            Class.forName(driver);
            con = DriverManager.getConnection(url, user, password);
            con.setAutoCommit(false);
	}"
1,"Enabling wire logging changes isEof/isStale behaviorIf you enable wire logging, DefaultClientConnection wraps the SocketInputBuffer with a LoggingSessionInputBuffer. This hides the EofSensor interface implemented by SocketInputBuffer (but not LoggingSessionInputBuffer), which makes at least AbstractHttpClientConnection.isEof() and isStale() methods behave differently.

(That is, stale connection checks won't really work as intended if wire logging is enabled. Which makes it a bit difficult to debug problems related to stale connections...)

Proposed fix: implement EofSensor interface in LoggingSessionInputBuffer (delegating it to wrapped buffer).
"
0,"replace collation/lib/icu4j.jar with a smaller icu jarCollation does not need all the icu data.
we can shrink the jar file a bit by using the data customizer, and excluding things like character set conversion tables."
0,"Change defaults in IndexWriter to maximize ""out of the box"" performanceThis is follow-through from LUCENE-845, LUCENE-847 and LUCENE-870;
I'll commit this once those three are committed.

Out of the box performance of IndexWriter is maximized when flushing
by RAM instead of a fixed document count (the default today) because
documents can vary greatly in size.

Likewise, merging performance should be faster when merging by net
segment size since, to minimize the net IO cost of merging segments
over time, you want to merge segments of equal byte size.

Finally, ConcurrentMergeScheduler improves indexing speed
substantially (25% in a simple initial test in LUCENE-870) because it
runs the merges in the backround and doesn't block
add/update/deleteDocument calls.  Most machines have concurrency
between CPU and IO and so it makes sense to default to this
MergeScheduler.

Note that these changes will break users of ParallelReader because the
parallel indices will no longer have matching docIDs.  Such users need
to switch IndexWriter back to flushing by doc count, and switch the
MergePolicy back to LogDocMergePolicy.  It's likely also necessary to
switch the MergeScheduler back to SerialMergeScheduler to ensure
deterministic docID assignment.

I think the combination of these three default changes, plus other
performance improvements for indexing (LUCENE-966, LUCENE-843,
LUCENE-963, LUCENE-969, LUCENE-871, etc.) should make for some sizable
performance gains Lucene 2.3!"
1,"Workspace.clone() fails the second time, if cloning referenceablesthe following testcode fails with the 2nd clone. please note, that if the 'folder' node is not made
referenceable, the test passes (copied an adapted from test in WorkspaceCloneTest).

    public void testCloneNodesTwice() throws RepositoryException {
        // clone referenceable node below non-referenceable node
        String dstAbsPath = node2W2.getPath() + ""/"" + node1.getName();

        Node folder = node1.addNode(""folder"");
        folder.addMixin(""mix:referenceable"");
        node1.save();
        workspaceW2.clone(workspace.getName(), node1.getPath(), dstAbsPath, true);
        workspaceW2.clone(workspace.getName(), node1.getPath(), dstAbsPath, true);

        // there should not be any pending changes after clone
        assertFalse(superuserW2.hasPendingChanges());
    }

"
0,"isHttp11 should have HttpClient scope-----Original Message-----
From: Kalnichevski, Oleg [mailto:oleg.kalnichevski@bearingpoint.com] 
Sent: Wednesday, January 15, 2003 8:24 AM
To: Commons HttpClient Project
Cc: Rob Owen
Subject: RE: isHttp11 and HTTP/1.0 servers 

Rob
You are basically right hands down. It does make sense for the HTTP version 
flag to have HttpClient scope. We should address this shortcoming as a part of 
the post-2.0-release redesign

Feel free to file a bug report to make sure the issue does not go forgotten

http://nagoya.apache.org/bugzilla/enter_bug.cgi?product=Commons

Many thanks for bring it up

Cheers

Oleg

-----Original Message-----
From: Rob Owen [mailto:Rob.Owen@sas.com]
Sent: Monday, January 13, 2003 18:31
To: Commons HttpClient Project
Subject: isHttp11 and HTTP/1.0 servers 


The boolean variable http11 is set on a method by method basis. For PutMethod, 
decisions (eg. Expect: 100-continue request header) are made prior to 
determining the value for Http11 (chicken and egg problem) and so the default 
(true) is used to produce the request. An HTTP/1.0 server hangs waiting for 
the extra data on the PUT method body. 

For applications that are using HttpClient (ie. they do not manipulate the 
HTTP methods directly and cannot be expected to set the value of Http11 for 
each method instance), shouldn't http11 have HttpClient scope ? This would 
allow an interaction (eg. OPTIONS) to set http11 and all methods thereafter 
would use this setting?
  
------
Rob Owen
SAS Institute Inc.
email: Rob.Owen@sas.com"
1,"Problem importing node with binary property in a repository configured with datastoreUsing the importXML method of workspace to import some node containing binary properties the nodes are imported correctly and the value of the binary data property is imported
However the binary data goes to the db (persistenceManager) an not to the datastore.

Creating a new node of the same type using the api, the binary data go to the datastore."
0,"Optimize TermsEnum.seek when caller doesn't need next termSome codecs are able to save CPU if the caller is only interested in
exact matches.  EG, Memory codec and SimpleText can do more efficient
FSTEnum lookup if they know the caller doesn't need to know the term
following the seek term.

We have cases like this in Lucene, eg when IW deletes documents by
Term, if the term is not found in a given segment then it doesn't need
to know the ceiling term.  Likewise when TermQuery looks up the term
in each segment.

I had done this change as part of LUCENE-3030, which is a new terms
index that's able to save seeking for exact-only lookups, but now that
we have Memory codec that can also save CPU I think we should commit
this today.

The change adds a ""boolean onlyExact"" param to seek(BytesRef).
"
0,"ParallelReader is now atomic, rename to ParallelAtomicReader and also add a ParallelCompositeReader (that requires LogDocMergePolicy to have identical subreader structure)The plan is:
- Move all subreaders to ctor (builder-like API. First build reader-set, then call build)
- Rename ParallelReader to ParallelAtomicReader
- Add a ParallelCompositeReader with same builder API, but taking any CompositeReader-set and checks them that they are aligned (docStarts identical). The subreaders are ParallelAtomicReaders."
1,"IndexReader.indexExists sometimes returns true when an index isn't presentIf you open a writer on a new dir and prepareCommit but don't finish the commit, IndexReader.indexExists incorrectly returns true, because it just checks for whether a segments_N file is present and not whether it can be successfully read."
0,"Remove ImportContextImpl#getDetectorthe method ImportContextImpl#getDetector refers an interface method on ImportContext that does not exist.
according to jukka that is a leftover. since the method is not used at all i would therefore suggest to remove it altogether, remove the instance field and deprecate the constructor taking the detector param."
0,"Improve parallel testsAs mentioned on the dev@ mailing list here: http://www.lucidimagination.com/search/document/93432a677917b9bd/lucenejunitresultformatter_sometimes_fails_to_lock

It would be useful to not create a lockfactory for each test suite (As they are run sequentially in the same separate JVM).
Additionally, we create a lot of JVMs (26) for each batch, because we have to run one for each letter.
Instead, we use a technique here to divide up the tests with a custom selector: http://blog.code-cop.org/2009/09/parallel-junit.html
(I emailed the blog author and received permission to use this code)

This gives a nice boost to the speed of overall tests, especially Solr tests, as many start with an ""S"", but this is no longer a problem.

"
1,"Findbugs reports and fixesRan findbugs 0.94.rc1 on 3.0RC4. 
Fixed a few of the obvious ones (patches to follow) and made notes on the 
remainder - see the //TODO markers in code.
Also created a findbugs target in build.xml - see appropriate patch file"
0,"Allow means force a Repository to synchronize with the clusterBased on the thread on the user mailing list I'm logging this to propose adding a sync() method to force cluster synchronization using the JackrabbitRepository extension API.

The purpose of the method is such that in a distributed clustered environment sometime cluster synchronization does or has not occurred such that certain repositories are in a stale state.  This method would provide a means to force a repository to update pull in possible changes made by other Jackrabbit repositories.

"
0,"Incorrect error message in AnalyzingQueryParser.getPrefixQueryThe error message of  getPrefixQuery is incorrect when tokens were added, for example by a stemmer. The message is ""token was consumed"" even if tokens were added.
Attached is a patch, which when applied gives a better description of what actually happened."
0,"SpecialOperations.isFinite can have TERRIBLE TERRIBLE runtime in certain situationsin an application of mine, i experienced some very slow query times with finite automata (all the DFAs are acyclic)

It turned out, the slowdown is some terrible runtime in SpecialOperations.isFinite <-- this is used to determine if the DFA is acyclic or not.

(in this case I am talking about even up to minutes of cpu).
"
0,"Change default value for maxMergeDocsThis is actually a left over from the time before JCR-197 was implemented. Back then index merges were performed with the client thread and would hold up execution for a long time if a large number of nodes were merged. The default value for maxMergeDocs limited this to 100'000 nodes, resulting in a couple of seconds for the merge operation.

This default value does not make sense anymore because index merges are performed in a background thread and may take a long time without an effect on regular workspace operations. If a workspace grows large it may cause performance degradation because the number of index segments increases linearly when there are more than 100'000 nodes.

I propose to set the new default to Integer.MAX_VALUE"
0,support offsets in MemoryPostingsReally we should add this for Sep & Pulsing too... but this is one more
0,"TCK: NodeTest#testAddNodeItemExistsException fails if validation deferred until saveThe test expects addNode to fail if a same-name sibling already exists.  JSR-170 allows this validation to be deferred until save.

Proposal: call save in the ""try"" block.

--- NodeTest.java       (revision 422074)
+++ NodeTest.java       (working copy)
@@ -380,6 +391,7 @@
         try {
             // try to add a node with same name again
             defaultTestNode.addNode(nodeName3, testNodeType);
+            defaultRootNode.save();
             fail(""Adding a node to a location where same name siblings are not allowed, but a node with same name"" +
                     "" already exists should throw ItemExistsException "");
         } catch (ItemExistsException e) {
"
0,"Grouped total countWhen grouping currently you can get two counts:
* Total hit count. Which counts all documents that matched the query.
* Total grouped hit count. Which counts all documents that have been grouped in the top N groups.

Since the end user gets groups in his search result instead of plain documents with grouping. The total number of groups as total count makes more sense in many situations. "
1,"MinPayloadFunction returns 0 when only one payload is presentIn some experiments with payload scoring through PayloadTermQuery, I'm seeing 0 returned when using MinPayloadFunction.  I believe there is a bug there.  No time at the moment to flesh out a unit test, but wanted to report it for tracking."
1,"MultiReader does not propagate readerFinishedListeners to clones/reopened readersWhile working on refactoring MultiReader/DirectoryReader in trunk, I found out that MultiReader does not correctly pass readerFinishedListeners to its clones and reopened readers."
0,Stop text extraction when the maxFieldLength limit is reachedWhen indexing large documents the text extraction often takes quite a while and uses lots of memory even if only the first maxFieldLength (by default 10000) tokens are used. I'd like to add a maxExtractLength parameter that can be used to set the maximum number of characters to extract from a binary. The default value of this parameter could be something like ten times the maxFieldLength setting.
0,JSR 283: Retention & Hold Management
1,"IndexReader.open(String|File) may incorrectly throw AlreadyClosedExceptionSpinoff from here:

    http://www.nabble.com/Runtime-exception-when-creating-IndexSearcher-to20226279.html

If you open an IndexSearcher/Reader, passing in String or File, then
closeDirectory is set to true in the reader.

If the index has a single segment, then SegmentReader.get is used to
open the index.  If an IOException is hit in there, the SegmentReader
closes itself and then closes the directory since closeDirectory is
true.

The problem is, the retry logic in SegmentInfos (to look for another
segments_N to try) kicks in and hits an AlreadyClosedException,
masking the original root cause.

Workaround is to separately get the Directory using
FSDirectory.getDirectory, and then instantiate IndexSearcher/Reader
from that.

This manifests as masking the root cause of a corrupted single-segment
index with a confusing AlreadyClosedException.  You could also hit
the false exception if the writer was in the process of committing
(ie, a retry was really needed) or if there is some transient IO
problem opening the index (eg too many open files).
"
0,"deprecate Directory.renameFile()Copied from my mailing list post so this issue can be tracked (if necessary). I will commit a patch.

I see that Directory.renameFile() isn't used anymore. I assume it has only 
been public for technical reasons, not because we expect this to be used 
from outside of Lucene? Should we deprecate this method? Its 
implementation e.g. in FSDirectory looks a bit scary anyway (the comment 
correctly says ""This is not atomic"" while the abstract class says ""This 
replacement should be atomic"").
"
1,"Incorrect sort by Numeric values for documents missing the sorting fieldWhile sorting results over a numeric field, documents which do not contain a value for the sorting field seem to get 0 (ZERO) value in the sort. (Tested against Double, Float, Int & Long numeric fields ascending and descending order).
This behavior is unexpected, as zero is ""comparable"" to the rest of the values. A better solution would either be allowing the user to define such a ""non-value"" default, or always bring those document results as the last ones.

Example scenario:
Adding 3 documents, 1st with value 3.5d, 2nd with -10d, and 3rd without any value.
Searching with MatchAllDocsQuery, with sort over that field in descending order yields the docid results of 0, 2, 1.

Asking for the top 2 documents brings the document without any value as the 2nd result - which seems as a bug?"
0,"Use of google-code-prettify for Lucene/Solr JavadocMy company, RONDHUIT uses google-code-prettify (Apache License 2.0) in Javadoc for syntax highlighting:

http://www.rondhuit-demo.com/RCSS/api/com/rondhuit/solr/analysis/JaReadingSynonymFilterFactory.html

I think we can use it for Lucene javadoc (java sample code in overview.html etc) and Solr javadoc (Analyzer Factories etc) to improve or simplify our life."
0,Add Amazon S3 authentication supportAdd support for the the Amazon S3 authentication scheme as defined by the online document: http://docs.amazonwebservices.com/AmazonS3/latest/index.html?RESTAuthentication.html
0,"Subclasses do not have write access to StatusLineHttpMethodBase provides the readStatusLine method explicitly designed for
subclasses to override. However, any attempt to do so quickly encounters issues
since the subclass does not have access to the statusLine member variable in
HttpMethodBase. The same holds true for several other member variables as well.

Recommend that all access to member variables occur through accessors and that
mutators be provided to set them. See patch below.
----------------------------------------------------------

Index: HttpMethodBase.java
===================================================================
--- HttpMethodBase.java	(revision 390815)
+++ HttpMethodBase.java	(working copy)
@@ -563,7 +563,7 @@
      * @return the status code associated with the latest response.
      */
     public int getStatusCode() {
-        return statusLine.getStatusCode();
+        return getStatusLine().getStatusCode();
     }
 
     /**
@@ -577,6 +577,13 @@
     }
 
     /**
+     * @param statusLine The statusLine to set.
+     */
+    protected final void setStatusLine(StatusLine statusLine) {
+        this.statusLine = statusLine;
+    }
+
+    /**
      * Checks if response data is available.
      * @return <tt>true</tt> if response data is available, <tt>false</tt>
otherwise.
      */
@@ -798,7 +805,7 @@
      * @return The status text.
      */
     public String getStatusText() {
-        return statusLine.getReasonPhrase();
+        return getStatusLine().getReasonPhrase();
     }
 
     /**
@@ -920,16 +927,16 @@
         }
         LOG.debug(""Resorting to protocol version default close connection policy"");
         // missing or invalid connection header, do the default
-        if (this.effectiveVersion.greaterEquals(HttpVersion.HTTP_1_1)) {
+        if (getEffectiveVersion().greaterEquals(HttpVersion.HTTP_1_1)) {
             if (LOG.isDebugEnabled()) {
-                LOG.debug(""Should NOT close connection, using "" +
this.effectiveVersion.toString());
+                LOG.debug(""Should NOT close connection, using "" +
getEffectiveVersion().toString());
             }
         } else {
             if (LOG.isDebugEnabled()) {
-                LOG.debug(""Should close connection, using "" +
this.effectiveVersion.toString());
+                LOG.debug(""Should close connection, using "" +
getEffectiveVersion().toString());
             }
         }
-        return this.effectiveVersion.lessEquals(HttpVersion.HTTP_1_0);
+        return getEffectiveVersion().lessEquals(HttpVersion.HTTP_1_0);
     }
     
     /**
@@ -980,14 +987,14 @@
         this.responseConnection = conn;
 
         checkExecuteConditions(state, conn);
-        this.statusLine = null;
+        setStatusLine(null);
         this.connectionCloseForced = false;
 
         conn.setLastResponseInputStream(null);
 
         // determine the effective protocol version
-        if (this.effectiveVersion == null) {
-            this.effectiveVersion = this.params.getVersion(); 
+        if (getEffectiveVersion() == null) {
+            setEffectiveVersion(this.params.getVersion()); 
         }
 
         writeRequest(state, conn);
@@ -996,7 +1003,7 @@
         // the method has successfully executed
         used = true; 
 
-        return statusLine.getStatusCode();
+        return getStatusCode();
     }
 
     /**
@@ -1048,8 +1055,8 @@
         getRequestHeaderGroup().clear();
         getResponseHeaderGroup().clear();
         getResponseTrailerHeaderGroup().clear();
-        statusLine = null;
-        effectiveVersion = null;
+        setStatusLine(null);
+        setEffectiveVersion(null);
         aborted = false;
         used = false;
         params = new HttpMethodParams();
@@ -1586,18 +1593,18 @@
         ""enter HttpMethodBase.readResponse(HttpState, HttpConnection)"");
         // Status line & line may have already been received
         // if 'expect - continue' handshake has been used
-        while (this.statusLine == null) {
+        while (getStatusLine() == null) {
             readStatusLine(state, conn);
             processStatusLine(state, conn);
             readResponseHeaders(state, conn);
             processResponseHeaders(state, conn);
             
-            int status = this.statusLine.getStatusCode();
+            int status = getStatusCode(); 
             if ((status >= 100) && (status < 200)) {
                 if (LOG.isInfoEnabled()) {
-                    LOG.info(""Discarding unexpected response: "" +
this.statusLine.toString()); 
+                    LOG.info(""Discarding unexpected response: "" +
getStatusLine().toString()); 
                 }
-                this.statusLine = null;
+                setStatusLine(null);
             }
         }
         readResponseBody(state, conn);
@@ -1675,7 +1682,7 @@
         if (Wire.CONTENT_WIRE.enabled()) {
             is = new WireLogInputStream(is, Wire.CONTENT_WIRE);
         }
-        boolean canHaveBody = canResponseHaveBody(statusLine.getStatusCode());
+        boolean canHaveBody = canResponseHaveBody(getStatusCode());
         InputStream result = null;
         Header transferEncodingHeader =
responseHeaders.getFirstHeader(""Transfer-Encoding"");
         // We use Transfer-Encoding if present and ignore Content-Length.
@@ -1714,7 +1721,7 @@
         } else {
             long expectedLength = getResponseContentLength();
             if (expectedLength == -1) {
-                if (canHaveBody &&
this.effectiveVersion.greaterEquals(HttpVersion.HTTP_1_1)) {
+                if (canHaveBody &&
getEffectiveVersion().greaterEquals(HttpVersion.HTTP_1_1)) {
                     Header connectionHeader =
responseHeaders.getFirstHeader(""Connection"");
                     String connectionDirective = null;
                     if (connectionHeader != null) {
@@ -1850,19 +1857,19 @@
         } while(true);
 
         //create the status line from the status string
-        statusLine = new StatusLine(s);
+        setStatusLine(new StatusLine(s));
 
         //check for a valid HTTP-Version
-        String versionStr = statusLine.getHttpVersion();
+        String versionStr = getStatusLine().getHttpVersion();
         if (getParams().isParameterFalse(HttpMethodParams.UNAMBIGUOUS_STATUS_LINE) 
            && versionStr.equals(""HTTP"")) {
             getParams().setVersion(HttpVersion.HTTP_1_0);
             if (LOG.isWarnEnabled()) {
                 LOG.warn(""Ambiguous status line (HTTP protocol version missing):"" +
-                statusLine.toString());
+                getStatusLine().toString());
             }
         } else {
-            this.effectiveVersion = HttpVersion.parse(versionStr);
+            setEffectiveVersion(HttpVersion.parse(versionStr));
         }
 
     }
@@ -1943,9 +1950,9 @@
                     readResponseHeaders(state, conn);
                     processResponseHeaders(state, conn);
 
-                    if (this.statusLine.getStatusCode() ==
HttpStatus.SC_CONTINUE) {
+                    if (getStatusCode() == HttpStatus.SC_CONTINUE) {
                         // Discard status line
-                        this.statusLine = null;
+                        setStatusLine(null);
                         LOG.debug(""OK to continue received"");
                     } else {
                         return;
@@ -2087,7 +2094,7 @@
      */
     private String getRequestLine(HttpConnection conn) {
         return  HttpMethodBase.generateRequestLine(conn, getName(),
-                getPath(), getQueryString(), this.effectiveVersion.toString());
+                getPath(), getQueryString(), getEffectiveVersion().toString());
     }
 
     /**
@@ -2128,6 +2135,13 @@
     }
 
     /**
+     * @param effectiveVersion The effectiveVersion to set.
+     */
+    protected final void setEffectiveVersion(HttpVersion effectiveVersion) {
+        this.effectiveVersion = effectiveVersion;
+    }
+
+    /**
      * Per RFC 2616 section 4.3, some response can never contain a message
      * body.
      *
@@ -2358,7 +2372,7 @@
     ) {
         // set used so that the response can be read
         this.used = true;
-        this.statusLine = statusline;
+        setStatusLine(statusline);
         this.responseHeaders = responseheaders;
         this.responseBody = null;
         this.responseStream = responseStream;"
0,"Refactor Searchable to not have RMI Remote dependencyPer http://lucene.markmail.org/message/fu34tuomnqejchfj?q=RemoteSearchable

We should refactor Searchable slightly so that it doesn't extend the java.rmi.Remote marker interface.  I believe the same could be achieved by just marking the RemoteSearchable and refactoring the RMI implementation out of core and into a contrib.

If we do this, we should deprecate/denote it for 2.9 and then move it for 3.0"
0,spi2dav: create RepositoryFactory implementation
0,"add LuceneTestCase.newSearcher()Most tests in the search package don't care about what kind of searcher they use.

we should randomly use MultiSearcher or ParallelMultiSearcher sometimes in tests."
1,"Using MultiSearcher and ParallelMultiSearcher can change the sort order.When using multiple sort criteria the first criterium that indicates a difference should be used.
When a field does not exist for a given document, special rules apply.
From what I see in the code, it is sorted as 0 for integer and float fields, and null Strings are sorted before others.

This works correctly in both Lucene 1.4.3 and in trunk as long as you use a single IndexSearcher (except perhaps in special cases, see other bug reports like LUCENE-374).

However, in MultiSearcher and ParallelMultiSearcher, the results of the separate IndexSearchers are merged and there an error occurs.
The bug is located in FieldDocSortedHitQueue.

It can even be demonstrated by passing a single indexSearcher to a MultiSearcher.

TestCase and patch follow."
1,FilteredQuery ignores boostFiltered query ignores it's own boost.
0,"some legal jcr names cause unneccessary server-roundtrips assume the following legal qualified jcr names:

""{foo}""
""{foo} bar""

when items with such names are read from the spi layer, they are first interpreted as expanded form names.
a prefix lookup for namespace 'foo' fails and the name is treated as qualified jcr name.

=> depending on the spi implementation, a server-roundtrip is required in order to determine that 'foo' is not a
registered namespace. "
1,"Registering nodetypes with empty namespace prefix causes a namespace exception in sync nodeRegistering a nodetype with empty namespace prefix causes a namespace exception in sync node. Stacktrace looks as follows:

03.03.2008 15:33:50 *ERROR* ClusterNode: Unable to read revision '10618'. (ClusterNode.java, line 1051)
o.a.j.core.journal.JournalException: Parse error while reading node type definition.
        at o.a.j.core.journal.AbstractRecord.readNodeTypeDef(AbstractRecord.java:256)
        at o.a.j.core.cluster.ClusterNode.consume(ClusterNode.java:1026)
        at o.a.j.core.journal.AbstractJournal.doSync(AbstractJournal.java:198)
        at o.a.j.core.journal.AbstractJournal.sync(AbstractJournal.java:173)
        at o.a.j.core.cluster.ClusterNode.sync(ClusterNode.java:303)
        at o.a.j.core.cluster.ClusterNode.run(ClusterNode.java:274)
        at java.lang.Thread.run(Thread.java:595)
Caused by: o.a.j.core.nodetype.compact.ParseException: Error while parsing 'bla' ((internal), line 3)
        at o.a.j.core.nodetype.compact.Lexer.fail(Lexer.java:152)
        at o.a.j.core.nodetype.compact.CompactNodeTypeDefReader.toQName(CompactNodeTypeDefReader.java:653)
        at o.a.j.core.nodetype.compact.CompactNodeTypeDefReader.doNodeTypeName(CompactNodeTypeDefReader.java:265)
        at o.a.j.core.nodetype.compact.CompactNodeTypeDefReader.parse(CompactNodeTypeDefReader.java:215)
        at o.a.j.core.nodetype.compact.CompactNodeTypeDefReader.<init>(CompactNodeTypeDefReader.java:178)
        at o.a.j.core.nodetype.compact.CompactNodeTypeDefReader.<init>(CompactNodeTypeDefReader.java:162)
        at o.a.j.core.journal.AbstractRecord.readNodeTypeDef(AbstractRecord.java:248)
        ... 6 more
Caused by: javax.jcr.NamespaceException: No URI for pefix '' declared.
        at o.a.j.spi.commons.namespace.NamespaceMapping.getURI(NamespaceMapping.java:74)
        at o.a.j.spi.commons.conversion.NameParser.parse(NameParser.java:116)
        at o.a.j.spi.commons.conversion.ParsingNameResolver.getQName(ParsingNameResolver.java:62)
        at o.a.j.spi.commons.conversion.DefaultNamePathResolver.getQName(DefaultNamePathResolver.java:61)
        at o.a.j.core.nodetype.compact.CompactNodeTypeDefReader.toQName(CompactNodeTypeDefReader.java:646)
        ... 11 more

"
0,Improve NodeTypeRegistry.effectiveNodeType()The current getEffectiveNodeType() implementation has a minor bug that prevents from proper caching for certain nodetype combinations. further performance enhancements can be made to the effective node type cache.
0,"SQL2 parser may infer type for UncastLiteral from static analysisThe spec says:

""An UncastLiteral is always interpreted as a Value of property type STRING. A CastLiteral, on the other hand, is interpreted as the string form of a Value of the PropertyType indicated.""

There are also two test cases in NodeNameTest that need to be fixed accordingly: testLongLiteral and testBooleanLiteral
"
0,"Setup nightly build website links and docsPer discussion on mailing list, we are going to setup a Nightly Build link on the website linking to the docs (and javadocs) generated by the nightly build process.  The build process may need to be modified to complete this task.

Going forward, the main website will, for the most part, only be updated per releases (I imagine exceptions will be made for News items and per committer's discretion).  The Javadocs linked to from the main website will always be for the latest release."
0,"Logger (Category) names don't follow common patternThe Wire class uses two loggers named unexpected. The ""org.apache.commons.""
prefix is missing - so you can't mute all debug level statements with a
one-liner in you log4j.properties for example:

  log4j.logger.org.apache.commons.httpclient INFO

You have to add this, too:

  log4j.logger.httpclient INFO

Please prepend the ""org.apache.commons."" before both names.

Cheers,
Christian

<code>
class Wire {

    public static Wire HEADER_WIRE = new
Wire(LogFactory.getLog(""httpclient.wire.header""));
    
    public static Wire CONTENT_WIRE = new
Wire(LogFactory.getLog(""httpclient.wire.content""));

</code>

http://svn.apache.org/viewcvs.cgi/jakarta/commons/proper/httpclient/trunk/src/java/org/apache/commons/httpclient/Wire.java?rev=155418&view=markup"
0,"Move QueryParsers from contrib/queryparser to queryparser moduleEach of the QueryParsers will be ported across.

Those which use the flexible parsing framework will be placed under the package flexible.  The StandardQueryParser will be renamed to FlexibleQueryParser and surround.QueryParser will be renamed to SurroundQueryParser."
0,"Synchronization bottleneck in FieldSortedHitQueue with many concurrent readersThe below is from a post by (my colleague) Paul Smith to the java-users list:

---

Hi ho peoples.

We have an application that is internationalized, and stores data from many languages (each project has it's own index, mostly aligned with a single language, maybe 2).

Anyway, I've noticed during some thread dumps diagnosing some performance issues, that there appears to be a _potential_ synchronization bottleneck using Locale-based sorting of Strings.  I don't think this problem is the root cause of our performance problem, but I thought I'd mention it here.  Here's the stack dump of a thread waiting:

""http-1001-Processor245"" daemon prio=1 tid=0x31434da0 nid=0x3744 waiting for monitor entry [0x2cd44000..0x2cd45f30]
        at java.text.RuleBasedCollator.compare(RuleBasedCollator.java)
        - waiting to lock <0x6b1e8c68> (a java.text.RuleBasedCollator)
        at org.apache.lucene.search.FieldSortedHitQueue$4.compare(FieldSortedHitQueue.java:320)
        at org.apache.lucene.search.FieldSortedHitQueue.lessThan(FieldSortedHitQueue.java:114)
        at org.apache.lucene.util.PriorityQueue.upHeap(PriorityQueue.java:120)
        at org.apache.lucene.util.PriorityQueue.put(PriorityQueue.java:47)
        at org.apache.lucene.util.PriorityQueue.insert(PriorityQueue.java:58)
        at org.apache.lucene.search.FieldSortedHitQueue.insert(FieldSortedHitQueue.java:90)
        at org.apache.lucene.search.FieldSortedHitQueue.insert(FieldSortedHitQueue.java:97)
        at org.apache.lucene.search.TopFieldDocCollector.collect(TopFieldDocCollector.java:47)
        at org.apache.lucene.search.BooleanScorer2.score(BooleanScorer2.java:291)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:132)
        at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:110)
        at com.aconex.index.search.FastLocaleSortIndexSearcher.search(FastLocaleSortIndexSearcher.java:90)
.....

In our case we had 12 threads waiting like this, while one thread had the lock on the RuleBasedCollator.  Turns out RuleBasedCollator's.compare(...) method is synchronized.  I wonder if a ThreadLocal based collator would be better here... ?  There doesn't appear to be a reason for other threads searching the same index to wait on this sort.  Be just as easy to use their own.  (Is RuleBasedCollator a ""heavy"" object memory wise?  Wouldn't have thought so, per thread)

Thoughts?

---

I've investigated this somewhat, and agree that this is a potential problem with a series of possible workarounds. Further discussion (including proof-of-concept patch) to follow."
0,"FilteredQuery should have getFilter()Unless you are in the same package, you can't access the filter in a FilteredQuery.
A getFilter() method should be added."
0,"Use correct version number in repository descriptorThe repository descriptor 'jcr.repository.version' always shows 1.0-dev.

The value should reflect the current jackrabbit version."
0,"JavaDoc getConnection methods in Connection ManagersThe JavaDoc for the getConnection() methods in the Simple and MultiThreaded
Connection managers is taken from the interface, and so is too generic.

The Javadoc for the doGetConnection() method in the MultiThreaded manager is
fine, but is not visible in the JavaDoc

The Simple Mangager JavaDoc could likewise be improved

[I hope to provide patches shortly]"
0,"Drop Maven 1 compatibilityWe migrated from Maven 1 to Maven 2 as the build system in Jackrabbit 1.2, but we kept compatibility with related Maven 1 build with the maven-one-plugin that deployed all builds also to the local Maven 1 repository.

Hardly any downstream project uses Maven 1 anymore, so it's safe for us to simplify our build now by dropping the use of the maven-one-plugin."
0,"Changes.html should be visible to users for closed releasesChanges.html is currently available only in the dev page, for trunk. 
See LUCENE-1157 for discussion on where exactly to expose this."
1,Incorrect results from joins on multivalued propertiesIt looks like join conditions on multivalued properties only use one of the multiple values for the comparison.
1,"Custom LoginModule configurations broken in 1.5.0Upgrading Jackrabbit from 1.4.5 to 1.5 has created an LDAP exception.  The configuration file which has not changed (except for the adding the new SimpleSecurityManager as required) is the default with the following substituted for the LoginModule:

        <LoginModule class=""com.sun.security.auth.module.LdapLoginModule"">
            <param name=""userProvider"" value=""ldap://localhost/ou=people,dc=example,dc=com"" />
            <param name=""userFilter"" value=""(&amp;(uid={USERNAME})(objectClass=inetOrgPerson))"" />
            <param name=""authzIdentity"" value=""{USERNAME}"" />
            <param name=""debug"" value=""true"" />
        </LoginModule>

This configuration worked correctly and I was able to authenticate properly with Jackrabbit 1.4.5
The same configuration with 1.5 throws the following exception:

javax.jcr.LoginException: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider
        at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1414)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.openSession(JCAManagedConnectionFactory.java:140)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createManagedConnection(JCAManagedConnectionFactory.java:176)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createManagedConnection(JCAManagedConnectionFactory.java:168)
        at com.sun.enterprise.resource.ConnectorAllocator.createResource(ConnectorAllocator.java:136)
        at com.sun.enterprise.resource.AbstractResourcePool.createSingleResource(AbstractResourcePool.java:891)
        at com.sun.enterprise.resource.AbstractResourcePool.createResourceAndAddToPool(AbstractResourcePool.java:1752)
        at com.sun.enterprise.resource.AbstractResourcePool.createResources(AbstractResourcePool.java:917)
        at com.sun.enterprise.resource.AbstractResourcePool.initPool(AbstractResourcePool.java:225)
        at com.sun.enterprise.resource.AbstractResourcePool.internalGetResource(AbstractResourcePool.java:516)
        at com.sun.enterprise.resource.AbstractResourcePool.getResource(AbstractResourcePool.java:443)
        at com.sun.enterprise.resource.PoolManagerImpl.getResourceFromPool(PoolManagerImpl.java:248)
        at com.sun.enterprise.resource.PoolManagerImpl.getResource(PoolManagerImpl.java:176)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.internalGetConnection(ConnectionManagerImpl.java:337)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.allocateConnection(ConnectionManagerImpl.java:189)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.allocateConnection(ConnectionManagerImpl.java:165)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.allocateConnection(ConnectionManagerImpl.java:158)
        at org.apache.jackrabbit.jca.JCARepositoryHandle.login(JCARepositoryHandle.java:98)
        at org.apache.jackrabbit.jca.JCARepositoryHandle.login(JCARepositoryHandle.java:89)
        at org.apache.jackrabbit.jca.JCARepositoryHandle.login(JCARepositoryHandle.java:73)
        at com.threesl.Sapphire.CradleJCR.login(CradleJCR.java:44)   

 try {
            InitialContext ctx = new InitialContext();
            repository = (Repository) ctx.lookup(""jcr/repository"");
            session = repository.login(credentials);
        } catch (Exception e) {

        at com.threesl.Sapphire.CradleWS.doLogin(CradleWS.java:68)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jersey.impl.model.method.dispatch.EntityParamDispatchProvider$TypeOutInvoker._dispatch(EntityParamDispatchProvider.java:136)
        at com.sun.jersey.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:85)
        at com.sun.jersey.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:123)
        at com.sun.jersey.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:111)
        at com.sun.jersey.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:71)
        at com.sun.jersey.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:111)
        at com.sun.jersey.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:63)
        at com.sun.jersey.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:722)
        at com.sun.jersey.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:692)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:344)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:831)
        at org.apache.catalina.core.ApplicationFilterChain.servletService(ApplicationFilterChain.java:411)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:290)
        at org.apache.catalina.core.StandardContextValve.invokeInternal(StandardContextValve.java:271)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:202)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:632)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:577)
        at com.sun.enterprise.web.WebPipeline.invoke(WebPipeline.java:94)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:206)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:632)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:577)
        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:571)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:1080)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:150)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:632)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:577)
        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:571)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:1080)
        at org.apache.coyote.tomcat5.CoyoteAdapter.service(CoyoteAdapter.java:272)
        at com.sun.enterprise.web.connector.grizzly.DefaultProcessorTask.invokeAdapter(DefaultProcessorTask.java:637)
        at com.sun.enterprise.web.connector.grizzly.DefaultProcessorTask.doProcess(DefaultProcessorTask.java:568)
        at com.sun.enterprise.web.connector.grizzly.DefaultProcessorTask.process(DefaultProcessorTask.java:813)
        at com.sun.enterprise.web.connector.grizzly.DefaultReadTask.executeProcessorTask(DefaultReadTask.java:341)
        at com.sun.enterprise.web.connector.grizzly.DefaultReadTask.doTask(DefaultReadTask.java:263)
        at com.sun.enterprise.web.connector.grizzly.DefaultReadTask.doTask(DefaultReadTask.java:214)
        at com.sun.enterprise.web.connector.grizzly.TaskBase.run(TaskBase.java:265)
        at com.sun.enterprise.web.connector.grizzly.ssl.SSLWorkerThread.run(SSLWorkerThread.java:106)
Caused by: javax.security.auth.login.LoginException: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider
        at org.apache.jackrabbit.core.security.authentication.LocalAuthContext.login(LocalAuthContext.java:68)
        at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1407)
        ... 62 more
javax.security.auth.login.LoginException: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider
        at org.apache.jackrabbit.core.security.authentication.LocalAuthContext.login(LocalAuthContext.java:68)
        at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1407)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.openSession(JCAManagedConnectionFactory.java:140)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createManagedConnection(JCAManagedConnectionFactory.java:176)
        at org.apache.jackrabbit.jca.JCAManagedConnectionFactory.createManagedConnection(JCAManagedConnectionFactory.java:168)
        at com.sun.enterprise.resource.ConnectorAllocator.createResource(ConnectorAllocator.java:136)
        at com.sun.enterprise.resource.AbstractResourcePool.createSingleResource(AbstractResourcePool.java:891)
        at com.sun.enterprise.resource.AbstractResourcePool.createResourceAndAddToPool(AbstractResourcePool.java:1752)
        at com.sun.enterprise.resource.AbstractResourcePool.createResources(AbstractResourcePool.java:917)
        at com.sun.enterprise.resource.AbstractResourcePool.initPool(AbstractResourcePool.java:225)
        at com.sun.enterprise.resource.AbstractResourcePool.internalGetResource(AbstractResourcePool.java:516)
        at com.sun.enterprise.resource.AbstractResourcePool.getResource(AbstractResourcePool.java:443)
        at com.sun.enterprise.resource.PoolManagerImpl.getResourceFromPool(PoolManagerImpl.java:248)
        at com.sun.enterprise.resource.PoolManagerImpl.getResource(PoolManagerImpl.java:176)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.internalGetConnection(ConnectionManagerImpl.java:337)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.allocateConnection(ConnectionManagerImpl.java:189)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.allocateConnection(ConnectionManagerImpl.java:165)
        at com.sun.enterprise.connectors.ConnectionManagerImpl.allocateConnection(ConnectionManagerImpl.java:158)
        at org.apache.jackrabbit.jca.JCARepositoryHandle.login(JCARepositoryHandle.java:98)
        at org.apache.jackrabbit.jca.JCARepositoryHandle.login(JCARepositoryHandle.java:89)
        at org.apache.jackrabbit.jca.JCARepositoryHandle.login(JCARepositoryHandle.java:73)
        at com.threesl.Sapphire.CradleJCR.login(CradleJCR.java:44)
        at com.threesl.Sapphire.CradleWS.doLogin(CradleWS.java:68)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jersey.impl.model.method.dispatch.EntityParamDispatchProvider$TypeOutInvoker._dispatch(EntityParamDispatchProvider.java:136)
        at com.sun.jersey.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:85)
        at com.sun.jersey.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.java:123)
        at com.sun.jersey.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:111)
        at com.sun.jersey.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.java:71)
        at com.sun.jersey.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.java:111)
        at com.sun.jersey.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.java:63)
        at com.sun.jersey.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:722)
        at com.sun.jersey.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:692)
        at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:344)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:831)
        at org.apache.catalina.core.ApplicationFilterChain.servletService(ApplicationFilterChain.java:411)
        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:290)
        at org.apache.catalina.core.StandardContextValve.invokeInternal(StandardContextValve.java:271)
        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:202)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:632)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:577)
        at com.sun.enterprise.web.WebPipeline.invoke(WebPipeline.java:94)
        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:206)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:632)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:577)
        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:571)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:1080)
        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:150)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:632)
        at org.apache.catalina.core.StandardPipeline.doInvoke(StandardPipeline.java:577)
        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:571)
        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:1080)
        at org.apache.coyote.tomcat5.CoyoteAdapter.service(CoyoteAdapter.java:272)
        at com.sun.enterprise.web.connector.grizzly.DefaultProcessorTask.invokeAdapter(DefaultProcessorTask.java:637)
        at com.sun.enterprise.web.connector.grizzly.DefaultProcessorTask.doProcess(DefaultProcessorTask.java:568)
        at com.sun.enterprise.web.connector.grizzly.DefaultProcessorTask.process(DefaultProcessorTask.java:813)
        at com.sun.enterprise.web.connector.grizzly.DefaultReadTask.executeProcessorTask(DefaultReadTask.java:341)
        at com.sun.enterprise.web.connector.grizzly.DefaultReadTask.doTask(DefaultReadTask.java:263)
        at com.sun.enterprise.web.connector.grizzly.DefaultReadTask.doTask(DefaultReadTask.java:214)
        at com.sun.enterprise.web.connector.grizzly.TaskBase.run(TaskBase.java:265)
        at com.sun.enterprise.web.connector.grizzly.ssl.SSLWorkerThread.run(SSLWorkerThread.java:106)
RAR5117 : Failed to obtain/create connection from connection pool [ jackrabbit-connection-pool ]. Reason : Failed to create session: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider: com.sun.security.auth.module.LdapLoginModule does not support 'userProvider

"
0,"Log level for message should be debug instead of error.In method org.apache.commons.httpclient.HttpMethodBase.getResponseBody() Log
message should be logged as debug instead of error. 

717             } catch (IOException e) {
718                 LOG.error(""I/O failure reading response body"", e);
719                 this.responseBody = null;
720             }

According to HTTPCLIENT-57:
2) Only/always log exception stack traces at the debug level
        } catch (Exception ex) {
            log.debug"
0,"""System Properties"" doc lists ""lockDir"" instead of ""lockdir""The ""System Properties"" documentation page states that the lock file directory
can be set with the system property ""org.apache.lucene.lockDir"".  However, as
implemented in org.apache.lucene.store.FSDirectory, line 56, the property name
is actually ""org.apache.lucene.lockdir"" (lower case ""d"" in ""lockdir""). 
Recommend changing documentation to match code."
0,"SetPropertyAssumeTypeTest check for non-protected string array propertySetPropertyAssumeTypeTest.testValuesConstraintViolationExceptionBecauseOfInvalidTypeParameter tries to find a property definition for a writable, multivalued string property. It consults NodeTypeUtil.locatePropertyDef() for that purpose.

In my setup, the property definition being returned is for jcr:valueConstraints, defined on nt:propertyDefinition. Nodes of that type in turn can not be created on the test node, thus the test fails already when trying to create the node.

It seems the test suite tries to be too smart here. Can we change this so that the node type and the property name are configuration parameters?"
0,"Change contrib/spatial to use trie's NumericUtils, and remove NumberUtilsCurrently spatial contrib includes a copy of NumberUtils from solr (otherwise it would depend on solr)

Once LUCENE-1496 is sorted out, this copy should be removed."
0,"move HttpRoute and related classes to separate packageThe route-related stuff in o.a.h.conn is detached from the rest of the connection management API.
Move HttpRoute, RouteTracker, HttpRouteDirector, HttpRoutePlanner to o.a.h.conn.route or ...routing.
Implementation classes have a dependency on Scheme and SchemeRegistry in o.a.h.conn,
but that does not introduce a recursive dependency between packages.
"
0,"spi2dav : EventJournal not  implementedi didn't look at the details just realized that all EventJournalTest of the TCK fail in the setup
jcr2spi - spi2dav(ex) - jcr-server.
i assume that this is due to missing implementation (the corresponding SPI method throws UnsupportedRepositoryOperationException)."
0,"[PATCH] Use entrySet iterators to avoid map look ups in loopsCode uses a keySet iterator in a loop, then does a map look up using the key retrieved from the iterator. 

Might as well use an entrySet iterator to avoid n map lookups.

Patch does this."
1,"AccessManager + CachingHierarchyManager problemThe problem we have is the implementation of the CachingHierarchyManager,
to which the SimpleAccessManager holds a reference.

Let's consider following example:
i add 3 subnodes (a,b,c) to a node and after that i reorder b and c ..
so i have a,c,b. in the process of reordering (using the function
orderBefore of javax.jcr.Node) our AccessManager is called several times to check the permissions of the nodes. In this AccessManager we use some
functions of the CachingHierarchyManager, f.ex.

Path itemPath = hierMgr.getPath(id);
return itemPath.denotesRoot();

or

Path itemPath = hierMgr.getPath(itemId);
Path parentPath = itemPath.getAncestor(1);
return hierMgr.resolvePath(parentPath);

the problem is, that when calling the methods of the
CachingHierarchyManager the nodes i ask for will be cached in the idCache in a wrong state (i. e.: before actually reordering the elements).
so if i want f.ex. delete the node b after reordering, the node will
be looked up in the idCache. in the cache the index of node b is still 2
(actually it should be 3) and so the wrong node will be deleted! "
0,"read/write .del as d-gaps when the deleted bit vector is sufficiently sparse.del file of a segment maintains info on deleted documents in that segment. The file exists only for segments having deleted docs, so it does not exists for newly created segments (e.g. resulted from merge). Each time closing an index reader that deleted any document, the .del file is rewritten. In fact, since the lock-less commits change a new (generation of) .del file is created in each such occasion.

For small indexes there is no real problem with current situation. But for very large indexes, each time such an index reader is closed, creating such new bit-vector seems like unnecessary overhead in cases that the bit vector is sparse (just a few docs were deleted). For instance, for an index with a segment of 1M docs, the sequence: {open reader; delete 1 doc from that segment; close reader;} would write a file of ~128KB. Repeat this sequence 8 times: 8 new files of total size of 1MB are written to disk.

Whether this is a bottleneck or not depends on the application deletes pattern, but for the case that deleted docs are sparse, writing just the d-gaps would save space and time. 

I have this (simple) change to BitVector running and currently trying some performance tests to, yet, convince myself on the worthiness of this.

"
1,"IndexWriter does not release its write lock when trying to open an index which does not yet existIn version 2.0.0, the private IndexWriter constructor does not properly remove its write lock in the event of an error. This can be seen when one attempts to open (not create) an index in a directory which exists, but in which there is no segments file. Here is the offending code:

    247   private IndexWriter(Directory d, Analyzer a, final boolean create, boolean closeDir)
    248     throws IOException {
    249       this.closeDir = closeDir;
    250       directory = d;
    251       analyzer = a;
    252 
    253       Lock writeLock = directory.makeLock(IndexWriter.WRITE_LOCK_NAME);
    254       if (!writeLock.obtain(writeLockTimeout)) // obtain write lock
    255         throw new IOException(""Index locked for write: "" + writeLock);
    256       this.writeLock = writeLock;                   // save it
    257 
    258       synchronized (directory) {        // in- & inter-process sync
    259         new Lock.With(directory.makeLock(IndexWriter.COMMIT_LOCK_NAME), commitLockTimeout) {
    260             public Object doBody() throws IOException {
    261               if (create)
    262                 segmentInfos.write(directory);
    263               else
    264                 segmentInfos.read(directory);
    265               return null;
    266             }
    267           }.run();
    268       }
    269   }

On line 254, a write lock is obtained by the constructor. If an exception is raised inside the doBody() method (on line 260), then that exception is propagated, the constructor will fail, but the lock is not released until the object is garbage collected. This is typically an issue except when using the IndexModifier class.

As of the filing of this bug, this has not yet been fixed in the trunk (IndexWriter.java#472959):

    251   private IndexWriter(Directory d, Analyzer a, final boolean create, boolean closeDir)
    252     throws IOException {
    253       this.closeDir = closeDir;
    254       directory = d;
    255       analyzer = a;
    256 
    257       Lock writeLock = directory.makeLock(IndexWriter.WRITE_LOCK_NAME);
    258       if (!writeLock.obtain(writeLockTimeout)) // obtain write lock
    259         throw new IOException(""Index locked for write: "" + writeLock);
    260       this.writeLock = writeLock;                   // save it
    261 
    262       synchronized (directory) {        // in- & inter-process sync
    263         new Lock.With(directory.makeLock(IndexWriter.COMMIT_LOCK_NAME), commitLockTimeout) {
    264             public Object doBody() throws IOException {
    265               if (create)
    266                 segmentInfos.write(directory);
    267               else
    268                 segmentInfos.read(directory);
    269               return null;
    270             }
    271           }.run();
    272       }
    273   }"
0,"Surround query languageThis is a copy of what I posted about a year ago. 
 
The whole thing is hereby licenced under the Apache Licence 2.0, 
copyright 2005 Apache Software Foundation. 
 
For inclusion in Lucene (sandbox perhaps?) it will need 
at least the following adaptations: 
- renaming of package names 
  (org.surround to somewhere org.apache.lucene ) 
- moves of the source files to corresponding directories 
 
Although it uses the identifier sncf in some places 
I'm not associated with French railroads, but I like the TGV. 
 
Regards, 
Paul Elschot"
1,"JCR2SPI: potential race condition in event listener registrationThere's a potential race condition when the first event listener is registered (ObservationManager.addEventListener). The observation manager should only start listening for events after the new SPI event filter has been created.

(Note there's a related problem when an *additional* event listener is getting registered, while a RepositoryService.getEvents call is already in progress).
"
0,"Move FunctionQuery, ValueSources and DocValues to Queries moduleHaving resolved the FunctionQuery sorting issue and moved the MutableValue classes, we can now move FunctionQuery, ValueSources and DocValues to a Queries module."
1,"CVE-2009-0026: Cross site scripting issues in webappSome of the jackrabbit-webapp forms don't properly escape user input when displaying it in the resulting HTML page. This leads to potential cross site scripting issues. For example:

    search.jsp?q=%25%22%3Cscript%3Ealert(1)%3C/script%3E
    swr.jsp?q=%25""<script>alert(1)</script>&swrnum=1

The CVE id for this issue is CVE-2009-0026. This issue was reported by the Red Hat Security Response Team."
0,"Helper Method to escape illegal XPath Search TermIf you try to perform a search like this

//element(*, nt:base)[jcr:contains(., 'test!')]

you get this exception

javax.jcr.RepositoryException: Exception building query: org.apache.jackrabbit.core.query.lucene.fulltext.ParseException: Encountered ""<EOF>"" at line 1, column 6.
"
0,"LengthFilter and other TokenFilters that skip tokens ignore relative positionIncrementSee for reference:
http://www.nabble.com/WordDelimiterFilter%2BLenghtFilter-results-in-termPosition%3D%3D-1-td16306788.html
and http://www.nabble.com/Lucene---Java-f24284.html

It seems that LengthFilter (at least) could produce a stream in which the first Token has a positionIncrement of 0, which make CheckIndex and Luke function ""Reconstruct&Edit"" to generate exception.

Should something be done to avoid this situation, or could the error be ignored (by allowing Term with a position of -1, and relaxing CheckIndex checks?)
"
1,"CacheBehaviour Observation brokenWhile trying to fix JCR-2293 I discovered that CacheBehaviour Observation is broken:

- HierarchyEventListener.onEvent ignores local event (despite the comment saying otherwise). Not sure which way it should be. However with local events being ignored, JCR-2293 will most probably also occur with CacheBehaviour Observation. 

- NodeEntryImpl.refresh(Event) does not set its child node entries to incomplete when a node/property was added.

- After tentatively fixing above issues, I discovered that NodeEntryImpl.refresh(Event) and my own event listener operate on different NodeEntryImpl and ChildNodeEntryImpl instances. That is, even though I set childNodeEntries.complete to false in NodeEntryImpl.refresh(Event), when my own event listener retrieves that node (entry), it gets a different instance which has childNodeEntries.complete still set to true.
"
1,"jcr-server should respect child node definition of jcr:contentWhen creating a new file, jcr:content defaults to nt:unstructured. This causes file creation to fail when the underlying persistent store (i.e. SPI implementation) does not support nt:unstructured for jcr:content. 

I suggest to check whether the underlying implementation provides its own node type for jcr:content and use that one. If not, default to nt:unstructured."
0,"FieldInfos should be read-only if loaded from diskCurrently FieldInfos create a private FieldNumberBiMap when they are loaded from a directory which is necessary due to some limitation we need to face with IW#addIndexes(Dir). If we add an index via a directory to an existing index field number can conflict with the global field numbers in the IW receiving the directories. Those field number conflicts will remain until those segments are merged and we stabilize again based on the IW global field numbers. Yet, we unnecessarily creating a BiMap here where we actually should enforce read-only semantics since nobody should modify this FieldInfos instance we loaded from the directory. If somebody needs to get a modifiable copy they should simply create a new one and all all FieldInfo instances to it.

"
0,PropertyTypeRegistry should also yield if property is multi-valuedCurrently only the PropertyType is available for a certain property name. In some cases it is also required to know if the property is single- or multi-valued.
0,"PostMethod Java doc refers to wrong section of RFC1945""The HTTP POST method is defined in section 9.5 of RFC1945"" should read ""The
HTTP POST method is defined in section 8.3 of RFC1945""

Change 9.5 to 8.3."
0,"Optimize concurrent queriesThere are a number of bottlenecks that prevent scalability of concurrent queries:

- Fake norms are created repeatedly because a new SearchIndex$CombinedIndexReader is created for each query. This prevents caching of fake norms on the level of the CombinedIndexReader. Creating fake norms for index readers that span multiple sub reader is inefficient and should be avoided. Like with other Jackrabbit specific queries, there should be one for TermQuery, which is aware of sub readers. Its weight should then create one scorer for each sub reader. This effectively reuses the fake norms on the sub reader.

- There should be a  UUID cache that maps document number to UUID. This is basically the inverse of the existing DocNumberCache. UUID lookup is regularly a bottleneck in the SegmentReader where the method document() is synchronized and does I/O.

- Queries often contain constraints that limit the result to nodes with a certain flag set to a literal. These constraints should be cached in the query handler."
1,"ArrayIndexOutOfBoundsException in NodeTypeDefDiffIt appears that the code for building diffs in child node definitions loops incorrectly, opening the possibility for an ArrayIndexOutOfBounds exception. The offending portion is in the ""buildChildNodeDefDiffs"" method:

<<
NodeDef[] cnda2 = newDef.getChildNodeDefs();
HashMap defs2 = new HashMap();
for (int i = 0; i < cnda1.length; i++) {
    defs2.put(cnda2[i].getId(), cnda2[i]);
}
>>

It seems like simply changing the length check to be cnda2 (as it is in ""buildPropDefsDiff"") would suffice."
0,Only load item definition when requiredSome item definitions are loaded when an item state is constructed. Whenever possible this should be delayed to a time when the definition is actually used.
0,FieldCache should support longs and doublesWould be nice if FieldCache supported longs and doubles
1,"BundleDbPersistenceManager.checkConsistency() only fixes inconsistency if consistencyFix is enabled in configurationThe method has a parameter that explicitly tells whether an inconsistency should be fixed, thus the configuration parameter should be ignored.

Suggested patch:

Index: BundleDbPersistenceManager.java
===================================================================
--- BundleDbPersistenceManager.java	(revision 648657)
+++ BundleDbPersistenceManager.java	(working copy)
@@ -864,7 +864,7 @@
         }
 
         // repair collected broken bundles
-        if (consistencyFix && !modifications.isEmpty()) {
+        if (fix && !modifications.isEmpty()) {
             log.info(name + "": Fixing "" + modifications.size() + "" inconsistent bundle(s)..."");
             Iterator iterator = modifications.iterator();
             while (iterator.hasNext()) {
"
0,"Remove support for pre-3.0 indexesWe should remove support for 2.x (and 1.9) indexes in 4.0. It seems that nothing can be done in 3x because there is no special code which handles 1.9, so we'll leave it there. This issue should cover:
# Remove the .zip indexes
# Remove the unnecessary code from SegmentInfo and SegmentInfos. Mike suggests we compare the version headers at the top of SegmentInfos, in 2.9.x vs 3.0.x, to see which ones can go.
# remove FORMAT_PRE from FieldInfos
# Remove old format from TermVectorsReader

If you know of other places where code can be removed, then please post a comment here.

I don't know when I'll have time to handle it, definitely not in the next few days. So if someone wants to take a stab at it, be my guest.
"
0,"Occasional IndexingQueueTest failuresEvery now and then, when doing a clean build of the latest jackrabbit trunk I see the following test failure in jackrabbit-core:

-------------------------------------------------------------------------------
Test set: org.apache.jackrabbit.core.query.lucene.TestAll
-------------------------------------------------------------------------------
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.665 sec <<< FAILURE!
testQueue(org.apache.jackrabbit.core.query.lucene.IndexingQueueTest)  Time elapsed: 1.654 sec  <<< FAILURE!
junit.framework.AssertionFailedError
        at junit.framework.Assert.fail(Assert.java:47)
        at junit.framework.Assert.assertTrue(Assert.java:20)
        at junit.framework.Assert.assertTrue(Assert.java:27)
        at org.apache.jackrabbit.core.query.lucene.IndexingQueueTest.testQueue(IndexingQueueTest.java:69)

Typically the problem disappears when I rebuild, but the test should still not have failed."
0,"JCRTest.java (First Steps example code) creates a StringValue with ""new""The JCRTest.java file described in the First Steps document (http://incubator.apache.org/jackrabbit/firststeps.html) on the jackrabbit incubator website contains a line that attempts to create a StringValue using new, rather than using the ValueFactory interface. This causes the code to fail to compile - perhaps an initiative test, but could be off-putting...

Simple fix is to swap the line:

 n.setProperty(""testprop"", new StringValue(""Hello, World.""));

to 

n.setProperty(""testprop"", session.getValueFactory().createValue(""Hello, World.""));

"
0,"implement PerFieldAnalyzerWrapper.getOffsetGapPerFieldAnalyzerWrapper does not delegates calls to getOffsetGap(Fieldable), instead it returns the default values from the implementation of Analyzer. (Similar to LUCENE-659 ""PerFieldAnalyzerWrapper fails to implement getPositionIncrementGap"")"
1,"NoSuchItemStateException on removing node (no versioning)I'm using jackrabbit 1.2.1
with no versioning
with a very simple SimpleAccessManager (this try to compute the path of the passed ItemId and verify permissions over that path)

when I remove a node (nt:file or nt:folder),  calling session.save() I obtains the exception reported below.

is it really a bug or am i wrong?
thanks

the following is the code I'm using to build the path
----------------------------------------- CODE START
public String getStringPath(ItemId id) throws ItemNotFoundException, RepositoryException, NoPrefixDeclaredException
	{
		String p = """";
		NamespaceResolver nsResolver = ((HierarchyManagerImpl) hierMgr).getNamespaceResolver();
		Path path = hierMgr.getPath(id);
		PathElement[] pe = path.getElements();
		for (int i = 0; i < pe.length; i++)
		{
			if (pe[i].denotesName())
				p += ""/"" + pe[i].toJCRName(nsResolver);
		}
		return p;
	}
----------------------------------------- CODE END


----------------------------------------- START
javax.jcr.ItemNotFoundException: failed to build path of d688e92f-26ae-4f7c-aba7-aaff1df62c2c: d688e92f-26ae-4f7c-aba7-aaff1df62c2c: d688e92f-26ae-4f7c-aba7-aaff1df62c2c
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:362)
	at org.apache.jackrabbit.core.CachingHierarchyManager.getPath(CachingHierarchyManager.java:224)
	at it.unict.faq.jackrabbit.SimpleAccessManager.getStringPath(SimpleAccessManager.java:238)
	at it.unict.faq.jackrabbit.SimpleAccessManager.controllo(SimpleAccessManager.java:215)
	at it.unict.faq.jackrabbit.SimpleAccessManager.isGranted(SimpleAccessManager.java:183)
	at org.apache.jackrabbit.core.ItemImpl.validateTransientItems(ItemImpl.java:645)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1162)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:821)
	at it.unict.faq.driver.manager.impl.DAO.JcrDAO.CancellaNodo(JcrDAO.java:638)
	at it.unict.faq.driver.manager.impl.DocumentServerManager.ds_del(DocumentServerManager.java:58)
	at elearn.portal.action.ds_del_portal.execute(ds_del_portal.java:28)
	at org.apache.struts.action.RequestProcessor.processActionPerform(RequestProcessor.java:421)
	at org.apache.struts.action.RequestProcessor.process(RequestProcessor.java:226)
	at org.apache.struts.action.ActionServlet.process(ActionServlet.java:1158)
	at org.apache.struts.action.ActionServlet.doPost(ActionServlet.java:415)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:709)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:252)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:173)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:264)
	at org.acegisecurity.intercept.web.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:107)
	at org.acegisecurity.intercept.web.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:72)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:110)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.providers.anonymous.AnonymousProcessingFilter.doFilter(AnonymousProcessingFilter.java:125)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.rememberme.RememberMeProcessingFilter.doFilter(RememberMeProcessingFilter.java:142)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.wrapper.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:81)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.AbstractProcessingFilter.doFilter(AbstractProcessingFilter.java:217)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.logout.LogoutFilter.doFilter(LogoutFilter.java:108)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.context.HttpSessionContextIntegrationFilter.doFilter(HttpSessionContextIntegrationFilter.java:193)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.util.FilterChainProxy.doFilter(FilterChainProxy.java:148)
	at org.acegisecurity.util.FilterToBeanProxy.doFilter(FilterToBeanProxy.java:98)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:202)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:173)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:213)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:178)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:126)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:105)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:107)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:148)
	at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:869)
	at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.processConnection(Http11BaseProtocol.java:664)
	at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpoint.java:527)
	at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFollowerWorkerThread.java:80)
	at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:684)
	at java.lang.Thread.run(Thread.java:595)
Caused by: org.apache.jackrabbit.core.state.NoSuchItemStateException: d688e92f-26ae-4f7c-aba7-aaff1df62c2c
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getTransientItemState(SessionItemStateManager.java:323)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:154)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getItemState(HierarchyManagerImpl.java:120)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:357)
	... 52 more
org.apache.jackrabbit.core.state.NoSuchItemStateException: d688e92f-26ae-4f7c-aba7-aaff1df62c2c
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getTransientItemState(SessionItemStateManager.java:323)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:154)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getItemState(HierarchyManagerImpl.java:120)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:357)
	at org.apache.jackrabbit.core.CachingHierarchyManager.getPath(CachingHierarchyManager.java:224)
	at it.unict.faq.jackrabbit.SimpleAccessManager.getStringPath(SimpleAccessManager.java:238)
	at it.unict.faq.jackrabbit.SimpleAccessManager.controllo(SimpleAccessManager.java:215)
	at it.unict.faq.jackrabbit.SimpleAccessManager.isGranted(SimpleAccessManager.java:183)
	at org.apache.jackrabbit.core.ItemImpl.validateTransientItems(ItemImpl.java:645)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1162)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:821)
	at it.unict.faq.driver.manager.impl.DAO.JcrDAO.CancellaNodo(JcrDAO.java:638)
	at it.unict.faq.driver.manager.impl.DocumentServerManager.ds_del(DocumentServerManager.java:58)
	at elearn.portal.action.ds_del_portal.execute(ds_del_portal.java:28)
	at org.apache.struts.action.RequestProcessor.processActionPerform(RequestProcessor.java:421)
	at org.apache.struts.action.RequestProcessor.process(RequestProcessor.java:226)
	at org.apache.struts.action.ActionServlet.process(ActionServlet.java:1158)
	at org.apache.struts.action.ActionServlet.doPost(ActionServlet.java:415)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:709)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:252)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:173)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:264)
	at org.acegisecurity.intercept.web.FilterSecurityInterceptor.invoke(FilterSecurityInterceptor.java:107)
	at org.acegisecurity.intercept.web.FilterSecurityInterceptor.doFilter(FilterSecurityInterceptor.java:72)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.ExceptionTranslationFilter.doFilter(ExceptionTranslationFilter.java:110)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.providers.anonymous.AnonymousProcessingFilter.doFilter(AnonymousProcessingFilter.java:125)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.rememberme.RememberMeProcessingFilter.doFilter(RememberMeProcessingFilter.java:142)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.wrapper.SecurityContextHolderAwareRequestFilter.doFilter(SecurityContextHolderAwareRequestFilter.java:81)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.AbstractProcessingFilter.doFilter(AbstractProcessingFilter.java:217)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.ui.logout.LogoutFilter.doFilter(LogoutFilter.java:108)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.context.HttpSessionContextIntegrationFilter.doFilter(HttpSessionContextIntegrationFilter.java:193)
	at org.acegisecurity.util.FilterChainProxy$VirtualFilterChain.doFilter(FilterChainProxy.java:274)
	at org.acegisecurity.util.FilterChainProxy.doFilter(FilterChainProxy.java:148)
	at org.acegisecurity.util.FilterToBeanProxy.doFilter(FilterToBeanProxy.java:98)
	at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:202)
	at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:173)
	at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:213)
	at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:178)
	at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:126)
	at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:105)
	at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:107)
	at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:148)
	at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:869)
	at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.processConnection(Http11BaseProtocol.java:664)
	at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpoint.java:527)
	at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFollowerWorkerThread.java:80)
	at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:684)
	at java.lang.Thread.run(Thread.java:595)
----------------------------------------- END"
0,"random analyzer testswe have been finding+fixing lots of bugs by randomizing lucene tests.
in r966878 I added a variant of random unicode string that gives you a random string within the same unicode block (for other purposes)

I think we should use this to test the analyzers better, for example we should pound tons of random greek strings against the greek analyzer and at least make sure there aren't exceptions.
"
0,[PATCH] don't use the reflective form of {Collection}.toArrayPassing a prototype array into {Collection}.toArray that is too small makes the toArray call expend alot of effort using reflection to do it's job. It is more performant to just pass in a correctly sized prototype. This patch does this.
1,"derelativizing of relative URIs with a scheme is incorrectURI constructor ""public URI(URI base, URI relative) throws URIException"" assumes that if given 'relative' URI has a scheme, it should provide an authority and complete path to the constructed URI. However, a URI can have a scheme but still be relative, requiring the authority and base path of the 'base' URI. 

Demonstration code:

URI base = new URI(""http://www.example.com/some/page"");
URI rel = new URI(""http:boo"");
URI derel = new URI(base,rel);
derel.toString();
(java.lang.String) http:boo

In fact, derel should be ""http://www.example.com/some/boo"". 

RFC2396 is a little confused about this; section 3.1 states """"Relative URI references are distinguished from absolute URI in that they do not begin with a scheme name."" But, in section 5, there are several sentences talking about relative URIs that begin with schemes (and how this prevents using relative URIs that have leading path segments that look like scheme identifiers). 

RFC3896, which supercedes RFC2396, removes the implication a relative URI cannot begin with a scheme, leaving the other text explcitly discussing relative URIs with schemes. 

Both Firefox (1.5) and IE (6.0) treat ""http:boo"" the same as ""boo"" for purposes of derelativization against an HTTP base URI, which would give the final URI ""http://www.example.com/some/boo"" in the example above. 

Even relative URIs like ""http:../../boo"" are explicitly legal. 

"
0,"warn on invalid set-cookie headerI had a problem on a WS server that comes from some proxy misconfiguration...
resulting in this reponse beeing received by HTTPclient :
17:26:36,489 DEBUG [header] << ""HTTP/1.1 200 OK[\r][\n]""
17:26:36,489 DEBUG [header] << ""Set-Cookie: =f448bb59feedbaaabaee; path=/[\r][\n]""
17:26:36,489 DEBUG [header] << ""Date: Tue, 15 Nov 2005 16:26:36 GMT[\r][\n]""
17:26:36,489 DEBUG [header] << ""Server: Apache[\r][\n]""
17:26:36,489 DEBUG [header] << ""Connection: close[\r][\n]""
17:26:36,489 DEBUG [header] << ""Transfer-Encoding: chunked[\r][\n]""
17:26:36,489 DEBUG [header] << ""Content-Type: text/xml;charset=utf-8[\r][\n]""

The set-cookie header is malformed, as cookie has no name, so the HTTP head may
be considered invalid.

This results in an error when building the NEXT request. I'd expect httpclient
to WARN on malformed header and drop it."
0,"Multi-level skipping on posting listsTo accelerate posting list skips (TermDocs.skipTo(int)) Lucene uses skip lists. 
The default skip interval is set to 16. If we want to skip e. g. 100 documents, 
then it is not necessary to read 100 entries from the posting list, but only 
100/16 = 6 skip list entries plus 100%16 = 4 entries from the posting list. This 
speeds up conjunction (AND) and phrase queries significantly.

However, the skip interval is always a compromise. If you have a very big index 
with huge posting lists and you want to skip over lets say 100k documents, then 
it is still necessary to read 100k/16 = 6250 entries from the skip list. For big 
indexes the skip interval could be set to a higher value, but then after a big 
skip a long scan to the target doc might be necessary.

A solution for this compromise is to have multi-level skip lists that guarantee a 
logarithmic amount of skips to any target in the posting list. This patch 
implements such an approach in the following way:

  Example for skipInterval = 3:
                                                      c            (skip level 2)
                  c                 c                 c            (skip level 1) 
      x     x     x     x     x     x     x     x     x     x      (skip level 0)
  d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d d  (posting list)
      3     6     9     12    15    18    21    24    27    30     (df)
 
  d - document
  x - skip data
  c - skip data with child pointer
 
Skip level i contains every skipInterval-th entry from skip level i-1. Therefore the 
number of entries on level i is: floor(df / ((skipInterval ^ (i + 1))).
 
Each skip entry on a level i>0 contains a pointer to the corresponding skip entry in 
list i-1. This guarantees a logarithmic amount of skips to find the target document.


Implementations details:

   * I factored the skipping code out of SegmentMerger and SegmentTermDocs to 
     simplify those classes. The two new classes AbstractSkipListReader and 
	 AbstractSkipListWriter implement the skipping functionality.
   * While AbstractSkipListReader and Writer take care of writing and reading the 
     multiple skip levels, they do not implement an actual skip data format. The two 
	 new subclasses DefaultSkipListReader and Writer implement the skip data format 
	 that is currently used in Lucene (with two file pointers for the freq and prox 
	 file and with payload length information). I added this extra layer to be 
	 prepared for flexible indexing and different posting list formats. 
      
   
File format changes: 

   * I added the new parameter 'maxSkipLevels' to the term dictionary and increased the
     version of this file. If maxSkipLevels is set to one, then the format of the freq 
	 file does not change at all, because we only have one skip level as before. For 
	 backwards compatibility maxSkipLevels is set to one automatically if an index 
	 without the new parameter is read. 
   * In case maxSkipLevels > 1, then the frq file changes as follows:
     FreqFile (.frq) --> <TermFreqs, SkipData>^TermCount
	 SkipData        --> <<SkipLevelLength, SkipLevel>^(Min(maxSkipLevels, 
	                       floor(log(DocFreq/log(skipInterval))) - 1)>, SkipLevel>
	 SkipLevel       --> <SkipDatum>^DocFreq/(SkipInterval^(Level + 1))

	 Remark: The length of the SkipLevel is not stored for level 0, because 1) it is not 
	 needed, and 2) the format of this file does not change for maxSkipLevels=1 then.
	 
	 
All unit tests pass with this patch."
0,spi2davex: reduce memory footprint of Node/PropertyInfoImplthe in-memory footprint of o.a.jackrabbit.spi2davex.NodeinfoImpl & PropertyInfoImp is quite big. 
0,"Tokenizers (which are the source of Tokens) should call AttributeSource.clearAttributes() firstThis is a followup for LUCENE-1796:
{quote}
Token.clear() used to be called by the consumer... but then it was switched to the producer here: LUCENE-1101 
I don't know if all of the Tokenizers in lucene were ever changed, but in any case it looks like at least some of these bugs were introduced with the switch to the attribute API - for example StandardTokenizer did clear it's reusableToken... and now it doesn't.
{quote}

As alternative to changing all core/contrib Tokenizers to call clearAttributes first, we could do this in the indexer, what would be a overhead for old token streams that itsself clear their reusable token. This issue should also update the Javadocs, to clearly state inside Tokenizer.java, that the source TokenStream (normally the Tokenizer) should clear *all* Attributes. If it does not do it and e.g. the positionIncrement is changed to 0 by any TokenFilter, but the filter does not change it back to 1, the TokenStream would stay with 0. If the TokenFilter would call PositionIncrementAttribute.clear() (because he is responsible), it could also break the TokenStream, because clear() is a general method for the whole attribute instance. If e.g. Token is used as AttributeImpl, a call to clear() would also clear offsets and termLength, which is not wanted. So the source of the Tokenization should rest the attributes to default values.

LUCENE-1796 removed the iterator creation cost, so clearAttributes should run fast, but is an additional cost during Tokenization, as it was not done consistently before, so a small speed degradion is caused by this, but has nothing to do with the new TokenStream API."
0,"Merge jcr-benchmark into the performance test suiteThe jackrabbit-jcr-benchmark component currently lives in the JCR Commons area, but there have been no active plans to release the component and AFAIUI it's so far only been used for the performance test suite we set up in JCR-2695. To avoid the extra complexity of spreading the test code over multiple components and trunks, I'd like to merge the jcr-benchmark component back to Jackrabbit trunk into the performance test suite we have in tests/performance."
0,"when checking tvx/fdx size mismatch, also include whether the file existsIndexWriter checks, during flush and during merge, that the size of the index file for stored fields (*.fdx) and term vectors (*.tvx) matches how many bytes it has just written.

This originally was added for LUCENE-1282, ie, as a safety to catch the nasty ""off by 1"" JRE hotspot bug that would otherwise silently corrupt the index.

However, this check also seems to catch a different case, where the size of the file is zero.   The most recent example is LUCENE-1521.  I'd like to improve the message in the exception to include whether or not the file exists, to help understand why users are sometimes hitting this exception.  My best theory at this point is something external is removing the file out from under the IndexWriter.
"
1,"PUT method blocks against older serversTo reproduce, attempt a PUT request against an appropriate servlet under TC3.2
(yes I know that needs an upgrade - sigh)

RFC 2616 says:
""Because of the presence of older implementations, the protocol allows ambiguous
situations in which a client may send ""Expect: 100- continue"" without receiving
either a 417 (Expectation Failed) status or a 100 (Continue) status. Therefore,
when a client sends this header field to an origin server (possibly via a proxy)
from which it has never seen a 100 (Continue) status, the client SHOULD NOT wait
for an indefinite period before sending the request body.""

This isn't how HttpClient behaves. After sending the headers,
PutMethod.writeRequestBody() returns false. HttpMethodBase then calls
readStatusCode(), which blocks waiting for a read (or I guess you could time out
the whole request). Right now this makes it impossible to use HttpClient to PUT
to older Http 1.1 implementations.

A suggested resolution: since the spec allows for clients to avoid waiting if
they know the 100 response will not arrive, why not simply provide a boolean
flag to allow the 'wait for 100' behaviour in PutMethod.writeResponseBody() to
be turned off, on a per-request basis? This solution puts the burden of knowing
""origin server[s]...from which it has never seen a 100 (Continue) status"" on the
user of HttpClient. Less than perfect as you can only find out that this has
happened by trial and error.

A more correct solution, is to maintain a list of servers that ignore the Expect
header in PutMethod, and override PutMethod.readStatusCode() to time out, send
the body, remember this server is buggy, and read the status code again."
1,"DatabaseFileSystem's logger references the wrong classIn DatabaseFileSystem, the logger is constructed as
private static Logger log = LoggerFactory.getLogger(DbFileSystem.class);

It should be constructed as:
private static Logger log = LoggerFactory.getLogger(DatabaseFileSystem.class);"
1,"HTTPS Post Does Not WorkUsing Java 1.4.1_01 on Windows 2000. An HTTPS Post results in HTTP/100-Continue 
messages. The same code posting to a non HTTPS URL works. The code populates 
the request body using a NameValuePair array."
0,"Make BooleanWeight and DisjunctionMaxWeight protectedCurrently, BooleanWeight is private, yet it has 2 protected members (similarity, weights) which are unaccessible from custom code

i have some use cases where it would be very useful to crawl a BooleanWeight to get at the sub Weight objects

however, since BooleanWeight is private, i have no way of doing this

If BooleanWeight is protected, then i can subclass BooleanQuery to hook in and wrap BooleanWeight with a subclass to facilitate this walking of the Weight objects

Would also want DisjunctionMaxWeight to be protected, along with its ""weights"" member

Would be even better if these Weights were made public with accessors to their sub ""weights"" objects (then no subclassing would be necessary on my part)

this should be really trivial and would be great if it can get into 2.9

more generally, it would be nice if all Weight classes were public with nice accessors to relevant ""sub weights""/etc so custom code can get its hooks in where and when desired"
0,"Wire log is incomplete if HttpParser detects an errorIf HttpParser detects an error in any of the headers, it throws a ProtocolException

Although the failing header is included in the Exception detail, the headers leading up to the failure are not logged, which makes it hard to debug (and is quite confusing, as the PE does not appear to be related to the data that has been received).

This is because the wire-logging is done in the caller (HttpMethodDirector) which only logs the header if the parse succeeds.

Perhaps the Wire logging should be done at the point where the HttpParser reads the line."
1,"TimeLimitingCollector's TimeExceededException contains useless relative docidWe found another bug with the RandomIndexWriter: When TimeLimitingCollector breaks collection after timeout, it records the last/next collected docid. It does this without rebasing, so the docid is useless. TestTimeLimitingCollector checks the docid, but correctly rebases it (as only this makes sense). Because the RandomIndexWriter uses different merge settings, the index is now sometimes not optimized and so the test fails (which is correct, as the docid is useless for non-optimized index).

Attached is a patch that fixes this. Please tell me if I should backport to 2.9 and 3.0!"
0,"Further parallelizaton of ParallelMultiSearcherWhen calling {{search(Query, Filter, int)}} on a ParallelMultiSearcher, the {{createWeights}} function of MultiSearcher is called, and sequentially calls {{docFreqs()}} on every sub-searcher. This can take a significant amount of time when there are lots of remote sub-searchers.

"
1,"javax.jcr.RepositoryException when a JOIN SQL2 query is send via Davex and has resultssee the following thread for details:
http://www.mail-archive.com/users@jackrabbit.apache.org/msg17975.html

assuming a data structure as follows:
/foo [nt:unstructured]
/foo/bar [nt:unstructured]
/foo/bar@lala = huii (lala is string property of bar)
/ding [nt:unstructured]
/ding@dong = ##barUUID### (dong is a property of type ""Reference"")

then the following code will throw an exception:

DavexClient Client = new DavexClient(url);
Repository repo = Client.getRepository();
Credentials sc = new SimpleCredentials(""admin"",""admin"".toCharArray());
Session s = repo.login(sc,workspace);

QueryManager qm = s.getWorkspace().getQueryManager();

String sql = ""SELECT data.* FROM [nt:unstructured] AS data WHERE data.lala= 'huii'"";
sql = ""SELECT * FROM [nt:unstructured] AS data INNER JOIN [nt:unstructured] AS referring ON referring.[dong] = data.[jcr:uuid] WHERE data.lala = 'huii'"";
sql = ""SELECT * FROM [nt:unstructured] AS data INNER JOIN [nt:unstructured] AS referring ON ISDESCENDANTNODE(data, referring) WHERE data.lala = 'huii'"";
Query query = qm.createQuery(sql, Query.JCR_SQL2);
QueryResult qr = query.execute();

The first query works just fine and I can iterate over the result. Neither the second nor the third query works.
In both cases I end up with a javax.jcr.RepositoryException. Note the exception only happens if the query returns results. Aka a join will work just fine if it matches no rows."
1,"need to ensure that sims that use collection-level stats (e.g. sumTotalTermFreq) handle non-existent fieldBecause of things like queryNorm, unfortunately similarities have to handle the case where they are asked to computeStats() for a term, where the field does not exist at all.
(Note they will never have to actually score anything, but unless we break how queryNorm works for TFIDF, we have to deal with this case).

I noticed this while doing some benchmarking, so i created a test to test some cases like this across all the sims."
1,MSSql and MySQL bunlde PM schemas missing definition for name indexthe mssql and mysql ddl files of the respective bundle persistence managers are missing the definitions for the name index.
1,"IW.optimize() can do too many merges at the very endThis was fixed on trunk in LUCENE-1044 but I'd like to separately
backport it to 2.3.

With ConcurrentMergeScheduler there is a bug, only when CFS is on,
whereby after the final merge of an optimize has finished and while
it's building its CFS, the merge policy may incorrectly ask for
another merge to collapse that segment into a compound file.  The net
effect is optimize can spend many extra iterations unecessarily
merging a single segment to collapse it to compound file.

I believe the case is rare (hard to hit), and maybe only if you have
multiple threads calling optimize at once (the TestThreadedOptimize
test can hit it), but it's a low-risk fix so I plan to commit to 2.3
shortly.

"
0,"cache revalidation of variants does not update original variant entryWhen the cache stories multiple variant entries due to Vary headers in responses, the cache correctly sends a conditional request containing the etags of any existing variants on a ""variant miss"" (incoming request does not match the request variants already cached). In addition, when it receives a 304 response, it correctly returns the indicated variant to the request that causes the variant miss. However, it does not update the pre-existing variant cache entry as recommended by RFC 2616.

For example:

request 1, User-Agent: agent1 results in a 200 OK with Etag: etag1 and Vary: User-Agent.
request 2, User-Agent: agent2 causes an If-None-Match to the origin; if it returns 304 Not Modified with Etag: etag1
request 3, User-Agent: agent1 results in a 200 OK but gets the (outdated) entry that resulted from request 1

in other words, the origin response from request 2 does not update the variant for ""agent1"".

This does not cause incorrect behavior (this is a SHOULD) but does miss out on some caching opportunities here.
"
0,"TCK: Test that expect that modifications made by Session1 are automatically visible to Session2While changes made by session1 are automatically visible to any other session2 with the RI, this is not required by the
specification. Therefore i would suggest to modify the following test cases:

- NodeUUIDTest.testSaveMovedRefNode()
- SessionUUIDTest.testSaveMovedRefNode()

-> no patch. sorry.

- NodeTest.testRemoveInvalidItemStateException()

-> see patch."
0,"Enhanced JCR remoting (extending webdav SPI impl, basic remoting servlet)"
0,"Check if a DAV-Request has a Label in the header, before checking if it's version-controlledWhen looking at our MySQL logs, I realized that jackrabbit on each DAV Request calls the VERSION table every time I get a new node (which is not cached yet), even if I only do a simple getNode. 
As a versioning table can get pretty large, this may have a performance impact.

I found out, that DavResourceFactoryImpl checks, if a node is versioned to decide, if we have to check for the Label header to later check out another version for the GET request. I re-ordered those checks now so that it first checks, if there's an http Label-header and only then checks, if the node is versioned. The check for a Label header should be much faster than checking a DB, if it's versioned (and scale much better, too)



"
0,Index update overhead on cluster slave due to JCR-905JCR-905 is a quick and dirty fix and causes overhead on a cluster slave node when it processes revisions.
0,"OracleBundlePersistenceManager needs special blob handling for JDBC drivers prior to oracle 10the new oracle bundle persistence manager (see JCR-755) needs special blob handling for oracle jdbc drivers prior to version 10. since the pm works for newer versions, i suggest to add a separate pm eg: Oracle9PersistenceManager that contains this special blob handling."
0,"jackrabbit-webapp faceliftStill before 1.4, I meant to make the jackrabbit-webapp look a bit nicer. I'm taking the skin from JCR-1236 and applying it to jackrabbit-webapp."
0,"contrib.ssl.HostConfigurationWithHostFactoryI'd like to contribute an example specialized HostConfiguration, to replace the one I contributed in HTTPCLIENT-634."
0,"All spatial contrib shape classes implement equals but not hashCodeviolates contract - at a min, need to implement return constant."
1,"[patch] fix uppercase/lowercase handling for not equal tocode is missing breaks in switch statements, which causes both uppercase and lowercase terms to the not equal to lucene search. patch fixes."
1,FilterIndexReader should overwrite isOptimized()A call of FilterIndexReader.isOptimized() results in a NPE because FilterIndexReader does not overwrite isOptimized().
1,"o.a.l.analysis.de.GermanStemmer crashes on some inputsSee the tests from LUCENE-2560. 

GermanAnalyzer no longer uses this stemmer by default, but we should fix it."
0,"client cache may be a shared cache but is caching responses to requests with Authorization headers""      When a shared cache (see section 13.7) receives a request
      containing an Authorization field, it MUST NOT return the
      corresponding response as a reply to any other request, unless one
      of the following specific exceptions holds:

      1. If the response includes the ""s-maxage"" cache-control
         directive, the cache MAY use that response in replying to a
         subsequent request. But (if the specified maximum age has
         passed) a proxy cache MUST first revalidate it with the origin
         server, using the request-headers from the new request to allow
         the origin server to authenticate the new request. (This is the
         defined behavior for s-maxage.) If the response includes ""s-
         maxage=0"", the proxy MUST always revalidate it before re-using
         it.

      2. If the response includes the ""must-revalidate"" cache-control
         directive, the cache MAY use that response in replying to a
         subsequent request. But if the response is stale, all caches
         MUST first revalidate it with the origin server, using the
         request-headers from the new request to allow the origin server
         to authenticate the new request.

      3. If the response includes the ""public"" cache-control directive,
         it MAY be returned in reply to any subsequent request.""

http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.8

It isn't clear whether the CachingHttpClient is a shared cache or not (it depends on where it gets used), so the conservative compliant behavior is to assume we are a shared cache. The current implementation is caching responses regardless of whether the original requests had Authorization headers or not.

Patch and discussion forthcoming.

"
0,"Static index pruning by in-document term frequency (Carmel pruning)This module provides tools to produce a subset of input indexes by removing postings data for those terms where their in-document frequency is below a specified threshold. The net effect of this processing is a much smaller index that for common types of queries returns nearly identical top-N results as compared with the original index, but with increased performance. 

Optionally, stored values and term vectors can also be removed. This functionality is largely independent, so it can be used without term pruning (when term freq. threshold is set to 1).

As the threshold value increases, the total size of the index decreases, search performance increases, and recall decreases (i.e. search quality deteriorates). NOTE: especially phrase recall deteriorates significantly at higher threshold values. 

Primary purpose of this class is to produce small first-tier indexes that fit completely in RAM, and store these indexes using IndexWriter.addIndexes(IndexReader[]). Usually the performance of this class will not be sufficient to use the resulting index view for on-the-fly pruning and searching. 

NOTE: If the input index is optimized (i.e. doesn't contain deletions) then the index produced via IndexWriter.addIndexes(IndexReader[]) will preserve internal document id-s so that they are in sync with the original index. This means that all other auxiliary information not necessary for first-tier processing, such as some stored fields, can also be removed, to be quickly retrieved on-demand from the original index using the same internal document id. 

Threshold values can be specified globally (for terms in all fields) using defaultThreshold parameter, and can be overriden using per-field or per-term values supplied in a thresholds map. Keys in this map are either field names, or terms in field:text format. The precedence of these values is the following: first a per-term threshold is used if present, then per-field threshold if present, and finally the default threshold.

A command-line tool (PruningTool) is provided for convenience. At this moment it doesn't support all functionality available through API."
0,"Swap URL+Email recognizing StandardTokenizer and UAX29TokenizerCurrently, in addition to implementing the UAX#29 word boundary rules, StandardTokenizer recognizes email adresses and URLs, but doesn't provide a way to turn this behavior off and/or provide overlapping tokens with the components (username from email address, hostname from URL, etc.).

UAX29Tokenizer should become StandardTokenizer, and current StandardTokenizer should be renamed to something like UAX29TokenizerPlusPlus (or something like that).

For rationale, see [the discussion at the reopened LUCENE-2167|https://issues.apache.org/jira/browse/LUCENE-2167?focusedCommentId=12929325&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#action_12929325]."
0,Include the README file in the generated jar filesThe Incubator would prefer if we had the incubation notice included in the binary jar files we release. It should be a simple Maven configuration change to get the README.txt file included in the binary jars.
1,"Test failures with spi2jcr in AddEventListenerTestTwo tests fail:

- AddEventListenerTest.testUUID
- AddEventListenerTest.testNodeType"
0,"Remove noLockHack in SharedItemStateManagerWith the increased test coverage, specifically the recently added multi-threaded tests, I'm reasonably confident that the noLockHack in SharedItemStateManager is not needed anymore.

Attached patch removes the hack. All tests still pass, including the daily integration tests."
0,"EnwikiDocMaker id fieldThe EnwikiDocMaker is fairly usable outside of the benchmarking class, but it would benefit from indexing the ID field of the docs.

Patch to follow that adds an ID field."
0,"Enhance indexing of binary contentIndexing of binary content should be enhanced in order to allow either configuration what fields are indexed or provide better support for custom NodeIndexer implementations.

The current design has a couple of flaws that should be addressed at the same time:
- Reader instances are requested from the text filters even though the reader might never be used
- only jcr:data properties of nt:resource nodes are fulltext indexed
- It is up to the text filter implementation to decide the lucene field name for the text representation, responsibility should be moved to the NodeIndexer. A text filter should only provide a Reader instance.

With those changes a custom NodeIndexer can then decide if a binary property has one or more representations in the index."
0,"Some files are missing the license headersJukka provided the following list of files that are missing the license headers.
In addition there might be other files (like build scripts) that don't have the headers.

src/java/org/apache/lucene/document/MapFieldSelector.java
src/java/org/apache/lucene/search/PrefixFilter.java
src/test/org/apache/lucene/TestHitIterator.java
src/test/org/apache/lucene/analysis/TestISOLatin1AccentFilter.java
src/test/org/apache/lucene/index/TestAddIndexesNoOptimize.java
src/test/org/apache/lucene/index/TestBackwardsCompatibility.java
src/test/org/apache/lucene/index/TestFieldInfos.java
src/test/org/apache/lucene/index/TestIndexFileDeleter.java
src/test/org/apache/lucene/index/TestIndexWriter.java
src/test/org/apache/lucene/index/TestIndexWriterDelete.java
src/test/org/apache/lucene/index/TestIndexWriterLockRelease.java
src/test/org/apache/lucene/index/TestIndexWriterMergePolicy.java
src/test/org/apache/lucene/index/TestNorms.java
src/test/org/apache/lucene/index/TestParallelTermEnum.java
src/test/org/apache/lucene/index/TestSegmentTermEnum.java
src/test/org/apache/lucene/index/TestTerm.java
src/test/org/apache/lucene/index/TestTermVectorsReader.java
src/test/org/apache/lucene/search/TestRangeQuery.java
src/test/org/apache/lucene/search/TestTermScorer.java
src/test/org/apache/lucene/store/TestBufferedIndexInput.java
src/test/org/apache/lucene/store/TestWindowsMMap.java
src/test/org/apache/lucene/store/_TestHelper.java
src/test/org/apache/lucene/util/_TestUtil.java
contrib/benchmark/src/java/org/apache/lucene/benchmark/byTask/feeds/SimpleSloppyPhraseQueryMaker.java
contrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/server/FeedNotFoundException.java
contrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/server/registry/ComponentType.java
contrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/server/registry/RegistryException.java
contrib/gdata-server/src/core/src/java/org/apache/lucene/gdata/storage/lucenestorage/StorageAccountWrapper.java
contrib/gdata-server/src/core/src/test/org/apache/lucene/gdata/storage/lucenestorage/TestModifiedEntryFilter.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/AtomUriElementTest.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMEntryImplTest.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMFeedImplTest.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMGenereatorImplTest.java
contrib/gdata-server/src/gom/src/test/org/apache/lucene/gdata/gom/core/GOMSourceImplTest.java
contrib/highlighter/src/java/org/apache/lucene/search/highlight/TokenSources.java
contrib/javascript/queryConstructor/luceneQueryConstructor.js
contrib/javascript/queryEscaper/luceneQueryEscaper.js
contrib/javascript/queryValidator/luceneQueryValidator.js
contrib/queries/src/java/org/apache/lucene/search/BooleanFilter.java
contrib/queries/src/java/org/apache/lucene/search/BoostingQuery.java
contrib/queries/src/java/org/apache/lucene/search/FilterClause.java
contrib/queries/src/java/org/apache/lucene/search/FuzzyLikeThisQuery.java
contrib/queries/src/java/org/apache/lucene/search/TermsFilter.java
contrib/queries/src/java/org/apache/lucene/search/similar/MoreLikeThisQuery.java
contrib/queries/src/test/org/apache/lucene/search/BooleanFilterTest.java
contrib/regex/src/test/org/apache/lucene/search/regex/TestSpanRegexQuery.java
contrib/snowball/src/java/net/sf/snowball/Among.java
contrib/snowball/src/java/net/sf/snowball/SnowballProgram.java
contrib/snowball/src/java/net/sf/snowball/TestApp.java
contrib/spellchecker/src/test/org/apache/lucene/search/spell/TestSpellChecker.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/BooleanQueryTst.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/ExceptionQueryTst.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/SingleFieldTestDb.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/Test01Exceptions.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/Test02Boolean.java
contrib/surround/src/test/org/apache/lucene/queryParser/surround/query/Test03Distance.java
contrib/wordnet/src/java/org/apache/lucene/wordnet/SynExpand.java
contrib/wordnet/src/java/org/apache/lucene/wordnet/SynLookup.java
contrib/wordnet/src/java/org/apache/lucene/wordnet/Syns2Index.java
"
0,"Supporting deleteDocuments in IndexWriter (Code and Performance Results Provided)Today, applications have to open/close an IndexWriter and open/close an
IndexReader directly or indirectly (via IndexModifier) in order to handle a
mix of inserts and deletes. This performs well when inserts and deletes
come in fairly large batches. However, the performance can degrade
dramatically when inserts and deletes are interleaved in small batches.
This is because the ramDirectory is flushed to disk whenever an IndexWriter
is closed, causing a lot of small segments to be created on disk, which
eventually need to be merged.

We would like to propose a small API change to eliminate this problem. We
are aware that this kind change has come up in discusions before. See
http://www.gossamer-threads.com/lists/lucene/java-dev/23049?search_string=indexwriter%20delete;#23049
. The difference this time is that we have implemented the change and
tested its performance, as described below.

API Changes
-----------
We propose adding a ""deleteDocuments(Term term)"" method to IndexWriter.
Using this method, inserts and deletes can be interleaved using the same
IndexWriter.

Note that, with this change it would be very easy to add another method to
IndexWriter for updating documents, allowing applications to avoid a
separate delete and insert to update a document.

Also note that this change can co-exist with the existing APIs for deleting
documents using an IndexReader. But if our proposal is accepted, we think
those APIs should probably be deprecated.

Coding Changes
--------------
Coding changes are localized to IndexWriter. Internally, the new
deleteDocuments() method works by buffering the terms to be deleted.
Deletes are deferred until the ramDirectory is flushed to disk, either
because it becomes full or because the IndexWriter is closed. Using Java
synchronization, care is taken to ensure that an interleaved sequence of
inserts and deletes for the same document are properly serialized.

We have attached a modified version of IndexWriter in Release 1.9.1 with
these changes. Only a few hundred lines of coding changes are needed. All
changes are commented by ""CHANGE"". We have also attached a modified version
of an example from Chapter 2.2 of Lucene in Action.

Performance Results
-------------------
To test the performance our proposed changes, we ran some experiments using
the TREC WT 10G dataset. The experiments were run on a dual 2.4 Ghz Intel
Xeon server running Linux. The disk storage was configured as RAID0 array
with 5 drives. Before indexes were built, the input documents were parsed
to remove the HTML from them (i.e., only the text was indexed). This was
done to minimize the impact of parsing on performance. A simple
WhitespaceAnalyzer was used during index build.

We experimented with three workloads:
  - Insert only. 1.6M documents were inserted and the final
    index size was 2.3GB.
  - Insert/delete (big batches). The same documents were
    inserted, but 25% were deleted. 1000 documents were
    deleted for every 4000 inserted.
  - Insert/delete (small batches). In this case, 5 documents
    were deleted for every 20 inserted.

                                current       current          new
Workload                      IndexWriter  IndexModifier   IndexWriter
-----------------------------------------------------------------------
Insert only                     116 min       119 min        116 min
Insert/delete (big batches)       --          135 min        125 min
Insert/delete (small batches)     --          338 min        134 min

As the experiments show, with the proposed changes, the performance
improved by 60% when inserts and deletes were interleaved in small batches.


Regards,
Ning


Ning Li
Search Technologies
IBM Almaden Research Center
650 Harry Road
San Jose, CA 95120"
1,"trunk tests hang/deadlock TestIndexWriterWithThreadstrunk tests have been hanging often lately in hudson, this time i was careful to kill and get a good stacktrace:"
1,"ClassCastException org.apache.xerces.parsers.XIncludeAwareParserConfiguration cannot be cast to org.apache.xerces.xni.parser.XMLParserConfiguration when deploying in JBoss 5.1I tried to follow the steps given on http://wiki.apache.org/jackrabbit/JackrabbitOnJBoss
To get over an exception I had to use jcr-2.0.jar (instead of jcr-1.0.jar)
The following exception happens when the jboss server is started.
=======================================================================

2009-10-02 11:49:05,630 ERROR [org.jboss.web.tomcat.service.deployers.JBossContextConfig] (main) XML error parsing: context.xml
org.jboss.xb.binding.JBossXBRuntimeException: Failed to create a new SAX parser
	at org.jboss.xb.binding.UnmarshallerFactory$UnmarshallerFactoryImpl.newUnmarshaller(UnmarshallerFactory.java:100)
	at org.jboss.web.tomcat.service.deployers.JBossContextConfig.processContextConfig(JBossContextConfig.java:549)
	at org.jboss.web.tomcat.service.deployers.JBossContextConfig.init(JBossContextConfig.java:536)
	at org.apache.catalina.startup.ContextConfig.lifecycleEvent(ContextConfig.java:279)
	at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:117)
	at org.apache.catalina.core.StandardContext.init(StandardContext.java:5436)
	at org.apache.catalina.core.StandardContext.start(StandardContext.java:4148)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeployInternal(TomcatDeployment.java:310)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeploy(TomcatDeployment.java:142)
	at org.jboss.web.deployers.AbstractWarDeployment.start(AbstractWarDeployment.java:461)
	at org.jboss.web.deployers.WebModule.startModule(WebModule.java:118)
	at org.jboss.web.deployers.WebModule.start(WebModule.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java:157)
	at org.jboss.mx.server.Invocation.dispatch(Invocation.java:96)
	at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
	at org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:264)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:668)
	at org.jboss.system.microcontainer.ServiceProxy.invoke(ServiceProxy.java:206)
	at $Proxy38.start(Unknown Source)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:42)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:37)
	at org.jboss.dependency.plugins.action.SimpleControllerContextAction.simpleInstallAction(SimpleControllerContextAction.java:62)
	at org.jboss.dependency.plugins.action.AccessControllerContextAction.install(AccessControllerContextAction.java:71)
	at org.jboss.dependency.plugins.AbstractControllerContextActions.install(AbstractControllerContextActions.java:51)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.system.microcontainer.ServiceControllerContext.install(ServiceControllerContext.java:286)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.ServiceController.doChange(ServiceController.java:688)
	at org.jboss.system.ServiceController.start(ServiceController.java:460)
	at org.jboss.system.deployers.ServiceDeployer.start(ServiceDeployer.java:163)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:99)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:46)
	at org.jboss.deployers.spi.deployer.helpers.AbstractSimpleRealDeployer.internalDeploy(AbstractSimpleRealDeployer.java:62)
	at org.jboss.deployers.spi.deployer.helpers.AbstractRealDeployer.deploy(AbstractRealDeployer.java:50)
	at org.jboss.deployers.plugins.deployers.DeployerWrapper.deploy(DeployerWrapper.java:171)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doDeploy(DeployersImpl.java:1439)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1157)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1178)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.install(DeployersImpl.java:1098)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.process(DeployersImpl.java:781)
	at org.jboss.deployers.plugins.main.MainDeployerImpl.process(MainDeployerImpl.java:702)
	at org.jboss.system.server.profileservice.repository.MainDeployerAdapter.process(MainDeployerAdapter.java:117)
	at org.jboss.system.server.profileservice.repository.ProfileDeployAction.install(ProfileDeployAction.java:70)
	at org.jboss.system.server.profileservice.repository.AbstractProfileAction.install(AbstractProfileAction.java:53)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.install(AbstractProfileService.java:361)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.activateProfile(AbstractProfileService.java:306)
	at org.jboss.system.server.profileservice.ProfileServiceBootstrap.start(ProfileServiceBootstrap.java:271)
	at org.jboss.bootstrap.AbstractServerImpl.start(AbstractServerImpl.java:461)
	at org.jboss.Main.boot(Main.java:221)
	at org.jboss.Main$1.run(Main.java:556)
	at java.lang.Thread.run(Thread.java:619)
Caused by: org.jboss.xb.binding.JBossXBException: Failed to create a new SAX parser
	at org.jboss.xb.binding.parser.sax.SaxJBossXBParser.<init>(SaxJBossXBParser.java:97)
	at org.jboss.xb.binding.UnmarshallerImpl.<init>(UnmarshallerImpl.java:56)
	at org.jboss.xb.binding.UnmarshallerFactory$UnmarshallerFactoryImpl.newUnmarshaller(UnmarshallerFactory.java:96)
	... 73 more
Caused by: java.lang.ClassCastException: org.apache.xerces.parsers.XIncludeAwareParserConfiguration cannot be cast to org.apache.xerces.xni.parser.XMLParserConfiguration
	at org.apache.xerces.parsers.SAXParser.<init>(Unknown Source)
	at org.apache.xerces.parsers.SAXParser.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserFactoryImpl.newSAXParser(Unknown Source)
	at org.jboss.xb.binding.parser.sax.SaxJBossXBParser.<init>(SaxJBossXBParser.java:92)
	... 75 more
2009-10-02 11:49:05,645 ERROR [org.jboss.web.tomcat.service.deployers.JBossContextConfig] (main) XML error parsing: jboss.web/localhost/context.xml.default
org.jboss.xb.binding.JBossXBRuntimeException: Failed to create a new SAX parser
	at org.jboss.xb.binding.UnmarshallerFactory$UnmarshallerFactoryImpl.newUnmarshaller(UnmarshallerFactory.java:100)
	at org.jboss.web.tomcat.service.deployers.JBossContextConfig.processContextConfig(JBossContextConfig.java:549)
	at org.jboss.web.tomcat.service.deployers.JBossContextConfig.init(JBossContextConfig.java:537)
	at org.apache.catalina.startup.ContextConfig.lifecycleEvent(ContextConfig.java:279)
	at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:117)
	at org.apache.catalina.core.StandardContext.init(StandardContext.java:5436)
	at org.apache.catalina.core.StandardContext.start(StandardContext.java:4148)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeployInternal(TomcatDeployment.java:310)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeploy(TomcatDeployment.java:142)
	at org.jboss.web.deployers.AbstractWarDeployment.start(AbstractWarDeployment.java:461)
	at org.jboss.web.deployers.WebModule.startModule(WebModule.java:118)
	at org.jboss.web.deployers.WebModule.start(WebModule.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java:157)
	at org.jboss.mx.server.Invocation.dispatch(Invocation.java:96)
	at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
	at org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:264)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:668)
	at org.jboss.system.microcontainer.ServiceProxy.invoke(ServiceProxy.java:206)
	at $Proxy38.start(Unknown Source)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:42)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:37)
	at org.jboss.dependency.plugins.action.SimpleControllerContextAction.simpleInstallAction(SimpleControllerContextAction.java:62)
	at org.jboss.dependency.plugins.action.AccessControllerContextAction.install(AccessControllerContextAction.java:71)
	at org.jboss.dependency.plugins.AbstractControllerContextActions.install(AbstractControllerContextActions.java:51)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.system.microcontainer.ServiceControllerContext.install(ServiceControllerContext.java:286)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.ServiceController.doChange(ServiceController.java:688)
	at org.jboss.system.ServiceController.start(ServiceController.java:460)
	at org.jboss.system.deployers.ServiceDeployer.start(ServiceDeployer.java:163)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:99)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:46)
	at org.jboss.deployers.spi.deployer.helpers.AbstractSimpleRealDeployer.internalDeploy(AbstractSimpleRealDeployer.java:62)
	at org.jboss.deployers.spi.deployer.helpers.AbstractRealDeployer.deploy(AbstractRealDeployer.java:50)
	at org.jboss.deployers.plugins.deployers.DeployerWrapper.deploy(DeployerWrapper.java:171)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doDeploy(DeployersImpl.java:1439)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1157)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1178)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.install(DeployersImpl.java:1098)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.process(DeployersImpl.java:781)
	at org.jboss.deployers.plugins.main.MainDeployerImpl.process(MainDeployerImpl.java:702)
	at org.jboss.system.server.profileservice.repository.MainDeployerAdapter.process(MainDeployerAdapter.java:117)
	at org.jboss.system.server.profileservice.repository.ProfileDeployAction.install(ProfileDeployAction.java:70)
	at org.jboss.system.server.profileservice.repository.AbstractProfileAction.install(AbstractProfileAction.java:53)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.install(AbstractProfileService.java:361)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.activateProfile(AbstractProfileService.java:306)
	at org.jboss.system.server.profileservice.ProfileServiceBootstrap.start(ProfileServiceBootstrap.java:271)
	at org.jboss.bootstrap.AbstractServerImpl.start(AbstractServerImpl.java:461)
	at org.jboss.Main.boot(Main.java:221)
	at org.jboss.Main$1.run(Main.java:556)
	at java.lang.Thread.run(Thread.java:619)
Caused by: org.jboss.xb.binding.JBossXBException: Failed to create a new SAX parser
	at org.jboss.xb.binding.parser.sax.SaxJBossXBParser.<init>(SaxJBossXBParser.java:97)
	at org.jboss.xb.binding.UnmarshallerImpl.<init>(UnmarshallerImpl.java:56)
	at org.jboss.xb.binding.UnmarshallerFactory$UnmarshallerFactoryImpl.newUnmarshaller(UnmarshallerFactory.java:96)
	... 73 more
Caused by: java.lang.ClassCastException: org.apache.xerces.parsers.XIncludeAwareParserConfiguration cannot be cast to org.apache.xerces.xni.parser.XMLParserConfiguration
	at org.apache.xerces.parsers.SAXParser.<init>(Unknown Source)
	at org.apache.xerces.parsers.SAXParser.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserFactoryImpl.newSAXParser(Unknown Source)
	at org.jboss.xb.binding.parser.sax.SaxJBossXBParser.<init>(SaxJBossXBParser.java:92)
	... 75 more
2009-10-02 11:49:05,645 ERROR [org.jboss.web.tomcat.service.deployers.JBossContextConfig] (main) XML error parsing: WEB-INF/context.xml
org.jboss.xb.binding.JBossXBRuntimeException: Failed to create a new SAX parser
	at org.jboss.xb.binding.UnmarshallerFactory$UnmarshallerFactoryImpl.newUnmarshaller(UnmarshallerFactory.java:100)
	at org.jboss.web.tomcat.service.deployers.JBossContextConfig.processContextConfig(JBossContextConfig.java:549)
	at org.jboss.web.tomcat.service.deployers.JBossContextConfig.init(JBossContextConfig.java:540)
	at org.apache.catalina.startup.ContextConfig.lifecycleEvent(ContextConfig.java:279)
	at org.apache.catalina.util.LifecycleSupport.fireLifecycleEvent(LifecycleSupport.java:117)
	at org.apache.catalina.core.StandardContext.init(StandardContext.java:5436)
	at org.apache.catalina.core.StandardContext.start(StandardContext.java:4148)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeployInternal(TomcatDeployment.java:310)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeploy(TomcatDeployment.java:142)
	at org.jboss.web.deployers.AbstractWarDeployment.start(AbstractWarDeployment.java:461)
	at org.jboss.web.deployers.WebModule.startModule(WebModule.java:118)
	at org.jboss.web.deployers.WebModule.start(WebModule.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java:157)
	at org.jboss.mx.server.Invocation.dispatch(Invocation.java:96)
	at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
	at org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:264)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:668)
	at org.jboss.system.microcontainer.ServiceProxy.invoke(ServiceProxy.java:206)
	at $Proxy38.start(Unknown Source)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:42)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:37)
	at org.jboss.dependency.plugins.action.SimpleControllerContextAction.simpleInstallAction(SimpleControllerContextAction.java:62)
	at org.jboss.dependency.plugins.action.AccessControllerContextAction.install(AccessControllerContextAction.java:71)
	at org.jboss.dependency.plugins.AbstractControllerContextActions.install(AbstractControllerContextActions.java:51)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.system.microcontainer.ServiceControllerContext.install(ServiceControllerContext.java:286)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.ServiceController.doChange(ServiceController.java:688)
	at org.jboss.system.ServiceController.start(ServiceController.java:460)
	at org.jboss.system.deployers.ServiceDeployer.start(ServiceDeployer.java:163)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:99)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:46)
	at org.jboss.deployers.spi.deployer.helpers.AbstractSimpleRealDeployer.internalDeploy(AbstractSimpleRealDeployer.java:62)
	at org.jboss.deployers.spi.deployer.helpers.AbstractRealDeployer.deploy(AbstractRealDeployer.java:50)
	at org.jboss.deployers.plugins.deployers.DeployerWrapper.deploy(DeployerWrapper.java:171)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doDeploy(DeployersImpl.java:1439)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1157)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1178)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.install(DeployersImpl.java:1098)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.process(DeployersImpl.java:781)
	at org.jboss.deployers.plugins.main.MainDeployerImpl.process(MainDeployerImpl.java:702)
	at org.jboss.system.server.profileservice.repository.MainDeployerAdapter.process(MainDeployerAdapter.java:117)
	at org.jboss.system.server.profileservice.repository.ProfileDeployAction.install(ProfileDeployAction.java:70)
	at org.jboss.system.server.profileservice.repository.AbstractProfileAction.install(AbstractProfileAction.java:53)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.install(AbstractProfileService.java:361)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.activateProfile(AbstractProfileService.java:306)
	at org.jboss.system.server.profileservice.ProfileServiceBootstrap.start(ProfileServiceBootstrap.java:271)
	at org.jboss.bootstrap.AbstractServerImpl.start(AbstractServerImpl.java:461)
	at org.jboss.Main.boot(Main.java:221)
	at org.jboss.Main$1.run(Main.java:556)
	at java.lang.Thread.run(Thread.java:619)
Caused by: org.jboss.xb.binding.JBossXBException: Failed to create a new SAX parser
	at org.jboss.xb.binding.parser.sax.SaxJBossXBParser.<init>(SaxJBossXBParser.java:97)
	at org.jboss.xb.binding.UnmarshallerImpl.<init>(UnmarshallerImpl.java:56)
	at org.jboss.xb.binding.UnmarshallerFactory$UnmarshallerFactoryImpl.newUnmarshaller(UnmarshallerFactory.java:96)
	... 73 more
Caused by: java.lang.ClassCastException: org.apache.xerces.parsers.XIncludeAwareParserConfiguration cannot be cast to org.apache.xerces.xni.parser.XMLParserConfiguration
	at org.apache.xerces.parsers.SAXParser.<init>(Unknown Source)
	at org.apache.xerces.parsers.SAXParser.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserImpl.<init>(Unknown Source)
	at org.apache.xerces.jaxp.SAXParserFactoryImpl.newSAXParser(Unknown Source)
	at org.jboss.xb.binding.parser.sax.SaxJBossXBParser.<init>(SaxJBossXBParser.java:92)
	... 75 more
2009-10-02 11:49:05,786 ERROR [org.apache.catalina.startup.ContextConfig] (main) Marking this application unavailable due to previous error(s)
2009-10-02 11:49:05,786 ERROR [org.apache.catalina.core.StandardContext] (main) Context [/jackrabbit-webapp-2.0-alpha9] startup failed due to previous errors
2009-10-02 11:49:06,473 ERROR [org.jboss.kernel.plugins.dependency.AbstractKernelController] (main) Error installing to Start: name=jboss.web.deployment:war=/jackrabbit-webapp-2.0-alpha9 state=Create mode=Manual requiredState=Installed
org.jboss.deployers.spi.DeploymentException: URL file:/C:/applications/jboss-5.1.0.GA/server/default/tmp/ahn1p-6tv4p6-g0bacatm-1-g0bageil-9t/jackrabbit-webapp-2.0-alpha9.war/ deployment failed
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeployInternal(TomcatDeployment.java:331)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeploy(TomcatDeployment.java:142)
	at org.jboss.web.deployers.AbstractWarDeployment.start(AbstractWarDeployment.java:461)
	at org.jboss.web.deployers.WebModule.startModule(WebModule.java:118)
	at org.jboss.web.deployers.WebModule.start(WebModule.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java:157)
	at org.jboss.mx.server.Invocation.dispatch(Invocation.java:96)
	at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
	at org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:264)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:668)
	at org.jboss.system.microcontainer.ServiceProxy.invoke(ServiceProxy.java:206)
	at $Proxy38.start(Unknown Source)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:42)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:37)
	at org.jboss.dependency.plugins.action.SimpleControllerContextAction.simpleInstallAction(SimpleControllerContextAction.java:62)
	at org.jboss.dependency.plugins.action.AccessControllerContextAction.install(AccessControllerContextAction.java:71)
	at org.jboss.dependency.plugins.AbstractControllerContextActions.install(AbstractControllerContextActions.java:51)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.system.microcontainer.ServiceControllerContext.install(ServiceControllerContext.java:286)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.ServiceController.doChange(ServiceController.java:688)
	at org.jboss.system.ServiceController.start(ServiceController.java:460)
	at org.jboss.system.deployers.ServiceDeployer.start(ServiceDeployer.java:163)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:99)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:46)
	at org.jboss.deployers.spi.deployer.helpers.AbstractSimpleRealDeployer.internalDeploy(AbstractSimpleRealDeployer.java:62)
	at org.jboss.deployers.spi.deployer.helpers.AbstractRealDeployer.deploy(AbstractRealDeployer.java:50)
	at org.jboss.deployers.plugins.deployers.DeployerWrapper.deploy(DeployerWrapper.java:171)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doDeploy(DeployersImpl.java:1439)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1157)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1178)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.install(DeployersImpl.java:1098)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.process(DeployersImpl.java:781)
	at org.jboss.deployers.plugins.main.MainDeployerImpl.process(MainDeployerImpl.java:702)
	at org.jboss.system.server.profileservice.repository.MainDeployerAdapter.process(MainDeployerAdapter.java:117)
	at org.jboss.system.server.profileservice.repository.ProfileDeployAction.install(ProfileDeployAction.java:70)
	at org.jboss.system.server.profileservice.repository.AbstractProfileAction.install(AbstractProfileAction.java:53)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.install(AbstractProfileService.java:361)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.activateProfile(AbstractProfileService.java:306)
	at org.jboss.system.server.profileservice.ProfileServiceBootstrap.start(ProfileServiceBootstrap.java:271)
	at org.jboss.bootstrap.AbstractServerImpl.start(AbstractServerImpl.java:461)
	at org.jboss.Main.boot(Main.java:221)
	at org.jboss.Main$1.run(Main.java:556)
	at java.lang.Thread.run(Thread.java:619)
2009-10-02 11:49:06,598 ERROR [org.jboss.kernel.plugins.dependency.AbstractKernelController] (main) Error installing to Real: name=vfszip:/C:/applications/jboss-5.1.0.GA/server/default/deploy/jackrabbit-webapp-2.0-alpha9.war/ state=PreReal mode=Manual requiredState=Real
org.jboss.deployers.spi.DeploymentException: URL file:/C:/applications/jboss-5.1.0.GA/server/default/tmp/ahn1p-6tv4p6-g0bacatm-1-g0bageil-9t/jackrabbit-webapp-2.0-alpha9.war/ deployment failed
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeployInternal(TomcatDeployment.java:331)
	at org.jboss.web.tomcat.service.deployers.TomcatDeployment.performDeploy(TomcatDeployment.java:142)
	at org.jboss.web.deployers.AbstractWarDeployment.start(AbstractWarDeployment.java:461)
	at org.jboss.web.deployers.WebModule.startModule(WebModule.java:118)
	at org.jboss.web.deployers.WebModule.start(WebModule.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java:157)
	at org.jboss.mx.server.Invocation.dispatch(Invocation.java:96)
	at org.jboss.mx.server.Invocation.invoke(Invocation.java:88)
	at org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:264)
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:668)
	at org.jboss.system.microcontainer.ServiceProxy.invoke(ServiceProxy.java:206)
	at $Proxy38.start(Unknown Source)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:42)
	at org.jboss.system.microcontainer.StartStopLifecycleAction.installAction(StartStopLifecycleAction.java:37)
	at org.jboss.dependency.plugins.action.SimpleControllerContextAction.simpleInstallAction(SimpleControllerContextAction.java:62)
	at org.jboss.dependency.plugins.action.AccessControllerContextAction.install(AccessControllerContextAction.java:71)
	at org.jboss.dependency.plugins.AbstractControllerContextActions.install(AbstractControllerContextActions.java:51)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.system.microcontainer.ServiceControllerContext.install(ServiceControllerContext.java:286)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.ServiceController.doChange(ServiceController.java:688)
	at org.jboss.system.ServiceController.start(ServiceController.java:460)
	at org.jboss.system.deployers.ServiceDeployer.start(ServiceDeployer.java:163)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:99)
	at org.jboss.system.deployers.ServiceDeployer.deploy(ServiceDeployer.java:46)
	at org.jboss.deployers.spi.deployer.helpers.AbstractSimpleRealDeployer.internalDeploy(AbstractSimpleRealDeployer.java:62)
	at org.jboss.deployers.spi.deployer.helpers.AbstractRealDeployer.deploy(AbstractRealDeployer.java:50)
	at org.jboss.deployers.plugins.deployers.DeployerWrapper.deploy(DeployerWrapper.java:171)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doDeploy(DeployersImpl.java:1439)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1157)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1178)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.install(DeployersImpl.java:1098)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.deployers.plugins.deployers.DeployersImpl.process(DeployersImpl.java:781)
	at org.jboss.deployers.plugins.main.MainDeployerImpl.process(MainDeployerImpl.java:702)
	at org.jboss.system.server.profileservice.repository.MainDeployerAdapter.process(MainDeployerAdapter.java:117)
	at org.jboss.system.server.profileservice.repository.ProfileDeployAction.install(ProfileDeployAction.java:70)
	at org.jboss.system.server.profileservice.repository.AbstractProfileAction.install(AbstractProfileAction.java:53)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.install(AbstractProfileService.java:361)
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:348)
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:1631)
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:934)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1082)
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:984)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:822)
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:553)
	at org.jboss.system.server.profileservice.repository.AbstractProfileService.activateProfile(AbstractProfileService.java:306)
	at org.jboss.system.server.profileservice.ProfileServiceBootstrap.start(ProfileServiceBootstrap.java:271)
	at org.jboss.bootstrap.AbstractServerImpl.start(AbstractServerImpl.java:461)
	at org.jboss.Main.boot(Main.java:221)
	at org.jboss.Main$1.run(Main.java:556)
	at java.lang.Thread.run(Thread.java:619)
"
0,"Prepare CharArraySet for Unicode 4.0CharArraySet does lowercaseing if created with the correspondent flag. This causes that  String / char[] with uncode 4 chars which are in the set can not be retrieved in ""ignorecase"" mode.
"
0,"Use Java 5 enumsReplace the use of o.a.l.util.Parameter with Java 5 enums, deprecating Parameter.

Replace other custom enum patterns with Java 5 enums."
1,"When using QueryImpl.setLimit() and QueryImpl.setOffset(), then NodeIterator.getSize() reports wrong sizeWhen using QueryImpl.setLimit() and QueryImpl.setOffset(), then NodeIterator.getSize() reports wrong size. Returned size seems to be allways the same as the limit."
0,"Use common base classes in jackrabbit-core and jcr2spiAs part of JCR-742 I've implemented a number of generic JCR base classes and adapters in org.apache.jackrabbit.commons. These classes are based on existing code in jackrabbit-core.

To encourage code reuse across jackrabbit-core and jcr2spi, I'd like to make both components use these generic base classes.

"
1,"No equals operation for Credentials implementationsI tripped across a scenario where I wanted to compare credentials, so I could
know to discard connection state (and thus any associated cookies).

Patch to follow shortly."
0,"David Spencer Spell Checker improvedhy,
i developed a SpellChecker based on the David Spencer code (DSc) but more flexible.
the structure of the index is inspired of the DSc (for a 3-4 gram):
word:
gram3:
gram4:
 
3start:
4start:
..
3end:
4end:
..
transposition:
 
This index is a dictonary so there isn't the ""freq"" field like with DSc version.
it's independant of the user index. So we can add words becoming to several
fields of several index for example or, why not, to a file with a list of words.
The suggestSimilar method return a list of suggests word sorted by the
Levenshtein distance and optionaly to the popularity of the word for a specific
field in a user index. More of that, this list can be restricted only to words
present in a specific field of a user index.
 
See the test case.
 
i hope this code will be put in the lucene sandbox. 
 
Nicolas Maisonneuve"
1,"TopFieldCollector throws AIOOBE if numHits is 0See solr-user thread ""ArrayIndexOutOfBoundsException for query with rows=0 and sort param"".

I think we should just create a null collector (only tallies up totalHits) if numHits is 0?"
0,"ServerQuery does not use RemoteAdapterFactory for creating ServerQueryResultThe ServerQuery sould use the Factory for creating ServerQueryResult.

Siehe the method ServerQuery.execute():

{code}
public RemoteQueryResult execute() throws RepositoryException, RemoteException {
        return new ServerQueryResult(query.execute(), getFactory());
    }
{code}

it should be:
{code}
    public RemoteQueryResult execute() throws RepositoryException, RemoteException {
        return getFactory().getRemoteQueryResult(this.query.execute());
    }
{code}"
0,"Change SortField types to an EnumWhen updating my SOLR-2533 patch, one issue was that the int value I had given my new type had been used by another change in the mean time.  Since we don't use these fields in a bitset kind of way, we can convert them to an enum."
0,"RFC4918 feature: absolute paths in ""Destination"" and ""If"" headersRFC4918 allows absolute paths (instead of absolute URIs) in the ""Destination"" and ""If"" headers (<http://greenbytes.de/tech/webdav/rfc4918.html#rfc.section.14.8>). This makes it simpler to deal with situations where reverse proxies are involved (because those usually are not aware of WebDAV request headers and do not rewrite them)."
0,Text.escapeIllegalJCRChars should be adjusted to match the 2.0 set of illegal charsText.escapeIllegalJCRChars still contains chars that were illegal in JCR 1.0 but were removed from the set for JCR 2.0
0,"Document order of result nodes should be configurableQueries without an order by clause are performed with document order for the result nodes. This is a quite expensive operation, because the document order is available in the search index itself. The document order is calculated with the help of the ItemStateManager and requires loading of all result node states including their ancestors.

Queries with a lot of result nodes become quite expensive, even though the actual query execution is fast. Because most use cases will not care for the document order, this feature should be made configurable. Some parameter for the QueryHandler that disables the document order on result nodes."
0,"TCK: NodeReadMethodsTest.testGetName fails with NPE if  'testroot' has no child nodeThe 'testGetName' does not assert, that  the childnode field has been populated during setup.
if  for whatever reason  the test data don't provide a single childnode below the test root, this test will fail with nullpointer
exception.

i would like to suggest to use the same assertion as in testGetPath and throw a NotExecutableException in case of
missing child node.

patch attached.

"
0,"IndexWriter should prune 100% deleted segs even in the NRT caseWe now prune 100% deleted segs on commit from IW or IR (LUCENE-2010),
but this isn't quite aggressive enough, because in the NRT case you
rarely call commit.

Instead, the moment we delete the last doc of a segment, it should be
pruned from the in-memory segmentInfos.  This way, if you open an NRT
reader, or a merge kicks off, or commit is called, the 100% deleted
segment is already gone.
"
0,"IndexWriter commits unnecessarily on fresh DirectoryI've noticed IndexWriter's ctor commits a first commit (empty one) if a fresh Directory is passed, w/ OpenMode.CREATE or CREATE_OR_APPEND. This seems unnecessarily, and kind of brings back an autoCommit mode, in a strange way ... why do we need that commit? Do we really expect people to open an IndexReader on an empty Directory which they just passed to an IW w/ create=true? If they want, they can simply call commit() right away on the IW they created.

I ran into this when writing a test which committed N times, then compared the number of commits (via IndexReader.listCommits) and was surprised to see N+1 commits.

Tried to change doCommit to false in IW ctor, but it got IndexFileDeleter jumping on me .. so the change might not be that simple. But I think it's manageable, so I'll try to attack it (and IFD specifically !) back :)."
0,"Extract a generic framework for running randomized tests.{color:red}The work on this issue is temporarily at github{color} (lots of experiments and tweaking):
https://github.com/carrotsearch/randomizedtesting
Or directly: git clone git://github.com/carrotsearch/randomizedtesting.git
{color}
----

RandomizedRunner is a JUnit runner, so it is capable of running @Test-annotated test cases. It
respects regular lifecycle hooks such as @Before, @After, @BeforeClass or @AfterClass, but it
also adds the following:

Randomized, but repeatable execution and infrastructure for dealing with randomness:

- uses pseudo-randomness (so that a given run can be repeated if given the same starting seed)
  for many things called ""random"" below,
- randomly shuffles test methods to ensure they don't depend on each other,
- randomly shuffles hooks (within a given class) to ensure they don't depend on each other,
- base class RandomizedTest provides a number of methods for generating random numbers, strings
  and picking random objects from collections (again, this is fully repeatable given the
  initial seed if there are no race conditions),
- the runner provides infrastructure to augment stack traces with information about the initial
  seeds used for running the test, so that it can be repeated (or it can be determined that
  the test is not repeatable -- this indicates a problem with the test case itself).

Thread control:

- any threads created as part of a test case are assigned the same initial random seed 
  (repeatability),
- tracks and attempts to terminate any Threads that are created and not terminated inside 
  a test case (not cleaning up causes a test failure),
- tracks and attempts to terminate test cases that run for too long (default timeout: 60 seconds,
  adjustable using global property or annotations),

Improved validation and lifecycle support:

- RandomizedRunner uses relaxed contracts of hook methods' accessibility (hook methods _can_ be
  private). This helps in avoiding problems with method shadowing (static hooks) or overrides
  that require tedious super.() chaining). Private hooks are always executed and don't affect
  subclasses in any way, period.
- @Listeners annotation on a test class allows it to hook into the execution progress and listen
  to events.
- @Validators annotation allows a test class to provide custom validation strategies 
  (project-specific). For example a base class can request specific test case naming strategy
  (or reject JUnit3-like methods, for instance).
- RandomizedRunner does not ""chain"" or ""suppress"" exceptions happening during execution of 
  a test case (including hooks). All exceptions are reported as soon as they happened and multiple
  failure reports can occur. Most environments we know of then display these failures sequentially
  allowing a clearer understanding of what actually happened first.
"
0,"SessionImpl#isSupportedOption: Skip descriptor evaluation if descriptor has not been loadedfollowup issue for JCR-3076.

as jukka stated changing the jcr-server to serve the repository-descriptor without mandating a successful login would
require quite some changes on the server side as the current flow demands a successful repository login in order
to be access any resource including the root resource that acts as parent for all (available) workspaces. since the
repository-descriptor report has be requested one of the resources it also mandates a successful login although
retrieving descriptors on the jcr-level is possible when just having the repository at hand.

on the other hand i would assume that the descriptor functionality present on the Repository is rarely used.
therefore i would suggest to just relax the check for supported options in the jcr2spi session implementation
and skip the evaluation if the descriptor isn't available at all. consequently the failure of a non-supported
feature would be postponed to the point it reaches the SPI (instead of informing the API consumer upfront). 
on the other hand supported operations would not fail just because the descriptors have not been loaded."
0,"Query path constraints like foo//*/bar do not scaleTo resolve the * step the LuceneQueryBuilder currently creates a MatchAllQuery and checks every node for a foo ancestor. Instead, it should search for bar nodes and check for foo ancestors with at least one arbitrary hierarchy level in between."
0,"Allow whitespaces in base64 encoded binary fields of XML import filesWhen importing files using Session.importXML(), the Binary property values are Base64 encoded.  However you cannot put whitespaces in them, and XML files with binaries in them become very long lines.  The files are more manageable if whilespaces could be put in them, as is common to do in base base64 encoded files."
0,"Improve architecture of FieldSortedHitQueuePer the discussion (quite some time ago) on issue LUCENE-806, I'd like to propose an architecture change to the way FieldSortedHitQueue works, and in particular the way it creates SortComparatorSources. I think (I hope) that anyone who looks at the FSHQ code will agree that the class does a lot and much of it's repetitive stuff that really has no business being in that class.

I am about to attach a patch which, in and of itself, doesn't really achieve much that's concrete but does tidy things up a great deal and makes it easier to plug in different behaviours. I then have a subsequent patch which provides a fairly simple and flexible example of how you might replace an implementation, in this case the field-local-String-comparator version from LUCENE-806.

The downside to this patch is that it involved changing the signature of SortComparatorSource.newComparator to take a Locale. There would be ways around this (letting FieldSortedHitQueue take in either a SortComparatorSource or some new, improved interface which takes a Locale (and possibly extends SortComparatorSource). I'm open to this but personally I think that the Locale version makes sense and would suggest that the code would be nicer by breaking the API (and hence targeting this to, presumably, 3.0 at a minimum).

This code does not include specific tests (I will add these, if people like the general idea I'm proposing here) but all current tests pass with this change.

Patch to follow."
1,"ExportSysViewTest fails with: System property org.xml.sax.driver not specifiedThe ExportSysViewTest class uses the XMLReaderFactory.createXMLReader() method that depends on the system property ""org.xml.sax.driver"" being specified. Apparently using a TransformerFactory works around this issue in some way, as the problem only appeared once we changed the XML export feature to use a custom serializer class instead of a JAXP Transformer for serialization (see JCR-1952).

The current workaround is to explicitly force a Transformer to be loaded, but we really should fix the cause of this issue for example by replacing the XMLReader instance with a SAXParser."
1,"when opening the merged SegmentReader, IW attempts to open store files that were deletedThe issue happens when a merge runs that does not merge the doc stores, those doc stores are still being written to, IW is using CFS, and while the merge is running the doc stores get closed and turned into a cfx file.

When we then try to open the reader (for warming), which as of LUCENE-2311 will now [correctly] open the doc stores, we hit FNFE because the SegmentInfo for the merge does not realize that the doc stores were turned into  a cfx.

This issue does affect trunk; if you crank up the #docs in the test, it happens consistently (I will tie this to _TestUtil.getRandomMultiplier!)."
0,"DirectoryTaxonomyWriter should throw a proper exception if it was closedDirTaxoWriter may throw random exceptions (NPE, Already Closed - depend on what API you call) after it has been closed/rollback. We should detect up front that it is already closed, and throw AlreadyClosedException.

Also, on LUCENE-3573 Doron pointed out a problem with DTW.rollback -- it should call close() rather than refreshReader. I will fix that as well in this issue."
0,Using explain may double ram reqs for fieldcaches when using ValueSourceQuery/CustomScoreQuery or for ConstantScoreQuerys that use a caching Filter.
1,"Tika regressions in 0.8There are a few notable problems in Tika 0.8, namely TIKA-548 and TIKA-556, that may adversely affect users of Jackrabbit 2.2.

Since we don't have a Tika 0.9 release available yet, I'll add workarounds for these issues in Jackrabbit."
1,"sysview import does not resolve referenceswhen importing a sysview with references, those are not resolved against the new uuids of the mix:referenceable nodes"
0,"Journal: Use buffered input / output streamsThe journal should use buffered input / output streams wherever possible. Currently there are some places where bytes are directly written to the journal file, which degrades performance."
1,"SpellChecker.clearIndex calls unlock inappropriatelyAs noted in LUCENE-1050, fixing a bug in SimpleLockFactory related to not reporting success/filure of lock file deletion has surfaced bad behavior in SpellChecker.clearIndex...

Grant...
{quote}
It seems the SpellChecker is telling the IndexReader to delete the lockFile, but the lockFile doesn't exist.
  ...
I don't know much about the locking mechanism, but it seems like this should check to see if the lockFile exists before trying to delete it.
{quote}

Hoss...
{quote}
Grant: my take on this is that SpellChecker.clearIndex is in the wrong. it shouldn't be calling unlock unless it has reason to think there is a ""stale lock"" that needs to be closed - ie: this is a bug in SpellChecker that you have only discovered because this bug LUCENE-1050 was fixed.

I would suggest a new issue for tracking, and a patch in which SpellChecker.clearIndex doesn't call unlock unless isLocked returns true. Even then, it might make sense to catch and ignore LockReleaseFailedException and let whatever resulting exception may originate from ""new IndexWriter"" be returned.
{quote}

marking for 2.3 since it seems like a fairly trivial fix, and if we don't deal with it now it will be a bug introduced in 2.3.

"
1,"Problems with custom nodes in journalI have an application that uses custom node types and I am having problems in a clustered configuration.

Issue 1: the following definition in a nodetype is incorrectly read from the journal:
  + * (nt:hierarchyNode) version

The * is stored in the journal as _x002a_ since it should be a QName and it gets escaped.
When read, the code ...core.nodetype.compact.CompactNodeTypeDefReader.doChildNodeDefinition does the following test:

        if (currentTokenEquals('*')) {
            ndi.setName(ItemDef.ANY_NAME); 
        } else {
            ndi.setName(toQName(currentToken));
        }

Since currentToken is _x002a_ and not * toQName(currentToken) is called but it fails.
I changed the test to:
        if (currentTokenEquals('*') || currentTokenEquals(""_x002a_""))
            ....
and that fixes the problem.

Issue 2: when storing a nodeType in the journal the superclass nt:base is not store, but when reading I get an error saying the node should be a subclass of nt:base.

The code in...core.nodetype.compact.CompactNodeTypeDefWriter.writeSupertypes skips nt:base when writing the node.

When reading the nodetype definition from the journal the following exception is thrown:

Unable to deliver node type operation: [{http://namespace/app/repository/1.0}resource] all primary node types except nt:base itself must be (directly or indirectly) derived from nt:base

probably because nt:base is not re-added to the nodetype definition

 "
0,"Includes new (old) mimetypes that OpenOfficeTextExtractor can handleThe following patch adds the old openoffice (1.0 version) mimetypes to have their contents extracted. 
I've tested with simple files and it worked here. 


$ cat OpenOfficeTextExtractor-mimetype.patch
--- jackrabbit-1.4/jackrabbit-text-extractors/src/main/java/org/apache/jackrabbit/extractor/OpenOfficeTextExtractor.java    2007-12-19 12:57:58.000000000 -0200
+++ jackrabbit-1.4-modified/jackrabbit-text-extractors/src/main/java/org/apache/jackrabbit/extractor/OpenOfficeTextExtractor.java  2008-07-24 15:01:08.000000000 -0300
@@ -54,7 +54,11 @@
                            ""application/vnd.oasis.opendocument.graphics"",
                            ""application/vnd.oasis.opendocument.presentation"",
                            ""application/vnd.oasis.opendocument.spreadsheet"",
-                           ""application/vnd.oasis.opendocument.text""});
+                           ""application/vnd.oasis.opendocument.text"",
+                           ""application/vnd.sun.xml.calc"",
+                           ""application/vnd.sun.xml.draw"",
+                           ""application/vnd.sun.xml.impress"",
+                           ""application/vnd.sun.xml.writer""});
     }

     //-------------------------------------------------------< TextExtractor >
"
0,"Add Searcher.search(Query, int)Now that we've deprecated Hits (LUCENE-1290), I think we should add this trivial convenience method to Searcher, which is just sugar for Searcher.search(Query, null, int) ie null filter, returning a TopDocs.

This way there is a simple API for users to retrieve the top N results for a Query.
"
0,"Remove SortField.AUTOI'd like to remove SortField.AUTO... it's dangerous for Lucene to
guess the type of your field, based on the first term it encounters.
It can easily be wrong, and, whether it's wrong or right could
suddenly change as you index different documents.

It unexepctedly binds SortField to needing an IndexReader to do the
guessing.

It's caused various problems in the past (most recently, for me on
LUCENE-1656) as we fix other issues/make improvements.

I'd prefer that users of Lucene's field sort be explicit about the
type that Lucene should cast the field to.  Someday, if we have
optional strong[er] typing of Lucene's fields, such type information
would already be known.  But in the meantime, I think users should be
explicit.
"
0,"[PATCH] MultiFieldQueryParser and BooleanQuery do not provide adequate support for queries across multiple fieldsThe attached test case demonstrates this problem and provides a fix:
  1.  Use a custom similarity to eliminate all tf and idf effects, just to 
isolate what is being tested.
  2.  Create two documents doc1 and doc2, each with two fields title and 
description.  doc1 has ""elephant"" in title and ""elephant"" in description.  
doc2 has ""elephant"" in title and ""albino"" in description.
  3.  Express query for ""albino elephant"" against both fields.
Problems:
      a.  MultiFieldQueryParser won't recognize either document as containing 
both terms, due to the way it expands the query across fields.
      b.  Expressing query as ""title:albino description:albino title:elephant 
description:elephant"" will score both documents equivalently, since each 
matches two query terms.
  4.  Comparison to MaxDisjunctionQuery and my method for expanding queries 
across fields.  Using notation that () represents a BooleanQuery and ( | ) 
represents a MaxDisjunctionQuery, ""albino elephant"" expands to:
        ( (title:albino | description:albino)
          (title:elephant | description:elephant) )
This will recognize that doc2 has both terms matched while doc1 only has 1 
term matched, score doc2 over doc1.

Refinement note:  the actual expansion for ""albino query"" that I use is:
        ( (title:albino | description:albino)~0.1
          (title:elephant | description:elephant)~0.1 )
This causes the score of each MaxDisjunctionQuery to be the score of highest 
scoring MDQ subclause plus 0.1 times the sum of the scores of the other MDQ 
subclauses.  Thus, doc1 gets some credit for also having ""elephant"" in the 
description but only 1/10 as much as doc2 gets for covering another query term 
in its description.  If doc3 has ""elephant"" in title and both ""albino"" 
and ""elephant"" in the description, then with the actual refined expansion, it 
gets the highest score of all (whereas with pure max, without the 0.1, it 
would get the same score as doc2).

In real apps, tf's and idf's also come into play of course, but can affect 
these either way (i.e., mitigate this fundamental problem or exacerbate it)."
0,"Remove SVN.exe and revision numbers from build.xml by svn-copy the backwards branch and linking snowball tests by svn:externalsAs we often need to update backwards tests together with trunk and always have to update the branch first, record rev no, and update build xml, I would simply like to do a svn copy/move of the backwards branch.

After a release, this is simply also done:
{code}
svn rm backwards
svn cp releasebranch backwards
{code}

By this we can simply commit in one pass, create patches in one pass.

The snowball tests are currently downloaded by svn.exe, too. These need a fixed version for checkout. I would like to change this to use svn:externals. Will provide patch, soon."
0,Use only one scheduler for repository tasksThere are still a few Timer instances being used by Jackrabbit. It would be better if all tasks were scheduled by the central ScheduledExecutorService thread pool of the repository.
0,"Make Jackrabbit compile on Java 7Compiling on Java 7 fails with the following error:

    jackrabbit-core/src/main/java/org/apache/jackrabbit/core/util/db/DataSourceWrapper.java:[30,7] error:
    DataSourceWrapper is not abstract and does not override abstract method getParentLogger() in CommonDataSource

We should fix that."
1,"HttpClient does not compile 'out of the box' in IBM's VisualAge IDEThis was observed with IBM VisualAge 3.5, which runs JDK1.2.2:

Importing the HTTPClient source code into the IDE brings up a
compilation error in 

org.apache.commons.httpclient.HttpMethodBase.

The initialization of ""private ResponseConsumedWatcher m_responseWatcher""
using an anyonymous inner class seems to cause some trouble. Implicated code:

private ResponseConsumedWatcher m_responseWatcher = new ResponseConsumedWatcher
() {
	public void responseConsumed() {
		responseBodyConsumed();
	}
};

The error message is: ""Field initialization: The constructor invoked to create
org.apache.commons.httpclient.HttpMethodBase$1 with arguments () is not defined""

...but only in the context of HttpMethodBase(String uri) constructor, i.e.
the HttpMethodBase() constructor *can* be compiled, HttpMethodBase(String uri)
*cannot* be compiled with error ""Cannot create constructor due to incorrect
field initialization"".

I interpret this to mean that the compiler is looking for a parameterless
constructor for the anonymous class in the context of 
HttpMethodBase(String uri). The message did not really make sense to me. 
Checked the syntax, checked in the Language Definition whether setting up an
anonymous class like that is permitted; found nothing obviously wrong.

Fix:

The code above is equivalent to constructing the instance at the beginning
of each constructor of the enclosing class. A copy and paste of the
construction code into each of the two constructors fixes things...until
the next update."
0,"Method to create default RepositoryConfig from just the repository directoryIt would be useful to have a static method like RepositoryConfig.create(File) that would take the repository directory and expect to find the repository configuration in a ""repository.xml"" file inside that directory.

If the directory does not exist, it would be created. And if the repository configuration file does not exist, then it would be created from the default configuration included in Jackrabbit."
1,"Text.unescape(""%"") throws a StringIndexOutOfBoundsExceptionYou get the following exception:

java.lang.StringIndexOutOfBoundsException: String index out of range: 3
	at java.lang.String.substring(String.java:1935)
	at org.apache.jackrabbit.util.Text.unescape(Text.java:407)
	at org.apache.jackrabbit.util.Text.unescape(Text.java:438)

It would be better if it failed with IllegalArgumentException."
1,"it is not possible to register an event listener which listens to mixin nodetypesit would be a nice enhancement if one could as well define mixin nodetypes to be listened:
...
om.addEventListener(this,
                        Event.PROPERTY_ADDED | Event.PROPERTY_CHANGED | Event.PROPERTY_REMOVED,
                        ""/"",
                        true,
                        null,
                        new String[]{""mix:Custom""},
                        false);
..."
0,bug form doesn't list latest version 
0,"Rate-limit IO used by mergingLarge merges can mess up searches and increase NRT reopen time (see
http://blog.mikemccandless.com/2011/06/lucenes-near-real-time-search-is-fast.html).

A simple rate limiter improves the spikey NRT reopen times during big
merges, so I think we should somehow make this possible.  Likely this
would reduce impact on searches as well.

Typically apps that do indexing and searching on same box are in no
rush to see the merges complete so this is a good tradeoff.
"
0,"+ - operators allow any amount of whitespaceAs an example, (foo - bar) is treated like (foo -bar).
It seems like for +- to be treated as unary operators, they should be immediately followed by the operand."
0,"[RFE] Allow streaming of POST methods via chunked transfer encoding.This is an RFE with a possible implementation attached. The implementation does
not modify any existing code.

We're using HTTP POST to send a large amount of data with an unknown size. We
don't want to buffer the entire request, so we implemented a streaming POST
method. The implementation has 3 classes: StreamedPostMethod,
BufferedChunkedOutputStream and OutputStreamWriter. The bulk of the code is in
the BufferedChunkedOutputStream, which may be a good target for replacing
ChunkedOutputStream from the main distribution.

BufferedChunkedOutputStream has the following charactersitics:
1) It has an internal 2K buffer. Without the buffer, chunk sizes would be too
small in many cases (e.g. ObjectOutputStream likes to call write(byte[]) with 4
byte long arguments). 2K was chosen to minimize the chunk overhead to less than 1%.
2) If the entire entity body fits within the 2K buffer, it does not use
chunking. This implies that the headers are only sent out when the first chunk
(or the entire body) has to be written, but no sooner.
3) The chunk size is not limited to 2K: if write(byte[]) is called with a large
argument, the internal buffer and the new request are sent out as a single chunk.
4) Because of (2) it's tightly coupled to StreamedPostMethod.reallyWriteHeaders.
5) StreamedPostMethod calls BufferedChunkedOutputStream.finish() to write the
last buffer and ending chunk.

Because of 4 and 5, we didn't want to touch ChunkedOutputStream. Interestingly,
EntityEnclosingMethod is already tightly coupled to ChunkedOutputStream because
it has to call writeClosingChunk. There is probably some room for refactoring here.

The package is just a suggestion; feel free to move the files as appropirate.
This code was written against 2.0rc2. We're hoping it will get included in time
for the 2.1 release.

To use the code, you must implement OutputStreamWriter and pass it to
StreamedPostMethod's constructor. Execute the method as usual.

Caveats: StreamedPostMethod does not implement Expect/continue logic. We had no
way to test this. It is also strictly for POST. In general, the same methodology
is applicable to PUT, etc. It should be fairly simple to generalize.

Legal: Goldman, Sachs & Co. is making this code available under the Apache License."
1,CompactNodeTypeDefReader does not recognise MIXIN ORDERABLE sequencethe code in 'doOptions' misses to set the setOrderableChildNodes flag if the order of the tokens is MIXIN ORDERABLE.
0,"Patch for ShingleFilter.enablePositions (or PositionFilter)Make it possible for *all* words and shingles to be placed at the same position, that is for _all_ shingles (and unigrams if included) to be treated as synonyms of each other.

Today the shingles generated are synonyms only to the first term in the shingle.
For example the query ""abcd efgh ijkl"" results in:
   (""abcd"" ""abcd efgh"" ""abcd efgh ijkl"") (""efgh"" efgh ijkl"") (""ijkl"")

where ""abcd efgh"" and ""abcd efgh ijkl"" are synonyms of ""abcd"", and ""efgh ijkl"" is a synonym of ""efgh"".

There exists no way today to alter which token a particular shingle is a synonym for.
This patch takes the first step in making it possible to make all shingles (and unigrams if included) synonyms of each other.

See http://comments.gmane.org/gmane.comp.jakarta.lucene.user/34746 for mailing list thread."
0,"Optimization for IndexWriter.addIndexes()One big performance problem with IndexWriter.addIndexes() is that it has to optimize the index both before and after adding the segments.  When you have a very large index, to which you are adding batches of small updates, these calls to optimize make using addIndexes() impossible.  It makes parallel updates very frustrating.

Here is an optimized function that helps out by calling mergeSegments only on the newly added documents.  It will try to avoid calling mergeSegments until the end, unless you're adding a lot of documents at once.

I also have an extensive unit test that verifies that this function works correctly if people are interested.  I gave it a different name because it has very different performance characteristics which can make querying take longer."
1,"SpellChecker does not work properly on calling indexDictionary after clearIndexWe have to call clearIndex and indexDictionary to rebuild dictionary from fresh. The call to SpellChecker clearIndex() function does not reset the searcher. Hence, when we call indexDictionary after that, many entries that are already in the stale searcher will not be indexed.

Also, I see that IndexReader reader is used for the sole purpose of obtaining the docFreq of a given term in exist() function. This functionality can also be obtained by using just the searcher by calling searcher.docFreq. Thus, can we get away completely with reader and code associated with it like
      if (IndexReader.isLocked(spellIndex)){
	IndexReader.unlock(spellIndex);
      }
and the reader related code in finalize?

"
0,"Use AtomicReaderContext also for CustomScoreProviderWhen moving to AtomicReaderContext, one place was not changed to use it: CustomScoreQuery's CustomScoreProvider. It should also take AtomicReaderContext instead of IndexReader, as this may help users to effectively implement custom scoring there absolute DocIds are needed."
1,"intermittent failure in TestIndexWriter. testRandomIWReaderRarely, this test (which was added with LUCENE-1516) fails in MockRAMDirectory.close because some files were not closed, eg:
{code}
   [junit] NOTE: random seed of testcase 'testRandomIWReader' was: -5001333286299627079
   [junit] ------------- ---------------- ---------------
   [junit] Testcase: testRandomIWReader(org.apache.lucene.index.TestStressIndexing2):        Caused an ERROR
   [junit] MockRAMDirectory: cannot close: there are still open files: {_cq.tvx=3, _cq.fdx=3, _cq.tvf=3, _cq.tvd=3, _cq.fdt=3}
   [junit] java.lang.RuntimeException: MockRAMDirectory: cannot close: there are still open files: {_cq.tvx=3, _cq.fdx=3, _cq.tvf=3, _cq.tvd=3, _cq.fdt=3}
   [junit]     at org.apache.lucene.store.MockRAMDirectory.close(MockRAMDirectory.java:292)
   [junit]     at org.apache.lucene.index.TestStressIndexing2.testRandomIWReader(TestStressIndexing2.java:66)
   [junit]     at org.apache.lucene.util.LuceneTestCase.runTest(LuceneTestCase.java:88)
{code}"
0,"Misleading method names in SetValueBinaryTestSome of the method names in SetValueBinaryTest say ""Boolean"" when they should say ""Binary"" (copy&paste error from SetValueBooleanTest?).

"
0,"nightly builds depend on cloveras reported by Michael Pelz Sherman on java-dev@lucene and solr-user@lucene the nightly builds coming out of hudson current depend on clover...

  [root@crm.test.bbhmedia.net tmp]# strings lucene-core-nightly.jar | grep -i clover|more
org/apache/lucene/LucenePackage$__CLOVER_0_0.class
org/apache/lucene/analysis/Analyzer$__CLOVER_1_0.class
...

the old nightly.sh dealt with this by running ant nightly twice, first without clover to get the jars and then with clover to get the report.  it loks like maybe this logic never made it into the hudson setup.

someone with hudson admin access/knowledge will need to look into this."
0,"Add an explicit method to invoke IndexDeletionPolicyToday, if one uses an IDP which holds onto segments, such as SnapshotDeletionPolicy, or any other IDP in the tests, those segments are left in the index even if the IDP no longer references them, until IW.commit() is called (and actually does something). I'd like to add a specific method to IW which will invoke the IDP's logic and get rid of the unused segments w/o forcing the user to call IW.commit(). There are a couple of reasons for that:

* Segments take up sometimes valuable HD space, and the application may wish to reclaim that space immediately. In some scenarios, the index is updated once in several hours (or even days), and waiting until then may not be acceptable.
* I think it's a cleaner solution than waiting for the next commit() to happen. One can still wait for it if one wants, but otherwise it will give you the ability to immediately get rid of those segments.
* TestSnapshotDeletionPolicy includes this code, which only strengthens (IMO) the need for such method:
{code}
// Add one more document to force writer to commit a
// final segment, so deletion policy has a chance to
// delete again:
Document doc = new Document();
doc.add(new Field(""content"", ""aaa"", Field.Store.YES, Field.Index.ANALYZED, Field.TermVector.WITH_POSITIONS_OFFSETS));
writer.addDocument(doc);
{code}

If IW had an explicit method, that code would not need to exist there at all ...

Here comes the fun part - naming the baby:
* invokeDeletionPolicy -- describes exactly what is going to happen. However, if the user did not set IDP at all (relying on default, which I think many do), users won't understand what is it.
* deleteUnusedSegments - more user-friendly, assuming users understand what 'segments' are.

BTW, IW already has deleteUnusedFiles() which only tries to delete unreferenced files that failed to delete before (such as on Windows, due to e.g. open readers). Perhaps instead of inventing a new name, we can change IW.deleteUnusedFiles to call IndexFileDeleter.checkpoint (instead of deletePendingFiles) which deletes those files + calls IDP.onCommit()."
0,Hide ugly repository init code for OCMHide repository namespace registration and ocm:discriminator node type registration in implementation of OCM
1,"NullPointerException when iterating over propertiesRunning ConcurrentReadWriteTest (NUM_NODES=5, NUM_THREADS=3, RUN_NUM_SECONDS=120) resulted in a NullPointerException:

Exception in thread ""Thread-11"" java.lang.NullPointerException
	at org.apache.commons.collections.map.AbstractReferenceMap$ReferenceEntry.getValue(AbstractReferenceMap.java:596)
	at org.apache.commons.collections.map.AbstractReferenceMap.containsKey(AbstractReferenceMap.java:204)
	at org.apache.jackrabbit.core.state.ItemStateMap.contains(ItemStateMap.java:66)
	at org.apache.jackrabbit.core.state.ItemStateReferenceCache.isCached(ItemStateReferenceCache.java:91)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.hasItemState(LocalItemStateManager.java:173)
	at org.apache.jackrabbit.core.state.XAItemStateManager.hasItemState(XAItemStateManager.java:252)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:174)
	at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:495)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:326)
	at org.apache.jackrabbit.core.LazyItemIterator.prefetchNext(LazyItemIterator.java:90)
	at org.apache.jackrabbit.core.LazyItemIterator.next(LazyItemIterator.java:203)
	at org.apache.jackrabbit.core.LazyItemIterator.nextProperty(LazyItemIterator.java:118)
	at org.apache.jackrabbit.core.ConcurrentReadWriteTest$1$1.execute(ConcurrentReadWriteTest.java:64)
	at org.apache.jackrabbit.core.AbstractConcurrencyTest$Executor.run(AbstractConcurrencyTest.java:110)
	at java.lang.Thread.run(Thread.java:619)

The cache is not synchronized and is accessed at the same time by the current thread and another thread that notified ItemStates about changes."
0,"HttpClient throws java.net.SocketException instead of org.apache.http.conn.ConnectionTimeoutException when connection timeout occursWhen sending an http request a connection timeout occurs, the HttpClient.execute method throws a java.net.SocketException instead of a org.apache.http.conn.ConnectionTimeoutException.

java.net.SocketTimeoutException: connect timed out
        at java.net.PlainSocketImpl.socketConnect(Native Method)
        at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
        at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
        at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
        at java.net.Socket.connect(Socket.java:519)
        at org.apache.http.conn.scheme.PlainSocketFactory.connectSocket(PlainSocketFactory.java:119)
        at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:129)
        at org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:164)
        at org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:119)
        at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:349)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:555)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:487)
        at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:465)
"
0,Jackrabbit JCR Nodemanagement API implementationThere needs to be a Jackrabbit implementation of the org.apache.portals.graffito.jcr.nodemanagement.NodeTypeManager interface.
1,"IndexMerger blocks client threads when obsolete index segments are deletedWhen index segments have been merged, the obsolete indexes are replaced with the new one an deleted afterwards. Currently deleting the obsolete segments is inside a MultiIndex synchronized block, which may block other threads from updating the index concurrently."
0,"[API Doc] Improve the description of the preemptive authenticationHttpClient authentication guide does not reflect the fact that preemptive
authentication requires default credentials to be set. It should also mention
the security implications of preemptive authentication (default credentials sent
with EVERY request to ANY target / proxy server)"
0,"cache module does not recognize equivalent URIshttp://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.2.3

""When comparing two URIs to decide if they match or not, a client SHOULD use a case-sensitive octet-by-octet comparison of the entire URIs, with these exceptions:
  * A port that is empty or not given is equivalent to the default port for that URI-reference;
  * Comparisons of host names MUST be case-insensitive;
  * Comparisons of scheme names MUST be case-insensitive;
  * An empty abs_path is equivalent to an abs_path of ""/"".
Characters other than those in the ""reserved"" and ""unsafe"" sets (see RFC 2396 [42]) are equivalent to their """"%"" HEX HEX"" encoding.

For example, the following three URIs are equivalent:
      http://abc.com:80/~smith/home.html
      http://ABC.com/%7Esmith/home.html
      http://ABC.com:/%7esmith/home.html""

The current implementation does not canonicalize the URIs it uses for cache keys, and so is missing potential cache hits. More importantly, though, required invalidations due to PUT/POST/DELETE to a URI (as well as those mentioned in Location or Content-Location headers) may not occur properly due to this bug."
0,"Support for handling large files should be addedYou should probably change the EntityEnclosingMethod content length field 
to a long in a future release. Currently it's an int."
1,"bad normalization in sorted search returning TopDocsFieldSortedHitQueue.maxscore is maintained in the lessThan method (which never gets called if a single document is added to the queue).

I've checked in a test to TestSort.testTopDocsScores() with the final assertion commented out."
0,"Allow basic regexp in namespace prefix of index-ruleCurrently a regular expression is limited to the local name, which makes fallback declarations that should match everything else difficult to write. I.e. you have to write a line per namespace in the node type registry, which bloats the index-rule unnecessarily.

Currently:

<property isRegexp=""true"">.*</property>

will only match properties with the empty namespace URI.

I propose we allow a basic regular expression in the prefix. That is the match all pattern: '.*' (dot star).

The following would match any property, including any namespace:

<property isRegexp=""true"">.*:.*</property>
"
0,"misleading lack of javadoc in StringRequestEntityWhen using httpclient2, we were doing the following:

	// Add the Content-type header.  This sets the charset to UTF-8.
	method.setRequestHeader( ""Content-type"", ""text/xml; charset=UTF-8"" );
	// The given string is converted internally by the post method into
	// a UTF-8 encoded byte array.
	method.setRequestBody( xmlstring );

The comments show that this was the way we used to obtain a UTF-8 encoded XML
document (if this was wrong, that may be the origin of the problem?).


When upgrading to httpclient3 and killing deprecated code, this was converted to:

	// Add the Content-type header.  This sets the charset to UTF-8.
	method.setRequestHeader( ""Content-type"", ""text/xml; charset=UTF-8"" );
	// The given string is converted internally by the post method into
	// a UTF-8 encoded byte array.
        method.setRequestEntity( new StringRequestEntity( xmlstring ) );

which went without problem during the tests on my machine and on test production
machine.. because platforms charset were UTF-8, which is not the case for
production machines :(

I think the javadoc of the used StringRequestEntity constructor should strongly
state that it uses String#getBytes for the content, which uses the platform
charset. Also, I didn't notice any ""upgrade to 3.x"" documentation which would
have helped me :/"
0,"Data Store: remove kill switch ""InternalValue.USE_DATA_STORE""There is still a ""kill switch"" (public static final boolean USE_DATA_STORE) in the class org.apache.jackrabbit.core.value.InternalValue. In version 2.0 this constant should be removed. Also, the system property ""org.jackrabbit.useDataStore"" will no longer be used. 

It is still possible to disable the DataStore (don't include a DataStore configuration in repository.xml)."
0,"Include diagnostics per-segment when writing a new segmentIt would be very helpful if each segment in an index included
diagnostic information, such as the current version of Lucene.

EG, in LUCENE-1474 this would be very helpful to see if certain
segments were written under 2.4.0.

We can start with just the current version.

We could also consider making this extensible, so you could provide
your own arbitrary diagnostics, but SegmentInfo/s is not public so I
think such an API would be ""one-way"" in that you'd have to use
CheckIndex to check on it later.  Or we could wait on such extensibility
until we provide some consistent way to access per-segment details
in the index.
"
0,"Add tests.iter.min to improve controlling tests.iter's behaviorAs discussed here: http://lucene.472066.n3.nabble.com/Stop-iterating-if-testsFailed-td2747426.html, this issue proposes to add tests.iter.min in order to allow one better control over how many iterations are run:

* Keep tests.iter as it is today
* Add tests.iter.min (default to tests.iter) to denote that at least N instances of the test should run until there's either a failure or tests.iter is reached.

If one wants to run until the first failure, he can set tests.iter.min=1 and tests.iter=X -- up to X instances of the test will run, until the first failure.

Similarly, one can set tests.iter=N to denote that at least N instances should run, regardless if there were failures, but if after N runs a failure occurred, the test should stop.

Note: unlike what's proposed on the thread, tests.iter.max is dropped from this proposal as it's exactly like tests.iter, so no point in having two similar parameters.

I will work on a patch tomorrow."
1,"Formatting error in ReportTask in contrib/benchmarkI am building a new Task, AnalyzerTask, that lets you change the Analyzer in the loop, thus allowing for the comparison of the same Analyzers over the set of documents.

My algorithm declaration looks like:
NewAnalyzer(WhitespaceAnalyzer, SimpleAnalyzer, StopAnalyzer, standard.StandardAnalyzer)

And it could be longer.

The exception is:
Error: cannot execute the algorithm! String index out of range: 85
java.lang.StringIndexOutOfBoundsException: String index out of range: 85
	at java.lang.String.substring(String.java:1765)
	at org.apache.lucene.benchmark.byTask.utils.Format.format(Format.java:85)
	at org.apache.lucene.benchmark.byTask.tasks.ReportTask.tableTitle(ReportTask.java:85)
	at org.apache.lucene.benchmark.byTask.tasks.ReportTask.genPartialReport(ReportTask.java:140)
	at org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask.reportSumByName(RepSumByNameTask.java:77)
	at org.apache.lucene.benchmark.byTask.tasks.RepSumByNameTask.doLogic(RepSumByNameTask.java:39)
	at org.apache.lucene.benchmark.byTask.tasks.PerfTask.runAndMaybeStats(PerfTask.java:83)
	at org.apache.lucene.benchmark.byTask.tasks.TaskSequence.doSerialTasks(TaskSequence.java:112)
	at org.apache.lucene.benchmark.byTask.tasks.TaskSequence.doLogic(TaskSequence.java:93)
	at org.apache.lucene.benchmark.byTask.utils.Algorithm.execute(Algorithm.java:228)
	at org.apache.lucene.benchmark.byTask.Benchmark.execute(Benchmark.java:73)
	at org.apache.lucene.benchmark.byTask.Benchmark.main(Benchmark.java:109)

The error seems to be caused by the fact that ReportTask uses the OP (operation) column for the String, but then uses the length of the algorithm declaration to index into the String, resulting in the index out of bounds exception.

The line in question is:
return (s + padd).substring(0, col.length());

And probably should be changed to something like:
    String s1 = (s + padd);
    return s1.substring(0, Math.min(col.length(), s1.length()));

Either that or the column should be trimmed.  The workaround is to explicitly name the task.

If no objections, I will make the change, tomorrow.  "
0,Update Lucene to 3.0Lucene 3.0 was released on 2009/11/25. They migrated to Java 1.5 as Jackrabbit is doing with 2.0. Also they added some new optimizations. It would be nice if Jackrabbit could switch to the new lucene version too.
0,"Root exception not logged in ClusterNode for ClusterExceptionWhen our MySQL server is down or failed queries we have the following log :
ERROR (ClusterNode-node1) [org.apache.jackrabbit.core.cluster.ClusterNode] Periodic sync of journal failed: Unable to return record iterater.

So the root exception (SQLException in my case) is missing from the log and this prevent me from quickly finding the reason."
1,"Version.isSame(Object) not workingVersion interface is implemented (on the frontend) by the VersionImpl class (extending NodeWrapper), which delegates to an internal NodeImpl class, which in turn extends ItemImpl.

Say you have :
      Node node = // at Version 1.0
      Version version = // retrieved as 1.0 for the node
      Version baseVersion = node.getBaseVersion()

You now expect
      baseVersion.isSame(version)
even if
      baseVersion != version

This fails, because VersionImpl delegates the isSame call to its delegatee, thus above call becomes
      ((VersionImpl) baseVersion).delegatee.isSame(version)
where this method is implemented by the ItemImpl class from which the delegatee NodeImpl extends.

That latter implementation ItemImpl.isSame() only returns true if the other is an ItemImpl, too. But this is not the case because VersionImpl is a Version, NodeWrapper, Node but not an ItemImpl.

Probably the best solution would be for NodeImpl.isSame() to check whether the otherItem is a NodeWrapper und use ((NodeWrapper) otherItem).delegatee as the otherItem for the delegatee call.

On another track: ItemImpl.isSame() should probably do a fast check whether the otherItem is actually the same instance to prevent type checks..."
1,"NullPointerException in ItemManagerWe have a lot of these occurring:
java.lang.NullPointerException
	at org.apache.jackrabbit.core.ItemManager.getDefinition(ItemManager.java:206)
	at org.apache.jackrabbit.core.ItemData.getDefinition(ItemData.java:99)
	at org.apache.jackrabbit.core.AbstractNodeData.getNodeDefinition(AbstractNodeData.java:73)
	at org.apache.jackrabbit.core.NodeImpl.getDefinition(NodeImpl.java:2430)
	at org.apache.jackrabbit.core.ItemValidator.isProtected(ItemValidator.java:373)
	at org.apache.jackrabbit.core.ItemValidator.checkCondition(ItemValidator.java:273)
	at org.apache.jackrabbit.core.ItemValidator.checkRemove(ItemValidator.java:254)
	at org.apache.jackrabbit.core.ItemRemoveOperation.perform(ItemRemoveOperation.java:63)
	at org.apache.jackrabbit.core.session.SessionState.perform(SessionState.java:200)
	at org.apache.jackrabbit.core.ItemImpl.perform(ItemImpl.java:91)
	at org.apache.jackrabbit.core.ItemImpl.remove(ItemImpl.java:322)
	at org.apache.jackrabbit.core.NPEandCMETest$TestTask.run(NPEandCMETest.java:87)
	at java.lang.Thread.run(Thread.java:679)

I'll attach a junit test to reproduce this exception."
0,"SpanQueryFilter additionSimilar to the QueryFilter (or whatever it is called now) the SpanQueryFilter is a regular Lucene Filter, but it also can return Spans-like information.  This is useful if you not only want to filter based on a Query, but you then want to be able to compare how a given match from a new query compared to the positions of the filtered SpanQuery.  Patch to come shortly also contains a caching mechanism for the SpanQueryFilter"
1,"DirectoryReader ignores NRT SegmentInfos in #isOptimized()DirectoryReader  only takes shared (with IW) SegmentInfos into account in DirectoryReader#isOptimized(). This can return true even if the actual realtime reader sees more than one segments. 

{code}
public boolean isOptimized() {
    ensureOpen();
   // if segmentsInfos changes in IW this can return false positive
    return segmentInfos.size() == 1 && !hasDeletions();
  }
{code}

DirectoryReader should check if this reader has a non-nul segmentInfosStart and use that instead"
1,"Calling size method of a ManageableArrayList causes NullPointerExceptionWhen using the NTCollectionConverterImpl with proxy=""true"" a call on the size () method of a ManageableArrayList causes a NullPointerException if there is no underlying List. LazyCollectionLoader doLoad returns null because there is are no children.

The ManageableArrayList is created because the isNull method of the NTCollectionConverterImpl class always returns false. 
According to the comment line this is done because the getCollectionNodes always returns a list. 
But after the fix for JCR-882 this is not correct anymore.

The attached fix corrects this. 

The only question remaining is how to differ between an empty list and a null-value for the field containing the list."
0,"don't spawn thread statically in FSDirectory on Mac OS Xon the Mac, creating the digester starts up a PKCS11 thread.

I do not think threads should be created statically (I have this same issue with TimeLimitedCollector and also FilterManager).

Uwe discussed removing this md5 digester, I don't care if we remove it or not, just as long as it doesn't create a thread,
and just as long as it doesn't use the system default locale."
0,"[PATCH] Efficiently retrieve sizes of field valuesSometimes an application would like to know how large a document is before retrieving it.  This can be important for memory management or choosing between algorithms, especially in cases where documents might be very large.

This patch extends the existing FieldSelector mechanism with two new FieldSelectorResults:  SIZE and SIZE_AND_BREAK.  SIZE creates fields on the retrieved document that store field sizes instead of actual values.  SIZE_AND_BREAK is especially efficient if one field comprises the bulk of the document size (e.g., the body field) and can thus be used as a reasonable size approximation.
"
0,"CharArraySet cannot be made generic, because it violates the Set<char[]> interfaceI tried to make CharArraySet using generics (extends AbstractSet<char[]>) but this is not possible, as it e.g. returns sometimes String instances in the Iterator instead of []. Also its addAll method accepts both String and char[]. I think this class is a complete mis-design and violates almost everything (sorry).

What to do? Make it Set<?> or just place a big @SuppressWarnings(""unchecked""> in front of it?

Because of this problem also a lot of Set declarations inside StopAnalyzer cannot be made generic as you never know whats inside."
1,"Query parser builds invalid parse treeCalling org.apache.jackrabbit.spi.commons.query.QueryParser.parse on 

SELECT prop1 FROM nt:unstructured WHERE prop1 IS NOT NULL ORDER BY prop1 ASC 

results in the following parse tree

+ Select properties: {}prop1
 + PathQueryNode
   + LocationStepQueryNode:  NodeTest=* Descendants=true Index=NONE
     + RelationQueryNode: Op: NOT NULL Prop=@{}prop1 Type=STRING Value=%
     + NodeTypeQueryNode:  Prop={http://www.jcp.org/jcr/1.0}primaryType Value={http://www.jcp.org/jcr/nt/1.0}unstructured
 + OrderQueryNode
   {}prop1 asc=true

The RelationQueryNode should not have a second operand since the NOT NULL operator is unary.

"
1,Update monitor is not releasedWhen the timer thread in MultiIndex commits the volatile index after some idle time it does not release / reset the updateInProgress flag. This results in queries that hang until another thread writes to the workspace.
1,"IndexWriter.synced  field accumulates data leading to a Memory LeakI am running into a strange OutOfMemoryError. My small test application does
index and delete some few files. This is repeated for 60k times. Optimization
is run from every 2k times a file is indexed. Index size is 50KB. I did analyze
the HeapDumpFile and realized that IndexWriter.synced field occupied more than
half of the heap. That field is a private HashSet without a getter. Its task is
to hold files which have been synced already.

There are two calls to addAll and one call to add on synced but no remove or
clear throughout the lifecycle of the IndexWriter instance.

According to the Eclipse Memory Analyzer synced contains 32618 entries which
look like file names ""_e065_1.del"" or ""_e067.cfs""

The index directory contains 10 files only.

I guess synced is holding obsolete data "
0,"Use a System.arraycopy more than a forIn org.apache.lucene.index.DocumentWriter. The patch will explain by itself. I didn't make any performance test, but I think it is obvious that it will be faster.
All tests passed."
0,"extensibility patch for simple WebDAV servletattaching a patch that makes the simple WebDAV servlet more extensible - subclasses can now provide their own support objects such as DavResourceFactory and DavLocatorFactory. also patches DavResourceImpl to make importXml and importFile methods protected, for subclass use.
"
0,"Open access modifier for RepositoryImpl.doShutdown()This is required for a subclass of RepositoryImpl that wants to run additional code on shutdown, otherwise a deadlock may occur because the sequence of lock acquisition cannot be ensured.

Jackrabbit requires that the shutdownLock is first acquired and then the actual shutdown code is executed."
1,"Yet another race in IW#nrtIsCurrentIn IW#nrtIsCurrent looks like this:

{code}
  synchronized boolean nrtIsCurrent(SegmentInfos infos) {
    ensureOpen();
    return infos.version == segmentInfos.version && !docWriter.anyChanges() && !bufferedDeletesStream.any();
  }
{code}

* the version changes once we checkpoint the IW
* docWriter has changes if there are any docs in ram or any deletes in the delQueue
* bufferedDeletes contain all frozen del packages from the delQueue

yet, what happens is 1. we decrement the numDocsInRam in DWPT#doAfterFlush (which is executed during DWPT#flush) but before we checkpoint. 2. if we freeze deletes (empty the delQueue) we put them in the flushQueue to maintain the order.  This means they are not yet in the bufferedDeleteStream.

Bottom line, there is a window where we could see IW#nrtIsCurrent returning true if we check within this particular window. Phew, I am not 100% sure if that is the reason for our latest failure in SOLR-2861 but from what the logs look like this could be what happens. If we randomly hit low values for maxBufferedDocs & maxBufferedDeleteTerms this is absolutely possible."
0,"The 1.5.0 webapp points to 1.4 javadocsThere's a ""Jackrabbit API"" link on the Jackrabbit webapp 1.5.0 that points to http://jackrabbit.apache.org/api/1.4/. It should be updated to point to http://jackrabbit.apache.org/api/1.5/."
0,"Add set/getLocalAddress methods to HostConfigurationOn clustered or multi-homed systems, there's a need to specify the local bind
address of sockets, to ensure that they're created on the right interface.  To
do this, the local address needs to be passed to the 4-argument version of
ProtcolSocketFactory.createSocket.

After discussion on the mailing list, the best approach for this seems to be
adding the local address as a property on HostConfiguration and HttpConnection.  

I've attached a patch which does the following:
- Add public set/getLocalAddress methods to HostConfiguration and HttpConnection.
- HttpConnection uses the local address when opening connections.
- Modify HostConfiguration.equals and hostEquals to compare the local address too.
- SimpleHttpConnectionManager uses the local address from the provided config. 
I also cleaned up its getConnection method a bit.
- HttpClient.executeMethod uses the local address from its default
HostConfiguration if the method's config doesn't specify one."
1,"PriorityQueue is inheriently broken if subclass attempts to use ""heap"" w/generic T bound to anything other then ""Object""as discovered in SOLR-2410 the fact that the protected ""heap"" variable in PriorityQueue is initialized using an Object[] makes it impossible for subclasses of PriorityQueue to exist and access the ""heap"" array unless they bind the generic to Object."
0,"Deprecate all String/File ctors/opens in IndexReader/IndexWriter/IndexSearcherDuring investigation of LUCENE-1658, I found out, that even LUCENE-1453 is not completely fixed.
As 1658 deprecates all FSDirectory.getDirectory() static factories, we should not use them anymore. As the user is now free to choose the correct directory implementation using direct instantiation or using FSDir.open() he should no longer use all ctors/methods in IndexWriter/IndexReader/IndexSearcher & Co. that simply take path names as String or File and always instantiate the Directory himself.

LUCENE-1453 currently works for the cached directory implementations from FSDir.getDirectory, but not with uncached, non refcounting FSDirs. Sometime reopen() closes the directory (as far as I see, when a SegmentReader changes to a MultiSegmentReader and/or deletes apply). This is hard to track. In Lucene 3.0 we then can remove the whole bunch of closeDirectory parameters/fields in these classes and simply do not care anymore about closing directories.

To remove this closeDirectory parameter now (before 3.0) and also fix 1453 correctly, an additional idea would be to change these factories that take the File/String to return the IndexReader wrapped by a FilteredIndexReader, that keeps track on closing the underlying directory after close and reopen. This is simplier than passing this boolean between different DirectoryIndexReader instances. The small performance impact by wrapping with FilterIndexReader should not be so bad, because the method is deprecated and we can state, that it is better to user the factory method with Directory parameter."
1,"org.apache.lucene.analysis.cn.ChineseTokenizer missing offset decrementApparently, in ChineseTokenizer, offset should be decremented like bufferIndex
when Character is OTHER_LETTER.  This directly affects startOffset and endOffset
values.

This is critical to have Highlighter working correctly because Highlighter marks
matching text based on these offset values."
0,"PayloadTermQuery refers to a deprecated documentation for redirection When PayloadTermQuery refers to the function for scoring Similarity - it refers to override the deprecated method - 

Similarity#scorePayload(String, byte[],int,int) . 

That method has been deprecated by  Similarity#scorePayload(int, String, int, int, byte[],int,int) . 


This javadoc patch addresses the class level javadoc for the class to provide the right signature in Similarity to be overridden. "
0,standard codec's terms dict seek should only scan if new term is in same index blockTermInfosReader in trunk already optimizes for this case... just need to do the same on flex.
0,"add IndexWriter.removeUnferencedFiles, so apps can more immediately delete index files when readers are closedThis has come up several times on the user's list.

On Windows, which prevents deletion of still-open files, IndexWriter cannot remove files that are in-use by open IndexReaders.  This is fine, and IndexWriter periodically retries the delete, but it doesn't retry very often (only on open, on flushing a new segment, and on committing a merge).  So it lacks immediacy.

With this expert method, apps that want faster deletion can call this method."

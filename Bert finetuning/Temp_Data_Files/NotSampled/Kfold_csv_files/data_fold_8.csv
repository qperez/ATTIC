label,summmarydescription
0,Update LICENSE and NOTICE files to match the updated dependenciesWe've made quite a few dependency updates since Jackrabbit 1.6 and need to update the license metadata accordingly.
0,"Buffered output to socket--Posted by Slavik Markovich:

Hi all,

This is probably a known issue (but I haven't found the answer for it yet).
I'm using httpclient to post data to a remote server but as far as I can see
(using ethereal) the client is writing every line to the wire without buffering.
After examining the code, I can see that the HttpConnection class is using the
output stream received from the socket directly.
Is there a reason for the direct writing?
This is a problem for me 'cause the remote server sets a very low timeout and
returns a bad request response after receiving the request line (without any
other header line or request body).

Can I easily add a buffered behavior to the http client?

10x"
0,"InstantiatedIndex - faster but memory consuming indexRepresented as a coupled graph of class instances, this all-in-memory index store implementation delivers search results up to a 100 times faster than the file-centric RAMDirectory at the cost of greater RAM consumption.

Performance seems to be a little bit better than log2n (binary search). No real data on that, just my eyes.

Populated with a single document InstantiatedIndex is almost, but not quite, as fast as MemoryIndex.    

At 20,000 document 10-50 characters long InstantiatedIndex outperforms RAMDirectory some 30x,
15x at 100 documents of 2000 charachters length,
and is linear to RAMDirectory at 10,000 documents of 2000 characters length.

Mileage may vary depending on term saturation.


"
1,"Wrong cookie matching port number reported when using a proxyFollowing the example given in https://issues.apache.org/jira/browse/HTTPCLIENT-852 and the route HttpRoute[{}->http://xyz.webfactional.com:7295->http://www.seoconsultants.com]:

one of the new cookies is reported to be added as:

[java] 2009/05/28 19:58:23:398 CEST [DEBUG] RequestAddCookies - Cookie [version: 0][name: ASPSESSIONIDCSARBQBA][value: MAMPAMKCBDJJFKNAAPKPMDAA][domain: www.seoconsultants.com][path: /][expiry: null] match [www.seoconsultants.com:7295/]

whereas it should be:

[java] 2009/05/28 19:57:46:667 CEST [DEBUG] RequestAddCookies - Cookie [version: 0][name: ASPSESSIONIDCSARBQBA][value: AAMPAMKCMBINHNEHPFEBFADA][domain: www.seoconsultants.com][path: /][expiry: null] match [www.seoconsultants.com:80/]

i.e. the same as without using a proxy. 7295 is the port number used to access the proxy. The target domain www.seoconsultants.com is accessed through the regular HTTP port number 80, thus the cookie matching should also refer to port 80 and not the proxy's port."
1,"SegmentReader.doCommit should be sync'd; norms methods need not be sync'dI fixed the failure in TestNRTThreads, but in the process tripped an assert because SegmentReader.doCommit isn't sync'd.

So I sync'd it, but I don't think the norms APIs need to be sync'd -- we populate norms up front and then never change them.  Un-sync'ing them is important so that in the NRT case calling IW.commit doesn't block searches trying to pull norms.

Also some small code refactoring."
0,"IndexReader.cloneBased on discussion http://www.nabble.com/IndexReader.reopen-issue-td18070256.html.  The problem is reopen returns the same reader if there are no changes, so if docs are deleted from the new reader, they are also reflected in the previous reader which is not always desired behavior."
1,"NullPointerException thrown by equals method in SpanOrQueryPart of our code utilizes the equals method in SpanOrQuery and, in certain cases (details to follow, if necessary), a NullPointerException gets thrown as a result of the String ""field"" being null.  After applying the following patch, the problem disappeared:

Index: src/java/org/apache/lucene/search/spans/SpanOrQuery.java
===================================================================
--- src/java/org/apache/lucene/search/spans/SpanOrQuery.java    (revision 465065)
+++ src/java/org/apache/lucene/search/spans/SpanOrQuery.java    (working copy)
@@ -121,7 +121,8 @@
     final SpanOrQuery that = (SpanOrQuery) o;

     if (!clauses.equals(that.clauses)) return false;
-    if (!field.equals(that.field)) return false;
+    if (field != null && !field.equals(that.field)) return false;
+    if (field == null && that.field != null) return false;

     return getBoost() == that.getBoost();
   }

"
1,"XA Transaction RecoveryIf i add a node to the repository i get a XAException because i run into a Timeout ... 
I see the Warn Message: Transaction rolled back because timeout expired.
The default Timeout is set to 5 sec and i dont know how to set it to a higher value
The Problem is if i restart my server websphere has a RecoveryManager and he try to recover this Transaction
and then i get a NullpointerException in JCAManagedConnectionFactory. createManagedConnection beacuse the given 
ConnectionRequestInfo is null.
So i dont know why the RecoveryManager tries to recover the Transaction ? The only solution for me is to delete the Tran-Log Files wich keep Websphere to recvoer
XA Trasnactions.
"
0,"Add some more constants for newer Java versions to Constants.class, remove outdated ones.Preparation for LUCENE-3235:
This adds constants to quickly detect Java6 and Java7 to Constants.java. It also deprecated and removes the outdated historical Java versions."
0,"unit tests should use private directoriesThis only affects our unit tests...

I run ""ant test"" and ""ant test-tag"" concurrently, but some tests have false failures (eg TestPayloads) because they use a fixed test directory in the filesystem for testing.

I've added a simple method to _TestUtil to get a temp dir, and switched over those tests that I've hit false failures on."
0,"Open IndexWriter API to allow custom MergeScheduler implementationIndexWriter's getNextMerge() and merge(OneMerge) are package-private, which makes it impossible for someone to implement his own MergeScheduler. We should open up these API, as well as any other that can be useful for custom MS implementations."
0,"TimeoutHandler visitor should be extracted into a dedicated classThis is a minor problem that I've stumbled upon when looking at a memory leak.
It seems that the TimeoutHandler thread runs each second and it uses a custom ElementVisitor to do its business. By creating a new instance of ElementVisitor each second this creates some garbage that could be avoided by using a predefined class.

Short story: during a unit test it creates 3x instances each second that have 16 bytes each. Having a dedicated visitor class creates just 3 instances for the lifespan of the repository."
0,Use creation tick instead of weak references in DocNumberCacheThis avoids the need to keep two maps in CachingMultiIndexReader. And I guess the fewer weak references the better...
0,NodeIndexer creates unnecessary string representation of Name valueNodeIndexer.addNameValue() calls Name.toString() but never uses it.
0,"Tika-based type detection in jcr-serverAs discussed on dev@, I'd like to make the jackrabbit-jcr-server component use Apache Tika for automatic media type detection."
1,The move method doesn't remove the source nodeHere is a small unit test that demonstrate that the method move doesn't remove the source node. 
0,"add one setter for start and end offset to OffsetAttributeadd OffsetAttribute. setOffset(startOffset, endOffset);

trivial change, no JUnit needed

Changed CharTokenizer to use it"
0,"Make EasySimilarityProvider a full-fledged class The {{EasySimilarityProvider}} in {{TestEasySimilarity}} would be a good candidate for a full-fledged class. Both {{DefaultSimilarity}} and {{BM25Similarity}} have their own providers, which are effectively the same,so I don't see why we couldn't add one generic provider for convenience."
0,"Allow to pass an instance of RateLimiter to FSDirectory allowing to rate limit merge IO across several directories / instancesThis can come in handy when running several Lucene indices in the same VM, and wishing to rate limit merge across all of them."
0,"TCK: PredefinedNodeTypeTest does not allow additions to the predefined node types hierarchyAs explained in section 6.7.22.2 (page 147) of the JSR 170 specification, an implementation is allowed to customize a predefined noe type definition with additional supertypes. The tests in PredefinedNodeTypeTest do not account for that and expect an exact match with the predefined node types."
1,"TestIndexWriter.testThreadInterruptDeadlock failed (can't reproduce)trunk: r1134163 

ran it a few times with tests.iter=200 and couldn't reproduce, but i believe you like an issue anyway.

{code}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter
    [junit] Testcase: testThreadInterruptDeadlock(org.apache.lucene.index.TestIndexWriter):     FAILED
    [junit]
    [junit] junit.framework.AssertionFailedError:
    [junit]     at org.apache.lucene.index.TestIndexWriter.testThreadInterruptDeadlock(TestIndexWriter.java:1203)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1403)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1321)
    [junit]
    [junit]
    [junit] Tests run: 40, Failures: 1, Errors: 0, Time elapsed: 23.79 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] CheckIndex failed
    [junit] ERROR: could not read any segments file in directory
    [junit] java.io.FileNotFoundException: segments_2w
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:407)
    [junit]     at org.apache.lucene.index.codecs.DefaultSegmentInfosReader.openInput(DefaultSegmentInfosReader.java:112)
    [junit]     at org.apache.lucene.index.codecs.DefaultSegmentInfosReader.read(DefaultSegmentInfosReader.java:45)
    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:257)
    [junit]     at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:287)
    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:698)
    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:533)
    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:283)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:311)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:154)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:144)
    [junit]     at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:1154)
    [junit]
    [junit] CheckIndex FAILED: unexpected exception
    [junit] java.lang.RuntimeException: CheckIndex failed
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:158)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:144)
    [junit]     at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:1154)
    [junit] IndexReader.open FAILED: unexpected exception
    [junit] java.io.FileNotFoundException: segments_2w
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:407)
    [junit]     at org.apache.lucene.index.codecs.DefaultSegmentInfosReader.openInput(DefaultSegmentInfosReader.java:112)
    [junit]     at org.apache.lucene.index.codecs.DefaultSegmentInfosReader.read(DefaultSegmentInfosReader.java:45)
    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:257)
    [junit]     at org.apache.lucene.index.DirectoryReader$1.doBody(DirectoryReader.java:88)
    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:698)
    [junit]     at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:84)
    [junit]     at org.apache.lucene.index.IndexReader.open(IndexReader.java:500)
    [junit]     at org.apache.lucene.index.IndexReader.open(IndexReader.java:293)
    [junit]     at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:1161)

    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testThreadInterruptDeadlock -Dtests.seed=6733070832417768606:3130345095020099096
    [junit] NOTE: test params are: codec=RandomCodecProvider: {=MockRandom, f6=SimpleText, f7=MockRandom, f8=MockSep, f9=Standard, f1=SimpleText, f0=Standard, f3=Standard, f2=MockSep, f5=Pulsing(freqCutoff=12),
 f4=MockFixedIntBlock(blockSize=552), c=MockVariableIntBlock(baseBlockSize=43), d9=MockVariableIntBlock(baseBlockSize=43), d8=MockRandom, d5=MockSep, d4=Pulsing(freqCutoff=12), d7=MockFixedIntBlock(blockSize=55
2), d6=MockVariableIntBlock(baseBlockSize=43), d25=MockSep, d0=MockVariableIntBlock(baseBlockSize=43), c29=MockFixedIntBlock(blockSize=552), d24=Pulsing(freqCutoff=12), d1=MockFixedIntBlock(blockSize=552), c28=
Standard, d23=SimpleText, d2=SimpleText, c27=MockSep, d22=Standard, d3=MockRandom, d21=MockRandom, d20=SimpleText, c22=MockSep, c21=Pulsing(freqCutoff=12), c20=SimpleText, d29=SimpleText, c26=MockVariableIntBlo
ck(baseBlockSize=43), d28=Standard, c25=MockRandom, d27=MockVariableIntBlock(baseBlockSize=43), c24=Pulsing(freqCutoff=12), d26=MockRandom, c23=MockFixedIntBlock(blockSize=552), e9=Pulsing(freqCutoff=12), e8=St
andard, e7=MockSep, e6=MockRandom, e5=SimpleText, c17=MockFixedIntBlock(blockSize=552), e3=MockFixedIntBlock(blockSize=552), d12=Pulsing(freqCutoff=12), c16=MockVariableIntBlock(baseBlockSize=43), e4=Pulsing(fr
eqCutoff=12), d11=MockFixedIntBlock(blockSize=552), c19=MockRandom, e1=MockSep, d14=MockVariableIntBlock(baseBlockSize=43), c18=SimpleText, e2=Standard, d13=MockRandom, e0=SimpleText, d10=MockSep, d19=MockVaria
bleIntBlock(baseBlockSize=43), c11=MockVariableIntBlock(baseBlockSize=43), c10=MockRandom, d16=Standard, c13=MockRandom, c12=SimpleText, d15=MockSep, d18=Pulsing(freqCutoff=12), c15=Standard, d17=MockFixedIntBl
ock(blockSize=552), c14=MockSep, b3=Standard, b2=MockSep, b5=Pulsing(freqCutoff=12), b4=MockFixedIntBlock(blockSize=552), b7=MockFixedIntBlock(blockSize=552), b6=MockVariableIntBlock(baseBlockSize=43), d50=Mock
Random, b9=MockRandom, b8=SimpleText, d43=SimpleText, d42=Standard, d41=MockVariableIntBlock(baseBlockSize=43), d40=MockRandom, d47=Pulsing(freqCutoff=12), d46=MockFixedIntBlock(blockSize=552), b0=MockRandom, d
45=Standard, b1=MockVariableIntBlock(baseBlockSize=43), d44=MockSep, d49=MockRandom, d48=SimpleText, c6=SimpleText, c5=Standard, c4=MockVariableIntBlock(baseBlockSize=43), c3=MockRandom, c9=MockFixedIntBlock(bl
ockSize=552), c8=Standard, c7=MockSep, d30=Standard, d32=Pulsing(freqCutoff=12), d31=MockFixedIntBlock(blockSize=552), c1=Pulsing(freqCutoff=12), d34=MockFixedIntBlock(blockSize=552), c2=MockSep, d33=MockVariab
leIntBlock(baseBlockSize=43), d36=MockRandom, c0=SimpleText, d35=SimpleText, d38=MockSep, d37=Pulsing(freqCutoff=12), d39=MockVariableIntBlock(baseBlockSize=43), e92=MockFixedIntBlock(blockSize=552), e93=Pulsin
g(freqCutoff=12), e90=MockSep, e91=Standard, e89=Standard, e88=MockVariableIntBlock(baseBlockSize=43), e87=MockRandom, e86=MockFixedIntBlock(blockSize=552), e85=MockVariableIntBlock(baseBlockSize=43), e84=MockS
ep, e83=Pulsing(freqCutoff=12), e80=MockFixedIntBlock(blockSize=552), e81=SimpleText, e82=MockRandom, e77=Standard, e76=MockSep, e79=Pulsing(freqCutoff=12), e78=MockFixedIntBlock(blockSize=552), e73=MockVariabl
eIntBlock(baseBlockSize=43), e72=MockRandom, e75=SimpleText, e74=Standard, binary=MockSep, f98=MockRandom, f97=SimpleText, f99=MockSep, f94=Pulsing(freqCutoff=12), f93=MockFixedIntBlock(blockSize=552), f96=Mock
VariableIntBlock(baseBlockSize=43), f95=MockRandom, e95=MockRandom, e94=SimpleText, e97=Standard, e96=MockSep, e99=MockSep, e98=Pulsing(freqCutoff=12), id=Standard, f34=SimpleText, f33=Standard, f32=MockVariabl
eIntBlock(baseBlockSize=43), f31=MockRandom, f30=MockFixedIntBlock(blockSize=552), f39=SimpleText, f38=MockVariableIntBlock(baseBlockSize=43), f37=MockRandom, f36=Pulsing(freqCutoff=12), f35=MockFixedIntBlock(b
lockSize=552), f43=MockSep, f42=Pulsing(freqCutoff=12), f45=MockFixedIntBlock(blockSize=552), f44=MockVariableIntBlock(baseBlockSize=43), f41=Standard, f40=MockSep, f47=SimpleText, f46=Standard, f49=MockSep, f4
8=Pulsing(freqCutoff=12), content=Standard, e19=Standard, e18=MockSep, e17=SimpleText, f12=MockRandom, e16=Standard, f11=SimpleText, f10=MockFixedIntBlock(blockSize=552), e15=MockVariableIntBlock(baseBlockSize=
43), e14=MockRandom, f16=MockFixedIntBlock(blockSize=552), e13=MockSep, e12=Pulsing(freqCutoff=12), f15=MockVariableIntBlock(baseBlockSize=43), e11=SimpleText, f14=MockSep, e10=Standard, f13=Pulsing(freqCutoff=
12), f19=Standard, f18=MockVariableIntBlock(baseBlockSize=43), f17=MockRandom, e29=MockRandom, e26=MockSep, f21=Standard, e25=Pulsing(freqCutoff=12), f20=MockSep, e28=MockFixedIntBlock(blockSize=552), f23=Pulsi
ng(freqCutoff=12), e27=MockVariableIntBlock(baseBlockSize=43), f22=MockFixedIntBlock(blockSize=552), f25=MockRandom, e22=MockFixedIntBlock(blockSize=552), f24=SimpleText, e21=MockVariableIntBlock(baseBlockSize=
43), f27=Standard, e24=MockRandom, f26=MockSep, e23=SimpleText, f29=MockSep, f28=Pulsing(freqCutoff=12), e20=Pulsing(freqCutoff=12), field=MockSep, string=MockVariableIntBlock(baseBlockSize=43), e30=MockFixedIn
tBlock(blockSize=552), e31=Pulsing(freqCutoff=12), a98=MockSep, e34=SimpleText, a99=Standard, e35=MockRandom, f79=MockSep, e32=MockVariableIntBlock(baseBlockSize=43), e33=MockFixedIntBlock(blockSize=552), b97=M
ockRandom, f77=MockRandom, e38=MockVariableIntBlock(baseBlockSize=43), b98=MockVariableIntBlock(baseBlockSize=43), f78=MockVariableIntBlock(baseBlockSize=43), e39=MockFixedIntBlock(blockSize=552), b99=Standard,
 f75=MockFixedIntBlock(blockSize=552), e36=Pulsing(freqCutoff=12), f76=Pulsing(freqCutoff=12), e37=MockSep, f73=Pulsing(freqCutoff=12), f74=MockSep, f71=Standard, f72=SimpleText, f81=Standard, f80=MockSep, e40=
MockVariableIntBlock(baseBlockSize=43), e41=Standard, e42=SimpleText, e43=MockSep, e44=Standard, e45=MockFixedIntBlock(blockSize=552), e46=Pulsing(freqCutoff=12), f86=Standard, e47=SimpleText, f87=SimpleText, e
48=MockRandom, f88=Pulsing(freqCutoff=12), e49=MockSep, f89=MockSep, f82=MockVariableIntBlock(baseBlockSize=43), f83=MockFixedIntBlock(blockSize=552), f84=SimpleText, f85=MockRandom, f90=Pulsing(freqCutoff=12),
 f92=MockVariableIntBlock(baseBlockSize=43), f91=MockRandom, str=MockRandom, a76=Standard, e56=Standard, f59=Pulsing(freqCutoff=12), a77=SimpleText, e57=SimpleText, a78=Pulsing(freqCutoff=12), e54=MockRandom, f
57=Standard, a79=MockSep, e55=MockVariableIntBlock(baseBlockSize=43), f58=SimpleText, e52=MockVariableIntBlock(baseBlockSize=43), e53=MockFixedIntBlock(blockSize=552), e50=Pulsing(freqCutoff=12), e51=MockSep, f
51=MockSep, f52=Standard, f50=MockRandom, f55=MockVariableIntBlock(baseBlockSize=43), f56=MockFixedIntBlock(blockSize=552), f53=Pulsing(freqCutoff=12), e58=MockFixedIntBlock(blockSize=552), f54=MockSep, e59=Pul
sing(freqCutoff=12), a80=Pulsing(freqCutoff=12), e60=Pulsing(freqCutoff=12), a82=MockVariableIntBlock(baseBlockSize=43), a81=MockRandom, a84=MockRandom, a83=SimpleText, a86=Standard, a85=MockSep, a89=SimpleText
, f68=MockVariableIntBlock(baseBlockSize=43), e65=Pulsing(freqCutoff=12), f69=MockFixedIntBlock(blockSize=552), e66=MockSep, a87=MockVariableIntBlock(baseBlockSize=43), e67=MockVariableIntBlock(baseBlockSize=43
), a88=MockFixedIntBlock(blockSize=552), e68=MockFixedIntBlock(blockSize=552), e61=SimpleText, e62=MockRandom, e63=MockSep, e64=Standard, f60=MockFixedIntBlock(blockSize=552), f61=Pulsing(freq

Cutoff=12), f62=MockRandom, f63=MockVariableIntBlock(baseBlockSize=43), e69=Standard, f64=SimpleText, f65=MockRandom, f66=MockSep, f67=Standard, f70=MockFixedIntBlock(blockSize=552), a93=MockSep, a92=Pulsing(freqCutoff=12), a91=SimpleText, e71=SimpleText, a90=Standard, e70=Standard, a97=MockVariableIntBlock(baseBlockSize=43), a96=MockRandom, a95=Pulsing(freqCutoff=12), a94=MockFixedIntBlock(blockSize=552), c58=MockRandom, a63=MockFixedIntBlock(blockSize=552), a64=Pulsing(freqCutoff=12), c59=MockVariableIntBlock(baseBlockSize=43), c56=MockFixedIntBlock(blockSize=552), d59=MockRandom, a61=MockSep, c57=Pulsing(freqCutoff=12), a62=Standard, c54=Pulsing(freqCutoff=12), c55=MockSep, a60=SimpleText, c52=Standard, c53=SimpleText, d53=SimpleText, d54=MockRandom, d51=MockVariableIntBlock(baseBlockSize=43), d52=MockFixedIntBlock(blockSize=552), d57=Pulsing(freqCutoff=12), b62=Standard, d58=MockSep, b63=SimpleText, d55=Standard, b60=MockRandom, d56=SimpleText, b61=MockVariableIntBlock(baseBlockSize=43), b56=Standard, b55=MockSep, b54=MockRandom, b53=SimpleText, d61=MockVariableIntBlock(baseBlockSize=43), b59=MockVariableIntBlock(baseBlockSize=43), d60=MockRandom, b58=MockSep, b57=Pulsing(freqCutoff=12), c62=Standard, c61=MockSep, a59=MockVariableIntBlock(baseBlockSize=43), c60=MockRandom, a58=MockRandom, a57=MockFixedIntBlock(blockSize=552), a56=MockVariableIntBlock(baseBlockSize=43), a55=MockSep, a54=Pulsing(freqCutoff=12), a72=MockRandom, c67=Standard, a73=MockVariableIntBlock(baseBlockSize=43), c68=SimpleText, a74=Standard, c69=Pulsing(freqCutoff=12), a75=SimpleText, c63=MockVariableIntBlock(baseBlockSize=43), c64=MockFixedIntBlock(blockSize=552), a70=MockVariableIntBlock(baseBlockSize=43), c65=SimpleText, a71=MockFixedIntBlock(blockSize=552), c66=MockRandom, d62=MockSep, d63=Standard, d64=MockFixedIntBlock(blockSize=552), b70=Standard, d65=Pulsing(freqCutoff=12), b71=Pulsing(freqCutoff=12), d66=MockVariableIntBlock(baseBlockSize=43), b72=MockSep, d67=MockFixedIntBlock(blockSize=552), b73=MockVariableIntBlock(baseBlockSize=43), d68=SimpleText, b74=MockFixedIntBlock(blockSize=552), d69=MockRandom, b65=Pulsing(freqCutoff=12), b64=MockFixedIntBlock(blockSize=552), b67=MockVariableIntBlock(baseBlockSize=43), b66=MockRandom, d70=SimpleText, b69=MockRandom, b68=SimpleText, d72=MockSep, d71=Pulsing(freqCutoff=12), c71=Pulsing(freqCutoff=12), c70=MockFixedIntBlock(blockSize=552), a69=Pulsing(freqCutoff=12), c73=MockVariableIntBlock(baseBlockSize=43), c72=MockRandom, a66=MockRandom, a65=SimpleText, a68=Standard, a67=MockSep, c32=MockSep, c33=Standard, c30=SimpleText, c31=MockRandom, c36=MockVariableIntBlock(baseBlockSize=43), a41=Pulsing(freqCutoff=12), c37=MockFixedIntBlock(blockSize=552), a42=MockSep, a0=MockRandom, c34=Pulsing(freqCutoff=12), c35=MockSep, a40=SimpleText, b84=MockSep, d79=MockFixedIntBlock(blockSize=552), b85=Standard, b82=SimpleText, d77=MockSep, c38=Standard, b83=MockRandom, d78=Standard, c39=SimpleText, b80=MockRandom, d75=Standard, b81=MockVariableIntBlock(baseBlockSize=43), d76=SimpleText, d73=MockRandom, d74=MockVariableIntBlock(baseBlockSize=43), d83=MockRandom, a9=MockFixedIntBlock(blockSize=552), d82=SimpleText, d81=MockFixedIntBlock(blockSize=552), d80=MockVariableIntBlock(baseBlockSize=43), b79=MockFixedIntBlock(blockSize=552), b78=MockSep, b77=Pulsing(freqCutoff=12), b76=SimpleText, b75=Standard, a1=Pulsing(freqCutoff=12), a35=Pulsing(freqCutoff=12), a2=MockSep, a34=MockFixedIntBlock(blockSize=552), a3=MockVariableIntBlock(baseBlockSize=43), a33=Standard, a4=MockFixedIntBlock(blockSize=552), a32=MockSep, a5=MockRandom, a39=MockRandom, c40=SimpleText, a6=MockVariableIntBlock(baseBlockSize=43), a38=SimpleText, a7=Standard, a37=MockFixedIntBlock(blockSize=552), a8=SimpleText, a36=MockVariableIntBlock(baseBlockSize=43), c41=MockFixedIntBlock(blockSize=552), c42=Pulsing(freqCutoff=12), c43=MockRandom, c44=MockVariableIntBlock(baseBlockSize=43), c45=SimpleText, a50=MockVariableIntBlock(baseBlockSize=43), c46=MockRandom, a51=MockFixedIntBlock(blockSize=552), c47=MockSep, a52=SimpleText, c48=Standard, a53=MockRandom, b93=MockFixedIntBlock(blockSize=552), d88=MockRandom, c49=MockVariableIntBlock(baseBlockSize=43), b94=Pulsing(freqCutoff=12), d89=MockVariableIntBlock(baseBlockSize=43), b95=MockRandom, b96=MockVariableIntBlock(baseBlockSize=43), d84=Pulsing(freqCutoff=12), b90=SimpleText, d85=MockSep, b91=Pulsing(freqCutoff=12), d86=MockVariableIntBlock(baseBlockSize=43), b92=MockSep, d87=MockFixedIntBlock(blockSize=552), d92=Standard, d91=MockSep, d94=Pulsing(freqCutoff=12), d93=MockFixedIntBlock(blockSize=552), b87=MockFixedIntBlock(blockSize=552), b86=MockVariableIntBlock(baseBlockSize=43), d90=SimpleText, b89=MockRandom, b88=SimpleText, a44=MockVariableIntBlock(baseBlockSize=43), a43=MockRandom, a46=SimpleText, a45=Standard, a48=Standard, a47=MockSep, c51=MockFixedIntBlock(blockSize=552), a49=MockFixedIntBlock(blockSize=552), c50=MockVariableIntBlock(baseBlockSize=43), d98=MockFixedIntBlock(blockSize=552), d97=MockVariableIntBlock(baseBlockSize=43), d96=MockSep, d95=Pulsing(freqCutoff=12), d99=MockRandom, a20=MockVariableIntBlock(baseBlockSize=43), c99=SimpleText, c98=Standard, c97=MockVariableIntBlock(baseBlockSize=43), c96=MockRandom, b19=MockVariableIntBlock(baseBlockSize=43), a16=Pulsing(freqCutoff=12), a17=MockSep, b17=Pulsing(freqCutoff=12), a14=Standard, b18=MockSep, a15=SimpleText, a12=SimpleText, a13=MockRandom, a10=MockVariableIntBlock(baseBlockSize=43), a11=MockFixedIntBlock(blockSize=552), b11=MockFixedIntBlock(blockSize=552), b12=Pulsing(freqCutoff=12), b10=Standard, b15=SimpleText, b16=MockRandom, a18=MockRandom, b13=MockVariableIntBlock(baseBlockSize=43), a19=MockVariableIntBlock(baseBlockSize=43), b14=MockFixedIntBlock(blockSize=552), b30=MockRandom, a31=MockSep, a30=Pulsing(freqCutoff=12), b28=SimpleText, a25=MockVariableIntBlock(baseBlockSize=43), b29=MockRandom, a26=MockFixedIntBlock(blockSize=552), a27=SimpleText, a28=MockRandom, a21=MockSep, a22=Standard, a23=MockFixedIntBlock(blockSize=552), a24=Pulsing(freqCutoff=12), b20=MockRandom, b21=MockVariableIntBlock(baseBlockSize=43), b22=Standard, b23=SimpleText, a29=Pulsing(freqCutoff=12), b24=MockSep, b25=Standard, b26=MockFixedIntBlock(blockSize=552), b27=Pulsing(freqCutoff=12), b41=Pulsing(freqCutoff=12), b40=MockFixedIntBlock(blockSize=552), c77=MockRandom, c76=SimpleText, c75=MockFixedIntBlock(blockSize=552), c74=MockVariableIntBlock(baseBlockSize=43), c79=SimpleText, c78=Standard, c80=MockSep, c83=MockRandom, c84=MockVariableIntBlock(baseBlockSize=43), c81=MockFixedIntBlock(blockSize=552), b39=MockFixedIntBlock(blockSize=552), c82=Pulsing(freqCutoff=12), b37=Standard, b38=SimpleText, b35=MockRandom, b36=MockVariableIntBlock(baseBlockSize=43), b33=MockVariableIntBlock(baseBlockSize=43), b34=MockFixedIntBlock(blockSize=552), b31=Pulsing(freqCutoff=12), b32=MockSep, str2=MockSep, b50=MockVariableIntBlock(baseBlockSize=43), b52=SimpleText, str3=MockVariableIntBlock(baseBlockSize=43), b51=Standard, c86=Standard, tvtest=MockSep, c85=MockSep, c88=Pulsing(freqCutoff=12), c87=MockFixedIntBlock(blockSize=552), c89=MockVariableIntBlock(baseBlockSize=43), c90=SimpleText, c91=MockRandom, c92=Standard, c93=SimpleText, c94=Pulsing(freqCutoff=12), c95=MockSep, content1=MockRandom, b46=Pulsing(freqCutoff=12), b47=MockSep, content3=MockFixedIntBlock(blockSize=552), b48=MockVariableIntBlock(baseBlockSize=43), content4=MockVariableIntBlock(baseBlockSize=43), b49=MockFixedIntBlock(blockSize=552), content5=Pulsing(freqCutoff=12), b42=SimpleText, b43=MockRandom, b44=MockSep, b45=Standard}, locale=sk, timezone=America/Rainy_River
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDateTools, TestDeletionPolicy, TestDocsAndPositions, TestFlex, TestIndexReaderCloneNorms, TestIndexWriter]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=86065896,total=127401984
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.index.TestIndexWriter FAILED
{code}"
1,"IndexReader.isCurrent incorrectly returns false after writer.prepareCommit has been calledSpinoff from thread ""2 phase commit with external data"" on java-user.

The IndexReader should not see the index as changed, after a prepareCommit has been called but before commit is called."
0,"Utility to output total term frequency and df from a lucene indexThis is a pair of command line utilities that provide information on the total number of occurrences of a term in a Lucene index.  The first  takes a field name, term, and index directory and outputs the document frequency for the term and the total number of occurrences of the term in the index (i.e. the sum of the tf of the term for each document).   The second reads the index to determine the top N most frequent terms (by document frequency) and then outputs a list of those terms along with  the document frequency and the total number of occurrences of the term. Both utilities are useful for estimating the size of the term's entry in the *prx files and consequent Disk I/O demands. "
0,"Work around ThreadLocal's ""leak""Java's ThreadLocal is dangerous to use because it is able to take a
surprisingly very long time to release references to the values you
store in it.  Even when a ThreadLocal instance itself is GC'd, hard
references to the values you had stored in it are easily kept for
quite some time later.

While this is not technically a ""memory leak"", because eventually
(when the underlying Map that stores the values cleans up its ""stale""
references) the hard reference will be cleared, and GC can proceed,
its end behavior is not different from a memory leak in that under the
right situation you can easily tie up far more memory than you'd
expect, and then hit unexpected OOM error despite allocating an
extremely large heap to your JVM.

Lucene users have hit this many times.  Here's the most recent thread:

  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200809.mbox/%3C6e3ae6310809091157j7a9fe46bxcc31f6e63305fcdc%40mail.gmail.com%3E

And here's another:

  http://mail-archives.apache.org/mod_mbox/lucene-java-dev/200807.mbox/%3CF5FC94B2-E5C7-40C0-8B73-E12245B91CEE%40mikemccandless.com%3E

And then there's LUCENE-436 and LUCENE-529 at least.

A google search for ""ThreadLocal leak"" yields many compelling hits.

Sun does this for performance reasons, but I think it's a terrible
trap and we should work around it with Lucene."
1,"Missing Content-Length header causes a SocketExceptionEssentially, we have an invalid HTTP server (Stellent CMS actually and we will file a bug with them), 
which is returning headers like:

HTTP/1.1 401 Unauthorized
WWW-Authenticate: Basic ""Secure Realm""
Connection: keep-alive

Which is clearly missing the Content-Length header.  Now, previously HttpClient handled this 
perfectly by reading until the end of the connection (ie: treating it like it was a Connection: close), 
however for some reason a socket exception is being thrown and the invalid connection is added 
back into the connection pool and then every connection to the server after that thows an 
exception.

See the thread ""SocketException with invalid server"" for the full discussion of the issue.

I'll attach a patch that fixes the problem.  The biggest thing to consider is the changes to the 
duplicate Connection header test cases which resolves around the question: if Connection: keep-
alive is present but no Content-Length is provided, should the connection be closed?  The patch 
requires the answer to be yes and I really can't see any other way to do it."
0,"[PATCH] reduce duplicate conversions from OffsetCharSequence to (lower/upper) stringscode repetitively converts OffsetCharSequence to strings, and then repetitively converts to lower/upper case, when generating search terms.

Patch fixes this."
0,"Avoid element arrays in PathImplThe path handling code in spi-commons shows quite often in thread dumps and profiling results, as the current implementation does quite a bit of repetitive allocating and copying of path element arrays. We should be able to streamline and simplify the path handling code by only tracking the latest path element and a reference to the parent path. To do this efficiently we may need to adjust some of the Path and PathFactory method declarations (that currently assume element array -based paths) also in the SPI.
"
0,"Realm from authentication challenge unavailableThere is currently no way to extract the authentication realm from HttpClient 
except to extract the authentication challenge header and parse it manually.

Either the realm needs to be available to the client or a method in 
Authenticator should extract the realm from a given authentication header.

The same problems occurs with determining which type of authentication is 
being used and what other options there are (basic, digest, NTLM, others)."
1,"ItemInfoCacheImpl.getNodeInfo() and .getPropertyInfo() might not clear all relevant entriesItemInfoCacheImpl.getNodeInfo() and .getPropertyInfo() remove the retrieved entry from the cache.

since entries might be cached by id AND path, entires identified by path are not removed from the cache if they're retrieved by id."
0,"Caching in QueryHandler does not scale wellCaching in class CachingIndexReader uses too much memory. It uses around 500 bytes per node and does not use any strategy to limit the cache.

This improvement covers two goals:
- lower per-node memory cost for caching
- implement a caching strategy using e.g LRU algorithm"
0,"[PATCH] FilteredTermEnum code cleanupFilteredTermEnum's constructor takes two parameters but doesn't use them. This 
patch changes that and thus makes the code easier readable. Maybe the old 
constructor should be kept (as deprecated)? I'm not sure, this version seems 
cleaner to me."
0,Provide additional test coverage for HTTP and HTTPS over proxyHTTP and HTTPS over proxy test coverage is still insufficient
0,"[PATCH] XPathQueryBuilder reports misleading column numbers for faulty queriesXPathQueryBuilder returns an error string with the column offset where a parsing error occurred. Unfortunately this value is difficult to correlate to the users query string, as XPathQueryBuilder embellishes the query by doing

statement = ""for $v in "" + statement + "" return $v"";

This patch appends the modified statement to the query message so that the user can get the real position of the error."
1,"JVM bug 4949631 causes BufferOverflowException in HttpMethodBase.getResponseBodyAsStringava.nio.BufferOverflowException
        at java.nio.charset.CoderResult.throwException(CoderResult.java:259)
        at java.lang.StringCoding$CharsetSD.decode(StringCoding.java:188)
        at java.lang.StringCoding.decode(StringCoding.java:224)
        at java.lang.String.<init>(String.java:320)
        at
org.apache.commons.httpclient.HttpConstants.getContentString(HttpConstants.java:199)
        at
org.apache.commons.httpclient.HttpConstants.getContentString(HttpConstants.java:233)
        at
org.apache.commons.httpclient.HttpMethodBase.getResponseBodyAsString(HttpMethodBase.java:735)


This seems to be caused by a known JVM bug:
http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4949631

Strings over 16Mb can cause the problem.   Some workarounds are listed, the
essence being to split the string and call getBytes on each piece and reassemble
with a ByteBuffer."
0,"HttpState#PREEMPTIVE_PROPERTY removed.Our code no longer compiles as HttpState#PREEMPTIVE_PROPERTY has been removed.
Our code compiles with 2.0.1.

See: 
http://jakarta.apache.org/commons/httpclient/apidocs/org/apache/commons/httpclient/HttpState.html#PREEMPTIVE_PROPERTY"
0,"Setup nightly builds for JackrabbitOnce the Jackrabbit zone is created (see INFRA-1008), setup nightly Jackrabbit builds as discussed in http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/9190/.
"
0,"Contrib Analyzer Setters should be deprecated and replace with ctor argumentsSome analyzers in contrib provide setters for stopword / stem exclusion sets / hashtables etc. Those setters should be deprecated as they yield unexpected behaviour. The way they work is they set the reusable token stream instance to null in a thread local cache which only affects the tokenstream in the current thread. Analyzers itself should be immutable except of the threadlocal. 

will attach a patch soon."
1,"norms reading fails with FileNotFound in exceptional caseIf we can't get to the bottom of this, we can always add the fileExists check back...

{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriterExceptions
    [junit] Testcase: testRandomExceptionsThreads(org.apache.lucene.index.TestIndexWriterExceptions):	Caused an ERROR
    [junit] No sub-file with id _nrm.cfs found (fileName=_19_nrm.cfs files: [.fdt, .fnm, .per, .fdx])
    [junit] java.io.FileNotFoundException: No sub-file with id _nrm.cfs found (fileName=_19_nrm.cfs files: [.fdt, .fnm, .per, .fdx])
    [junit] 	at org.apache.lucene.store.CompoundFileDirectory.createSlicer(CompoundFileDirectory.java:313)
    [junit] 	at org.apache.lucene.store.CompoundFileDirectory.<init>(CompoundFileDirectory.java:65)
    [junit] 	at org.apache.lucene.codecs.lucene40.Lucene40DocValuesProducer.<init>(Lucene40DocValuesProducer.java:48)
    [junit] 	at org.apache.lucene.codecs.lucene40.Lucene40NormsFormat$Lucene40NormsDocValuesProducer.<init>(Lucene40NormsFormat.java:70)
    [junit] 	at org.apache.lucene.codecs.lucene40.Lucene40NormsFormat.docsProducer(Lucene40NormsFormat.java:49)
    [junit] 	at org.apache.lucene.codecs.lucene40.Lucene40NormsFormat.docsProducer(Lucene40NormsFormat.java:62)
    [junit] 	at org.apache.lucene.index.SegmentCoreReaders.<init>(SegmentCoreReaders.java:122)
    [junit] 	at org.apache.lucene.index.SegmentReader.<init>(SegmentReader.java:54)
    [junit] 	at org.apache.lucene.index.DirectoryReader$1.doBody(DirectoryReader.java:65)
    [junit] 	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:660)
    [junit] 	at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:55)
    [junit] 	at org.apache.lucene.index.IndexReader.open(IndexReader.java:242)
    [junit] 	at org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:304)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$3$1.evaluate(LuceneTestCase.java:530)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit] 
    [junit] 
    [junit] Tests run: 22, Failures: 0, Errors: 1, Time elapsed: 3.439 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testRandomExceptionsThreads -Dtests.seed=-4ea45cb40d17460b:-459bfb455a2351b9:1abd8f0f3a0611b9 -Dargs=""-Dfile.encoding=UTF-8""
    [junit] NOTE: test params are: codec=Lucene40: {field=MockVariableIntBlock(baseBlockSize=31), id=PostingsFormat(name=NestedPulsing), content=Pulsing40(freqCutoff=2 minBlockSize=58 maxBlockSize=186), contents=MockVariableIntBlock(baseBlockSize=31), content1=MockVariableIntBlock(baseBlockSize=31), content2=PostingsFormat(name=MockSep), content4=Pulsing40(freqCutoff=2 minBlockSize=58 maxBlockSize=186), content5=MockFixedIntBlock(blockSize=964), content6=PostingsFormat(name=Memory), content7=PostingsFormat(name=MockRandom), crash=PostingsFormat(name=NestedPulsing), subid=PostingsFormat(name=NestedPulsing)}, sim=RandomSimilarityProvider(queryNorm=false,coord=true): {other=DFR GB3(800.0), contents=IB SPL-L3(800.0), content=DFR GL3(800.0), id=DFR I(F)L1, field=IB LL-DZ(0.3), content1=DFR I(ne)BZ(0.3), content2=DFR I(n)3(800.0), content3=DFR GZ(0.3), content4=DFR I(ne)B2, content5=IB LL-L3(800.0), content6=IB SPL-D2, crash=DFR I(F)3(800.0), content7=DFR I(F)B3(800.0), subid=IB LL-L1}, locale=de_CH, timezone=Canada/Saskatchewan
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestAssertions, TestNumericTokenStream, TestSimpleAttributeImpl, TestImpersonation, TestPulsingReuse, TestDocument, TestAddIndexes, TestAtomicUpdate, TestByteSlices, TestCheckIndex, TestConcurrentMergeScheduler, TestConsistentFieldNumbers, TestCrashCausesCorruptIndex, TestDocCount, TestDocumentWriter, TestFlex, TestForceMergeForever, TestIndexInput, TestIndexReader, TestIndexWriterConfig, TestIndexWriterExceptions]
    [junit] NOTE: Linux 3.0.0-14-generic amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=186661872,total=245104640
{noformat}
"
0,"[PATCH] new method: Document.remove()Here's a patch that adds a remove() method to the Document class (+test case). This 
is very useful if you have converter classes that return a Lucene Document object but 
you need to make changes to that object. 
 
In my case, I wanted to index PDF files that were saved as BLOBs in a database. The 
files need to be saved to a temporary file and that file name is given to the PDF 
converter class. The PDF converter then saves the name of the temporary file name 
as the file name, which doesn't make sense. So my code needs to remove the 
'filename' field and re-add it, this time with the columns primary ID. This is only possible 
with the attached patch."
0,JSR 283: Shareable nodes support in query
0,"Document number integrity merge policyThis patch allows for document numbers stays the same even after merge of segments with deletions.

Consumer needs to do this:
indexWriter.setSkipMergingDeletedDocuments(false);

The effect will be that deleted documents are replaced by a new Document() in the merged segment, but not marked as deleted. This should probably be some policy thingy that allows for different solutions such as keeping the old document, et c.

Also see http://www.nabble.com/optimization-behaviour-tf3723327.html#a10418880
"
0,"Implement search facility for users and groupsImplement a search facility for users and groups supporting:
- search for users and/or groups with a certain property (value) either directly on the user/group node or on any of its sub nodes
- full text search on user and/or group nodes and its sub nodes
- inclusion/exclusion based on group membership: i.e. restricting search to members of a group or to groups with a certain member
- ordering 
- paging"
0,"Upgrade to Tika 0.8Apache Tika version 0.8 is now available, and we should upgrade to benefit from the various fixes and improvements included in that version."
1,"org.apache.lucene.search.BooleanQuery$TooManyClauses when using '>' operatorwhen using a query with a '>' operator, the query engine does not scale with number of matching properties and a org.apache.lucene.search.BooleanQuery$TooManyClauses exception is thrown"
1,SQL2 ISDESCENDANTNODE can throw BooleanQuery#TooManyClauses if there are too many matching child nodesRunning a query that has a ISDESCENDANTNODE clause can easily go over the max clause limit from lucene's BooleanQuery when there's a bigger hierarchy involved.
0,"Add support for query result highlightingHighlighting matches in a query result list is regularly needed for an application. The query languages should support a pseudo property or function that allows one to retrieve text fragments with highlighted matches from the content of the matching node.

To support this feature the following enhancements are required:
- define a pseudo property or function that returns the text excerpt and can be used in the select clause
- the index needs to *store* the original text it used when the node was indexed. this also includes extracted text from binary properties.
- text fragments must be created based on the original text, the query and index information"
1,"Bundle Persistence Manager error - failing to read bundle the first timeCode:
NodeIterator entiter = null;
Node root = null, contNode = null, entsNode = null;

try
{
    root = session.getRootNode();
    contNode = root.getNode(""sr:cont"");
    entsNode = contNode.getNode(""sr:ents"");
    entiter = entsNode.getNodes();
}
catch (Exception e)
{
    logger.error(""Getting ents nodes"", e);
}

Output:
12359 [http-8080-Processor24] ERROR org.apache.jackrabbit.core.persistence.bundle.BundleDbPersistenceManager - failed to read bundle: c3a09c19-cc6b-45bd-a42e-c4c925b67d02: java.io.IOException: ERROR 40XD0: Container has been closed
12375 [http-8080-Processor24] ERROR com.taxila.editor.sm.RepoOperations - Getting ents nodes
javax.jcr.PathNotFoundException: sr:ents
    at org.apache.jackrabbit.core.NodeImpl.getNode(NodeImpl.java:2435)
    at com.taxila.editor.sm.RepoOperations.getEntityNodes (RepoOperations.java:4583)
    at com.taxila.editor.sm.RepoOperations.displayEntities(RepoOperations.java:4159)
    at com.taxila.editor.sm.RepoOperations.displayEntities(RepoOperations.java:4114)
    at com.taxila.editor.em.um.MainEntityForm.reset (MainEntityForm.java:215)
    at org.apache.struts.taglib.html.FormTag.doStartTag(FormTag.java:640)
    at org.apache.jsp.pages.jsp.entity.MainEntity_jsp._jspService(MainEntity_jsp.java:414)
    at org.apache.jasper.runtime.HttpJspBase.service (HttpJspBase.java:97)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
    at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:332)
    at org.apache.jasper.servlet.JspServlet.serviceJspFile (JspServlet.java:314)
    at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:264)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter (ApplicationFilterChain.java:252)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:173)
    at org.apache.catalina.core.ApplicationDispatcher.invoke(ApplicationDispatcher.java :672)
    at org.apache.catalina.core.ApplicationDispatcher.processRequest(ApplicationDispatcher.java:463)
    at org.apache.catalina.core.ApplicationDispatcher.doForward(ApplicationDispatcher.java:398)
    at org.apache.catalina.core.ApplicationDispatcher.forward (ApplicationDispatcher.java:301)
    at org.apache.struts.action.RequestProcessor.doForward(RequestProcessor.java:1014)
    at org.apache.struts.action.RequestProcessor.processForwardConfig(RequestProcessor.java:417)
    at org.apache.struts.action.RequestProcessor.processActionForward(RequestProcessor.java:390)
    at org.apache.struts.action.RequestProcessor.process(RequestProcessor.java:271)
    at org.apache.struts.action.ActionServlet.process (ActionServlet.java:1292)
    at org.apache.struts.action.ActionServlet.doPost(ActionServlet.java:510)
    at javax.servlet.http.HttpServlet.service(HttpServlet.java:709)
    at javax.servlet.http.HttpServlet.service (HttpServlet.java:802)
    at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:252)
    at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java :173)
    at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:213)
    at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:178)
    at org.apache.catalina.core.StandardHostValve.invoke (StandardHostValve.java:126)
    at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:105)
    at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:107)
    at org.apache.catalina.connector.CoyoteAdapter.service (CoyoteAdapter.java:148)
    at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:869)
    at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.processConnection(Http11BaseProtocol.java :664)
    at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpoint.java:527)
    at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFollowerWorkerThread.java:80)
    at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run (ThreadPool.java:684)
    at java.lang.Thread.run(Unknown Source)

On the other hand if I do this:
Code:
try
{
    root = session.getRootNode ();
    contNode = root.getNode(""sr:cont"");
    entsNode = contNode.getNode(""sr:ents"");
    entiter = entsNode.getNodes();
}
catch (Exception e)
{
    logger.error(""Getting ents nodes"", e);
    try
    {
        entsNode = contNode.getNode(""sr:ents"");
        entiter = entsNode.getNodes();
    }
    catch (Exception e1)
    {
        e1.printStackTrace();
    }
}

Output:
The first error as in the previous case comes, but the second execution of the entsNode = contNode.getNode(""sr:ents""); statement returns the right node, and hence the iterator."
1,"Not configuring the adminId, anonymousId, or defaultuserId causes login module to ignore credentialsUsing the DefaultLoginModule, DefaultAccessManager, and DefaultSecurityManager and calling Repository.login(Credentials) causes the following stack trace to be thrown.  

javax.jcr.LoginException: LoginModule ignored Credentials: LoginModule ignored Credentials: LoginModule ignored Credentials
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1353)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:53)
	at com.cerner.system.configuration.repository.jcr.JackrabbitTest.testLoginWithCredentials(JackrabbitTest.java:23)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:59)
	at org.junit.internal.runners.MethodRoadie.runTestMethod(MethodRoadie.java:98)
	at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:79)
	at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:87)
	at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77)
	at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42)
	at org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88)
	at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51)
	at org.junit.internal.runners.JUnit4ClassRunner$1.run(JUnit4ClassRunner.java:44)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37)
	at org.junit.internal.runners.JUnit4ClassRunner.run(JUnit4ClassRunner.java:42)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:45)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)
Caused by: javax.security.auth.login.FailedLoginException: LoginModule ignored Credentials
	at org.apache.jackrabbit.core.security.authentication.LocalAuthContext.login(LocalAuthContext.java:73)
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1346)
	... 24 more
javax.security.auth.login.FailedLoginException: LoginModule ignored Credentials
	at org.apache.jackrabbit.core.security.authentication.LocalAuthContext.login(LocalAuthContext.java:73)
	at org.apache.jackrabbit.core.RepositoryImpl.login(RepositoryImpl.java:1346)
	at org.apache.jackrabbit.commons.AbstractRepository.login(AbstractRepository.java:53)
	at com.cerner.system.configuration.repository.jcr.JackrabbitTest.testLoginWithCredentials(JackrabbitTest.java:23)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.junit.internal.runners.TestMethod.invoke(TestMethod.java:59)
	at org.junit.internal.runners.MethodRoadie.runTestMethod(MethodRoadie.java:98)
	at org.junit.internal.runners.MethodRoadie$2.run(MethodRoadie.java:79)
	at org.junit.internal.runners.MethodRoadie.runBeforesThenTestThenAfters(MethodRoadie.java:87)
	at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:77)
	at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:42)
	at org.junit.internal.runners.JUnit4ClassRunner.invokeTestMethod(JUnit4ClassRunner.java:88)
	at org.junit.internal.runners.JUnit4ClassRunner.runMethods(JUnit4ClassRunner.java:51)
	at org.junit.internal.runners.JUnit4ClassRunner$1.run(JUnit4ClassRunner.java:44)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:27)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:37)
	at org.junit.internal.runners.JUnit4ClassRunner.run(JUnit4ClassRunner.java:42)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:45)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)

A testcase and repository.xml file will be attached shortly."
0,"Directory createOutput and openInput should take an IOContextToday for merging we pass down a larger readBufferSize than for searching because we get better performance.

I think we should generalize this to a class (IOContext), which would hold the buffer size, but then could hold other flags like DIRECT (bypass OS's buffer cache), SEQUENTIAL, etc.

Then, we can make the DirectIOLinuxDirectory fully usable because we would only use DIRECT/SEQUENTIAL during merging.

This will require fixing how IW pools readers, so that a reader opened for merging is not then used for searching, and vice/versa.  Really, it's only all the open file handles that need to be different -- we could in theory share del docs, norms, etc, if that were somehow possible."
0,"Transfer-Encoding: identity not supported + possible patchIn HttpMethodBase.readResponseBody only chunked transfer encoding is 
supported.  Some proxy servers like Privoxy, etc send a Transfer-Encoding: 
identity header and HttpClient fails quietly and returns a null result input 
stream.  At line 2037 in HttpMethodBase.java revision 1.160 I inserted the 
following two lines and it appeared to work fine:

} else if (""identity"".equalsIgnoreCase(transferEncodingHeader.getValue())) {
   result = is;

I think it should at least throw an exception or do something when it 
encounters an unsupported Transfer-Encoding instead of returning a null input 
stream."
0,"CheckPermissionTest-testCheckPermission() doesn't allow config of node type to be createdCheckPermissionTest-testCheckPermission() doesn't allow configuration of node type to be created. Proposal to re-use the testNodeType config property.
"
0,"TCK: observation tests are too restrictiveThe basic sequence in all observation tests is:

1) add listener
2) modify workspace
3) remove listener
4) wait for events on listener

This sequence forces an implementation to maintain a logical order for listener registrations and content changes. In the light of the asynchronous nature of observation events this seems too restrictive for certain implementations.

The sequence should be changed to:

1) add listener
2) modify workspace
3) wait for events on listener
4) remove listener

Which is also more intuitive from a user perspective."
1,"errors in text filters can cause indexing to fail without warning the clienti've opened this issue to track the discussion at <http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/5086>. briefly, exceptions thrown by text filters are logged and swallowed by jackrabbit; there's no way for a text filter to signal to the jcr client that indexing failed.

some solutions have been proposed, including throwing an unchecked exception, which doesn't allow jackrabbit to maintain transactional integrity, and giving filters veto power over the observed repository operation. depending on the difficulty of the solution that is eventually determined to be correct, it may be sufficient for 1.0 to document the issue and perhaps improve the warning/error logging.
"
0,"ShingleFilter improvementsShingleFilter should allow configuration of minimum shingle size (in addition to maximum shingle size), so that it's possible to (e.g.) output only trigrams instead of bigrams mixed with trigrams.  The token separator used in composing shingles should be configurable too."
0,workspace-wide default for lock timeoutThere should be a way to define a workspace-wide default for JCR lock timeouts (in case the code creating the lock did not specify one).
0,"Reduce memory usage of ParentAxisScorerThe ParentAxisScorer keeps a map of non-default scores while it calculates the parent matches of the context scorer. In most cases the scores are not equal to the default score, but still may be all the same.

The scorer should therefore use the first score value as the default instead of the currently used 1.0f."
0,"add checks to MockTokenizer to enforce proper consumptionwe can enforce things like consumer properly iterates through tokenstream lifeycle
via MockTokenizer. this could catch bugs in consumers that don't call reset(), etc."
1,"Jcr-Server: BasicCredentialsProviderTest throws NPE if defaultAuthHeader init param misses the passwordissue reported by dominique jaeggi:

a missing-auth-header init param that has the form ""uid"" instead of ""uid:pw"" or ""uid:"" results in NPE upon SimpleCredentials creation.



"
1,"Query Builder and jcr:deref problem. Can't add predicate after jcr:derefCannot add a predicate (like [@property = 'value'] after a jcr:deref function.
The query builder throws an ""InvalidQueryException: Unsupported location for jcr:deref()"".

So for example, the query :

//element(*,nt:category)[@member]/jcr:deref(@member, '*')[@property='value'] 

is invalid and it should be valid.

"
1,"Connection with the proxy is not reopened if an proxy auth failure occurs while SSL tunnel is being establishedConnection with the proxy is not reopened if an proxy auth failure occurs while
SSL tunnel is being established.

This problem has been reported by on the httpclient-user by Gebhard Gaukler
<gebhard.gaukler at db.com>.

My bad.

Oleg"
0,"IndexReader's add/removeCloseListener should not use ConcurrentHashMap, just a synchronized setThe use-case for ConcurrentHashMap is when many threads are reading and less writing to the structure. Here this is just funny: The only reader is close(). Here you can just use a synchronized HashSet. The complexity of CHM is making this just a joke :-)"
0,provide option to automatically dispose idle workspaces
1,NPE in event polling threadThis exception occurs when running the jcr2dav integration tests. This surfaces as a  side effect of JCR-3046. The root cause is refresh(Event) not guarding against null values returned from Event.getItemId().
1,"ConstantScoreRangeQuery - fixes ""too many clauses"" exceptionConstantScoreQuery wraps a filter (representing a set of documents) and returns
a constant score for each document in the set.

ConstantScoreRangeQuery implements a RangeQuery that works for any number of
terms in the range.  It rewrites to a ConstantScoreQuery that wraps a RangeFilter.

Still needed:
  - unit tests (these classes have been tested and work fine in-house, but the
current tests rely on too much application specific code)
  - code review of Weight() implementation (I'm unsure If I got all the score
normalization stuff right)
  - explain() implementation

NOTE: requires Java 1.4 for BitSet.nextSetBit()"
0,"Weight is not serializable for some of the queriesIn order to work with ParallelMultiSearcher, Query weights need to be serializable.  The interface Weight extends java.io.Serializable, but it appears that some of the newer queries unnecessarily store global state in their weights, thus causing serialization errors."
0,"Spellchecker should take IndexWriterConfig... deprecate old methods?When looking at LUCENE-3490, i realized there was no way to specify the codec for the spellchecker to use.

It has the following current methods:
* indexDictionary(Dictionary dict): this causes optimize!
* indexDictionary(Dictionary dict, int mergeFactory, int ramMB): this causes optimize!
* indexDictionary(Dictionary dict, int mergeFactor, int ramMB, boolean optimize)

But no way to specify an IndexwriterConfig. Additionally, I don't like that several of these ctors force an optimize in a tricky way,
even though it was like this all along.

So I think we should add indexDictionary(Dictionary dict, IndexWriterConfig config, boolean optimize).

We should either deprecate all the other ctors in 3.x and nuke in trunk, or at least add warnings to the ones that optimize."
1,"DistanceFilter problem with deleted documentsI know this is the locallucene lib, but wanted to make sure we don't get this bug when it gets into lucene contrib.

I suspect that the issue is that deleted documents are trying to be evaluated by the filter.  I did some debugging and I confirmed that it is bombing on a document that is marked as deleted (using Luke).


Thanks!

Using the locallucene library 1.51, I get a NullPointerException at line 123 of DistanceFilter
The method is 	public BitSet bits(IndexReader reader) 
The line is double x = NumberUtils.SortableStr2double(sx);

The stack trace is:
java.lang.NullPointerException
	at org.apache.solr.util.NumberUtils.SortableStr2long(NumberUtils.java:149)
	at org.apache.solr.util.NumberUtils.SortableStr2double(NumberUtils.java:104)
	at com.pjaol.search.geo.utils.DistanceFilter.bits(DistanceFilter.java:123)
	at org.apache.lucene.search.Filter.getDocIdSet(Filter.java:49)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:140)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:112)
	at org.apache.lucene.search.Hits.getMoreDocs(Hits.java:113)
	at org.apache.lucene.search.Hits.<init>(Hits.java:90)
	at org.apache.lucene.search.Searcher.search(Searcher.java:72)"
0,"A Property and a Node Can Have the Same Name according to paragraph ""3.3.4"" of the of the JSR 283 specification (Public Review Draft), a property and a node can have the same name. "
0,"Change default value for respectDocumentOrderThe current default value for the search index configuration parameter respectDocumentOrder is true. Almost all applications are not interested in document order, while this default adds significant overhead to query execution because document order information is present in the index but has to be calculated over the complete result set.

I propose to change the default value to false and document this change in the 1.4 release notes. If an application relies on document order one can still explicitly set the parameter in the configuration to true."
1,"Directory#copy leaks file handlesDirectory#copy doesn't close the target directories output stream if sourceDir.openInput(srcFile) throws an Exception. Before LUCENE-3218 Directory#copy wasn't used extensively so this wasn't likely to happen during tests. Today we had a failure on the 3.x branch that is likely caused by this bug:

{noformat}
[junit] Testsuite: org.apache.lucene.index.TestAddIndexes
    [junit] Testcase: testAddIndexesWithRollback(org.apache.lucene.index.TestAddIndexes):	Caused an ERROR
    [junit] MockDirectoryWrapper: cannot close: there are still open files: {_co.cfs=1}
    [junit] java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_co.cfs=1}
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:483)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads.closeDir(TestAddIndexes.java:693)
    [junit] 	at org.apache.lucene.index.TestAddIndexes.testAddIndexesWithRollback(TestAddIndexes.java:924)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1277)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1195)
    [junit] Caused by: java.lang.RuntimeException: unclosed IndexOutput: _co.cfs
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.addFileHandle(MockDirectoryWrapper.java:410)
    [junit] 	at org.apache.lucene.store.MockCompoundFileDirectoryWrapper.<init>(MockCompoundFileDirectoryWrapper.java:39)
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.createCompoundOutput(MockDirectoryWrapper.java:439)
    [junit] 	at org.apache.lucene.index.SegmentMerger.createCompoundFile(SegmentMerger.java:128)
    [junit] 	at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:3101)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes3.doBody(TestAddIndexes.java:839)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:667)
    [junit] 
    [junit] 
    [junit] Tests run: 18, Failures: 0, Errors: 1, Time elapsed: 9.034 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] java.lang.IllegalStateException: CFS has pending open files
    [junit] 	at org.apache.lucene.store.CompoundFileWriter.close(CompoundFileWriter.java:143)
    [junit] 	at org.apache.lucene.store.CompoundFileDirectory.close(CompoundFileDirectory.java:181)
    [junit] 	at org.apache.lucene.store.DefaultCompoundFileDirectory.close(DefaultCompoundFileDirectory.java:58)
    [junit] 	at org.apache.lucene.store.MockCompoundFileDirectoryWrapper.close(MockCompoundFileDirectoryWrapper.java:55)
    [junit] 	at org.apache.lucene.index.SegmentMerger.createCompoundFile(SegmentMerger.java:139)
    [junit] 	at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:3101)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$CommitAndAddIndexes3.doBody(TestAddIndexes.java:839)
    [junit] 	at org.apache.lucene.index.TestAddIndexes$RunAddIndexesThreads$1.run(TestAddIndexes.java:667)
{noformat}"
0,"wrong exception from NativeFSLockFactory (LIA2 test case)As part of integrating Lucene In Action 2 test cases (LUCENE-2661), I found one of the test cases fail

the test is pretty simple, and passes on 3.0. The exception you get instead (LockReleaseFailedException) is 
pretty confusing and I think we should fix it.
"
0,"Query Syntax page does not make it clear that wildcard searches are not allowed in Phrase QueriesThe queryparsersyntax page which is where I expect most novices (such as myself) start with lucene seems to indicate that wildcards can be used in phrase terms

Quoting:
'Terms: A query is broken up into terms and operators. There are two types of terms: Single Terms and Phrases.
A Single Term is a single word such as ""test"" or ""hello"".
A Phrase is a group of words surrounded by double quotes such as ""hello dolly"".

....

Wildcard Searches
Lucene supports single and multiple character wildcard searches.
To perform a multiple character wildcard search use the ""*"" symbol.
Multiple character wildcard searches looks for 0 or more characters. For example, to search for test, tests or tester, you can use the search:

test*
You can also use the wildcard searches in the middle of a term.

'
there is nothing to indicate in the section on Wildcard Searches that it can be performed only on Single word terms not Phrase terms.

Chris  argues 'that there is nothing in the description of a Phrase to indicate that it can be anything other then what it says ""a group of words surrounded by double quotes"" .. at no point does it
suggest that other types of queries or syntax can be used inside the quotes.  likewise the discussion of Wildcards makes no mention of phrases to suggest that wildcard characters can be used in a phrase.'
but I don't accept this because there is nothing in the description of a Single Term either to indicate it can use wildcards either. Wildcards are only mentioned in the Wildcard section and there it says thay can be used in a term, it does not restrict the type of term


I Propose a simple solution modify:

Lucene supports single and multiple character wildcard searches.

to 

Lucene supports single and multiple character wildcard searches within single terms.

(Chris asked for a patch, but Im not sure how to do this, but the change is simple enough)



"
0,"TermAttribute.termLength() optimization
   public int termLength() {
     initTermBuffer(); // This patch removes this method call 
     return termLength;
   }

I see no reason to initTermBuffer() in termLength()... all tests pass, but I could be wrong?

"
0,Create default repository in targetOne of the JCR API tests gets a default repository from the RepositoryFactory. This default repository should use a home directory under target.
1,"While you could use a custom Sort Comparator source with remote searchable before, you can no longer do so with FieldComparatorSourceFieldComparatorSource is not serializable, but can live on a SortField"
0,"Pass resultFetchSize/limit hint to SortedLuceneQueryHitsThe SortedLuceneQueryHits currently uses a default value of 100 (taken from lucene) for initially retrieved and sorted results. For larger result sets this is not optimal because it will cause re-execution of the underlying query with values 200, 400, 800, 1600, 3200, 6400, etc. Instead the query hits should get the limit that is set on the query or the resultFetchSize configured for the SearchIndex."
1,"MultiReader.numDocs incorrect after undeleteAllCalling MultiReader.undeleteAll does not clear cached numDocs value. So the subsequent numDocs() call returns a wrong value if there were deleted documents in the index. Following patch fixes the bug and adds a test showing the issue.


Index: src/test/org/apache/lucene/index/TestMultiReader.java
===================================================================
--- src/test/org/apache/lucene/index/TestMultiReader.java       (revision 354923)
+++ src/test/org/apache/lucene/index/TestMultiReader.java       (working copy)
@@ -69,6 +69,18 @@
     assertTrue(vector != null);
     TestSegmentReader.checkNorms(reader);
   }
+
+  public void testUndeleteAll() throws IOException {
+    sis.read(dir);
+    MultiReader reader = new MultiReader(dir, sis, false, readers);
+    assertTrue(reader != null);
+    assertEquals( 2, reader.numDocs() );
+    reader.delete(0);
+    assertEquals( 1, reader.numDocs() );
+    reader.undeleteAll();
+    assertEquals( 2, reader.numDocs() );
+  }
+

   public void testTermVectors() {
     MultiReader reader = new MultiReader(dir, sis, false, readers);
Index: src/java/org/apache/lucene/index/MultiReader.java
===================================================================
--- src/java/org/apache/lucene/index/MultiReader.java   (revision 354923)
+++ src/java/org/apache/lucene/index/MultiReader.java   (working copy)
@@ -122,6 +122,7 @@
     for (int i = 0; i < subReaders.length; i++)
       subReaders[i].undeleteAll();
     hasDeletions = false;
+    numDocs = -1;      // invalidate cache
   }

   private int readerIndex(int n) {    // find reader for doc n:"
0,InstantiatedIndex supports non-optimized IndexReadersInstantiatedIndex does not currently support non-optimized IndexReaders.  
1,"PayloadTermQuery's explain is broken when span score is not includedWhen setting includeSpanScore to false with PayloadTermQuery, the explain is broken."
1,"recoverable exceptions when reading are not retriedIf a recoverable exception occurs after a request is written then the method is
not retried."
1,"Embedded Derby fails under JBoss because of JMX-related conflictsJBoss fails to start due to a bug in Derby-10.4.2.0. The dependency should be agains derby-10.4.2.1 which seems to has this bug fixed. More info at https://issues.apache.org/jira/browse/DERBY-3887

Please, include this fix in the upcoming 1.6.3"
0,jcr-tests: make property value(s) and property type(s) configurabletest-cases using Node.setProperty or Property.setValue mostly hardcode the value... there should be the possibility to specify type and value in the config.
1,"creating empty field + empty term leads to invalid indexSpinoff from LUCENE-3526.

* if you create new Field("""", """"), you get IllegalArgumentException from Field's ctor: ""name and value cannot both be empty""
* But there are tons of other ways to index an empty term for the empty field (for example initially make it ""garbage"" then .setValue(""""), or via tokenstream).
* If you do this, and you have assertions enabled, you will trip an assert (the assert is fixed in trunk, in LUCENE-3526)
* But If you don't have assertions enabled, you will create a corrupt index: test: terms, freq, prox...ERROR [term : docFreq=1 != num docs seen 0 + num docs deleted 0]
"
0,"Spellchecker ""Suggest Mode"" SupportThis is a spin-off from SOLR-2585.

Currently o.a.l.s.s.SpellChecker and o.a.l.s.s.DirectSpellChecker support two ""Suggest Modes"":
1. Suggest for terms that are not in the index.
2. Suggest ""more popular"" terms.

This issue is to add a third Suggest Mode:
3. Suggest always.

This will assist users in developing context-sensitive spell suggestions and/or did-you-mean suggestions.  See SOLR-2585 for a full discussion.

Note that o.a.l.s.s.SpellChecker already can support this functionality, if the user passes in a NULL term & IndexReader.  This, however, is not intutive.  o.a.l.s.s.DirectSpellChecker currently has no support for this third Suggest Mode."
1,"wrong charset indication in HttpConstants.getContentString()Around line 236 in HttpConstants.getConstentString() the charset is wrongly indicated as 
""DEFAULT_CONTENT_CHARSET"" where it should have been indicated as ""charset"" like in the 
getContentBytes function.

            if (LOG.isWarnEnabled()) {
                LOG.warn(""Unsupported encoding: "" 
                    + DEFAULT_CONTENT_CHARSET // <== should be the variable ""charset"" here
                    + "". Default HTTP encoding used"");
            }

Wrong copy/paste I guess :-)

ZC."
0,"move JDK collation to core, ICU collation to ICU contribAs mentioned on the list, I propose we move the JDK-based CollationKeyFilter/CollationKeyAnalyzer, currently located in contrib/collation into core for collation support (language-sensitive sorting)

These are not much code (the heavy duty stuff is already in core, IndexableBinaryString). 

And I would also like to move the ICUCollationKeyFilter/ICUCollationKeyAnalyzer (along with the jar file they depend on) also currently located in contrib/collation into a contrib/icu.

This way, we can start looking at integrating other functionality from ICU into a fully-fleshed out icu contrib.
"
0,"[PATCH] Use filter bits for next() and skipTo() in FilteredQueryThis improves performance of FilteredQuery by not calling score() 
on documents that do not pass the filter. 
This passes the current tests for FilteredQuery, but these tests 
have not been adapted/extended."
0,"ClientConnectionRelease example is incorrecthttp://svn.apache.org/repos/asf/httpcomponents/httpclient/tags/4.0.1/httpclient/src/examples/org/apache/http/examples/client/ClientConnectionRelease.java

is incorrect: 

1. if error happens in BufferedReader constructor (OutOfMemoryError, StackOverflowError), reader.close() is not called and connection is not released

2. if error happens in reader.readLine(), reader.close() is called, but httpget.abort() is not."
0,"Add the Data Store to the Jackrabbit APICurrently, the garbage collection is not part of the Jackrabbit API. However, the data store garbage collection must be used once in a while if the data store is enabled. I propose to add the required interfaces to the Jackrabbit API. This will also allow to call garbage collection using RMI."
0,"MultiStatusResponse should not call resource.getPropertiescurrent constructor MultiStatusResponse() calls resource.getProperties() even if propFindType == PROPFIND_BY_PROPERTY.

This is inconvenient, because some properties are expensive to generate if they are not requested. MultiStatusResponse() constructor with parameter PROPFIND_BY_PROPERTY should do:

===
if (propFindType == PROPFIND_BY_PROPERTY) {
  for (propName : propNameSet) {
    prop = resource.getProperty(propName);
    if (prop != null)
      status200.addContent(prop);
    else
      status404.addContent(propName);
  }
} else {
  ...
}
==="
1,"NTLM Proxy and basic host authorizationUsing a Microsoft proxy with NTLM validation enabled the authorization against a
remote host does not work. This, of course, assuming that the page is correctly
fetched (which currently is not), see the NTLM authentication bug number 24327"
1,"basetokenstreamtestcase should fail if tokenstream starts with posinc=0This is meaningless for a tokenstream to start with posinc=0,

Its also caused problems and hairiness in the indexer (LUCENE-1255, LUCENE-1542),
and it makes senseless tokenstreams. We should add a check and fix any that do this.

Furthermore the same bug can exist in removing-filters if they have enablePositionIncrements=false.
I think this option is useful: but it shouldnt mean 'allow broken tokenstream', it just means we
don't add gaps. 

If you remove tokens with enablePositionIncrements=false it should not cause the TS to start with
positionincrement=0, and it shouldnt 'restructure' the tokenstream (e.g. moving synonyms on top of a different word).
It should just not add any 'holes'.
"
0,"Problem with formerly escaped JCR node names when upgrading to Jackrabbit 2.2.9The following unit test fails:

{code}
import static org.junit.Assert.*;

import org.apache.jackrabbit.util.Text;
import org.junit.Test;

public class TestEscaping
{
   @Test
   public void testEscaping() throws Exception
   {
      // expect this as an escaped string (e.g. formerly escaped with jackrabbit 1.6)
      String escaped = ""nam%27e"";
      String unescaped = Text.unescapeIllegalJcrChars(escaped);
      assertEquals(escaped, Text.escapeIllegalJcrChars(unescaped));
   }
}
{code}

This is a major problem when upgrading from 1.6.x to 2.2.9. The node names that were escaped in jackrabbit 1.6 are not longer escaped and that breaks the backward compatibility. I think the problem comes in with JCR-2198. "
0,"add target jvm in maven properties for compilationactually the compatibility level for sources/binaries is not defined in project.properties, so if you compile jackrabbit with a 1.5 jdk it will not run on older vm.

It would be nice to add the following properties to assure that the generated jar will work on different jvms:
maven.compile.target=1.4
maven.compile.source=1.4

(or 1.3 if you are targetting also java 1.3)"
1,"DocumentWriter closes TokenStreams too earlyThe DocumentWriter closes a TokenStream as soon as it has consumed its tokens. The javadoc of TokenStream.close() says that it releases resources associated with the stream. However, the DocumentWriter keeps references of the resources (i. e. payload byte arrays, term strings) until it writes the postings to the new segment, which means that DocumentWriter should call TokenStream.close() after it has written the postings.

This problem occurs in multithreaded applications where e. g. pooling is used for the resources. My patch adds a new test to TestPayloads which shows this problem. Multiple threads add documents with payloads to an index and use a pool of byte arrays for the payloads. TokenStream.close() puts the byte arrays back into the pool. The test fails with the old version but runs successfully with the patched version. 

All other units tests pass as well.
"
0,Backport FSTs to 3.x
0,"integrate snowball stopword listsThe snowball project creates stopword lists as well as stemmers, example: http://svn.tartarus.org/snowball/trunk/website/algorithms/english/stop.txt?view=markup

This patch includes the following:
* snowball stopword lists for 13 languages in contrib/snowball/resources
* all stoplists are unmodified, only added license header and converted each one from whatever encoding it was in to UTF-8
* added getSnowballWordSet  to WordListLoader, this is because the format of these files is very different, for example it supports multiple words per line and embedded comments.

I did not add any changes to SnowballAnalyzer to actually automatically use these lists yet, i would like us to discuss this in a future issue proposing integrating snowball with contrib/analyzers.
"
1,"Contrib query org.apache.lucene.search.BoostingQuery sets boost on constructor Query, not cloned copyBoostingQuery sets the boost value on the passed context Query

    public BoostingQuery(Query match, Query context, float boost) {
      this.match = match;
      this.context = (Query)context.clone();        // clone before boost
      this.boost = boost;

      context.setBoost(0.0f);                      // ignore context-only matches
    }

This should be 
      this.context.setBoost(0.0f);                      // ignore context-only matches

Also, boost value of 0.0 may have wrong effect - see discussion at

http://www.mail-archive.com/java-user@lucene.apache.org/msg12243.html 

"
1,NRTCachingDirectory.deleteFile always throws exceptionSilly bug.
0,CND support in jackrabbit-jcr-commonsIt would be nice if the CND parsing functionality in spi-commons could be made available in jcr-commons for use by JCR clients that shouldn't have to know anything about the SPI.
1,"Binary field content lost during optimizeScenario:

* create an index with arbitrary content, and close it
* open IndexWriter again, and add a document with binary field (stored but not compressed)
* close IndexWriter _without_ optimizing, so that the new document is in a separate segment.
* open IndexReader. You can read the last document and its binary field just fine.
* open IndexWriter, optimize the index, close IndexWriter
* open IndexReader. Now the field is still present (not null) and is marked as binary, but the data is not there - Field.getBinaryLength() returns 0.
"
1,"fix reverseStringFilter for unicode 4.0ReverseStringFilter is not aware of supplementary characters: when it reverses it will create unpaired surrogates, which will be replaced by U+FFFD by the indexer (but not at query time).
The wrong words will conflate to each other, and the right words won't match, basically the whole thing falls apart.

This patch implements in-place reverse with the algorithm from apache harmony AbstractStringBuilder.reverse0()
"
1,"charset in Content-Type header shouldn't be in quotesThe charset value in the Content-Type header returned from IOUtil.buildContentType is enclosed in quotes. This value should be a token which does not include double quotes.

Index: C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/server/io/IOUtil.java
===================================================================
--- C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/server/io/IOUtil.java	(revision 397215)
+++ C:/jprojects/eclipse/jackrabbit/jcr-server/server/src/java/org/apache/jackrabbit/server/io/IOUtil.java	(working copy)
@@ -112,7 +112,7 @@
     public static String buildContentType(String mimeType, String encoding) {
         String contentType = mimeType;
         if (contentType != null && encoding != null) {
-            contentType += ""; charset=\"""" + encoding + ""\"""";
+            contentType += ""; charset="" + encoding;
         }
         return contentType;
     }
"
1,"HttpClient:- Connections not released when SSL Tunneling fails.Trying to use HTTPS, and SSL tunneling fails as expected because the host is not accepted by the squid proxy, so squid proxy return 403. 

The problem I am seeing is that, when ever this happens the connections are not released to the pool. I traced the code and it appears that in 
HttpMethidDirector.java:  executeWithRetry()
when executeConnect() return false and there is no retry, the connections are not released.

Is this expected? Or am I doing something wrong."
0,"SPI: Javadoc Issue with QNodeTypeDefinition#getPropertyDefs and #getChildNodeDefsJavadoc of the mentioned methods currently states:

@return an array containing the property definitions or
     *         <code>null</code> if not set.

while the default implementation returns an empty array, which i find much nicer.

if nobody objects, i would fix the javadoc accordingly."
0,"EdgeNGram* documentation improvementTo clarify what ""edge"" means, I added some description. That edge means the beggining edge of a term or ending edge of a term."
1,"Error downloading text file with gzip content encodingHello I am getting an exception when I try to download certain files.

I don't have control over the host server, only the client.  Here's my client code:

		HttpParams params = new BasicHttpParams();
		params.setParameter(CoreConnectionPNames.CONNECTION_TIMEOUT, 300000L);
		params.setParameter(ClientPNames.HANDLE_REDIRECTS, true);

		// This client indicates to servers that it will support 'gzip'
		// and 'deflate' compressed responses.
		ContentEncodingHttpClient.setDefaultHttpParams(params);
		ContentEncodingHttpClient client = new ContentEncodingHttpClient();

		if (user != null && password != null) {
			String hostname = url.getHost();
			HttpHost hostHttp = new HttpHost(hostname, 80, ""http"");
			HttpHost hostHttps = new HttpHost(hostname, 443, ""https"");
			client.getCredentialsProvider().setCredentials(
			        new AuthScope(hostname, 80), 
			        new UsernamePasswordCredentials(user, password));
	
			client.getCredentialsProvider().setCredentials(
			        new AuthScope(hostname, 443), 
			        new UsernamePasswordCredentials(user, password));
	
			// Create AuthCache instance
			AuthCache authCache = new BasicAuthCache();
			// Generate BASIC scheme object and add it to the local auth cache
			BasicScheme basicAuth = new BasicScheme();
			authCache.put(hostHttp, basicAuth);
			authCache.put(hostHttps, basicAuth);
	
			// Add AuthCache to the execution context
			BasicHttpContext localcontext = new BasicHttpContext();
			localcontext.setAttribute(ClientContext.AUTH_CACHE, authCache);
		}
		HttpGet httpget = new HttpGet(url.toString());
		httpget.setHeader(""If-Modified-Since"", lastModified);


		HttpResponse response = client.execute(httpget);
		responseCode = response.getStatusLine().getStatusCode();
		HttpEntity entity = response.getEntity();
		if (responseCode == HttpStatus.SC_NOT_MODIFIED) {
			
		} else if (responseCode == HttpStatus.SC_OK && entity != null) {
			outStream = new BufferedOutputStream(new FileOutputStream(outFilename));
			entity.writeTo(outStream);
		}

Here's the log output:

DEBUG [2011-08-02 01:23:01,031] [org.apache.http.impl.conn.SingleClientConnManager:212] Get connection for route HttpRoute[{}->http://<host>]
DEBUG [2011-08-02 01:23:01,036] [org.apache.http.impl.conn.DefaultClientConnectionOperator:145] Connecting to <host>/<IP>:80
DEBUG [2011-08-02 01:23:01,057] [org.apache.http.client.protocol.RequestAddCookies:132] CookieSpec selected: best-match
DEBUG [2011-08-02 01:23:01,057] [org.apache.http.client.protocol.RequestAuthCache:75]   Auth cache not set in the context
DEBUG [2011-08-02 01:23:01,058] [org.apache.http.impl.client.DefaultRequestDirector:631]        Attempt 1 to execute request
DEBUG [2011-08-02 01:23:01,058] [org.apache.http.impl.conn.DefaultClientConnection:264] Sending request: GET <file> HTTP/1.1
DEBUG [2011-08-02 01:23:01,058] [org.apache.http.impl.conn.Wire:63]     >> ""GET <file> HTTP/1.1[\r][\n]""
DEBUG [2011-08-02 01:23:01,058] [org.apache.http.impl.conn.Wire:63]     >> ""If-Modified-Since: Mon, 01 Aug 2011 18:26:09 CEST[\r][\n]""
DEBUG [2011-08-02 01:23:01,059] [org.apache.http.impl.conn.Wire:63]     >> ""Host: <host>[\r][\n]""
DEBUG [2011-08-02 01:23:01,059] [org.apache.http.impl.conn.Wire:63]     >> ""Connection: Keep-Alive[\r][\n]""
DEBUG [2011-08-02 01:23:01,059] [org.apache.http.impl.conn.Wire:63]     >> ""User-Agent: Apache-HttpClient/4.1.1 (java 1.5)[\r][\n]""
DEBUG [2011-08-02 01:23:01,059] [org.apache.http.impl.conn.Wire:63]     >> ""Accept-Encoding: gzip,deflate[\r][\n]""
DEBUG [2011-08-02 01:23:01,060] [org.apache.http.impl.conn.Wire:63]     >> ""[\r][\n]""
DEBUG [2011-08-02 01:23:01,060] [org.apache.http.impl.conn.DefaultClientConnection:268] >> GET <file> HTTP/1.1
DEBUG [2011-08-02 01:23:01,060] [org.apache.http.impl.conn.DefaultClientConnection:271] >> If-Modified-Since: Mon, 01 Aug 2011 18:26:09 CEST
DEBUG [2011-08-02 01:23:01,060] [org.apache.http.impl.conn.DefaultClientConnection:271] >> Host: <host>
DEBUG [2011-08-02 01:23:01,061] [org.apache.http.impl.conn.DefaultClientConnection:271] >> Connection: Keep-Alive
DEBUG [2011-08-02 01:23:01,061] [org.apache.http.impl.conn.DefaultClientConnection:271] >> User-Agent: Apache-HttpClient/4.1.1 (java 1.5)
DEBUG [2011-08-02 01:23:01,061] [org.apache.http.impl.conn.DefaultClientConnection:271] >> Accept-Encoding: gzip,deflate
DEBUG [2011-08-02 01:23:01,085] [org.apache.http.impl.conn.Wire:63]     << ""HTTP/1.1 200 OK[\r][\n]""
DEBUG [2011-08-02 01:23:01,086] [org.apache.http.impl.conn.Wire:63]     << ""Server: nginx/0.8.54[\r][\n]""
DEBUG [2011-08-02 01:23:01,086] [org.apache.http.impl.conn.Wire:63]     << ""Date: Mon, 01 Aug 2011 23:23:01 GMT[\r][\n]""
DEBUG [2011-08-02 01:23:01,086] [org.apache.http.impl.conn.Wire:63]     << ""Content-Type: text/plain[\r][\n]""
DEBUG [2011-08-02 01:23:01,086] [org.apache.http.impl.conn.Wire:63]     << ""Last-Modified: Wed, 20 Jul 2011 14:39:57 GMT[\r][\n]""
DEBUG [2011-08-02 01:23:01,087] [org.apache.http.impl.conn.Wire:63]     << ""Transfer-Encoding: chunked[\r][\n]""
DEBUG [2011-08-02 01:23:01,087] [org.apache.http.impl.conn.Wire:63]     << ""Connection: keep-alive[\r][\n]""
DEBUG [2011-08-02 01:23:01,087] [org.apache.http.impl.conn.Wire:63]     << ""Vary: Accept-Encoding[\r][\n]""
DEBUG [2011-08-02 01:23:01,087] [org.apache.http.impl.conn.Wire:63]     << ""Expires: Wed, 31 Aug 2011 23:23:01 GMT[\r][\n]""
DEBUG [2011-08-02 01:23:01,088] [org.apache.http.impl.conn.Wire:63]     << ""Cache-Control: max-age=2592000[\r][\n]""
DEBUG [2011-08-02 01:23:01,088] [org.apache.http.impl.conn.Wire:63]     << ""Content-Encoding: gzip[\r][\n]""
DEBUG [2011-08-02 01:23:01,088] [org.apache.http.impl.conn.Wire:63]     << ""[\r][\n]""
DEBUG [2011-08-02 01:23:01,088] [org.apache.http.impl.conn.DefaultClientConnection:249] Receiving response: HTTP/1.1 200 OK
DEBUG [2011-08-02 01:23:01,089] [org.apache.http.impl.conn.DefaultClientConnection:252] << HTTP/1.1 200 OK
DEBUG [2011-08-02 01:23:01,089] [org.apache.http.impl.conn.DefaultClientConnection:255] << Server: nginx/0.8.54
DEBUG [2011-08-02 01:23:01,089] [org.apache.http.impl.conn.DefaultClientConnection:255] << Date: Mon, 01 Aug 2011 23:23:01 GMT
DEBUG [2011-08-02 01:23:01,089] [org.apache.http.impl.conn.DefaultClientConnection:255] << Content-Type: text/plain
DEBUG [2011-08-02 01:23:01,089] [org.apache.http.impl.conn.DefaultClientConnection:255] << Last-Modified: Wed, 20 Jul 2011 14:39:57 GMT
DEBUG [2011-08-02 01:23:01,090] [org.apache.http.impl.conn.DefaultClientConnection:255] << Transfer-Encoding: chunked
DEBUG [2011-08-02 01:23:01,090] [org.apache.http.impl.conn.DefaultClientConnection:255] << Connection: keep-alive
DEBUG [2011-08-02 01:23:01,090] [org.apache.http.impl.conn.DefaultClientConnection:255] << Vary: Accept-Encoding
DEBUG [2011-08-02 01:23:01,090] [org.apache.http.impl.conn.DefaultClientConnection:255] << Expires: Wed, 31 Aug 2011 23:23:01 GMT
DEBUG [2011-08-02 01:23:01,090] [org.apache.http.impl.conn.DefaultClientConnection:255] << Cache-Control: max-age=2592000
DEBUG [2011-08-02 01:23:01,091] [org.apache.http.impl.conn.DefaultClientConnection:255] << Content-Encoding: gzip
DEBUG [2011-08-02 01:23:01,091] [org.apache.http.impl.client.DefaultRequestDirector:477]        Connection can be kept alive indefinitely
DEBUG [2011-08-02 01:23:01,131] [org.apache.http.impl.conn.Wire:63]     << ""600a[\r][\n]""
DEBUG [2011-08-02 01:23:01,132] [org.apache.http.impl.conn.Wire:77]     << ""[0x1f]""
DEBUG [2011-08-02 01:23:03,838] [org.apache.http.impl.conn.Wire:63]     << ""[\r][\n]""

.... (Content)

DEBUG [2011-08-02 01:23:03,839] [org.apache.http.impl.conn.SingleClientConnManager:267] Releasing connection org.apache.http.impl.conn.SingleClientConnManager$ConnAdapter@2aa3873
DEBUG [2011-08-02 01:23:03,839] [org.apache.http.impl.conn.SingleClientConnManager:285] Released connection open but not reusable.
DEBUG [2011-08-02 01:23:03,839] [org.apache.http.impl.conn.DefaultClientConnection:152] Connection shut down
ERROR [2011-08-02 01:23:03,840] [app]        Exception downloading file
java.io.EOFException
        at java.util.zip.GZIPInputStream.readUByte(GZIPInputStream.java:224)
        at java.util.zip.GZIPInputStream.readUShort(GZIPInputStream.java:214)
        at java.util.zip.GZIPInputStream.readHeader(GZIPInputStream.java:153)
        at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:75)
        at java.util.zip.GZIPInputStream.<init>(GZIPInputStream.java:85)
        at org.apache.http.client.entity.GzipDecompressingEntity.getContent(GzipDecompressingEntity.java:63)
        at org.apache.http.util.EntityUtils.consume(EntityUtils.java:65)
        at org.apache.http.conn.BasicManagedEntity.ensureConsumed(BasicManagedEntity.java:98)
        at org.apache.http.conn.BasicManagedEntity.writeTo(BasicManagedEntity.java:115)
        at util.FileDownload.download(FileDownload.java:188) <--- my app

Does this happen because the server doesn't specify the content length?"
0,"Extend FieldCache architecture to multiple ValuesI would consider this a bug. It appears lots of people are working around this limitation, 
why don't we just change the underlying data structures to natively support multiValued fields in the FieldCache architecture?

Then functions() will work properly, and we can do things like easily geodist() on a multiValued field.

Thoughts?"
0,"Restore mix:referenceable check to SessionImpl.getNodeByUUIDIn revision 504623 we commented out the mix:referenceable check in the SessionImpl.getNodeByUUID() method:

    // since the uuid of a node is only exposed through jcr:uuid declared
    // by mix:referenceable it's rather unlikely that a client can possibly
    // know the internal uuid of a non-referenceable node; omitting the
    // check for mix:referenceable seems therefore to be a reasonable
    // compromise in order to improve performance.
    /*
    if (node.isNodeType(Name.MIX_REFERENCEABLE)) {
        return node;
    } else {
        // there is a node with that uuid but the node does not expose it
        throw new ItemNotFoundException(uuid.toString());
    }
    */

This solved a minor performance issue issue with client code that used the node UUID as a quick way to access a node. The downside was a slight incompatibility with the spec that says that the getNodeByUUID method is only supposed to work with mix:referenceable nodes.

Now with JCR 2.0 clients can (and should) use the Session.getNodeByIdentifier method that does not have the mix:referenceable limitation. Thus we can restore the original and correct functionality of the getNodeByUUID method."
1,"IndexWriter.setMaxMergeDocs gives non-backwards-compatible exception ""out of the box""Yonik hit this (see details in LUCENE-994): because we have switched
to LogByteSizeMergePolicy by default in IndexWriter, which uses MB to
limit max size of merges (setMaxMergeMB), when an existing app calls
setMaxMergeDocs (or getMaxMergeDocs) it will hit an
IllegalArgumentException on dropping in the new JAR.

I think the simplest solution is to fix LogByteSizeMergePolicy to
allow setting of the max by either MB or by doc count, just like how
in LUCENE-1007 allows flushing by either MB or docCount or both."
1,"Base64 bug - last buffer not flushedI found an issue when using the org.apache.jackrabbit.util.Base64.encode(InputStream in, OutputStream out) method. It appears that the issue is that the last buffer is not flushed on the Writer that it creates before returning from the method. I was able to work around this issue by creating a Writer my own program, and call another encode method, and then call the flush() method before using the data. The source in trunk appears the same.
"
0,"HttpMethodBase logger uses wrong class.I just noticed a minor error in HttpMethodBase: it initializes its Log object
with HttpMethod.class instead of HttpMethodBase.class.  No big deal, but it
probably ought to be fixed at some point.  I'll attach the patch."
0,"Move common implementations of SPI interfaces to spi-commons moduleSome of the spi modules use nearly duplicate code, which should be moved to the spi-commons module."
0,Add workspace population toolAdd a simple tool to jackrabbit-webapp to populate the workspace with content.
1,"if you use setNorm, lucene writes a headerless separate norms fileIn this case SR.reWrite just writes the bytes with no header...
we should write it always.

we can detect in these cases (segment written <= 3.1) with a 
sketchy length == maxDoc check.
"
0,"When 3.1 is released, update backwards tests in 3.x branchWhen we have released the official artifacts of Lucene 3.1 (the final ones!!!), we need to do the following:

- svn rm backwards/src/test
- svn cp https://svn.apache.org/repos/asf/lucene/dev/branches/lucene_solr_3_1/lucene/src/test backwards/src/test
- Copy the lucene-core-3.1.0.jar from the last release tarball to lucene/backwards/lib and delete old one.
- Check that everything is correct: The backwards folder should contain a src/ folder that now contains ""test"". The files should be the ones from the branch.
- Run ""ant test-backwards""

Uwe will take care of this!"
1,"MS Proxy with NTLM authentication set up does not workWhen I try to go via a MS Proxy which is set up with NTLM authentication I
always get a ""407"" error, no matter which credentials used."
0,"Payload QueriesNow that payloads have been implemented, it will be good to make them searchable via one or more Query mechanisms.  See http://wiki.apache.org/lucene-java/Payload_Planning for some background information and https://issues.apache.org/jira/browse/LUCENE-755 for the issue that started it all.  "
1,JCR2SPI: incomplete changelog when combining move with removal of new destination parent
1,"Bug in duplicate mapping checkThere is a bug in the MappingDescriptor for checking if a mapping for a node type is already available. The following patch solves this problem:

Index: /Users/cziegeler/Developer/workspaces/default/jackrabbit/jackrabbit-ocm/src/main/java/org/apache/jackrabbit/ocm/mapper/model/MappingDescriptor.java
===================================================================
--- /Users/cziegeler/Developer/workspaces/default/jackrabbit/jackrabbit-ocm/src/main/java/org/apache/jackrabbit/ocm/mapper/model/MappingDescriptor.java	(revision 614136)
+++ /Users/cziegeler/Developer/workspaces/default/jackrabbit/jackrabbit-ocm/src/main/java/org/apache/jackrabbit/ocm/mapper/model/MappingDescriptor.java	(working copy)
@@ -75,7 +75,7 @@
         if (null != classDescriptor.getJcrType() && !  """".equals(classDescriptor.getJcrType()) && 
         		 ! ManagerConstant.NT_UNSTRUCTURED.equals(classDescriptor.getJcrType()))
         {
-        	if ((classDescriptorsByNodeType.get(classDescriptor.getClassName()) != null) &&
+        	if ((classDescriptorsByNodeType.get(classDescriptor.getJcrType()) != null) &&
         		classDescriptor.usesNodeTypePerConcreteClassStrategy()	)
         	{
         	    log.warn(""Duplicate classdescriptor for node type : "" + classDescriptor.getJcrType());	
"
0,"Add N-Gram String Matching for Spell CheckingN-Gram version of edit distance based on paper by Grzegorz Kondrak, ""N-gram similarity and distance"". Proceedings of the Twelfth International Conference on String Processing and Information Retrieval (SPIRE 2005), pp. 115-126,  Buenos Aires, Argentina, November 2005. 
http://www.cs.ualberta.ca/~kondrak/papers/spire05.pdf
"
1,"[SPI] Node.setProperty with null value throws ItemNotFoundExceptionNode.setProperty with a null value should not throw a ItemNotFoundException in the case a property with the given name does not exist. Rather should it return a stale property which throws an InvalidItemStateException when its methods are accessed. 

This behavior is also consistent with jackrabbit-core.
"
1,"highlighting exact phrase with overlapping tokens fails.Fields with overlapping token are not highlighted in search results when searching exact phrases, when using TermVector.WITH_OFFSET.

The document builded in MemoryIndex for highlight does not preserve positions of tokens in this case. Overlapping tokens get ""flattened"" (position increment always set to 1), the spanquery used for searching relevant fragment will fail to identify the correct token sequence because the position shift.

I corrected this by adding a position increment calculation in sub class StoredTokenStream. I added junit test covering this case.

I used the eclipse codestyle from trunk, but style add quite a few format differences between repository and working copy files. I tried to reduce them, but some linewrapping rules still doesn't match.

Correction patch joined"
0,"Remove sanityCheck() from ItemImpl.getSession()The following code causes an InvalidItemStateException to be thrown for no good reason:

    Property property = ...;
    property.setValue((Value) null);
    property.getSession();

There are cases (I'm looking at one right now) where it's good to be able to access the session of an Item even if it has already been invalidated.

The simple fix is to remove the sanityCheck() call from ItemImpl.getSession(). I'll do that unless someone has a good reason why the sanity check should be kept."
0,"FieldCache introspection APIFieldCache should expose an Expert level API for runtime introspection of the FieldCache to provide info about what is in the FieldCache at any given moment.  We should also provide utility methods for sanity checking that the FieldCache doesn't contain anything ""odd""...
   * entries for the same reader/field with different types/parsers
   * entries for the same field/type/parser in a reader and it's subreader(s)
   * etc...


"
0,decorator enhancementsadded some decorating enhancements as we discussed on the mailing list (apparently there is nothing yet in the gmane /marc archives).
0,"CacheManager (Memory Management in Jackrabbit)Jackrabbit can run out of memory because the the combined size of the various caches is not managed. The biggest problem (for me) is the combined size of the o.a.j.core.state.MLRUItemStateCache caches. Each session seems to create a few (?) of those caches, and each one is limited to 4 MB by default.

I have implemented a dynamic (cache-) memory management service that distributes a fixed amount of memory dynamically to all those caches.

Here is the patch"
0,Update Spatial Lucene sort to use FieldComparatorSourceUpdate distance sorting to use FieldComparator sorting as opposed to SortComparator
0,"Observation tests should throw NotExecutableException when repository does not support observationThe observation tests should throw NotExecutableException when repository does not support observation.
"
0,"Link javadocs of HttpClient, HttpCore and HttpMimePresently the javadocs for HttpCore, HttpClient and HttpMime are isolated from each other.  For new users this can create a great deal of confusion and the appearance of limited functionality of HttpClient.  Please set the javadoc creation task to link the javadoc of these three projects together.  "
1,rep:excerpt() may return malformed XMLThe rep:excerpt() function does not encode the prefined XML entities but writes them as is into the excerpt XML. This may produce malformed XML.
0,"Easier way to run benchmarkMove Benchmark.main to a more easily accessible method exec() that can be easily invoked by external programs.
"
1,"StringIndexOutOfBound exception in RFC2109 cookie validate when host name contains no domain information and is short in length than the cookie domain.If the target server is identified by hostname only (no domain) and the domain
of the cookie is greater in length than the target hostname, a
StringIndexOutOfBoundsException occurs.

Offending line(s) of code: 174-176 in o.a.c.h.cookie.RFC2109Spec.java"
0,"Observation logs error when a node is moved in placeAn error message is written to the log when the following sequence of operations is executed:

- create node 'parent'
- create node 'child' as a child of 'parent'
- save
- create node 'tmp'
- move 'child' under 'tmp'
- remove 'parent'
- move 'tmp' to former path of 'parent'

The log will say: EventStateCollection: Unable to calculate old path of moved node

This is because the zombie path of 'child' is equal to the new path after the move. The EventStateCollection detects a new parentId assigned to 'child' and expects a new path that is different from the zombie path. The above case however shows that there is a use case where the paths are equal and events should be generated."
0,"consolidate FieldCache and ExtendedFieldCache instancesIt's confusing and error prone having two instances of FieldCache... FieldCache .DEFAULT and ExtendedFieldCache .EXT_DEFAULT.
Accidentally use the wrong one and you silently double the memory usage for that field.  Since ExtendedFieldCache extends FieldCache, there's no reason not to share the same instance across both."
0,"Making Tokenizer.reset(Reader) publicIn order to implement reusableTokenStream and be able to reset a Tokenizer, Tokenizer defines a reset(Reader) method. The problem is that this method is protected. I need to call this reset(Reader) method without having to know in advance what will be the type of the Tokenizer (I plan to have several).
I noticed that almost all Tokenizer extensions define this method as public, and I wonder if this can be changed for Tokenizer also (I can't simply create my general Tokenizer extension and inherit from it because I want to use StandardTokenizer as well). "
0,"Include OCM in the main Jackrabbit build when using Java 5Currently the OCM component are separate from the rest of Jackrabbit build due to the fact that they need Java 5 to compile. I'd like to add a java5 profile to the main Jackrabbit build that contains the OCM components and is automatically activated when building with Java 5 or higher.

This would simplify build instructions and allow us to remove the extra Jackrabbit-ocm Hudson build that we currently use to build the OCM component."
0,"TimeLimitingCollector starts thread in static {} with no way to stop themSee the comment in LuceneTestCase.

If you even do Class.forName(""TimeLimitingCollector"") it starts up a thread in a static method, and there isn't a way to kill it.

This is broken."
1,"HttpState.clearCookies() should be synchronizedThe HttpState class has a clearCookies method that is not synchronized but
should be considering it modifies an ArrayList (which is unsynchronized). All
other methods which modify or read from the ArrayList are synchronized except
the clearCookies method. 

I stumbled upon this fact because a webapp I am working on that uses HttpClient
threw an IllegalArgumentException indicating that one of the cookies in the
array returned from HttpState.getCookies() was null, which shouldn't be
possible.  Upon further inspection and testing, the only possible option is that
the threadsafety hole left by the unsynchronized clearCookies method caused the
issue."
1,"ConnectionTimeoutException doesn't releaseConnection()When a ConnectionTimeoutException is thrown, HttpConnection doesn't seem to
release the connection. Instead, the connection is properly released if an
InterruptedIOException is thrown.

This is the pattern I use:

Try {
     method.execute(...);
     method.getResponseBodyAsString();
 } catch (ConnectionTimeoutException cte) {
     ...
 } catch (InterruptedIOException ioe) {
     ...
 } finally {
     method.releaseConnection();
     LOG.info(""RELEASED"");   
 }

The following log shows that no actual release is performed, while the message
""RELEASED"" is logged.

10544  DEBUG [MainCheck2] httpclient.HttpConnection - enter
HttpConnection.isResponseAvailable(int)
10930  WARN  [MainCheck1] httpclient.HttpConnection - The host
www.pccomputing.com:80 (or proxy null:-1) did not accept the connection within
timeout of 3000 milliseconds
10931  WARN  [MainCheck1] CheckPerformer - Connection Timeout occurred..
org.apache.commons.httpclient.HttpConnection$ConnectionTimeoutException
at org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:659) 
...
at PersistenceCheck$MainCheck.run(PersistenceCheck.java:306)
10932  INFO  [MainCheck1] CheckPerformer - RELEASED

->Here no call to HttpConnection.releaseConnection() is performed. 

Thanks"
0,"ItemManager issues WARN message on Node.checkIn and Node.checkOutWenn checking in or checking out a node, the ItemManager.cacheItem method issues a WARN message because a cache entry is being replaced.

While this message might be valuable in certain contexts, in the contetx of checking in or out a node, this is not valuable and harms confidence :-)"
1,"Registering NodeType with defaultvalues fails with IndexOutOfBoundsWhen trying to register more than one nodetpye with default values I get the following exception:

Caused by: java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.toNodeTypeDef(NodeTypeManagerImpl.java:790)
	at org.apache.jackrabbit.core.nodetype.NodeTypeManagerImpl.registerNodeTypes(NodeTypeManagerImpl.java:560)

I assume there is an index missmatch in the implementation

                Value[] values = pdefs[i].getDefaultValues();
                if (values != null) {
                    InternalValue[] qvalues = new InternalValue[values.length];
                    for (int j = 0; j < values.length; j++) {
                        try {
-->                            qvalues[j] = InternalValue.create(values[i], session);
                        } catch (ValueFormatException e) {
                            throw new InvalidNodeTypeDefinitionException(
                                    ""Invalid default value format"", e);
                        }
                    }
                    qpdef.setDefaultValues(qvalues);
                }
"
0,"SSL contrib files do not use standard javax.net.ssl package provided from JDK 1.4.2Hi all,

While trying to use ssl on AIX, i found that some of the files contributed in 
src/contrib/org/apache/commons/httpclient/contrib/ssl were making hard 
references to com.sun.net.ssl package. Since JDK 1.4.2, one shall use the 
javax.net.ssl package instead.

I have then:
1/ fixed the source files appropriately
2/ updated the build.xml to also build a commons-http-client-contrib.jar 

I will attached to this bug report the resulting unified diff to include in svn"
1,"wordnet parsing bugA user reported that wordnet parses the prolog file incorrectly.

Also need to check the wordnet parser in the memory contrib for this problem.

If this is a false alarm, i'm not worried, because the test will be the first unit test wordnet package ever had.

{noformat}
For example, looking up the synsets for the
word ""king"", we get:

java SynLookup wnindex king
baron
magnate
mogul
power
queen
rex
scrofula
struma
tycoon

Here, ""scrofula"" and ""struma"" are extraneous. This happens because, the line
parser code in Syns2Index.java interpretes the two consecutive single quotes
in entry s(114144247,3,'king''s evil',n,1,1) in  wn_s.pl file, as
termination
of the string and separates into ""king"". This entry concerns
synset of words ""scrofula"" and ""struma"", and thus they get inserted in the
synset of ""king"". *There 1382 such entries, in wn_s.pl* and more in other
WordNet
Prolog data-base files, where such use of two consecutive single quotes
appears.

We have resolved this by adding a statement in the line parsing portion of
Syns2Index.java, as follows:

           // parse line
           line = line.substring(2);
          * line = line.replaceAll(""\'\'"", ""`""); // added statement*
           int comma = line.indexOf(',');
           String num = line.substring(0, comma);  ... ... etc.
In short we replace ""''"" by ""`"" (a back-quote). Then on recreating the
index, we get:

java SynLookup zwnindex king
baron
magnate
mogul
power
queen
rex
tycoon
{noformat}"
0,"Remove background initialization of hierarchy cacheThis is a follow up to JCR-1998.

Rethinking the initialization in a background thread again, I now come to the conclusion that it should be initialized either completely on startup or not at all. A background thread puts additional load on the process, possibly fighting for I/O with other startup procedures.
"
1,"TestNRTThreads test failurehit a fail in TestNRTThreads running tests over and over:
"
0,"Additional excerpt provider implementationThe current DefaultHTMLExcerpt implementation is very simple. It basically picks the first three fragments, regardless of how many matches it contains. There should be an alternative implementation that weights the fragments based on the number of matching terms and the whether phrases have matched."
0,"NodeAddMixinTest assumptions on addMixin behaviourNodeAddMixinTest.testAddMixinReferencable() assumes that mix:referenceable can be added to the test node type. In practice, the node type may already inherit mix:referenceable, but it may not be active until the node is saved. Thus, a ConstraintViolationException upon addMixin should be catched, and the mixin should be checked after save() again.
"
0,"Use bulk-byte-copy when merging term vectorsIndexing all of Wikipedia, with term vectors on, under the YourKit
profiler, shows that 26% of the time (!!) was spent merging the
vectors.  This was without offsets & positions, which would make
matters even worse.

Depressingly, merging, even with ConcurrentMergeScheduler, cannot in
fact keep up with the flushing of new segments in this test, and this
is on a strong IO system (Mac Pro with 4 drive RAID 0 array, 4 CPU
cores).

So, just like Robert's idea to merge stored fields with bulk copying
whenever the field name->number mapping is ""congruent"" (LUCENE-1043),
we can do the same with term vectors.

It's a little trickier because the term vectors format doesn't quite
make it easy to bulk-copy because it doesn't directly encode the
offset into the tvf file.

I worked out a patch that changes the tvx format slightly, by storing
the absolute position in the tvf file for the start of each document
into the tvx file, just like it does for tvd now.  This adds an extra
8 bytes (long) in the tvx file, per document.

Then, I removed a vLong (the first ""position"" stored inside the tvd
file), which makes tvd contents fully position independent (so you can
just copy the bytes).

This adds up to 7 bytes per document (less for larger indices) that
have term vectors enabled, but I think this small increase in index
size is acceptable for the gains in indexing performance?

With this change, the time spent merging term vectors dropped from 26%
to 3%.  Of course, this only applies if your documents are ""regular"".
I think in the future we could have Lucene try hard to assign the same
field number for a given field name, if it had been seen before in the
index...

Merging terms now dominates the merge cost (~20% over overall time
building the Wikipedia index).

I also beefed up TestBackwardsCompatibility unit test: test a non-CFS
and a CFS of versions 1.9, 2.0, 2.1, 2.2 index formats, and added some
term vector fields to these indices.
"
0,"Minimize use of fields in lucene indexCurrently every property name creates a field in the lucene index, bloating the size of the index because of the norm files created for each field.

When values are indexed as is (not tokenized for fulltext indexing), then the property name may be part of the term text. That way lucene must only maintain one field for all property names. With this approach the search terms are always a combination of property name and literal value. e.g. instead of using TermQuery(new Term(""prop"", ""foo"")) the query must be TermQuery(new TermQuery(""common-field"", ""prop:foo"")). this works for general comparison / value comparison operators and also for the like function. the contains function uses the fulltext index which uses a different field anyway.

Using the property name as part of the indexed term text, requires a custom SortComparator which is aware of the property name.

This change will not be backward compatible with earlier indexes created by jackrabbit."
0,"Jar manifest should not contain ${user.name} of the person buildingNot sure if it is a big deal, but I don't particularly like that my user id for my build machine is in the manifest of the JAR that I constructed.  It's a stretch, security-wise, I know, but I don't see how it serves any useful purpose.  We have signatures/logs/SVN tags so we know who built the particular item w/o needing to know what their local user account name is.

The fix is:

{code}
Index: common-build.xml
===================================================================
--- common-build.xml    (revision 661027)
+++ common-build.xml    (working copy)
@@ -281,7 +281,7 @@
                <attribute name=""Implementation-Title"" value=""org.apache.lucene""/>
                <!-- impl version can be any string -->
                <attribute name=""Implementation-Version""
-                          value=""${version} ${svnversion} - ${user.name} - ${DSTAMP} ${TSTAMP}""/>
+                          value=""${version} ${svnversion} - ${DSTAMP} ${TSTAMP}""/>
                <attribute name=""Implementation-Vendor""
                           value=""The Apache Software Foundation""/>
                <attribute name=""X-Compile-Source-JDK"" 
{code} "
0,"Real In-memory RepositoryFor unit tests it is desirable to have an in-memory repository which holds its whole data(even PropertyType.BINARY) in memory.
The actual implementation of org.apache.jackrabbit.core.persistence.mem.InMemPersistenceManager uses the FileSystemBLOBStore along with LocalFileSystem.
The binary properties are serialized to the OS-Filesystem.
"
1,"Spatial checks for a string in an int,double map{code}
  private Map<Integer,Double> distances;
{code}

{code}
    if (precise != null) {
      double xLat = getPrecision(lat, precise);
      double xLng = getPrecision(lng, precise);
      
      String k = new Double(xLat).toString() +"",""+ new Double(xLng).toString();
    
      Double d = (distances.get(k));
      if (d != null){
        return d.doubleValue();
      }
    }
{code}

Something is off here eh?"
1,"Cookie with domain .mydomain.com not sent to host mydomain.comA cookie with for example 
  .mydomain.com 
as domain property is not sent to the host
  mydomain.com
(without www. or anything else before ""mydomain.com"")

This concern all CookieSpec as the relevant code is located in CookieSpecBase:

    public boolean domainMatch(final String host, final String domain) {
        return host.endsWith(domain);
    }

It should be changed for instance to something like:

    public boolean domainMatch(final String host, final String domain) {
        // take care of host ""myDomain.com"" and domain "".myDomain.com""
        return host.endsWith(domain)
	|| _host.equals(_domain.substring(1));
    }"
0,"analysis consumers should use reusable tokenstreamsSome analysis consumers (highlighter, more like this, memory index, contrib queryparser, ...) are using Analyzer.tokenStream but should be using Analyzer.reusableTokenStream instead for better performance."
0,"Remove HitsLUCENE-1290 removed all references to Hits from core.

Most work to be done here is to remove all references from the contrib modules and some new ones that crept into core after 1290."
0,"Remove unnecessary NodeImpl references from LuceneQueryFactoryLuceneQueryFactory casts to NodeImpl just to get the node id. 
This info is available via the api as well, so the cast seems unnecessary.
I'll attach a patch for this tiny issue."
1,"cache returns cached responses even if validators not consistent with all conditional headersThis is a MUST-level requirement in the RFC, where if both ETags and Last-Modified dates are used as validators in a conditional request, a cache cannot return a cached response unless it is consistent with all the conditional headers in the request. There is a unit test for this already, but it is incorrect (it uses 'If-Unmodified-Since' instead of 'If-Modified-Since' in the test case).

"
0,"Use ConcurrentHashMap instead of HashMap wherever thread-safe access is neededConsider using ConcurrentHashMap instead of HashMap for any Maps that are used by multiple threads.

For example SchemeRegistry and AuthSchemeRegistry."
0,Add a testing implementation for DocumentsWriterPerThreadPoolcurrently we only have one impl for DocumentsWriterPerThreadPool. We should add some more to make sure the interface is sufficient and to beef up tests. For testing I'm working on a randomized impl. selecting and locking states randomly.
1,"An HTTP ""204 NO CONTENT"" response results in dropped connectionAfter receiving a ""204 NO CONTENT"" response, HttpClient always closes the 
connection.

This did not happen in earlier versions and appears to have been caused by a 
recent fix to bug# 34262."
0,"CustomScoreQuery should support multiple ValueSourceQueriesCustomScoreQuery's constructor currently accepts a subQuery, and a ValueSourceQuery.  I would like it to accept multiple ValueSourceQueries.  The workaround of nested CustomScoreQueries works for simple cases, but it quickly becomes either cumbersome to manage, or impossible to implement the desired function.

This patch implements CustomMultiScoreQuery with my desired functionality, and refactors CustomScoreQuery to implement the special case of a CustomMultiScoreQuery with 0 or 1 ValueSourceQueries.  This keeps the CustomScoreQuery API intact.

This patch includes basic tests, more or less taken from the original implementation, and customized a bit to cover the new cases."
0,temporary files created by some jUnit test are not automatically removed
0,"CloseableThreadLocal is now obsoleteSince Lucene 3 depends on Java 5, we can use ThreadLocal#remove() to take care or resource management."
0,"Data Store: garbage collection should ignore removed itemsThe GCConcurrentTest fails sometimes. The problem is that
the garbage collector stops if a node or property was removed
while scanning. Instead, the garbage collector should ignore the
removed item and continue.

-------------------------------------------------------------------------------
Test set: org.apache.jackrabbit.core.data.TestAll
-------------------------------------------------------------------------------
Tests run: 19, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 21.282 sec <<< FAILURE!
testGC(org.apache.jackrabbit.core.data.GCConcurrentTest)  Time elapsed: 0.578 sec  <<< ERROR!
javax.jcr.InvalidItemStateException: 5fc4130b-aee4-4bef-b51d-21420d78f315/{}data: the item does not exist anymore
	at org.apache.jackrabbit.core.ItemImpl.sanityCheck(ItemImpl.java:144)
	at org.apache.jackrabbit.core.PropertyImpl.getPropertyState(PropertyImpl.java:89)
	at org.apache.jackrabbit.core.PropertyImpl.getType(PropertyImpl.java:773)
	at org.apache.jackrabbit.core.data.GarbageCollector.recurse(GarbageCollector.java:310)
	at org.apache.jackrabbit.core.data.GarbageCollector.recurse(GarbageCollector.java:327)
	at org.apache.jackrabbit.core.data.GarbageCollector.recurse(GarbageCollector.java:327)
	at org.apache.jackrabbit.core.data.GarbageCollector.scanNodes(GarbageCollector.java:193)
	at org.apache.jackrabbit.core.data.GarbageCollector.scan(GarbageCollector.java:177)
	at org.apache.jackrabbit.core.data.GCThread.run(GCThread.java:52)
	at java.lang.Thread.run(Thread.java:619)
"
0,"Refactor DBMS support for JNDI datasourcesOur shop currently uses Oracle for most projects, most commonly in an application server (Tomcat, WebSphere, etc.), and use configured J2EE datasources. Unfortunately, many of the classes that fix quirks on specific DBMS force you to configure a JDBC connection (look at org.apache.jackrabbit.core.fs.db.OracleFileSystem for instance), which is a ""bad idea"" on an application server -- the application server should be managing resources like DB connections, etc.  If you want to use an DbFileSystem based on an Oracle database, you can't use a datasource from a JNDI lookup.  This in effect makes Jackrabbit unusable in clustered enterprise environments.

It would be much better to refactor the current database support to separate the method that an implementation obtains its connection from its functionality."
0,"allow case insensitive searcheswould be nice to be able to search specific properties like a fulltext search, e.g. with an ignore-case flag, so you could find a subset of the results of

  select * from nt:base where contains('bla')

using something like

  select * from nt:base where jcr:bla like '%bla%'

(currently, the value must contain 'bla' exactly as it is to be found by the second query)

i suggest to extend the contains function with an additional argument for the property to search in, e.g.

  select * from nt:base where contains('bla',jcr:bla)

this could then also easily be used in XPath.
"
0,"jcr-server: make auth-header configurable for JCR-ServerIn WEB-INF/web.xml, there is a section (commented-out by default) that reads:
        <init-param>
            <param-name>authenticate-header</param-name>
            <param-value>Basic realm=""Jackrabbit Webdav Server""</param-value>
            <description>
                Defines the value of the 'WWW-Authenticate' header.
            </description>
        </init-param>

This parameter is ignored (not loaded) by the code - the default string is always used instead
Note: this was more of a problem before JCR-286 had been fixed
"
0,"Provide rename method for nodesCurrently renaming a node is a nuisance if the node's parent has orderable child nodes: The parents child nodes must be searched for the successor of the node to be moved, the node must be moved to its new name and then ordered before the successor. Furthermore the case where the to be moved node is the last node must be special cased. 

I thus propose to provide functionality for directly renaming nodes.  "
0,"Provide support for unconnected socketsOverview description:
If Proxy settings are incorrect or host does not reply, the
HttpClient.executeMethod() hangs, and HttpMethod.abort() does not stop it. Thus,
you cannot assert that the entire application will stop immediately on demand.

Expected Results:
During a HttpMethod.executeMethod(), HttpMethod.abort() should cancel
immediately the executeMethod().

Actual Results:
If HttpMethod.executeMethod() freezes because of Proxy bad settings or not
responding hostname (in fact impossible to open the socket), the abort() method
does not do anything.

Platform:
I tested it on Windows XP and Linux Debian with HttpClient 3.0 RC2 (but if you
look further I point the problem and the source code of the nightly build is
identical).

See comments for the dialogue about the problem, and 2 Test cases. The solution
is described at the end, but it may implies a change in the API and works only
since Java 1.4."
1,"NPE in NearSpansUnordered from PayloadNearQueryThe following query causes a NPE in NearSpansUnordered, and is reproducible with the the attached unit test. The failure occurs on the last document scored.
"
1,"PropertyState binary type desirialsation only returns half of contentCreate a PropertyState for a binary Property (e.g jcr:data) set a value larger than the BLOBFileValues#MAX_BUFFER_SIZE  (e.g. 300Kbyte) serialse it.
On deserialisation the resulting PropertyState's InternalValue's size is only half as the origianl (e.g. 150Kbyte)

Most probably this is due to the States InputStream implementation marking bytes twice to be read.
Following fix solves the issue for call to #read(byte[], in, int),
but other Stream methods may fail as well.

Index: jackrabbit/java/org/apache/jackrabbit/core/state/PropertyState.java
===================================================================
--- jackrabbit/java/org/apache/jackrabbit/core/state/PropertyState.java (revision 399293)
+++ jackrabbit/java/org/apache/jackrabbit/core/state/PropertyState.java (working  copy)
@@ -305,7 +305,6 @@
                                 len = (int) (length - consumed);
                             }
                             int read = super.read(b, off, len);
-                            consumed += read;
                             return read;
                         }
"
0,"indexing-rules should allow wildcards for (global) property nameseg:

<indexing-rule nodeType=""*"">
  <property>text</property>
  <property>*Text</property>
</indexing-rule>

defines that all properties named 'text' and all that end with 'Text' should be fulltext indexed.
if the property name includes namespace prefixes, wildcards are only allowed for 'any' namespace. eg:

*:title

but not: j*:title
"
1,"XMLPersistenceManager incorrectly handles propertiesJCR Property instances are written by the XMLPersistenceManager as java.util.Properties files and loaded through the loadPropertyState() method as Properties files. Unfortunately the reload() method tries to re-load the Property states from XML files, which is not possible."
0,"Cookie Strict Mode independent of regular Strict ModeHi,

I'm having a problem where a web site I'm trying to access is using strict 
cookies (on one line) and a 302 redirect that fails in strict mode.  So I 
cannot access this website because in strict mode it fails because of the 302 
redirect and in non-strict mode the website doesn't recognize the Cookies on 
separate lines.

I'd love to see this added for the next release candidate.

Thanks,

Brent"
1,"Importing strings with special characters failsBoth Session.importXML and Workspace.importXML don't work correctly in some cases.

Importing very large foreign language (for example, Chinese) text property values could result in incorrect values on some platforms. The reason is, BufferedStringValue (buffers very large string to a temporary file) uses the platform default encoding to read and write the text.

BufferedStringValue is relatively slow on some systems when importing large texts or binary data because of using FD().sync().

If an exported string value contains a carriage return (\r), this character was truncated on some platforms.

If an exported string value contains a characters with code below 32 excluding newline (\n) and tab (\t) - for example form feed (\f) - the imported string value was base64 encoded.
"
0,"Intermittent failure in TestThreadedOptimizeFailure looks like this:

{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestThreadedOptimize
    [junit] Testcase: testThreadedOptimize(org.apache.lucene.index.TestThreadedOptimize):	FAILED
    [junit] null
    [junit] junit.framework.AssertionFailedError: null
    [junit] 	at org.apache.lucene.index.TestThreadedOptimize.runTest(TestThreadedOptimize.java:125)
    [junit] 	at org.apache.lucene.index.TestThreadedOptimize.testThreadedOptimize(TestThreadedOptimize.java:149)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.runBare(LuceneTestCase.java:253)
{noformat}

I just committed some verbosity so next time it strikes we'll have more details."
1,"XMLPersistanceManager doesn't preserve a property's 'multiValued' attributewhen a multi-valued property is persisted and later read using the XMLPersistenceManager the 'multiValued' attribute is lost, i.e. PropertyState.isMultiValued() returns always false."
0,"Trim whitespace from parameter names in configuration filesWe've had a couple of issues with extra whitespace in parameter names causing those configuration options being lost. Now with the more strict validation of configuration settings such mistakes can even prevent the repository from starting. On one hand that's a good thing, as the user would then explicitly need to fix such broken configurations, but it would be nice if no user intervention was needed.

Since leading and trailing whitespace is never allowed in parameter names, we can just as well trim it automatically."
1,"FieldsReader does not regard offset and position flagsWhen creating a Field the FieldsReader looks at the storeTermVector flag of the FieldInfo. If true Field.TermVector.YES is used as parameter. But it should be checked if storeOffsetWithTermVector and storePositionWithTermVector are set and Field.TermVector.WITH_OFFSETS, ...WITH_POSITIONS, or ...WITH_POSITIONS_OFFSETS should be used as appropriate."
1,"Writers on two machines over NFS can hit FNFE due to stale NFS client cachingIssue spawned from this thread:

  http://www.gossamer-threads.com/lists/lucene/java-user/50680

When IndexFileDeleter lists the directory, looking for segments_X
files to load, if it hits a FNFE on opening such a file it should
catch this and treat it as if the file does not exist.

On NFS (and possibly other file systems), a directory listing is not
guaranteed to be ""current""/coherent.  Specifically, if machine #1 has
just removed file ""segments_n"" and shortly thereafer machine #2 does a
dir listing, it's possible (likely?) that the dir listing will still
show that segments_n exists.

I think the fix is simple: catch the FNFE and just handle it as if the
segments_n does not in fact exist.

"
1,"entity returns the same stream for getContent()BasicHttpEntity and GzipDecompressingEntity will return the same stream
when getContent() is called multiple times. That is not allowed by the
HttpEntity interface. They should rather throw an IllegalStateException.

Some tests and EntityUtils rely on getContent to return the same stream
for multiple calls.

patch follows,
  Roland"
0,"the jcr:frozenUuid property is of type REFERENCE instead of STRINGThe spec says that jcr:frozenUuid is a STRING but jackrabbit 1.0.1 uses a REFERENCE for it.
"
0,PropertyReadMethodsTest should also work on NAME propertySome test cases in PropertyReadMethodsTest require a String property even though a NAME property like jcr:primaryType would be sufficient.
0,"NGramFilter -- construct n-grams from a TokenStreamThis filter constructs n-grams (token combinations up to a fixed size, sometimes
called ""shingles"") from a token stream.

The filter sets start offsets, end offsets and position increments, so
highlighting and phrase queries should work.

Position increments > 1 in the input stream are replaced by filler tokens
(tokens with termText ""_"" and endOffset - startOffset = 0) in the output
n-grams. (Position increments > 1 in the input stream are usually caused by
removing some tokens, eg. stopwords, from a stream.)

The filter uses CircularFifoBuffer and UnboundedFifoBuffer from Apache
Commons-Collections.

Filter, test case and an analyzer are attached."
0,"DateField class should be publicThe class org.apache.jackrabbit.core.query.lucene.DateField should be made public.  It has several public methods which are useful but are currently not accessible because the class itself is not accessible outside of its package.  All of the other Field classes in that package are public and accessible (LongField, DoubleField, etc.)"
0,"Make contrib/collation/(ICU)CollationKeyAnalyzer constructors publicIn contrib/collation, the constructors for CollationKeyAnalyzer and ICUCollationKeyAnalyzer are package private, and so are effectively unusable."
0,"create a simple test that indexes and searches byte[] termsCurrently, the only good test that does this is Test2BTerms (disabled by default)

I think we should test this capability, and also have a simpler example for how to do this.
"
0,"wrong class name in statemgmt.xml""BasicClientCookie"" should read ""BasicCookieStore"", see the patch for details"
0,JSR 283: Access Controlcontainer issue for JSR 283 access control functionality
0,"use isBinary cached variable instead of instanceof in FieldField class can hold three types of values, 
See: AbstractField.java  protected Object fieldsData = null; 

currently, mainly RTTI (instanceof) is used to determine the type of the value stored in particular instance of the Field, but for binary value we have mixed RTTI and cached variable ""boolean isBinary"" 

This patch makes consistent use of cached variable isBinary.

Benefit: consistent usage of method to determine run-time type for binary case  (reduces chance to get out of sync on cached variable). It should be slightly faster as well.

Thinking aloud: 
Would it not make sense to maintain type with some integer/byte""poor man's enum"" (Interface with a couple of constants)
code:java{
public static final interface Type{
public static final byte BOOLEAN = 0;
public static final byte STRING = 1;
public static final byte READER = 2;
....
}
}

and use that instead of isBinary + instanceof? "
0,"CompoundFileWriter should pre-set its file lengthI've read that if you are writing a large file, it's best to pre-set
the size of the file in advance before you write all of its contents.
This in general minimizes fragmentation and improves IO performance
against the file in the future.

I think this makes sense (intuitively) but I haven't done any real
performance testing to verify.

Java has the java.io.File.setLength() method (since 1.2) for this.

We can easily fix CompoundFileWriter to call setLength() on the file
it's writing (and add setLength() method to IndexOutput).  The
CompoundFileWriter knows exactly how large its file will be.

Another good thing is: if you are going run out of disk space, then,
the setLength call should fail up front instead of failing when the
compound file is actually written.  This has two benefits: first, you
find out sooner that you will run out of disk space, and, second, you
don't fill up the disk down to 0 bytes left (always a frustrating
experience!).  Instead you leave what space was available
and throw an IOException.

My one hesitation here is: what if out there there exists a filesystem
that can't handle this call, and it throws an IOException on that
platform?  But this is balanced against possible easy-win improvement
in performance.

Does anyone have any feedback / thoughts / experience relevant to
this?
"
0,"allow automatontermsenum to work on full byte rangeAutomatonTermsEnum is really agnostic to whats in your byte[], only that its in binary order.
so if you wanted to use this on some non-utf8 terms, thats just fine.

the patch just does some code cleanup and removes ""utf8"" references, etc.
additionally i changed the pkg-private, lucene-internal byte-oriented ctor, to public, lucene.experimental.
"
0,"Make ReqExclScorer package private, and use DocIdSetIterator for excluded part."
1,"Unusual Http status lineThe web server at http://alces.med.umn.edu/Candida.html returns the following
status line:

HTTP 200 Document follows

This page loads in the 3 browsers I tried (though Safari actually rendered the
headers).  The current version of HttpClient reads through the whole page
looking for a line that starts with HTTP/.  I don't know how big of a problem
this is, but it's a fairly easy fix.  Patch to follow."
0,"Log path of missing node when re-indexing failsIf one tries to re-index a corrupt workspace, then the UUID of the missing nodes is logged. If possible the log should also contain the path to the missing node."
0,"Re-index fails on corrupt bundleThe re-indexing process should be more resilient, log an error and simply continue with the next node. It doesn't seem useful to refuse repository startup in this case."
0,"Change MergePolicy & MergeScheduler to be abstract base classes instead of an interfacesThis gives us freedom to add methods with default base implementation over time w/o breaking backwards compatibility.

Thanks to Hoss for raising this!"
1,"DefaultHttpMethodRetryHandler does not check whether the failed method has been abortedDefaultHttpMethodRetryHandler does not check whether the failed method has been
aborted."
1,"Buffered deletes under count RAMI found this while working on LUCENE-2548: when we freeze the deletes (create FrozenBufferedDeletes), when we set the bytesUsed we are failing to account for RAM required for the term bytes (and now term field)."
0,"Arabic Analyzer: Stopwords list needs enhancementThe provided Arabic stopwords list needs some enhancements (e.g. it contains a lot of words that not stopwords, and some cleanup) . patch will be provided with this issue."
1,"Request is retried if preemptive authentication failsHello,

I'm using premptive authentification from an Axis client using BASIC Http
authentification. When the user isn't authenticated/authorized by server (in my
case, credentials are expired), httpclient runs a ""Chalenge"" that produces a
second request to server with same credentials.

when using preemptive mode, chalenge should be skipped if authentication scheme
hasn't changed !"
0,Add a ton of missing license headers throughout test/demo/contrib
0,"PostMethod - Chunked requests are not supported at the moment.For Apache Axis, we'd like send a POST request without needing to calculate the
content-length for HTTP 1.1 based servers. Of course if the server-side does not
support 1.1 then a fallback mechanism could calculate the total size under the
covers. 

Also see related request from ""Trevor O'Reilly"" <wtrevor@yahoo.com>:
http://marc.theaimsgroup.com/?l=jakarta-commons-user&m=102719653201792&w=2"
0,"Remove write access from SegmentReader and possibly move to separate class or IndexWriter/BufferedDeletes/...After LUCENE-3606 is finished, there are some TODOs:

SegmentReader still contains (package-private) all delete logic including crazy copyOnWrite for validDocs Bits. It would be good, if SegmentReader itsself could be read-only like all other IndexReaders.

There are two possibilities to do this:
# the simple one: Subclass SegmentReader and make a RWSegmentReader that is only used by IndexWriter/BufferedDeletes/... DirectoryReader will only use the read-only SegmentReader. This would move all TODOs to a separate class. It's reopen/clone method would always create a RO-SegmentReader (for NRT).
# Remove all write and commit stuff from SegmentReader completely and move it to IndexWriter's readerPool (it must be in readerPool as deletions need a not-changing view on an index snapshot).

Unfortunately the code is so complicated and I have no real experience in those internals of IndexWriter so I did not want to do it with LUCENE-3606, I just separated the code in SegmentReader and marked with TODO. Maybe Mike McCandless can help :-)"
1,"DatabaseJournal assigns same revision id to different revisionsRunning a transaction that updates multiple workspaces (e.g. a versioning operation) will fail in DatabaseJournal, because every individual update will ultimately be assigned the same revision id. An indication of this failure when e.g. using Oracle as backend for journaling will look as follows::

java.sql.SQLException: ORA-00001: unique constraint (JOURNAL_IDX) violated
 at oracle.jdbc.dbaccess.DBError.throwSqlException(DBError.java:134)
 at oracle.jdbc.ttc7.TTIoer.processError(TTIoer.java:289)
 at oracle.jdbc.ttc7.Oall7.receive(Oall7.java:590)
 at oracle.jdbc.ttc7.TTC7Protocol.doOall7(TTC7Protocol.java:1973)
 at oracle.jdbc.ttc7.TTC7Protocol.executeFetch(TTC7Protocol.java:977)
 at oracle.jdbc.driver.OracleStatement.executeNonQuery(OracleStatement.java:2205)
 at oracle.jdbc.driver.OracleStatement.doExecuteOther(OracleStatement.java:2064)
 at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:2989)
 at oracle.jdbc.driver.OraclePreparedStatement.executeUpdate(OraclePreparedStatement.java:658)
 at oracle.jdbc.driver.OraclePreparedStatement.execute(OraclePreparedStatement.java:736)
 at org.apache.jackrabbit.core.journal.DatabaseJournal.append(DatabaseJournal.java:293)
 ... 24 more

This bug has been reported by Rafał Kwiecień."
1,"SegmentReader.hasSeparateNorms always returns falseThe loop in that method looks like this: 
 
for(int i = 0; i < 0; i++){ 
 
I guess ""i < 0"" should be replaced by ""i < result.length""?"
1,"Cluster information is not persisted to database when connected to case sensitive MS SQL Server 2005After a call to Session::save, we observed that cluster information was not written to the ${schemaObjectPrefix}JOURNAL and ${schemaObjectPrefix}GLOBAL_REVISION tables. We tested against Oracle 10 database servers and MS Sql Server 2005 servers. The problem was noticed only with MS Sql Server 2005. 

Initially, the problem was masked since the test was written as part of our unit test environment and the exceptions generated by JDBC were not showing up in the logs. A separate test with was carried out as shown by the code below

<pre>
import java.io.FileInputStream;

import javax.jcr.Node;
import javax.jcr.Repository;
import javax.jcr.Session;
import javax.jcr.SimpleCredentials;

import org.apache.jackrabbit.core.TransientRepository;
import org.apache.jackrabbit.core.config.RepositoryConfig;

public class Main
{
    public static void main(String[] args)
        throws Exception
    {
        System.setProperty(""org.apache.jackrabbit.core.cluster.node_id"", ""testid"");
        
        RepositoryConfig config = RepositoryConfig.create(new FileInputStream(""repository.xml""), ""repository"");
        
        Repository repository = new TransientRepository();
        
        Session session = repository.login(new SimpleCredentials(""username"", ""password"".toCharArray()));
        
        Node root = session.getRootNode();
        
        root.addNode(""node1"");
        root.addNode(""node2"");
        root.addNode(""node3"");
        
        session.save();
    }
}
</pre>

The configuration file used to configure the repository is attached.

After debugging this, we obtained the exceptions that were previously not visible. Note that, JackRabbit continues to run (is that because the cluster code is running in a separate thread?) even after this exception. The problem was that the 'revision_id' field did not exist. The mssql.ddl schema file sets up the table names in capitals. However, at least two of the SQL statements in DatabaseJournal use lower case table names. For example:-

<pre>
        updateGlobalStmt = con.prepareStatement(
                ""update "" + schemaObjectPrefix + ""global_revision "" +
                ""set revision_id = revision_id + 1"");
        selectGlobalStmt = con.prepareStatement(
                ""select revision_id "" +
                ""from "" + schemaObjectPrefix + ""global_revision"");
</pre>

An additional error is that the mssql.ddl file is missing the following:

<pre>
# Inserting the one and only revision counter record now helps avoiding race conditions
insert into ${schemaObjectPrefix}GLOBAL_REVISION VALUES(0)
</pre>

Fixing the above two issues, fixed the problem with MS SQL Server 2005."
1,"HttpClient enter 100% for endless timeI was working masively using HttpClient (I was testing it for usage within a 
server) and it got to 100% CPU for an endless time.

I was querying urls of the type 
http://search.barnesandnoble.com/booksearch/results.asp?WRD=<text>&sort=R&SAT=1

To reproduce it, run 100-200 urls with random words instead of <text> and 
you'll probably reproduce the problem."
1,"SegmentReader.setNorm can fail to remove separate norms file, on Windows
While working through LUCENE-710 I hit this bug: on Windows
only, when SegmentReader.setNorm is called, but separate norms
(_X_N.sY) had already been previously saved, then, on closing the
reader, we will write the next gen separate norm file correctly
(_X_N+1.sY) but fail to delete the current one.

It's quite minor because the next writer to touch the index will
remove the stale file.

This is because the Norm class still holds the IndexInput open when
the reader commits."
0,"Add an option so skip the ""checkSchema"" methodsSometimes the ""checkSchema"" methods in the various components (DB filesystem, DB persistence manager, DB journal) fail to detect that the required tables already exist with as result that the startup  fails. (See the mail thread on the dev list: http://jackrabbit.markmail.org/message/jtq2sqis2aceh7ro).
An option to just skip the checkSchema methods on startup would solve this issue."
1,"HttpMethodBase#aborted variable mistakenly declared transient instead of volatileHttpMethodBase#aborted variable mistakenly declared transient instead of volatile. This is quite nasty. 

Do we want to cut an emergency release (3.0.2) because of that or can this wait until 3.1-beta1?

Fix attached.

Oleg"
0,"Reintegrate flex branch into trunkThis issue is for reintegrating the flex branch into current trunk. I will post the patch here for review and commit, when all contributors to flex have reviewed the patch.

Before committing, I will tag both trunk and flex."
1,"ShingleMatrixFilter eaily throws StackOverFlow as the complexity of a matrix growsShingleMatrixFilter#next makes a recursive function invocation when the current permutation iterator is exhausted or if the current state of the permutation iterator already has produced an identical shingle. In a not too complex matrix this will require a gigabyte sized stack per thread.

My solution is to avoid the recursive invocation by refactoring like this:

{code:java}
public Token next(final Token reusableToken) throws IOException {
    assert reusableToken != null;
    if (matrix == null) {
      matrix = new Matrix();
      // fill matrix with maximumShingleSize columns
      while (matrix.columns.size() < maximumShingleSize && readColumn()) {
        // this loop looks ugly
      }
    }

    // this loop exists in order to avoid recursive calls to the next method
    // as the complexity of a large matrix
    // then would require a multi gigabyte sized stack.
    Token token;
    do {
      token = produceNextToken(reusableToken);
    } while (token == request_next_token);
    return token;
  }

  
  private static final Token request_next_token = new Token();

  /**
   * This method exists in order to avoid reursive calls to the method
   * as the complexity of a fairlt small matrix then easily would require
   * a gigabyte sized stack per thread.
   *
   * @param reusableToken
   * @return null if exhausted, instance request_next_token if one more call is required for an answer, or instance parameter resuableToken.
   * @throws IOException
   */
  private Token produceNextToken(final Token reusableToken) throws IOException {

{code}

"
0,"TCK: Transfer of lock token should be tested using open-scoped locksdespite the fact that jsr170 does not limit the usage of Session.removeLockToken(String) and Session.addLockToken(String) to tokens obtained from open-scoped locks, i don't see too much benefit of it. Therefore (and due to the fact that this issue will be addressed within the scope of jsr283), i would  like to suggest to modify those test-cases dealing with transfer of lock tokens and create open-scoped locks.
"
0,ReadOnlyIndexReaders are re-created on every accessAbstractIndex.getReadOnlyIndexReader() creates a new instance on every call. The returned index reader should instead be cached and kept open as long as there are no changes on the underlying index.
0,"Generalize SearcherManagerI'd like to generalize SearcherManager to a class which can manage instances of a certain type of interfaces. The reason is that today SearcherManager knows how to handle IndexSearcher instances. I have a SearcherManager which manages a pair of IndexSearcher and TaxonomyReader pair.

Recently, few concurrency bugs were fixed in SearcherManager, and I realized that I need to apply them to my version as well. Which led me to think why can't we have an SM version which is generic enough so that both my version and Lucene's can benefit from?

The way I see SearcherManager, it can be divided into two parts: (1) the part that manages the logic of acquire/release/maybeReopen (i.e., ensureOpen, protect from concurrency stuff etc.), and (2) the part which handles IndexSearcher, or my SearcherTaxoPair. I'm thinking that if we'll have an interface with incRef/decRef/tryIncRef/maybeRefresh, we can make SearcherManager a generic class which handles this interface.

I will post a patch with the initial idea, and we can continue from there."
1,".toString on empty MultiPhraseQuery hits NPERoss Woolf hit this on java-user thread ""MultiPhraseQuery.toString() throws null pointer exception"".  It's still present on trunk..."
1,"Registering a Nodetype based on an existing NodeType failIf I create a new NodeTypeTemplate using the code show below,

           NodeTypeManagerImpl ntm = (NodeTypeManagerImpl) session.getWorkspace().getNodeTypeManager();
           NodeTypeDefinition nt = (NodeTypeDefinition) ntm.getNodeType(""wr:entity"");
           NodeTypeTemplate ntt = ntm.createNodeTypeTemplate(nt);

the list of declaredSuperType contains the same name of the original nodeType (repeted twice) and not the declaredSuperType of the original nodeType (in this example [nt:base, nt:file])

          ntt.getDeclaredSupertypeNames(); -> [wr:entity, wr:entity]"
1,"After IW.addIndexesNoOptimize, IW.close may hangSpinoff from here:

  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200804.mbox/%3c43128.192.168.1.71.1208561409.webmail@192.168.1.71%3e

The addIndexesNoOptimize method first merges eligible segments
according to the MergePolicy, and then copies over one by one any
remaining ""external"" segments.

That copy can possibly (rather rarely) result in new merges becoming
eligible because its size can change if the index being added was
created with autoCommit=false.

However, we fail to then invoke the MergeScheduler to run these
merges.  As a result, in close, where we wait until all running and
pending merges complete, we will never return.

The fix is simple: invoke the merge scheduler inside
copyExternalSegments() if any segments were copied.  I also added
defensive invocation of the merge scheduler during close, just in case
other code paths could allow for a merge to be added to the pending
queue but not scheduled.

"
0,Allow name to be set in PropertyInfoBuilder and NodeInfoBuilderCurrently the property name for new Properties and Nodes can only be set when their builder is created. I suggest to add methods for setting the names by themselves. 
1,"Constructor of IndexWriter let's runtime exceptions pop up, while keeping the writeLock obtainedConstructor of IndexWriter let's runtime exceptions pop up, while keeping the writeLock obtained.

The init method in IndexWriter catches IOException only (I got NegativeArraySize by reading up a _corrupt_ index), and now, there is no way to recover, since the writeLock will be kept obtained. Moreover, I don't have IndexWriter instance either, to ""grab"" the lock somehow, since the init() method is called from IndexWriter constructor.

Either broaden the catch to all exceptions, or at least provide some circumvention to clear up. In my case, I'd like to ""fallback"", just delete the corrupted index from disk and recreate it, but it is impossible, since the LOCK_HELD NativeFSLockFactory's entry about obtained WriteLock is _never_ cleaned out and is no (at least apparent) way to clean it out forcibly. I can't create new IndexWriter, since it will always fail with LockObtainFailedException."
1,"URIUtils.extractHost(...) throws a NumberFormatException line 310Original Jboss-seam-wicket-booking application in Jboss-4.2.3.GA started, post a login request thanks httpclient, then NumberFormatException.



regarding this page :
http://hc.apache.org/httpcomponents-client-dev/httpclient/clover/org/apache/http/client/utils/URIUtils.html

305 	   	// Extract the port suffix, if present
306 	   	if (host != null) {
307 	  	   int colon = host.indexOf(':');
308 	   	   if (colon >= 0) {
309 	   	      if (colon+1 < host.length()) {
310 	   	          port = Integer.parseInt(host.substring(colon+1));
311 	   	      }
312 	  	   host = host.substring(0,colon);
313 	   	   }
314 	   	}

resolving the port throw a NumberFormatException

java.lang.NumberFormatException: For input string: ""8080;jsessionid=9E9EDA0B6E1CDD499A0A15C4A8F212D8""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
	at java.lang.Integer.parseInt(Integer.java:458)
	at java.lang.Integer.parseInt(Integer.java:499)
	at org.apache.http.client.utils.URIUtils.extractHost(URIUtils.java:310)
	at org.apache.http.impl.client.AbstractHttpClient.determineTarget(AbstractHttpClient.java:764)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:754)
	at org.tagbrowser.api.TagBrowser.request(TagBrowser.java:109)


another case of this problem canbe found hier :
https://gitorious.org/yacy/rc1/commit/8b0920b0b5eb67ae17eec24c1bf3a059543cb6e8/diffs"
0,"Consolidate type safe wrappers for commons-collection classesVarious places define their own type safe wrappers for classes from commons-collections (i.e. FilterIterator, TransformIterator and the like). I would like to consolidate them into one single place. "
0,"Add FieldCache.getTermBytes, to load term data as byte[]With flex, a term is now an opaque byte[] (typically, utf8 encoded unicode string, but not necessarily), so we need to push this up the search stack.

FieldCache now has getStrings and getStringIndex; we need corresponding methods to load terms as native byte[], since in general they may not be representable as String.  This should be quite a bit more RAM efficient too, for US ascii content since each character would then use 1 byte not 2."
0,"Alternative depth-based DOT layout ordering in FST's UtilsUtils.toDot() dumps GraphViz's DOT file, but it can be quite difficult to read. This patch provides an alternative layout that is probably a little bit easier on the eyes (well, as far as larger FSTs can be ;)"
1,"FSDirectory.copy() impl is unsafeThere are a couple of issues with it:

# FileChannel.transferFrom documents that it may not copy the number of bytes requested, however we don't check the return value. So need to fix the code to read in a loop until all bytes were copied..
# When calling addIndexes() w/ very large segments (few hundred MBs in size), I ran into the following exception (Java 1.6 -- Java 1.5's exception was cryptic):
{code}
Exception in thread ""main"" java.io.IOException: Map failed
    at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:770)
    at sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:450)
    at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:523)
    at org.apache.lucene.store.FSDirectory.copy(FSDirectory.java:450)
    at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:3019)
Caused by: java.lang.OutOfMemoryError: Map failed
    at sun.nio.ch.FileChannelImpl.map0(Native Method)
    at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:767)
    ... 7 more
{code}

I changed the impl to something like this:
{code}
long numWritten = 0;
long numToWrite = input.size();
long bufSize = 1 << 26;
while (numWritten < numToWrite) {
  numWritten += output.transferFrom(input, numWritten, bufSize);
}
{code}

And the code successfully adds the indexes. This code uses chunks of 64MB, however that might be too large for some applications, so we definitely need a smaller one. The question is how small so that performance won't be affected, and it'd be great if we can let it be configurable, however since that API is called by other API, such as addIndexes, not sure it's easily controllable.

Also, I read somewhere (can't remember now where) that on Linux the native impl is better and does copy in chunks. So perhaps we should make a Linux specific impl?"
0,Buffered I/O in IndexInfosIndexInfos currently uses plain Input/OutputStreams without buffering.
0,Config: make all elements in the security configuration optionalin order not to introduce new mandatory elements in the security configuration.
1,"Problems with BundleDbPersistenceManager getAllNodeIdsWhen using MySQL:
The problem arises when the method parameter maxcount is less than the total amount of records in the bundle table.

First of all I found out that mysql orders the nodeid objects different than jackrabbit does. The following test describes this idea:

    public void testMySQLOrderByNodeId() throws Exception {
        NodeId nodeId1 = new NodeId(""7ff9e87c-f87f-4d35-9d61-2e298e56ac37"");
        NodeId nodeId2 = new NodeId(""9fd0d452-b5d0-426b-8a0f-bef830ba0495"");

        PreparedStatement stmt = connection.prepareStatement(""SELECT NODE_ID FROM DEFAULT_BUNDLE WHERE NODE_ID = ? OR NODE_ID = ? ORDER BY NODE_ID"");

        Object[] params = new Object[] { nodeId1.getRawBytes(), nodeId2.getRawBytes() };
        stmt.setObject(1, params[0]);
        stmt.setObject(2, params[1]);

        ArrayList<NodeId> nodeIds = new ArrayList<NodeId>();
        ResultSet resultSet = stmt.executeQuery();
        while(resultSet.next()) {
            NodeId nodeId = new NodeId(resultSet.getBytes(1));
            System.out.println(nodeId);
            nodeIds.add(nodeId);
        }
        Collections.sort(nodeIds);
        for (NodeId nodeId : nodeIds) {
            System.out.println(nodeId);
        }
    }

Which results in the following output:

7ff9e87c-f87f-4d35-9d61-2e298e56ac37
9fd0d452-b5d0-426b-8a0f-bef830ba0495
9fd0d452-b5d0-426b-8a0f-bef830ba0495
7ff9e87c-f87f-4d35-9d61-2e298e56ac37


Now the problem with the getAllNodeIds method is that it fetches an extra 10 records on top of maxcount (to avoid a problem where the first key is not the one you that is wanted). Afterwards it skips a number of records again, this time using nodeid.compareto. This compareto statement returns true unexpectedly for mysql because the code doesn't expect the mysql ordering.

I had the situation where I had about 17000 records in the bundle table but consecutively getting the ids a thousand records at a time returned only about 8000 records in all.
"
0,Add plugable mechanism for import/export of webdav-serveradd plugable mechanism to improve flexible configuration of the jcr-server
0,"Change IndexSearcher multisegment searches to search each individual segment using a single HitCollectorThis issue changes how an IndexSearcher searches over multiple segments. The current method of searching multiple segments is to use a MultiSegmentReader and treat all of the segments as one. This causes filters and FieldCaches to be keyed to the MultiReader and makes reopen expensive. If only a few segments change, the FieldCache is still loaded for all of them.

This patch changes things by searching each individual segment one at a time, but sharing the HitCollector used across each segment. This allows FieldCaches and Filters to be keyed on individual SegmentReaders, making reopen much cheaper. FieldCache loading over multiple segments can be much faster as well - with the old method, all unique terms for every segment is enumerated against each segment - because of the likely logarithmic change in terms per segment, this can be very wasteful. Searching individual segments avoids this cost. The term/document statistics from the multireader are used to score results for each segment.

When sorting, its more difficult to use a single HitCollector for each sub searcher. Ordinals are not comparable across segments. To account for this, a new field sort enabled HitCollector is introduced that is able to collect and sort across segments (because of its ability to compare ordinals across segments). This TopFieldCollector class will collect the values/ordinals for a given segment, and upon moving to the next segment, translate any ordinals/values so that they can be compared against the values for the new segment. This is done lazily.

All and all, the switch seems to provide numerous performance benefits, in both sorted and non sorted search. We were seeing a good loss on indices with lots of segments (1000?) and certain queue sizes / queries, but the latest results seem to show thats been mostly taken care of (you shouldnt be using such a large queue on such a segmented index anyway).

* Introduces
** MultiReaderHitCollector - a HitCollector that can collect across multiple IndexReaders. Old HitCollectors are wrapped to support multiple IndexReaders.
** TopFieldCollector - a HitCollector that can compare values/ordinals across IndexReaders and sort on fields.
** FieldValueHitQueue - a Priority queue that is part of the TopFieldCollector implementation.
** FieldComparator - a new Comparator class that works across IndexReaders. Part of the TopFieldCollector implementation.
** FieldComparatorSource - new class to allow for custom Comparators.
* Alters
** IndexSearcher uses a single HitCollector to collect hits against each individual SegmentReader. All the other changes stem from this ;)
* Deprecates
** TopFieldDocCollector
** FieldSortedHitQueue
"
1,"SloppyPhraseScorer sometimes computes Infinite freqreported on user list:
http://www.lucidimagination.com/search/document/400cbc528ed63db9/score_of_infinity_on_dismax_query
"
0,"Path should not be encoded in HttpMethodBaseI suggest to change the protocol or add a new method for this one

protected static String generateRequestLine(HttpConnection connection,
	String name, String reqPath,
	String qString, String protocol);
so that we can choose to use URIUtil.encode(reqPath, URIUtil.pathSafe()) or not

The reason is that after the encoding process, some server cannot recognize this
Actually, I am handling a project of the Method Propfind(for getting mail from 
Hotmail) and I find that the restriction of Hotmail server is quite high, and 
if the address is encoded, it does not work."
1,"Problems with File Copy using WebDAVWhen i make a copy of files from one workspace to other (CTRL-C -> CTRL-V). The file isnt copied, but the original file is deleted."
0,"SQL2 queries are not loggedSQL2 queries are constructed via QueryObjectModel, and ran via QueryObjectModelImpl which does not log the run time.
I'll attach a run time log similar to the old one."
0,Add user manager performance tests We should add some performance tests for validating JCR-2710 and related. That is we should measure performance for creating users and groups and adding/removing users to/from groups. This should be done for both repository configurations: one with the old content model (group membership in property) and one with the new content model introduced with JCR-2710 (group membership in b-tree like node structure). 
1,"NodeTypeDefDiff compares to restrictiveThe NodeTypeDefDiff class is used to compare NodeTypeDef instances. Unfortunately this class reports two NodeTypeDef instances which are not equal but have no structural difference as having trivial changes. The correct result would be to have no modification at all.

I suggest to modify the NodeTypeDefDiff.init() method such, that the initial type is ""NONE"" instead of ""TRIVIAL"" and to first compare the ""hasOrderableChildNodes"" first and raise the level to ""TRIVIAL"" if not equal. Next the rest of the current comparisons would follow."
1,"Use of java.net.URI.resolve() is buggyThe use of java.net.URI.resolve() is buggy (see <http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4708535>). Affected class: org.apache.http.impl.client.DefaultRedirectHandler.

Proposed solution: Create a resolve(URI, String) method in o.a.h.client.utils.URLUtils."
0,Make HighFreqTerms.TermStats class publicIt's not possible to use public methods in contrib/misc/... /HighFreqTerms from outside the package because the return type has package visibility. I propose to move TermStats class to a separate file and make it public.
0,"deprecate Document.fields(), add getFields()A simple API improvement that I'm going to commit if nobody objects."
1,"IndexReader.isCurrent() lies if documents were only removed by latest commitUsecase is as following:

1. Get indexReader via indexWriter.
2. Delete documents by Term via indexWriter. 
3. Commit indexWriter.
4. indexReader.isCurrent() returns true.

Usually there is a check if index reader is current. If not then it is reopened (re-obtained via writer or ect.). But this causes the problem when documents can still be found through the search after deletion.
Testcase is attached."
0,"Test class in the main source treeorg.apache.jackrabbit.core.TestRepository is in the main source folder
(src/main/java) instead of the test folder (src/test/java).

The build of jackrabbit-core is successful even if I move the class
to the test folder, so it looks like it was just a mistake."
1,"DisjunctionSumScorer gives slightly (float iotas) different scores when you .nextDoc vs .advanceSpinoff from LUCENE-1536.

I dug into why we hit a score diff when using luceneutil to benchmark
the patch.

At first I thought it was BS1/BS2 difference, but because of a bug in
the patch it was still using BS2 (but should be BS1) -- Robert's last
patch fixes that.

But it's actually a diff in BS2 itself, whether you next or advance
through the docs.

It's because DisjunctionSumScorer, when summing the float scores for a
given doc that matches multiple sub-scorers, might sum in a different
order, when you had .nextDoc'd to that doc than when you had .advance'd
to it.

This in turn is because the PQ used by that scorer (ScorerDocQueue)
makes no effort to break ties.  So, when the top N scorers are on the
same doc, the PQ doesn't care what order they are in.

Fixing ScorerDocQueue to break ties will likely be a non-trivial perf
hit, though, so I'm not sure whether we should do anything here..."
1,"TermSpans skipTo() doesn't always move forwardsIn TermSpans (or the anonymous Spans class returned by SpansTermQuery, depending on the version), the skipTo() method is improperly implemented if the target doc is less than or equal to the current doc:

  public boolean skipTo(int target) throws IOException {
          // are we already at the correct position?
          if (doc >= target) {
            return true;
          }

          ...


This violates the correct behavior (as described in the Spans interface documentation), that skipTo() should always move forwards, in other words the correct implementation would be:

if (doc >= target) {
  return next();
}

This bug causes particular problems if one wants to use the payloads feature - this is because if one loads a payload, then performs a skipTo() to the same document, then tries to load the ""next"" payload, the spans hasn't changed position and it attempts to load the same payload again (which is an error)."
0,"SweetSpotSimiliarityThis is a new Similarity implimention for the contrib/miscellaneous/ package, it provides a Similiarty designed for people who know the ""sweetspot"" of their data.  three major pieces of functionality are included:

1) a lengthNorm which creates a ""platuea"" of values.
2) a baseline tf that provides a fixed value for tf's up to a minimum, at which point it becomes a sqrt curve (this is used by the tf(int) function.
3) a hyperbolic tf function which is best explained by graphing the equation.  this isn't used by default, but is available for subclasses to call from their own tf functions.

All constants used in all functions are configurable.  In the case of lengthNorm, the constants are configurable per field, as well as allowing for defaults for unspecified fields."
0,SPI: provide batch read functionalityextend RepositoryService interface to allow for BatchRead and modify jcr2spi accordingly.
0,CharacterCache - references deleted CharacterCache is deprecated by Character.valueOf(c) . Hence the latter is chosen over the former. 
0,"Mark pending nodes in IndexingQueue directly in indexThe index currently writes an indexing_queue.log file which contains all nodes that timed out while text was extracted. Instead, the index itself should mark an indexed node as pending. This is more robust because no additional file must be written."
0,"JSR 283: Access Nodes and Properties by Array of ""NameGlob""The proposed final draft contains new variants of Node.getNodes and Node.getProperties:

- Node.getNodes(String[] nameGlobs)
- Node.getProperties(String[] nameGlobs)

see section 5.2.2 Iterating Over Child Items and 5.2.2.2 Name Globs"
0,"TopTermsScoringBooleanQueryRewrite minscorewhen using the TopTermsScoringBooleanQueryRewrite (LUCENE-2123), it would be nice if MultiTermQuery could set an attribute specifying the minimum required score once the Priority Queue is filled. 

This way, FilteredTermsEnums could adjust their behavior accordingly based on the minimal score needed to actually be a useful term (i.e. not just pass thru the pq)

An example is FuzzyTermsEnum: at some point the bottom of the priority queue contains words with edit distance of 1 and enumerating any further terms is simply a waste of time.
This is because terms are compared by score, then termtext. So in this case FuzzyTermsEnum could simply seek to the exact match, then end.

This behavior could be also generalized for all n, for a different impl of fuzzyquery where it is only looking in the term dictionary for words within edit distance of n' which is the lowest scoring term in the pq (they adjust their behavior during enumeration of the terms depending upon this attribute).

Other FilteredTermsEnums could make use of this minimal score in their own way, to drive the most efficient behavior so that they do not waste time enumerating useless terms.
"
0,"FastVectorHighlighter: support for additional queriesI am using fastvectorhighlighter for some strange languages and it is working well! 

One thing i noticed immediately is that many query types are not highlighted (multitermquery, multiphrasequery, etc)
Here is one thing Michael M posted in the original ticket:

{quote}
I think a nice [eventual] model would be if we could simply re-run the
scorer on the single document (using InstantiatedIndex maybe, or
simply some sort of wrapper on the term vectors which are already a
mini-inverted-index for a single doc), but extend the scorer API to
tell us the exact term occurrences that participated in a match (which
I don't think is exposed today).
{quote}

Due to strange requirements I am using something similar to this (but specialized to our case).
I am doing strange things like forcing multitermqueries to rewrite into boolean queries so they will be highlighted,
and flattening multiphrasequeries into boolean or'ed phrasequeries.
I do not think these things would be 'fast', but i had a few ideas that might help:

* looking at contrib/highlighter, you can support FilteredQuery in flatten() by calling getQuery() right?
* maybe as a last resort, try Query.extractTerms() ?
"
0,"Update link for javadocs from 1.0 to 1.3On this page: http://jackrabbit.apache.org/doc/arch/overview/jcrlevels.html

You see this link:
Browse current Jackrabbit API: http://jackrabbit.apache.org/api-1/index.html

This should point to the latest javadoc version."
0,"improve how IndexWriter uses RAM to buffer added documentsI'm working on a new class (MultiDocumentWriter) that writes more than
one document directly into a single Lucene segment, more efficiently
than the current approach.

This only affects the creation of an initial segment from added
documents.  I haven't changed anything after that, eg how segments are
merged.

The basic ideas are:

  * Write stored fields and term vectors directly to disk (don't
    use up RAM for these).

  * Gather posting lists & term infos in RAM, but periodically do
    in-RAM merges.  Once RAM is full, flush buffers to disk (and
    merge them later when it's time to make a real segment).

  * Recycle objects/buffers to reduce time/stress in GC.

  * Other various optimizations.

Some of these changes are similar to how KinoSearch builds a segment.
But, I haven't made any changes to Lucene's file format nor added
requirements for a global fields schema.

So far the only externally visible change is a new method
""setRAMBufferSize"" in IndexWriter (and setMaxBufferedDocs is
deprecated) so that it flushes according to RAM usage and not a fixed
number documents added.
"
1,FastVectorHighlighter: highlighted term is out of alignment in multi-valued NOT_ANALYZED field
1,"Highlighting overlapping tokens outputs doubled wordsIf for the text ""the fox did not jump"" we generate following tokens :
(the, 0, 0-3),({fox},0,0-7),(fox,1,4-7),(did,2,8-11),(not,3,12,15),(jump,4,16,18)

If TermVector for field is stored WITH_OFFSETS and not WITH_POSITIONS_OFFSETS, highlighing would output
""the<em>the fox</em> did not jump""

I join a patch with 2 additive JUnit tests and a fix of TokenSources class where token ordering by offset did'nt manage well overlapping tokens.
"
0,"[PATCH] remove minor unneeded code stutterCode has a repeated method call on isOrderable for no reason as such

{code}
public String getSupportedMethods() {
        String ms = super.getSupportedMethods();
        if (isOrderable()) {
            StringBuffer sb = new StringBuffer(ms);
            // Ordering
            if (isOrderable()) {
                sb.append("", "").append(OrderingResource.METHODS);
            }
            return sb.toString();
        } else {
            return ms;
        }
    }
{code}

patch cleans this up."
0,"internal hashing improvementsInternal power-of-two closed hashtable traversal in DocumentsWriter and CharArraySet could be better.

Here is the current method of resolving collisions:
    if (text2 != null && !equals(text, len, text2)) {
      final int inc = code*1347|1;
      do {
        code += inc;
        pos = code & mask;
        text2 = entries[pos];
      } while (text2 != null && !equals(text, len, text2));

The problem is that two different hashCodes with the same lower bits will keep picking the same slots (the upper bits will be ignored).
This is because multiplication (*1347) only really shifts bits to the left... so given that the two codes already matched on the right, they will both pick the same increment, and this will keep them on the same path through the table (even though it's being added to numbers that differ on the left).  To resolve this, some bits need to be moved to the right when calculating the increment.

"
0,"IndexReader.reopen()This is Robert Engels' implementation of IndexReader.reopen() functionality, as a set of 3 new classes (this was easier for him to implement, but should probably be folded into the core, if this looks good).
"
1,"Data Store: DB2 fails to create the tableDB2 throws an exception(1) when creating the table. The correct SQL sentence to create it is:
createTable=CREATE TABLE ${tablePrefix}${table}(ID VARCHAR(255) PRIMARY KEY NOT NULL, LENGTH BIGINT, LAST_MODIFIED BIGINT, DATA BLOB(1000M))
(1): Sorry but I don't have the exception information since I made this change a few weeks ago."
1,"Locking two same-name siblings and unlocking first apparently unlocks second instead.Executing the following test that unlocks the first of two locked same-name siblings:

public void testLocking() throws RepositoryException {
       Session jcrSession = ((S1SessionImpl) session).getSession();
       Node rootNode = jcrSession.getRootNode();

       Node n1 = rootNode.addNode(""path"");
       n1.addMixin(""mix:lockable"");
       Node n2 = rootNode.addNode(""path"");
       n2.addMixin(""mix:lockable"");

       jcrSession.save();

       n1.lock(true, true);
       n2.lock(true, true);

       System.out.println(""n1.isLocked() = "" + n1.isLocked());
       System.out.println(""n2.isLocked() = "" + n2.isLocked());
       assertTrue(n1.isLocked());
       assertTrue(n2.isLocked());

       n1.save();
       n1.unlock();

       System.out.println(""n1.isLocked() = "" + n1.isLocked());
       System.out.println(""n2.isLocked() = "" + n2.isLocked());
       assertFalse(n1.isLocked());
       assertTrue(n2.isLocked());
   }

Results in:

n1.isLocked() = true
n2.isLocked() = true
n1.isLocked() = true
n2.isLocked() = false

which is wrong."
0,"Allow Directory.copy() to accept a collection of file names to be copiedPar example, I want to copy files pertaining to a certain commit, and not everything there is in a Directory."
0,"Using deprecated class javax.servlet.http.HttpUtils[javac]
test-webapp/src/org/apache/commons/httpclient/RedirectServlet.java:74: warning:
javax.servlet.http.HttpUtils in javax.servlet.http has been deprecated
    [javac]             to =
HttpUtils.getRequestURL(request).append(""?"").append(request.getQueryString()).toString();

The javax.servlet.http.HttpUtils class is deprecated in Tomcat 4.1.18 and should
not be used."
0,"DateTools.java general improvementsApplying the attached patch shows the improvements to DateTools.java that I think should be done. All logic that does anything at all is moved to instance methods of the inner class Resolution. I argue this is more object-oriented.

1. In cases where Resolution is an argument to the method, I can simply invoke the appropriate call on the Resolution object. Formerly there was a big branch if/else.
2. Instead of ""synchronized"" being used seemingly everywhere, synchronized is used to sync on the object that is not threadsafe, be it a DateFormat or Calendar instance.
3. Since different DateFormat and Calendar instances are created per-Resolution, there is now less lock contention since threads using different resolutions will not use the same locks.
4. The old implementation of timeToString rounded the time before formatting it. That's unnecessary since the format only includes the resolution desired.
5. round() now uses a switch statement that benefits from fall-through (no break).

Another debatable improvement that could be made is putting the resolution instances into an array indexed by format length. This would mean I could remove the switch in lookupResolutionByLength() and avoid the length constants there. Maybe that would be a bit too over-engineered when the switch is fine."
1,"MultiSearcher.explain returns incorrect score/explanation relating to docFreqCreating 2 different indexes, searching  each individually and print score details and compare to searching both indexes with MulitSearcher and printing score details.  
 
The ""docFreq"" value printed isn't correct - the values it prints are as if each index was searched individually.

Code is like:
{code}
MultiSearcher multi = new MultiSearcher(searchables);
Hits hits = multi.search(query);
for(int i=0; i<hits.length(); i++)
{
  Explanation expl = multi.explain(query, hits.id(i));
  System.out.println(expl.toString());
}
{code}

I raised this in the Lucene user mailing list and was advised to log a bug, email thread given below.

{noformat} 
-----Original Message-----
From: Chris Hostetter  
Sent: Friday, December 07, 2007 10:30 PM
To: java-user
Subject: Re: does the MultiSearcher class calculate IDF properly?


a quick glance at the code seems to indicate that MultiSearcher has code 
for calcuating the docFreq accross all of the Searchables when searching 
(or when the docFreq method is explicitly called) but that explain method 
just delegates to Searchable that the specific docid came from.

if you compare that Explanation score you got with the score returned by 
a HitCollector (or TopDocs) they probably won't match.

So i would say ""yes MultiSearcher calculates IDF properly, but 
MultiSeracher.explain is broken.  Please file a bug about this, i can't 
think of an easy way to fix it, but it certianly seems broken to me.


: Subject: does the MultiSearcher class calculate IDF properly?
: 
: I tried the following.  Creating 2 different indexes, search each
: individually and print score details and compare to searching both
: indexes with MulitSearcher and printing score details.  
: 
: The ""docFreq"" value printed don't seem right - is this just a problem
: with using Explain together with the MultiSearcher?
: 
: 
: Code is like:
: MultiSearcher multi = new MultiSearcher(searchables);
: Hits hits = multi.search(query);
: for(int i=0; i<hits.length(); i++)
: {
:   Explanation expl = multi.explain(query, hits.id(i));
:   System.out.println(expl.toString());
: }
: 
: 
: Output:
: id = 14 score = 0.071
: 0.07073946 = (MATCH) fieldWeight(contents:climate in 2), product of:
:   1.0 = tf(termFreq(contents:climate)=1)
:   1.8109303 = idf(docFreq=1)
:   0.0390625 = fieldNorm(field=contents, doc=2)
{noformat} "
1,"Garbage data when reading a compressed, text field, lazilylazy compressed text fields is a case that was neglected during lazy field implementation.  TestCase and patch provided.

"
1,"PrecedenceQueryParser misinterprets queries starting with NOT""NOT foo AND baz"" is parsed as ""-(+foo +baz)"" instead of ""-foo +bar"".

(I'm setting parser.setDefaultOperator(PrecedenceQueryParser.AND_OPERATOR) but the issue applies otherwise too.)
"
0,"small speedups to bulk mergingThe bulk merging code, for stored fields & term vectors, was calling isDeleted twice for each deleted doc.

Patch also changes DocumentsWriter to use IndexWriter.message for its infoStream messages."
0,"random localization test failuresSome tests fail randomly (hard to reproduce). It appears to me that this is caused by uninitialized date fields. For example Uwe reported a failure today in this test of TestQueryParser:

{code}
 /** for testing legacy DateField support */
  public void testLegacyDateRange() throws Exception {
    String startDate = getLocalizedDate(2002, 1, 1, false);
    String endDate = getLocalizedDate(2002, 1, 4, false);
{code}

if you look at the helper getLocalizedDate, you can see if the 4th argument is false, it does not initialize all date field functions.
{code}
  private String getLocalizedDate(int year, int month, int day, boolean extendLastDate) {
 Calendar calendar = new GregorianCalendar();
 calendar.set(year, month, day);
 if (extendLastDate) {
      calendar.set(Calendar.HOUR_OF_DAY, 23);
      calendar.set(Calendar.MINUTE, 59);
      calendar.set(Calendar.SECOND, 59);
 ...
}
{code}

I think the solution to this is that in all tests, whereever we create new GregorianCalendar(), it should be followed by a call to Calendar.clear().
This will ensure that we always initialize unused calendar fields to zero, rather than being dependent on the local time."
0,Disable consistency check per defaultThere should be a way to disable the consistency check entirely. Currently a consistency check is performed on startup whenever the redo log is applied. For large workspaces this may take a long time and should only be performed when 'requested'.
1,"While indexing Turkish web pages, ""Parse Aborted: Lexical error...."" occursWhen I try to index Turkish page if there is a Turkish specific character in the HTML specific tag HTML parser gives ""Parse Aborted: Lexical error.on ... line"" error.
For this case ""<IMG SRC=""../images/head.jpg"" WIDTH=570 HEIGHT=47 BORDER=0 ALT=""ş"">"" exception address ""ş"" character (which has 351 ascii value) as an error. OR ı character in title tag.
<a title=""(ııı)"">

Turkish character in the content do not create any problem."
0,"Some concurrency improvements for NRTSome concurrency improvements for NRT

I found & fixed some silly thread bottlenecks that affect NRT:

  * Multi/DirectoryReader.numDocs is synchronized, I think so only 1
    thread computes numDocs if it's -1.  I removed this sync, and made
    numDocs volatile, instead.  Yes, multiple threads may compute the
    numDocs for the first time, but I think that's harmless?

  * Fixed BitVector's ctor to set count to 0 on creating a new BV, and
    clone to copy the count over; this saves CPU computing the count
    unecessarily.

  * Also strengthened assertions done in SR, testing the delete docs
    count.

I also found an annoying thread bottleneck that happens, due to CMS.
Whenever CMS hits the max running merges (default changed from 3 to 1
recently), and the merge policy now wants to launch another merge, it
forces the incoming thread to wait until one of the BG threads
finishes.

This is a basic crude throttling mechanism -- you force the mutators
(whoever is causing new segments to appear) to stop, so that merging
can catch up.

Unfortunately, when stressing NRT, that thread is the one that's
opening a new NRT reader.

So, the first serious problem happens when you call .reopen() on your
NRT reader -- this call simply forwards to IW.getReader if the reader
was an NRT reader.  But, because DirectoryReader.doReopen is
synchronized, this had the horrible effect of holding the monitor lock
on your main IR.  In my test, this blocked all searches (since each
search uses incRef/decRef, still sync'd until LUCENE-2156, at least).
I fixed this by making doReopen only sync'd on this if it's not simply
forwarding to getWriter.  So that's a good step forward.

This prevents searches from being blocked while trying to reopen to a
new NRT.

However... it doesn't fix the problem that when an immense merge is
off and running, opening an NRT reader could hit a tremendous delay
because CMS blocks it.  The BalancedSegmentMergePolicy should help
here... by avoiding such immense merges.

But, I think we should also pursue an improvement to CMS.  EG, if it
has 2 merges running, where one is huge and one is tiny, it ought to
increase thread priority of the tiny one.  I think with such a change
we could increase the max thread count again, to prevent this
starvation.  I'll open a separate issue....
"
0,TestFieldsReader - TestLazyPerformance problems w/ permissions in temp dir in multiuser environmentWas trying to setup some enhancements to the nightly builds and the testLazyPerformance test failed in TestFieldsReader since it couldn't delete the lazyDir directory from someone else's running of the test.  Will change it to append user.name System property to the directory name.
0,Backport JCR-1111: Access to version history results in reading all versions of versionable nodeBackport issue JCR-1111 (Accesss to version history results in reading all versions of versionable node) to 1.3 branch for 1.3.4 (separate issue to avoid re-opening JCR-1111 which was already released with 1.4).
0,"Http Client give sme message when proxy/http endpoint is downWhether Http sever endpoint is down or the proxy server is down we get the same stack trace as:

java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:333)
	at java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:195)
	at java.net.PlainSocketImpl.connect(PlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:366)
	at java.net.Socket.connect(Socket.java:518)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.commons.httpclient.protocol.ReflectionSocketFactory.createSocket(ReflectionSocketFactory.java:139)
	at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:124)
	at org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:706)
	at org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpConnectionAdapter.open(MultiThreadedHttpConnectionManager.java:1321)
	at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:386)
	at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:170)
	at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:396)
	at com.approuter.module.http.protocol.HttpTransportSender.perform(HttpTransportSender.java:214)
	at 


It will be good if we can get information whether the proxy was down or the Http endpoint.

"
0,"javadoc writing and generation with mvn""mvn -source 1.5 javadoc:javadoc"" does not work because following lines must be added to pom.xml
<plugin>
      <artifactId>maven-javadoc-plugin</artifactId>
      <configuration>
        <source>1.5</source>
      </configuration>
    </plugin> 
Please also write more comprehensive javadocs to allow better source understanding and developer co-operation"
0,"Safe namespace registrationThe namespace registration methods provided by the JCR NamespaceRegistry API are cumbersome to use and vulnerable to race conditions in the event of conflicting prefix mappings. This problem was discussed lately on the mailing list (see http://article.gmane.org/gmane.comp.apache.jackrabbit.devel/6805) and one symptom of the problem is the new code in NodeTypeManagerImpl (see JCR-349):

    //  Registers a namespace...
    try {
        nsReg.getPrefix(uri);
    } catch (NamespaceException e1) {
        String original = prefix;
        for (int i = 2; true; i++) {
            try {
                nsReg.registerNamespace(prefix, uri);
                return;
            } catch (NamespaceException e2) {
                prefix = original + i;
            }
        }
    }

We should add an internal convenience method like NamespaceRegistryImpl.safeRegisterNamespace(String prefixHint, String uri) that, instead of throwing an exception, would automatically generate and use a unique prefix based on the given hint when a prefix conflict is detected."
0,"JSR 283: References and Dereferencing of Property ValuesReferences
--------------------------------------------------------------------------------------------------------------------------
new methods are:

- Node.getReferences(String name) PropertyIterator 
- Node.getWeakReferences() PropertyIterator 
- Node.getWeakReferences(String name) PropertyIterator


Derferencing
--------------------------------------------------------------------------------------------------------------------------
As of JSR 283 the following property types may be dereferenced to a Node:

- REFERENCE
- WEAKREFERENCE
- PATH
- any type that can be converted to either of the types above

The new method
- Property.getProperty() returns the Property pointed to by a PATH value.
- any type that can be converted to PATH




"
1,"PathElement.equals doesn't handle INDEX_UNDEFINEDPathElement (and therefore Path) comparisons fail when INDEX_UNDEFINED is used (it's treated differently from INDEX_DEFAULT).
"
0,"It should be possible to create a non-transient Repository inside the JCARepositoryManagerWith JCR-2555 jukka changed the code to create a Repository with the RepositoryFactory mechanism.
It should be possible to create a non-transient Repository
"
1,"Deadlock in acl.EntryCollector / ItemManagerHere's another three-way deadlock that we've encountered:

* Thread A holds a downgraded SISM write lock and is about to start delivering observation events to synchronous listeners
* Thread B wants to write something and blocks waiting for the SISM write lock (since A holds the lock)
* Thread C wants to read something and blocks waiting for the SISM read lock (since B waits for the lock)

Normally such a scenario is handled without any problems, but there's a problem if the session used by thread C has a synchronous observation listener that attempts to read something from the repository during event delivery. In this case the following can happen:

* Thread C holds the ItemManager synchronization lock higher up in the call chain
* Observation listener code called by thread A attempts to read something from the repository, and blocks trying to acquire the ItemManager synchronization lock (since C holds it)

In principle such a scenario should never happen as an observation listener (much less a synchronous one) should never try to use the session that might already be in use by another thread.

Unfortunately the EntryCollector class in o.a.j.core.security.authorization.acl does not follow this guideline, which leads to the deadlock as shown below:

Thread A:
""127.0.0.1 [1297191119365] POST /bin/wcmcommand HTTP/1.0"" nid=1179 state=BLOCKED
    - waiting on <0x11c329fd> (a org.apache.jackrabbit.core.ItemManager)
    - locked <0x11c329fd> (a org.apache.jackrabbit.core.ItemManager)
     owned by 127.0.0.1 [1297191138443] POST /bin/wcmcommand HTTP/1.0 id=67
    at org.apache.jackrabbit.core.ItemManager.getNode(ItemManager.java:653)
    at org.apache.jackrabbit.core.ItemManager.getNode(ItemManager.java:605)
    at org.apache.jackrabbit.core.SessionImpl.getNode(SessionImpl.java:1406)
    at org.apache.jackrabbit.core.security.authorization.acl.EntryCollector.onEvent(EntryCollector.java:253)
    at org.apache.jackrabbit.core.observation.EventConsumer.consumeEvents(EventConsumer.java:246)
    at org.apache.jackrabbit.core.observation.ObservationDispatcher.dispatchEvents(ObservationDispatcher.java:214)
    at org.apache.jackrabbit.core.observation.EventStateCollection.dispatch(EventStateCollection.java:475)
    at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:786)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:1488)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:349)
    at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:354)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:324)
    at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:328)
    at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1141)
    at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:920)

Thread B:
""Thread-2438"" nid=2582 state=WAITING
    - waiting on <0x166e790e> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
    - locked <0x166e790e> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock)
    at java.lang.Object.wait(Native Method)
    at java.lang.Object.wait(Object.java:485)
    at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$WriterLock.acquire(Unknown Source)
    at org.apache.jackrabbit.core.state.DefaultISMLocking$WriteLockImpl.<init>(DefaultISMLocking.java:76)
    at org.apache.jackrabbit.core.state.DefaultISMLocking$WriteLockImpl.<init>(DefaultISMLocking.java:70)
    at org.apache.jackrabbit.core.state.DefaultISMLocking.acquireWriteLock(DefaultISMLocking.java:64)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.acquireWriteLock(SharedItemStateManager.java:1808)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.access$200(SharedItemStateManager.java:112)
    at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:565)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:1458)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:1488)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:349)
    at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:354)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:324)
    at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:328)
    at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1141)
    at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:920)

Thread C:
""127.0.0.1 [1297191138443] POST /bin/wcmcommand HTTP/1.0"" nid=67 state=WAITING
    - waiting on <0xf820edb> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$ReaderLock)
    - locked <0xf820edb> (a EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$ReaderLock)
    at java.lang.Object.wait(Native Method)
    at java.lang.Object.wait(Object.java:485)
    at EDU.oswego.cs.dl.util.concurrent.WriterPreferenceReadWriteLock$ReaderLock.acquire(Unknown Source)
    at org.apache.jackrabbit.core.state.DefaultISMLocking$ReadLockImpl.<init>(DefaultISMLocking.java:102)
    at org.apache.jackrabbit.core.state.DefaultISMLocking$ReadLockImpl.<init>(DefaultISMLocking.java:96)
    at org.apache.jackrabbit.core.state.DefaultISMLocking.acquireReadLock(DefaultISMLocking.java:53)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.acquireReadLock(SharedItemStateManager.java:1794)
    at org.apache.jackrabbit.core.state.SharedItemStateManager.getItemState(SharedItemStateManager.java:257)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.getNodeState(LocalItemStateManager.java:107)
    at org.apache.jackrabbit.core.state.LocalItemStateManager.getItemState(LocalItemStateManager.java:171)
    at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:200)
    at org.apache.jackrabbit.core.ItemManager.getItemData(ItemManager.java:391)
    at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:337)
    at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:638)
    at org.apache.jackrabbit.core.security.authorization.acl.ACLProvider$AclPermissions.canRead(ACLProvider.java:507)
      - locked java.lang.Object@6ad9b475
    at org.apache.jackrabbit.core.security.DefaultAccessManager.canRead(DefaultAccessManager.java:251)
    at org.apache.jackrabbit.core.query.lucene.QueryResultImpl.isAccessGranted(QueryResultImpl.java:374)
    at org.apache.jackrabbit.core.query.lucene.QueryResultImpl.collectScoreNodes(QueryResultImpl.java:353)
    at org.apache.jackrabbit.core.query.lucene.QueryResultImpl.getResults(QueryResultImpl.java:310)
    at org.apache.jackrabbit.core.query.lucene.SingleColumnQueryResult.<init>(SingleColumnQueryResult.java:70)
    at org.apache.jackrabbit.core.query.lucene.QueryImpl.execute(QueryImpl.java:133)
    at org.apache.jackrabbit.core.query.QueryImpl.execute(QueryImpl.java:127)
"
0,"Jackrabbit does not allow concurrent reads to the data store if copyWhenReading=falseJackrabbit does not allow concurrent reads to the data store if copyWhenReading=false, even if maxConnections>1.
See JCR-1184 for a test for this problem (run it with copyWhenReading=false).
"
1,"Error reading dataHi,

I have some problems with HttpClient HEAD. It works fine with a build of 
20020720 of HttpClient though.

It seems HttpClient is not reading correctly the returned HTTP response.

I'm attaching the logs.

Here is the output from Cactus build:



     [java]     [junit] Testcase: testLongProcess took 3.645 sec
     [java]     [junit]         Caused an ERROR
     [java]     [junit] Failed to get the test results. This is probably due 
to an error that happen
ed on the server side when trying to execute the tests. Here is what was 
returned by the server : [<
html><head><Long Process></head><body>Some data</body></html>
     [java]     [junit] ]
     [java]     [junit] org.apache.cactus.util.ChainedRuntimeException: Failed 
to get the test resul
ts. This is probably due to an error that happened on the server side when 
trying to execute the tes
ts. Here is what was returned by the server : [<html><head><Long 
Process></head><body>Some data</bod
y></html>
     [java]     [junit] ]
     [java]     [junit]         at 
org.apache.cactus.client.AbstractHttpClient.doTest(Unknown Source
)
     [java]     [junit]         at 
org.apache.cactus.AbstractWebTestCase.runWebTest(Unknown Source)
     [java]     [junit]         at 
org.apache.cactus.AbstractWebTestCase.runGenericTest(Unknown Sour
ce)
     [java]     [junit]         at org.apache.cactus.ServletTestCase.runTest
(Unknown Source)
     [java]     [junit]         at org.apache.cactus.AbstractTestCase.runBare
(Unknown Source)
     [java]     [junit] org.apache.cactus.client.ParsingException: Not a valid 
response. First 100 c
haracters of the reponse: [</webresult>HTTP/1.1 200 OK
     [java]     [junit] Server: Resin/2.1.2
     [java]     [junit] Content-Length: 23
     [java]     [junit] Date: Tue, 13 Aug 2002 08:45:2]
     [java]     [junit]         at 
org.apache.cactus.client.WebTestResultParser.readExceptionClassna
me(Unknown Source)
     [java]     [junit]         at 
org.apache.cactus.client.WebTestResultParser.parse(Unknown Source

Thanks
-Vincent"
1,"Observation tests failPath returned by Event.getPath() is wrong. It always returns the path to the parent node connected to the event. That is, if e.g. a node /foo/bar is created the path /foo is returned instead of /foo/bar.

This issue had been introduced with changed from api version 0.14 to 0.15."
1,"Bundle of events may be dropped due to NP.In [1], if the resolver fails to lookup a node entry, a NP is thrown. This exception will break the loop which forwards the events to the observer in [2].
This will result in an observer not receiving events that he should have.

[1] org.apache.jackrabbit.jcr2spi.hierarchy.HierarchyManagerImpl#lookup(ItemId workspaceItemId)
[2] org.apache.jackrabbit.jcr2spi.WorkspaceManager#onEventReceived(EventBundle[] eventBundles,InternalEventListener[] lstnrs)"
1,"An IOException or RuntimeException leaves the underlying socket in an undetermined stateIf an application level IOException or RuntimeException occurs, the underlying
socket will be in an undetermined state. In many cases, this will lead to zombie
connections in the pool that do not respond properly.

Simple example: uploading a file via POST. If we promise the server 1MB of data.
Shortly after starting the transfer an IOException occurs (e.g. the NFS server
the file was residing on stops responding). The connection is returned to the
pool (see HTTPCLIENT-302) but the the server is still expecting close to 1MB of data
on that socket. The next request on that socket (e.g. a GET) will send the HTTP
header but  the server thinks the header is part of the old stream and doesn't
respond."
0,JSR 283 support
0,"Typo in PropertyDefinitionTemplatesetValueConstarints should read setValueConstraints


"
1,"BlockJoinQuery advance fails on an assert in case of a single parent with child segmentThe BlockJoinQuery will fail on an assert when advance in called on a segment with a single parent with a child. The call to parentBits.prevSetBit(parentTarget - 1) will cause -1 to be returned, and the assert will fail, though its valid. Just removing the assert fixes the problem, since nextDoc will handle it properly.

Also, I don't understand the ""assert parentTarget != 0;"", with a comment of each parent must have one child. There isn't really a reason to add this constraint, as far as I can tell..., just call nextDoc in this case, no?"
1,"Benchmark package uses new TopFieldCollector but also still uses AUTO without resolving it - result is, our sort algorithms won't runAUTO does not work with TopFieldCollector. If you want to use AUTO with TopFieldCollector, we have a convienence method called detectType on SortField, but it is package protected and so cannot be used here as a stop gap or by users if they wanted to mix AUTO with TopFieldCollector. Lucene does still handle this for back compat internally. Solr got bit here when it was switched to use TopFieldCollector - no auto resolution was added (detectType help couldn't have been used due to visibility), and the result was that plugin code that used to be able to use AUTO would now blow up. You shouldn't use AUTO in Solr anyway though.

The Benchmark package got bit as well  when it moved to TopFieldCollector. Sort algorithms allowed auto if you specified it, or if you left off the type. Now our sort algs fail because they didn't specify a type.

I'll change to require the type to be specified to get the algs working again. I was thinking of just putting auto resolution in as a stop gap till 3.0 (when auto is removed), but since detectFieldType is package protected and I don't want to repeat it, disallowing auto seems the best way to go."
1,"Oracle bundle PM fails checking schema if 2 users use the same databaseWhen using the OracleBundlePersistenceManager there is an issue when two users use the same database for persistence. In  that case, the checkSchema() method of the BundleDbPersistenceManager  does not work like it should. More precisely, the call ""metaData.getTables(null,  null, tableName, null);"" will also includes table names of other  schemas/users. Effectively, only the first user of a database is able to create  the schema.

probably same issue as here: JCR-582"
1,"Benchmark alg line -  {[AddDoc(4000)]: 4} : * - causes an infinite loopBackground in http://www.mail-archive.com/java-dev@lucene.apache.org/msg10831.html 
The line  
   {[AddDoc(4000)]: 4} : * 
causes an infinite loop because the parallel sequence would mask the exhaustion from the outer sequential sequence.

To fix this the DocMaker exhaustion check should be modified to rely  on the doc maker instance only, and to be reset when the inputs are being reset. "
0,"Clean up spi-commons pom.xmlThe pom.xml contains lines that were copied from the jackrabbit-core but are not actually needed. A log4j.properties is also missing in test resources.

See attached patch."
0,"FastVectorHighlighter: some classes and members should be publicly accessible to implement FragmentsBuilderI intended to design custom FragmentsBuilder can be written and pluggable, though, when I tried to write it out of the FVH package, it came out that some classes and members should be publicly accessible."
1,"spi2dav: ItemInfoCache causes failure of (Workspace)RestoreTest#testRestoreWithUUIDConflict and variantswhile running the API version tests i found the (Workspace)RestoreTest.testRestoreWithUUIDConfict and variants failing. to be precise the test passes but
transiently removing the versionableNode2 in the teardown fails upon removal of the jcr:uuid property of the moved childnode.

having a closer look at it revealed that the problem is caused in the WorkspaceItemStateFactory where the property entry is retrieved from the
cache and subsequently checking if the path really matches fails. for test purposes i prevented the usage of the cached entry by returning false in WorkspaceItemStateFactory.isUpToDate  => the tests passed. 

as far as i know the same tests pass with spi2jcr.
michael, could it be that this is caused by a flaw in the iteminfo-cache logic? or is there something specific that needs to be adjusted in spi2dav?"
1,"non-contiguous LogMergePolicy should be careful to not select merges already runningNow that LogMP can do non-contiguous merges, the fact that it disregards which segments are already being merged is more problematic since it could result in it returning conflicting merges and thus failing to run multiple merges concurrently.
"
1,"Parsing built-in CND and XML nodetypes does not result in equal nt-definitionsi created a test in order to make sure builtin-nodetypes.xml and builtin-nodetypes.cnd provide the same definitions (actually i only wanted to test my own changes).

it reveals that the existing built-in NodeTypeDefinitions are not equal due to the following reason:

- in the xml-format nt:base is always specified if no other super type extends from nt:base
- in the cnd notation the nt:base is omitted (see below for quote from appendix of jsr 283) even if other super type(s) are
  defined and none of them extends from nt:base.

this affects the following nodetypes (all extending from mix:referenceable only):

nt:versionHistory
nt:version
nt:frozenNode
nt:resource


quote from public-review of jsr 283:

""7.2.2.4 Supertypes [...]
After the node type name comes the optional list of supertypes. If this element is not present and the node type is not a mixin (see 7.2.2.5 Options), then a supertype of nt:base is assumed.""


I'm not totally sure, if according to the quote above the built-in cnd-definitions are valid at all. since it states, that the nt:base is assumed if no other super type is defined. In the case of the node types above, mix:referenceable is defined to be the only super type, which is not totally true... the non-mixin types are always sub types of nt:base.

In either case: From my understanding the node types resulting from parsing the xml and the cnd file should be equal.
If the definitions are valid, we may need to adjust the CompactNodeTypeDefReader.




"
0,"potential memory leak when using ThreadSafeClientConnManagerWhen using ThreadSafeClientConnManager and developing with Jetty using auto-redeploy feature eventually I run into a PermGen out of memory exception.  I investigated with YourKit 8.0.6 and found a class loader circular reference in RefQueueWorker.  Not really sure what I was doing I made the refQueueHandler non-final and nulled it in the shutdown method of RedQueueWorker.  I don't seem to have the problem any longer with circular class loader references.

Here is a diff from 4.0-beta2


--- httpclient/src/main/java/org/apache/http/impl/conn/tsccm/RefQueueWorker.jav(revision 763223)
+++ httpclient/src/main/java/org/apache/http/impl/conn/tsccm/RefQueueWorker.jav(working copy)
@@ -50,7 +50,7 @@
     protected final ReferenceQueue<?> refQueue;
 
     /** The handler for the references found. */
-    protected final RefQueueHandler refHandler;
+    protected RefQueueHandler refHandler;
 
 
     /**
@@ -112,6 +112,8 @@
             this.workerThread = null; // indicate shutdown
             wt.interrupt();
         }
+
+        refHandler = null;
     }
 
 
"
1,"IW.getReader() returns inconsistent reader on RT BranchI extended the testcase TestRollingUpdates#testUpdateSameDoc to pull a NRT reader after each update and asserted that is always sees only one document. Yet, this fails with current branch since there is a problem in how we flush in the getReader() case. What happens here is that we flush all threads and then release the lock (letting other flushes which came in after we entered the flushAllThread context, continue) so that we could concurrently get a new segment that transports global deletes without the corresponding add. They sneak in while we continue to open the NRT reader which in turn sees inconsistent results.

I will upload a patch soon"
1,"incorrect jcr:uuid on frozen subnodeThe following program:

import javax.jcr.Repository;
import javax.jcr.Session;
import javax.jcr.SimpleCredentials;
import javax.jcr.Node;
import org.apache.jackrabbit.core.TransientRepository;

public class debug2 {
    public static void main(String[] args) throws Exception {
        Repository repository = new TransientRepository();
        Session session = repository.login(
                new SimpleCredentials(""username"", ""password"".toCharArray()));
        try {
            Node root = session.getRootNode();

            Node foo = root.addNode(""foo"");
            foo.addMixin(""mix:versionable"");

            Node bar = foo.addNode(""bar"");
            bar.addMixin(""mix:referenceable"");
            System.out.println(""bar:            "" + bar.getUUID());

            session.save();
            foo.checkin();

            Node frozenbar = foo.getBaseVersion().getNode(""jcr:frozenNode"").getNode(""bar"");
            System.out.println(""frozenbar UUID: "" + frozenbar.getUUID());
            System.out.println(""jcr:uuid:       "" + frozenbar.getProperty(""jcr:uuid"").getValue().getString());
            System.out.println(""jcr:frozenUuid: "" + frozenbar.getProperty(""jcr:frozenUuid"").getValue().getString());

        } finally {
            session.logout();
        }
    }
}

Gives as sample output:
bar:            fcf0affb-7476-4a64-a480-3039e8c53d53
frozenbar UUID: ed9fece9-9837-4ecc-9b7e-55bdfb8284e2
jcr:uuid:       fcf0affb-7476-4a64-a480-3039e8c53d53
jcr:frozenUuid: fcf0affb-7476-4a64-a480-3039e8c53d53

The jcr:uuid of the frozen bar is incorrect (althoug getUUID() returns the correct value).
"
1,"NPE Exception Thrown By FileJournal During Commit OperationThe following exception stack traces appearing repeatedly during a performance test of a JCR cluster at a customer site. 

ERROR - Unexpected error while preparing log entry.
java.lang.NullPointerException
	at org.apache.jackrabbit.core.cluster.FileRevision.unlock(FileRevision.java:117)
	at org.apache.jackrabbit.core.cluster.FileRevision.get(FileRevision.java:146)
	at org.apache.jackrabbit.core.cluster.FileJournal.sync(FileJournal.java:296)
	at org.apache.jackrabbit.core.cluster.FileJournal.begin(FileJournal.java:435)
	at org.apache.jackrabbit.core.cluster.ClusterNode.updatePrepared(ClusterNode.java:399)
	at org.apache.jackrabbit.core.cluster.ClusterNode.access$000(ClusterNode.java:40)
	at org.apache.jackrabbit.core.cluster.ClusterNode$WorkspaceUpdateChannel.updatePrepared(ClusterNode.java:559)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:647)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:778)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:808)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:302)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:295)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1204)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:821)
	
ERROR - Unexpected error while committing log entry.
java.lang.NullPointerException
	at org.apache.jackrabbit.core.cluster.FileJournal.commit(FileJournal.java:660)
	at org.apache.jackrabbit.core.cluster.ClusterNode.updateCommitted(ClusterNode.java:425)
	at org.apache.jackrabbit.core.cluster.ClusterNode$WorkspaceUpdateChannel.updateCommitted(ClusterNode.java:566)
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:712)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:808)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:326)
	at org.apache.jackrabbit.core.state.XAItemStateManager.update(XAItemStateManager.java:313)
	at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:302)
	at org.apache.jackrabbit.core.state.SessionItemStateManager.update(SessionItemStateManager.java:295)
	at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1204)
	at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:821)
	"
0,"jcr:deref and parent axis in xpath predicatesCurrently, the jcr:deref() function is not allowed in a xpath query predicate. Example :
book holds a reference property on its author(s)
authors have a name

We want all books from a specific author :

/jcr:root/element(*, bookType)[jcr:deref(@author, 'authorType')/@name = 'King']

This fails with an InvalidQueryException currently (not supported).

The error is raised in the XPathQueryBuilder class, in function : private QueryNode createFunction(SimpleNode node, QueryNode queryNode), in the block :
else if (NameFormat.format(JCR_DEREF, resolver).equals(fName))

Problem is that with this query, when evaluating the jcr:deref() function, then in this method at this point, queryNode.getType() is 0 and tests raise the exception if queryNode.getType() is neither QueryNode.TYPE_LOCATION nor QueryNode.TYPE_PATH.

I think this is a useful place to put a deref function in a query, as I don't know how we could test the referenced node properties another way.

Frederic Esnault"
0,Enable DataStore in default configurationCurrently the default configuration causes binary properties to be stored in the derby database. This is very inefficient. The standalone server (and the web application it contains) should run with a reasonable default configuration. 
1,"Registering cyclic dependent nodetypes does not workwhen registering the followin 2 nodetypes:

[foo] 
+ mybar (bar)

[bar]
+ myfoo (foo)

NodeTypeRegistry.registerNodeTypes(Collection) throws:

 org.apache.jackrabbit.core.nodetype.InvalidNodeTypeDefException: the following node types could not be registered because of unresolvable dependencies: {}foo {}bar 
"
0,"Simultaneous updates by multiple sessions might not appear in the journalIn a clustering environment, simultaneous updates by multiple sessions in the same cluster node might not appear in the journal, because only record at a time can be handled by the cluster's workspace-specific callback method. When such a situtation arises, the following warnings can be found in the log:

*WARN * ClusterNode: No record created.
*WARN * ClusterNode: No record prepared.
"
0,"Add MergePolicy to IndexWriterConfigNow that IndexWriterConfig is in place, I'd like to move MergePolicy to it as well. The change is not straightforward and so I've kept it for a separate issue. MergePolicy requires in its ctor an IndexWriter, however none can be passed to it before an IndexWriter actually exists. And today IW may create an MP just for it to be overridden by the application one line afterwards. I don't want to make iw member of MP non-final, or settable by extending classes, however it needs to remain protected so they can access it directly. So the proposed changes are:

* Add a SetOnce object (to o.a.l.util), or Immutable, which can only be set once (hence its name). It'll have the signature SetOnce<T> w/ *synchronized set<T>* and *T get()*. T will be declared volatile, so that get() won't be synchronized.
* MP will define a *protected final SetOnce<IndexWriter> writer* instead of the current writer. *NOTE: this is a bw break*. any suggestions are welcomed.
* MP will offer a public default ctor, together with a set(IndexWriter).
* IndexWriter will set itself on MP using set(this). Note that if set will be called more than once, it will throw an exception (AlreadySetException - or does someone have a better suggestion, preferably an already existing Java exception?).

That's the core idea. I'd like to post a patch soon, so I'd appreciate your review and proposals."
0,JSR 283 Node Identifier
0,Remove HitCollectorRemove the rest of HitCollectors
0,"Add IW.add/updateDocuments to support nested documentsI think nested documents (LUCENE-2454) is a very compelling addition
to Lucene.  It's also a popular (many votes) issue.

Beyond supporting nested document querying, which is already an
incredible addition since it preserves the relational model on
indexing normalized content (eg, DB tables, XML docs), LUCENE-2454
should also enable speedups in grouping implementation when you group
by a nested field.

For the same reason, it can also enable very fast post-group facet
counting impl (LUCENE-3097) when you what to
count(distinct(nestedField)), instead of unique documents, as your
""identifier"".  I expect many apps that use faceting need this ability
(to count(distinct(nestedField)) not distinct(docID)).

To support these use cases, I believe the only core change needed is
the ability to atomically add or update multiple documents, which you
cannot do today since in between add/updateDocument calls a flush (eg
due to commit or getReader()) could occur.

This new API (addDocuments(Iterable<Document>), updateDocuments(Term
delTerm, Iterable<Document>) would also further guarantee that the
documents are assigned sequential docIDs in the order the iterator
provided them, and that the docIDs all reside in one segment.

Segment merging never splits segments apart, so this invariant would
hold even as merges/optimizes take place.
"
1,"missing sync in InternalVersionManagerImpl.externalUpdate can cause ConcurrentModificationExceptionIn

        for (Map.Entry<ItemId, InternalVersionItem> entry : versionItems.entrySet()) {
            if (changes.has(entry.getKey())) {
                items.add(entry.getValue());
            }
        }

we need to sync on versionItems, I believe."
0,"Cookie guide lists RFC 2965 as unsupportedHttpClient 3.1 added support for RFC 2965 (port-sensitive cookies), but the Cookie guide on the 3.x website still lists that as unsupported.
http://jakarta.apache.org/httpcomponents/httpclient-3.x/cookies.html

cheers,
  Roland
 "
1,"Create or Append mode determined before obtaining write lockIf an IndexWriter(""writer1"") is opened in CREATE_OR_APPEND mode, it determines whether to CREATE or APPEND before obtaining the write lock.  When another IndexWriter(""writer2"") is in the process of creating the index, this can result in writer1 entering create mode and then waiting to obtain the lock.  When writer2 commits and releases the lock, writer1 is already in create mode and overwrites the index created by write2.

This bug was probably effected by LUCENE-2386 as prior to that Lucene generated an empty commit when a new index was created.  I think the issue could still have occurred prior to that but the two IndexWriters would have needed to be opened nearly simultaneously and the first IndexWriter would need to release the lock before the second timed out."
0,"fix LowerCaseFilter for unicode 4.0lowercase suppl. characters correctly. 

this only fixes the filter, the LowerCaseTokenizer is part of a more complex issue (CharTokenizer)
"
0,Move PersistenceManagerTest from o.a.j.core to o.a.j.core.persistenceThe subject pretty much sums it up. The PMTest should be placed together with the other PM related tests in o.a.j.core.persistence.
0,"FastVectorHighlighter: Make FragmentsBuilder use EncoderMake FragmentsBuilder use Encoder, as Highlighter does."
0,Add multi-part post supportAdd a new method to support multi-part post.
0,"MergePolicy should require an IndexWriter upon constructionMergePolicy does not require an IW upon construction, but requires one to be passed as method arg to various methods. This gives the impression as if a single MP instance can be shared across various IW instances, which is not true for all MPs (if at all). In addition, LogMergePolicy uses the IW instance passed to these methods incosistently, and is currently exposed to potential NPEs.

This issue will change MP to require an IW instance, however for back-compat reasons the following changes will be made:
# A new MP ctor w/ IW as arg will be introduced. Additionally, for back-compat a default ctor will also be declared which will assign null to the member IW.
# Methods that require IW will be deprecated, and new ones will be declared.
#* For back-compat, the new ones will not be made abstract, but will throw UOE, with a comment that they will become abstract in 3.0.
# All current MP impls will move to use the member instance.
# The code which calls MP methods will continue to use the deprecated methods, passing an IW even that it won't be necessary --> this is strictly for back-compat.

In 3.0, we'll remove the deprecated default ctor and methods, and change the code to not call the IW method variants anymore.

I hope that I didn't leave anything out. I'm sure I'll find out when I work on the patch :)."
1,"Version 1.3 reports IOException when re-creating an indexVersion: Lucene 1.3 final 
Error reported when I am (re-)doing an initialization on the index created 
previously:
java.io.IOException: couldn't delete _26a.f1

The problem disappearred after a re-start of the jvm, some files may be locked 
after the index writer action !
Problem does not appear in Version 1.2."
0,"Remove references to older versions of Lucene in ""per-release"" documentationSome of the documentation that is ""per release"" contains references to older versions, which is often confusing.  This is most noticeable in the file formats docs, but there might be other places too."
1,"Webdav: creating resource in case of RepositoryExceptionif accessing item fails for any other reason than PathNotFoundException, creating
the resource should rather fail (throwing 403).

(reported by brian)"
0,"WikipediaTokenizerI have extended StandardTokenizer to recognize Wikipedia syntax and mark tokens with certain attributes.  It isn't necessarily complete, but it does a good enough job for it to be consumed and improved by others.

It sets the Token.type() value depending on the Wikipedia syntax (links, internal links, bold, italics, etc.) based on my pass at http://en.wikipedia.org/wiki/Wikipedia:Tutorial

I have only tested it with the benchmarking EnwikiDocMaker wikipedia stuff and it seems to do a decent job.

Caveats:  I am not sure how to best handle testing, since the content is licensed under GNU Free Doc License, I believe I can't copy and paste a whole document into the unit test.  I have hand coded one doc and have another one that just generally runs over the benchmark Wikipedia download.

One more question is where to put it.  It could go in analysis, but the tests at least will have a dependency on Benchmark.  I am thinking of adding a new contrib/wikipedia where this could grow to have other wikipedia things (perhaps we would move EnwikiDocMaker there????) and reverse the dependency on Benchmark.

I will post a patch over the next few days."
0,"Drop deprecations from trunksubj.
Also, to each remaining deprecation add release version when it first appeared.

Patch incoming."
0,"XMLPersistenceManager fails after creating too many directories on linuxWhen using the  XMLPersistenceManager it creates a bunch of directories in jackrabbit/home/version/data. Eventually I reach 32000 directories in the data directory and subsequent writes fail.

I believe this is caused by XMLPersistenceManager.buildNodeFolderPath() method where it does 
   if (cnt == 4 || cnt == 8) {
      sb.append('/');
   }

This causes the subdirectories to be 4 characters, 0-f i.e. 16^4 which is 65536, if what I'm seeing is correct, on linux ext3, it's limited to 32000 entries. If the XMLPersistence manager used 2 or 3 characters this should fix the problem, or if it were configurable it would also solve this (I think).

an 
   ls jackrabbit/home/version/data | wc -l
returns 
   32001

A stack trace for when this happens is as follows :
Caused by: javax.jcr.RepositoryException: failed to write node state: da2cd5d1-9776-4dbe-a42b-842b0134dbfb: failed to write node state: da2cd5d1-9776-4dbe-a42b-842b0134dbfb
        at org.apache.jackrabbit.core.version.AbstractVersionManager.createVersionHistory(AbstractVersionManager.java:181)
        at org.apache.jackrabbit.core.version.VersionManagerImpl$1.run(VersionManagerImpl.java:194)
        at org.apache.jackrabbit.core.version.VersionManagerImpl$DynamicESCFactory.doSourced(VersionManagerImpl.java:526)
        at org.apache.jackrabbit.core.version.VersionManagerImpl.createVersionHistory(VersionManagerImpl.java:191)
        at org.apache.jackrabbit.core.version.XAVersionManager.createVersionHistory(XAVersionManager.java:140)
        at org.apache.jackrabbit.core.ItemImpl.initVersionHistories(ItemImpl.java:754)
        at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1166)
        at org.apache.jackrabbit.core.SessionImpl.save(SessionImpl.java:805)
        ... 166 more
Caused by: org.apache.jackrabbit.core.state.ItemStateException: failed to write node state: da2cd5d1-9776-4dbe-a42b-842b0134dbfb
        at org.apache.jackrabbit.core.state.xml.XMLPersistenceManager.store(XMLPersistenceManager.java:579)
        at org.apache.jackrabbit.core.state.AbstractPersistenceManager.store(AbstractPersistenceManager.java:66)
        at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.end(SharedItemStateManager.java:574)
        at org.apache.jackrabbit.core.state.SharedItemStateManager.update(SharedItemStateManager.java:697)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:315)
        at org.apache.jackrabbit.core.state.LocalItemStateManager.update(LocalItemStateManager.java:291)
        at org.apache.jackrabbit.core.version.AbstractVersionManager.createVersionHistory(AbstractVersionManager.java:174)
        ... 173 more
Caused by: org.apache.jackrabbit.core.fs.FileSystemException: failed to create folder /home/cms/pepsiaccess/jackrabbit/home/version/data/da2c/d5d1/97764dbea42b842b0134dbfb
        at org.apache.jackrabbit.core.fs.local.LocalFileSystem.createFolder(LocalFileSystem.java:208)
        at org.apache.jackrabbit.core.fs.BasedFileSystem.createFolder(BasedFileSystem.java:99)
        at org.apache.jackrabbit.core.fs.FileSystemResource.makeParentDirs(FileSystemResource.java:100)
        at org.apache.jackrabbit.core.state.xml.XMLPersistenceManager.store(XMLPersistenceManager.java:517)
        ... 179 more"
1,"TestTermsEnum.testIntersectRandom fail{noformat}
    [junit] Testsuite: org.apache.lucene.index.TestTermsEnum
    [junit] Testcase: testIntersectRandom(org.apache.lucene.index.TestTermsEnum):	FAILED
    [junit] (null)
    [junit] junit.framework.AssertionFailedError
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1530)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1432)
    [junit] 	at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$IntersectEnum.getState(BlockTreeTermsReader.java:894)
    [junit] 	at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$IntersectEnum.seekToStartTerm(BlockTreeTermsReader.java:969)
    [junit] 	at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader$IntersectEnum.<init>(BlockTreeTermsReader.java:786)
    [junit] 	at org.apache.lucene.index.codecs.BlockTreeTermsReader$FieldReader.intersect(BlockTreeTermsReader.java:483)
    [junit] 	at org.apache.lucene.index.TestTermsEnum.testIntersectRandom(TestTermsEnum.java:293)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1530)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1432)
    [junit] 
    [junit] 
    [junit] Tests run: 6, Failures: 1, Errors: 0, Time elapsed: 14.762 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestTermsEnum -Dtestmethod=testIntersectRandom -Dtests.seed=320d0949741fc6b1:3cabdb9b04d0d243:-4b7c80572775ed92 -Dtests.multiplier=3
{noformat}"
0,Snowball package contains BSD licensed code with ASL headerAll classes in org.tartarus.snowball (but not in org.tartarus.snowball.ext) has for some reason been given an ASL header. These classes are licensed with BSD. Thus the ASL header should be removed. I suppose this a misstake or possible due to the ASL header automation tool.
0,"java.util.UUID.fromString() too slowBenchmarking shows that the java.util.UUID.fromString() method is 10 times slower than the previous version we used from jackrabbit-jcr-commons. This method is quite heavily used in the query section or more generally whenever a NodeId is created from a String.

I'd like to introduce the custom String UUID parsing code again that we had in the jackrabbit-jcr-commons UUID class and use it in the NodeId(String) constructor.

WDYT?"
0,"light/minimal stemming for euro languagesThe snowball stemmers are very aggressive and it would be nice if there were lighter alternatives.

Some applications may want to perform less aggressive stemming, for example:
http://www.lucidimagination.com/search/document/5d16391e21ca6faf/plural_only_stemmer

Good, relevance tested algorithms exist and I think we should provide these alternatives."
0,"bogus javadocs for FieldValueHitQuery.fillFieldsFieldValueHitQuery.fillFields has javadocs that seem to be left over from a completely different method...

{code}
  /**
   * Given a FieldDoc object, stores the values used to sort the given document.
   * These values are not the raw values out of the index, but the internal
   * representation of them. This is so the given search hit can be collated by
   * a MultiSearcher with other search hits.
   * 
   * @param doc
   *          The FieldDoc to store sort values into.
   * @return The same FieldDoc passed in.
   * @see Searchable#search(Weight,Filter,int,Sort)
   */
  FieldDoc fillFields(final Entry entry) {
    final int n = comparators.length;
    final Comparable[] fields = new Comparable[n];
    for (int i = 0; i < n; ++i) {
      fields[i] = comparators[i].value(entry.slot);
    }
    //if (maxscore > 1.0f) doc.score /= maxscore;   // normalize scores
    return new FieldDoc(entry.docID, entry.score, fields);
  }

{code}"
0,"Build failed in the flexscoring branch because of Javadoc warningsAnt build log:
  [javadoc] Standard Doclet version 1.6.0_24
  [javadoc] Building tree for all the packages and classes...
  [javadoc] /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/src/java/org/apache/lucene/search/Similarity.java:93: warning - Tag @link: can't find tf(float) in org.apache.lucene.search.Similarity
  [javadoc] /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/src/java/org/apache/lucene/search/TFIDFSimilarity.java:588: warning - @param argument ""term"" is not a parameter name.
  [javadoc] /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/src/java/org/apache/lucene/search/TFIDFSimilarity.java:588: warning - @param argument ""docFreq"" is not a parameter name.
  [javadoc] /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/src/java/org/apache/lucene/search/TFIDFSimilarity.java:618: warning - @param argument ""terms"" is not a parameter name.
  [javadoc] Generating /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/org/apache/lucene/store/instantiated//package-summary.html...
  [javadoc] Copying file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/classdiagram.png to directory /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/org/apache/lucene/store/instantiated/doc-files...
  [javadoc] Copying file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/HitCollectionBench.jpg to directory /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/org/apache/lucene/store/instantiated/doc-files...
  [javadoc] Copying file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/contrib/instantiated/src/java/org/apache/lucene/store/instantiated/doc-files/classdiagram.uxf to directory /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/org/apache/lucene/store/instantiated/doc-files...
  [javadoc] Generating /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/serialized-form.html...
  [javadoc] Copying file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/prettify/stylesheet+prettify.css to file /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/stylesheet+prettify.css...
  [javadoc] Building index for all the packages and classes...
  [javadoc] Building index for all classes...
  [javadoc] Generating /home/savior/Development/workspaces/java/Lucene-GSoC/lucene/build/docs/api/all/help-doc.html...
  [javadoc] 4 warnings
"
0,"Unit tests for persistence managersCurrently we only test our persistence managers indirectly via JCR-level test cases. The downside of this approach is that we can only test one persistence manager implementation at a time, and need separate build profiles to switch from one implementation to another. To ensure better coverage and consistent behaviour across all our persistence managers I implemented a simple unit test that works directly against the PersistenceManager interface."
0,"Support for new Resources model in ant 1.7 in Lucene ant task.Ant Task for Lucene should use modern Resource model (not only FileSet child element).
There is a patch with required changes.

Supported by old (ant 1.6) and new (ant 1.7) resources model:
<index ....> <!-- Lucene Ant Task -->
  <fileset ... />
</index> 

Supported only by new (ant 1.7) resources model:
<index ....> <!-- Lucene Ant Task -->
  <filelist ... />
</index> 

<index ....> <!-- Lucene Ant Task -->
  <userdefinied-filesource ... />
</index> "
0,"Setting different MAX_HOST_CONNECTION values per host using a single MultiThreadedHttpConnectionManagerRight now, it's not possible to use the
MultiThreadedHttpConnectionManager.setMaxConnectionsPerHost(int) method in a per
HostConfiguration basis. The value applies to every HostConfiguration the
current connection manager is managing.

I would be quite useful to allow the connection manager to set different values
depending on the HostConfiguration."
0,"Could we get a way to know if the response has been served from the cache or not ?Is there a way to know if the response has been served from the cache or not ?
That's an information which might be useful for monitoring the activity of the cache.

If there's no current way, maybe a flag could be added in the request context whenever the response comes from the cache ... ?

"
0,"Javadoc of TokenStream.end() somehow confusingThe Javadocs of TokenStream.end() are somehow confusing, because they also refer to the old TokenStream API (""after next() returned null""). But one who implements his TokenStream with the old API cannot make use of the end() feature, as he would not use attributes and so cannot update the end offsets (he could, but then he should rewrite the whole TokenStream). To be conform to the old API, there must be an end(Token) method, which we will not add.

I would drop the old API from this docs."
0,Use jackrabbit 1.2.1Use Jackarabit 1.2.1
0,"FST should allow more than one output for the same inputFor the block tree terms dict, it turns out I need this case."
0,Replace xerces for serialization by JAXPThe org.apache.jackrabbit.rmi.xml.ImportContentHandler class currently uses Xerces to implement the SAX DocumentHandler and serialize XML into a byte[]. This dependency should be dropped and JAXP be used instead for this functionality.
0,"RFC4918 feature: PROPFIND/includeRFC4918, Section 14.8 (<http://greenbytes.de/tech/webdav/rfc4918.html#rfc.section.14.8>) defines an extension to PROPFIND that allows clients to retrieve all RFC2518 properties, the dead properties, plus a set of additional live properties. This can help avoiding a second roundtrip to retrieve really all properties.
"
1,"Removal of versions throws javax.jcr.ReferentialIntegrityExceptionFrom the following thread : http://www.mail-archive.com/jackrabbit-dev%40incubator.apache.org/msg03483.html

When trying to remove a version of a Node  the VersionHistory.removeVersion() method throws : ""javax.jcr.ReferentialIntegrityException: Unable to remove version. At least once referenced."".

Secton 8.2.2.10 (Removal of Versions) of the specification indicates that the version graph should be automatically repaired upon removal. Then, VersionHistory.removeVersion() should take care of references. (In fact, a user cannot alter the references (jcr:predecessors and jcr:successors), since they are protected properties.)

Here's the example (*updated) :

Node root1 = session.getRootNode() ;
Node test1 = root1.addNode(""test"") ;
test1.addMixin(""mix:versionable"");
test1.setProperty(""test"", ""1"");
session.save();
test1.checkin();

test1.checkout();
test1.setProperty(""test"", ""2"");
session.save();
test1.checkin();

test1.checkout();
test1.setProperty(""test"", ""3"");
session.save();
test1.checkin();

String baseVersion = test1.getBaseVersion().getName();
System.out.println(""Base version name: "" + baseVersion);

VersionHistory vh = test1.getVersionHistory();
for (VersionIterator vi = vh.getAllVersions(); vi.hasNext(); ) {
    Version currenVersion = vi.nextVersion();
    String versionName = currenVersion.getName();
    if (!versionName.equals(""jcr:rootVersion"") && !versionName.equals(baseVersion)) { 
        String propertyValue = currenVersion.getNode(""jcr:frozenNode"").getProperty(""test"").getString();
        System.out.println(""Removing version : "" + versionName + "" with value: "" + propertyValue);
        vh.removeVersion(versionName);
    }
}

Regards, 

Nicolas"
0,Allow parent path to be set explicitly in NodeInfoBuilderCurrently there is no way to explicitly set the parent of an NodeInfo build by NodeInfoBuilder. I suggest to add a setParentPath() method to NodeInfoBuilder to fix this.
0,"Revert subsequent token-node updates (tentatively introduced)i would like to revert this improvement has been tentatively introduced based on the following
thread on the dev list: http://www.mail-archive.com/dev@jackrabbit.apache.org/msg24437.html
as i am still concerned about undesired effects. in addition i still feel that this somehow
violates the basic contract."
0,"ShingleMatrixFilter, a three dimensional permutating shingle filterBacked by a column focused matrix that creates all permutations of shingle tokens in three dimensions. I.e. it handles multi token synonyms.

Could for instance in some cases be used to replaces 0-slop phrase queries with something speedier.

{code:java}
Token[][][]{
  {{hello}, {greetings, and, salutations}},
  {{world}, {earth}, {tellus}}
}
{code}

passes the following test  with 2-3 grams:

{code:java}
assertNext(ts, ""hello_world"");
assertNext(ts, ""greetings_and"");
assertNext(ts, ""greetings_and_salutations"");
assertNext(ts, ""and_salutations"");
assertNext(ts, ""and_salutations_world"");
assertNext(ts, ""salutations_world"");
assertNext(ts, ""hello_earth"");
assertNext(ts, ""and_salutations_earth"");
assertNext(ts, ""salutations_earth"");
assertNext(ts, ""hello_tellus"");
assertNext(ts, ""and_salutations_tellus"");
assertNext(ts, ""salutations_tellus"");
{code}

Contains more and less complex tests that demonstrate offsets, posincr, payload boosts calculation and construction of a matrix from a token stream.

The matrix attempts to hog as little memory as possible by seeking no more than maximumShingleSize columns forward in the stream and clearing up unused resources (columns and unique token sets). Can still be optimized quite a bit though."
0,Apply the supplied patch. Sets 2 variable in the base class to protectedThe patch attached to the main task contains minimal changes to allow the HttpMethodBase class to be overloaded by base class.
1,"Cookies with null path attribute are rejected in the compatibility modeWeblogic sends cookies with path empty, httpclient emits a warning
and doesn't send back the cookie to server.

Maybe httpclient works in the RFC's ways but this doesn't reproduce
common web browsers behaviours. Our application works well with IE,
Opera and Netscape, httpunit also sends back the cookie to the server.

When receving the response, httpclient emits the followin warning :

[WARN] HttpMethod - -Invalid cookie header: ""JTD=O%
2FdF13CDb1W7H2GNfUTS2YQ3Zt6bCW6ZKZRvVJ9FwaadQLxXVI7rgii%2FwbxeCsqym7dcWKDxSj%
2Bg1ubJRSVRhYGb7wRLjp5c0v2R3QrCIXVhMKDjuwuXDXnjbH3LHSWG7bfzJSmS7nXk9R%
2FqMIRHb5najLQkU7WkuPGgXUnUln%2BF51TajkVmXkrLMYN7MHDT48BEHvFQFNXBlmSRejWqrd%
2Fiiao0flObOrT3HcaWI09B1vekpAcPmgvMD2oZzXQWJwjDZIX6QoVVD6U8CXPSvVQjITyaxf6AqaS%
2BAFJgRsqbZBc0%2BV5G%2FnzE87ggOVIozfPFn99ny0kxiPGBEisJIy%3D%3D; Version=1; 
Path=; Max-Age=604800"". Missing value for path attribute

That's right, maybe the http header is not correct, but I think httpclient
should handle this case without error in order to have the same behaviour
as common browsers. We have no way to give a better value to this path."
0,"Can't build lucene 06/13/2004 CVS under jdk 1.5.0[root@shilo jakarta-lucene]# /usr/local/ant/bin/ant build
Buildfile: build.xml

BUILD FAILED
Target `build' does not exist in this project. 

Total time: 1 second
[root@shilo jakarta-lucene]# /usr/local/ant/bin/ant 
Buildfile: build.xml

init:
    [mkdir] Created dir: /usr/src/jakarta-lucene/build
    [mkdir] Created dir: /usr/src/jakarta-lucene/dist

compile-core:
    [mkdir] Created dir: /usr/src/jakarta-lucene/build/classes/java
    [javac] Compiling 160 source files to /usr/src/jakarta-
lucene/build/classes/java
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/FilterIndexReader.java:42: as of 
release 1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     public void seek(TermEnum enum) throws IOException { in.seek
(enum); }
    [javac]                               ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/FilterIndexReader.java:42: as of 
release 1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     public void seek(TermEnum enum) throws IOException { in.seek
(enum); }
    [javac]                                                                  ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/SegmentTermDocs.java:55: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]   public void seek(TermEnum enum) throws IOException {
    [javac]                             ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/SegmentTermDocs.java:59: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     if (enum instanceof SegmentTermEnum && ((SegmentTermEnum) 
enum).fieldInfos == parent.fieldInfos)          // optimized case
    [javac]         ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/SegmentTermDocs.java:59: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     if (enum instanceof SegmentTermEnum && ((SegmentTermEnum) 
enum).fieldInfos == parent.fieldInfos)          // optimized case
    [javac]                                                               ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/SegmentTermDocs.java:60: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]       ti = ((SegmentTermEnum) enum).termInfo();
    [javac]                               ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/SegmentTermDocs.java:62: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]       ti = parent.tis.get(enum.term());
    [javac]                           ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:63: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     SegmentTermEnum enum = (SegmentTermEnum)enumerators.get();
    [javac]                     ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:64: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     if (enum == null) {
    [javac]         ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:65: enum types 
must not be local
    [javac]       enum = terms();
    [javac]       ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:65: <identifier> 
expected
    [javac]       enum = terms();
    [javac]            ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:65: '{' expected
    [javac]       enum = terms();
    [javac]                     ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:66: <identifier> 
expected
    [javac]       enumerators.set(enum);
    [javac]                      ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:66: ';' expected
    [javac]       enumerators.set(enum);
    [javac]                       ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:66: <identifier> 
expected
    [javac]       enumerators.set(enum);
    [javac]                           ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:66: '{' expected
    [javac]       enumerators.set(enum);
    [javac]                            ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:68: illegal start 
of type
    [javac]     return enum;
    [javac]     ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:68: as of release 
1.5, 'enum' is a keyword, and may not be used as an identifier
    [javac] (try -source 1.4 or lower to use 'enum' as an identifier)
    [javac]     return enum;
    [javac]            ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:75: illegal start 
of expression
    [javac]   private final void readIndex() throws IOException {
    [javac]   ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:199: ';' expected
    [javac] }
    [javac] ^
    [javac] /usr/src/jakarta-
lucene/src/java/org/apache/lucene/index/TermInfosReader.java:200: '}' expected
    [javac] ^
    [javac] 21 errors

BUILD FAILED
/usr/src/jakarta-lucene/build.xml:140: Compile failed; see the compiler error 
output for details.

Total time: 4 seconds
[root@shilo jakarta-lucene]#"
0,add support for RFC 3253 to the simple serverhttp://www.ietf.org/rfc/rfc3253.txt
0,"benchmark tests always fail on windows because directory cannot be removedThis seems to be a bug recently introduced. I have no idea what's wrong. Attached is a log file, reproduces everytime.

"
0,"LowerCaseFilter for Turkish languagejava.lang.Character.toLowerCase() converts 'I' to 'i' however in Turkish alphabet lowercase of 'I' is not 'i'. It is LATIN SMALL LETTER DOTLESS I.

"
0,"Tests using Version.LUCENE_CURRENT will produce problems in backwards branch, when development for 3.2 startsA lot of tests for the most-recent functionality in Lucene use Version.LUCENE_CURRENT, which is fine in trunk, as we use the most recent version without hassle and changing this in later versions.

The problem is, if we copy these tests to backwards branch after 3.1 is out and then start to improve analyzers, we then will have the maintenance hell for backwards tests. And we loose backward compatibility testing for older versions. If we would specify a specific version like LUCENE_31 in our tests, after moving to backwards they must work without any changes!

To not always modify all tests after a new version comes out (e.g. after switching to 3.2 dev), I propose to do the following:
- declare a static final Version TEST_VERSION = Version.LUCENE_CURRENT (or better) Version.LUCENE_31 in LuceneTestCase(4J).
- change all tests that use Version.LUCENE_CURRENT using eclipse refactor to use this constant and remove unneeded import statements.

When we then move the tests to backward we must only change one line, depending on how we define this constant:
- If in trunk LuceneTestCase it's Version.LUCENE_CURRENT, we just change the backwards branch to use the version numer of the released thing.
- If trunk already uses the LUCENE_31 constant (I prefer this), we do not need to change backwards, but instead when switching version numbers we just move trunk forward to the next major version (after added to Version enum)."
1,HTML Text Extractor does not extract or index numericsNumerics such as addresses/dates/financial figures are not extracted or indexed by the current HTML Extractor.  These values are handled properly and searchable when done via the PlainTextExtractor
0,"Links in Section ""Example Code"" are brokenSteps to Reproduce: Go to http://hc.apache.org/user-docs.html
Click of one of the Links in the Section ""Example Code"""
0,"IndexWriter#addIndexesNoOptimize has redundent try/catchWith the new transaction code, the try/catch clause at the beginning of IndexWriter#addIndexesNoOptimize is redundant."
0,"TCK: more tests assuming that 'addMixin' immediately taking effectjsr170 allows an implementation to have Node.addMixin only taking affect upon a save-call.

some tests already got adjusted.
attached patch for additional tests, that make usage of addMixin"
0,NodeBasedGroup#isMember(Principal) should have shortcut for the everyone group.
0,"Make observation polling time configurableCurrently the polling time is hard coded to 3 seconds in org.apache.jackrabbit.jcr2spi.WorkspaceImpl. I suggest to make it configurable similar to CacheBehaviour. That is, add a respective setting to org.apache.jackrabbit.jcr2spi.config.RepositoryConfig"
0,"Deprecate SimilarityDelegator and Similarity.lengthNormSimilarityDelegator is a back compat trap (see LUCENE-2828).

Apps should just [statically] subclass Sim or DefaultSim; if they really need ""runtime subclassing"" then they can make their own app-level delegator.

Also, Sim.computeNorm subsumes lengthNorm, so we should deprecate lengthNorm in favor of computeNorm."
0,"Support for NTLM authenticationA late write in for this would be support for NTLM authenticatin as well as
basic and digest.  Obviously non-trivial but it would be a very big feature.

Adrian Sutton, Software Engineer
Ephox Corporation
www.ephox.com <http://www.ephox.com>"
0,"RMIC not working in subprojects when compiling parent using maven2This is because there is a bug such that if you have a child build which uses the ant plugin it inherits the plugin dependencies of the first time the plugin is declared.

The workaround is to put the antrun plugin in the toplevel, and add the java jar to its plugin dependencies.

(reference: http://mail-archives.apache.org/mod_mbox/maven-users/200602.mbox/%3CC2CDEFBECFC9A14892BCCFB4C95F4868044F8230@EX-201.mail.navisite.com%3E)"
0,"BindableRepositoryFactory requires exact resource typeThe org.apache.jackrabbit.jndi.BindableRepositoryFactory class requires the exact class name org.apache.jackrabbit.jndi.BindableRepository to be specified for the JNDI resource that the factory is responsible for. However the current deployment model 2 howto document suggest that the more generic interface name javax.jcr.Repository be used instead. Currently this suggested configuration results in a null JNDI resource .

This issue should be fixed by either fixing the documentation (I can do this) or by relaxing the code in BindableRepositoryFactory. I'll attach a patch that does the latter, please comment on what you think is the best solution.

This issue was detected during the mailing list thread
http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/2303"
0,"WebdavResponseImpl should cache TransformerFactoryJackrabbitResponeImpl.sendXmlResponse creates an instance of TransformerFactory on each invocation. We see, that this TransformerFactory initialization consumes significant amount of time, because of complex logic inside:

{code}
    at java.lang.String.intern(Native Method)
    at java.util.jar.Attributes$Name.<init>(Attributes.java:449)
    at java.util.jar.Attributes.putValue(Attributes.java:151)
    at java.util.jar.Attributes.read(Attributes.java:404)
    at java.util.jar.Manifest.read(Manifest.java:234)
    at sun.security.util.SignatureFileVerifier.processImpl(SignatureFileVerifier.java:188)
    at sun.security.util.SignatureFileVerifier.process(SignatureFileVerifier.java:176)
    at java.util.jar.JarVerifier.processEntry(JarVerifier.java:277)
    at java.util.jar.JarVerifier.update(JarVerifier.java:188)
    at java.util.jar.JarFile.initializeVerifier(JarFile.java:321)
    at java.util.jar.JarFile.getInputStream(JarFile.java:386)
    at sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:144)
    at java.net.URL.openStream(URL.java:1009)
    at java.lang.ClassLoader.getResourceAsStream(ClassLoader.java:1170)
    at javax.xml.transform.SecuritySupport$4.run(SecuritySupport.java:94)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.xml.transform.SecuritySupport.getResourceAsStream(SecuritySupport.java:87)
    at javax.xml.transform.FactoryFinder.findJarServiceProvider(FactoryFinder.java:250)
    at javax.xml.transform.FactoryFinder.find(FactoryFinder.java:223)
    at javax.xml.transform.TransformerFactory.newInstance(TransformerFactory.java:102)
    at org.apache.jackrabbit.webdav.WebdavResponseImpl.sendXmlResponse(WebdavResponseImpl.java:163)
{code}

TransformerFactory can be cached in static field:

private static final TransofmerFactory transformerFactory = TransformerFactory.newInstance()."
0,"Multivalued property sorted by last/random valueSorting on multivalued property may produce incorrect result because sorting is performed only by last value of multivalued property.
Steps to reproduce:
1. Create multivalued field in repository. Example from nodetypes file:
<propertyDefinition name=""MyProperty"" requiredType=""String"" autoCreated=""false"" mandatory=""false""
   onParentVersion=""COPY"" protected=""false"" multiple=""false"">
2. Create few records so that all records except one would contain single value for MyProperty and one record would contain 
first value which is greater then of any other record and the second value is somewhere in the middle. Here is an example:
1st record: ""aaaa""
2nd record: ""cccc""
3rd record: ""dddd"", ""bbbb""
3. Run some query which sorts Example of XPath query:
//*[...here are some criteria...] order by @MyProperty ascending
The query would return documents in such order:
""aaaa""
""dddd"", ""bbbb""
""cccc""
which is not expected order (expected same order as they were entered - as ""aaaa"" < ""cccc"", ""cccc"" < ""dddd"")

After some digging I found out that it happens because method 
org.apache.jackrabbit.core.query.lucene.SharedFieldCache.getValueIndex
(called from org.apache.jackrabbit.core.query.lucene.SharedFieldSortComparator.SimpleScoreDocComparator constructor)
returns only last Comparable of the document. Here is overwrites previous value:
retArray[termDocs.doc()] = getValue(value, type);

I tried to concatenate comparables (just to check if it would work for my case):
	if(retArray[termDocs.doc()] == null) {
		retArray[termDocs.doc()] = getValue(value, type);
	} else {
		retArray[termDocs.doc()] =
				retArray[termDocs.doc()] + "" "" + getValue(value, type);
	}
But it didn't worked well either - TermEnum returns terms not in the same order as JackRabbit returns values of multivalued field
(as an example [""qwer"", ""asdf""] may become [""asdf"", ""qwer""] ). So, simple concatenation doesn't help.
"
0,"Provide factory method to create DefaultHttpClient instances pre-configured based on JSSE and networking system propertiesProvide factory method or a factory class intended to create DefaultHttpClient instances pre-configured based on JSSE [1] and networking [2] system properties.

[1] http://download.oracle.com/javase/1,5.0/docs/guide/security/jsse/JSSERefGuide.html
[2] http://download.oracle.com/javase/1.5.0/docs/guide/net/properties.html"
1,MoreLikeThis ignores custom similarityMoreLikeThis only allows the use of the DefaultSimilarity.  Patch shortly
0,Add method to set uuid in NodeInfoBuilder
0,"Modified QueryImpl to enable external query builders to read and write JCR expressions in the orderBy ClauseThe QueryImpl does not create the JCR expression on the fly. The OrderByExpression does the job. If an external querybuilder class needs to dynamically collect properties against which order by is required, QueryImpl does not support updating the JCR Expression. It can only return the built expression since arrayList is used for collecting the properties. The change enables one to add JCRExpression to the QueryImpl object. A test has been added.

Changed files:
Path
src/main/java/org/apache/jackrabbit/ocm/query/impl/QueryImpl.java
src/test/java/org/apache/jackrabbit/ocm/manager/query/DigesterSimpleQueryTest.java
"
0,"Extend contrib Highlighter to properly support PhraseQuery, SpanQuery,  ConstantScoreRangeQueryThis patch adds a new Scorer class (SpanQueryScorer) to the Highlighter package that scores just like QueryScorer, but scores a 0 for Terms that did not cause the Query hit. This gives 'actual' hit highlighting for the range of SpanQuerys, PhraseQuery, and  ConstantScoreRangeQuery. New Query types are easy to add. There is also a new Fragmenter that attempts to fragment without breaking up Spans.

See http://issues.apache.org/jira/browse/LUCENE-403 for some background.

There is a dependency on MemoryIndex."
0,"IndexWriter.mergeSegments should not hold the commit lock while cleaning up.Same happens in IndexWriter.addIndexes(IndexReader[] readers).

The commit lock should be obtained whenever the Index structure/version is read or written.  It should be kept for as short a period as possible.

The write lock is needed to make sure only one IndexWriter or IndexReader instance can update the index (multiple IndexReaders can of course use the index for searching).

The list of files that can be deleted is stored in the file ""deletable"".  It is only read or written by the IndexWriter instance that holds the write lock, so there's no need to have the commit lock to to update it.

On my production system deleting the obsolete segment files after a mergeSegments() happens can occasionally take several seconds(!) and the commit lock blocks the searcher machines from updating their IndexReader instance.
Even on a standalone machine, the time to update the segments file is about 3ms, the time to delete the obsolete segments about 30ms.
"
0,"use packed ints for the terms dict indexTerms dict index needs to store large RAM resident arrays of ints, but, because their size is bound & variable (depending on the segment/docs), we should used packed ints for them."
0,"Move SimpleWebdavServlet to jcr-server and make it abstractIn line with isse JCR-417, I suggest to partially move the SimpleWebdavServlet from the jcr-webapp project to the jcr-server project. By partially I mean, that the new (moved) servlet will be abstract and the getRepository() method will be abstract. The jcr-webapp project will still contain a SimpleWebdavServlet (for backwards compatibility maintaing the same name) which just extends the new servlet and implements the getRepository() method using the RepositoryAccess servlet.

This allows for the reuse of the jcr-server project including the abstract SimpleWebdavServlet in other environments. My intention is to include this project (along with the webdav) project in Sling.

Will provide a patch for this proposal

(This issue is separated out of JCR-1262 as suggested by Angela)
"
1,"Wrong creation of AuthScope objectClass Name: org.apache.http.client.protocol.RequestAuthCache
Line #: 118-119

Issue: Want to create a new Object of AuthScope by passing host name, port and scheme name but due to incorrect constructor call, Getting a object with realm name as scheme name.
Current Code: Credentials creds = credsProvider.getCredentials(new AuthScope(host.getHostName(), host.getPort(), null, schemeName));"
0,"CMS merge throttling is not aggressive enoughI hit this crab while working on the NRT benchmarker (in luceneutil).

CMS today forcefully idles any incoming threads, when there are too many merges pending.

This is the last line of defense that it has, since it also juggles thread priorities (and forcefully idles the biggest merges) to try to reduce the outstanding merge count.

But when it cannot keep up it has no choice but to stall those threads responsible for making new segments.

However, the logic is in the wrong place now -- the stalling happens after pulling the next merge from IW.  This is poor because it means if you have N indexing threads, you allow max + N outstanding merges.

I have a simple fix, which is to just move the stall logic to before we pull the next merge from IW."
0,"Utility class to tranform JCR-SQL2 to/from JCR-JQOMThe JCR2 doc specify that both contain the same thing and can be translated from one to another
in a straightforward manner. The jackrabbit-jcr-commons module should offer a utility class to transform
from one language to another in a generic way, 

for exemple :
- String toSQL2(QueryObjectModel qom)
- QueryObjectModel toJQOM(QueryObjectModelFactory factory, String query)"
1,"Exception in HttpConnection because of unchecked buffer sizeFrom the httpclient-dev mailing list:

Date: Tue, 8 Mar 2005 19:08:35 +0100
Subject: Error with multiple connections

Hello,

 

I am having some problems while trying multiple connections over a
HttpClient object with a MultiThreadedHttpConnectionManager. I am
launching 10 threads and each thread executes some GetMethods using this
HttpClient object.

 

Some times I got an error like this:

 

java.lang.IllegalArgumentException: Buffer size <= 0

      at java.io.BufferedInputStream.<init>(Unknown Source)

      at
org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:70
3)

      at
org.apache.commons.httpclient.MultiThreadedHttpConnectionManager$HttpCon
nectionAdapter.open(MultiThreadedHttpConnectionManager.java:1170)

      at
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:6
28)

      at
org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:4
97)

      at Main$Hilo.run(Main.java:58)

 

Does anybody have any idea? 

 

Thanks in advance,

Jorge"
0,"Indonesian AnalyzerThis is an implementation of http://www.illc.uva.nl/Publications/ResearchReports/MoL-2003-02.text.pdf

The only change is that I added an option to disable derivational stemming, 
in case you want to just remove inflectional particles and possessive pronouns.

"
0,"Add ""testpackage"" to common-build.xmlOne can define ""testcase"" to execute just one test class, which is convenient. However, I didn't notice any equivalent for testing a whole package. I find it convenient to be able to test packages rather than test cases because often it is not so clear which test class to run.

Following patch allows one to ""ant test -Dtestpackage=search"" (for example) and run all tests under the \*/search/\* packages in core, contrib and tags, or do ""ant test-core -Dtestpackage=search"" and execute similarly just for core, or do ""ant test-core -Dtestpacakge=lucene/search/function"" and run all the tests under \*/lucene/search/function/\* (just in case there is another o.a.l.something.search.function package out there which we want to exclude."
1,"addIndexes unexpectedly closes indexIt seems that in 1.4rc2, a call to IndexWriter.addIndexes (IndexReader[]) will
close the provided IndexReader; in 1.3-final this does not happen.  So my code
which uses addIndexes to merge new information into an index and then calls
close() on the IndexReader now crashes with an ""already closed"" exception.  I
can attach test code which works in 1.3 but not in 1.4rc2 if that would be helpful.

If this is an intentional change in behavior, it needs to be documented.  Thanks!"
0,"NamespaceRegistryTest uses an invalid URI as namespace URIThe test cases use ""www.apache.org/..."" as a namespace URI, but this is not a URI.

Suggest to fix by using a proper URI, such as by prefixing with ""http://"".

A related question is what our expectation is for JCR implementations. Are they allowed to reject something that doesn't parse as a URI according to RFC3986?
"
1,"DocValues infinite loop caused by - a call to getMinValue | getMaxValue | getAverageValueorg.apache.lucene.search.function.DocValues offers 3 public (optional) methods to access value statistics like min, max and average values of the internal values. A call to one of the methods will result in an infinite loop. The internal counter is not incremented. 
I added a testcase, javadoc and a slightly different implementation to it. I guess this is not breaking any back compat. as a call to those methodes would have caused an infinite loop anyway.
I changed the return value of all of those methods to Float.NaN if the DocValues implementation does not contain any values.

It might be considerable to fix this in 2.4.2 and 2.3.3

"
0,"Add option to ReverseStringFilter to mark reversed tokensThis patch implements additional functionality in the filter to ""mark"" reversed tokens with a special marker character (Unicode 0001). This is useful when indexing both straight and reversed tokens (e.g. to implement efficient leading wildcards search)."
0,"Various improvment to Path and PathImplThere are various issues with Path and PathImpl which the following patch addresses:
- Fixed problem with normalization of some paths in PathImpl. 
- Fixed handling of relative paths in PathImpl. 
- Fixed wrong return value for depth and ancestor count in PathImpl. 
- Added method for determining equivalence of paths in PathImpl.
- Fixed subPath method in PathImpl. 
- Clarified blurry contract for Path.
- Added many new test cases

For many of the fixes credits are due to Angela."
0,"spellchecker: make hard-coded values configurablethe class org.apache.lucene.search.spell.SpellChecker uses the following hard-coded values in its method
indexDictionary:
        writer.setMergeFactor(300);
        writer.setMaxBufferedDocs(150);
this poses problems when the spellcheck index is created on systems with certain limits, i.e. in unix
environments where the ulimit settings are restricted for the user (http://www.gossamer-threads.com/lists/lucene/java-dev/47428#47428).

there are several ways to circumvent this:
1. add another indexDictionary method with additional parameters:
    public void indexDictionary (Dictionary dict, int mergeFactor, int maxBufferedDocs) throws IOException
    
2. add setter methods for mergeFactor and maxBufferedDocs 
    (see code in http://www.gossamer-threads.com/lists/lucene/java-dev/47428#47428 )

3. Make SpellChecker subclassing easier as suggested by Chris Hostetter 
   (see reply  http://www.gossamer-threads.com/lists/lucene/java-dev/47463#47463)

thanx,
karin
"
0,"norms file can become unexpectedly enormous
Spinoff from this user thread:

   http://www.gossamer-threads.com/lists/lucene/java-user/46754

Norms are not stored sparsely, so even if a doc doesn't have field X
we still use up 1 byte in the norms file (and in memory when that
field is searched) for that segment.  I think this is done for
performance at search time?

For indexes that have a large # documents where each document can have
wildly varying fields, each segment will use # documents times # fields
seen in that segment.  When optimize merges all segments, that product
grows multiplicatively so the norms file for the single segment will
require far more storage than the sum of all previous segments' norm
files.

I think it's uncommon to have a huge number of distinct fields (?) so
we would need a solution that doesn't hurt the more common case where
most documents have the same fields.  Maybe something analogous to how
bitvectors are now optionally stored sparsely?

One simple workaround is to disable norms.
"
0,"BUILD.txt instructions wrong for JavaCCThe text in BUILD.txt for javacc says to set the property to the bin directory in the javacc installation. It should actually be set to the javacc installation directory, the directory containing the bin directory. The comments common-build.xml correctly state this."
0,"Allow access to registered cookie policiesIt would be useful for JMeter (and perhaps other applications) to have access to the list of registered Cookie policy names.

[If this is acceptable, let me know if you want me to provide a patch.]"
0,"Add Google Analytics to Jackrabbit web siteI'd like to add Google Analytics to our web site to better track how the site is used and how much traffic we are generating.

Currently the best stats we have are at http://people.apache.org/~vgritsenko/stats/projects/jackrabbit.html, which is nice but not nearly as good as we could have."
1,"Jcr-Server: ValuesProperty missing property type informationJCR specific dav-property ValuesProperty does not reveal the PropertyType of the value, which is therefore lost during (de)serialization. 

Solution: 
- Pass type of the JCR-value as attribute to the xml-element containing the value."
0,"Maybe rename Field.omitTf, and strengthen the javadocsSpinoff from here:

  http://www.nabble.com/search-problem-when-indexed-using-Field.setOmitTf()-td22456141.html

Maybe rename omitTf to something like omitTermPositions, and make it clear what queries will silently fail to work as a result."
0,Add UserManager#getAuthorizableByPath(String) for symmetry with JCR-3037JCR-3037 added Authorizable#getPath. I would suggest to also add UserManager#getAuthorizableByPath that was symmetric to Authorizable#getPath
1,"when many query clases are specified in boolean or dismax query, highlighted terms are always ""yellow"" if multi-colored feature is usedThe problem is the following snippet:

{code}
protected String getPreTag( int num ){
  return preTags.length > num ? preTags[num] : preTags[0];
}
{code}

it should be:

{code}
protected String getPreTag( int num ){
  int n = num % preTags.length;
  return  preTags[n];
}
{code}
"
1,"TCK: OrderByMultiTypeTest doesn't respect nodetype configuration propertyOrderByMultiTypeTest creates test data by calling addNode(String).  This fails if there is no default primary type.

Proposal: call addNode(String, String)

--- OrderByMultiTypeTest.java   (revision 428760)
+++ OrderByMultiTypeTest.java   (working copy)
@@ -43,9 +43,9 @@
      * Tests order by queries with a String property and a long property.
      */
     public void testMultipleOrder() throws Exception {
-        Node n1 = testRootNode.addNode(nodeName1);
-        Node n2 = testRootNode.addNode(nodeName2);
-        Node n3 = testRootNode.addNode(nodeName3);
+        Node n1 = testRootNode.addNode(nodeName1, testNodeType);
+        Node n2 = testRootNode.addNode(nodeName2, testNodeType);
+        Node n3 = testRootNode.addNode(nodeName3, testNodeType);
  
         n1.setProperty(propertyName1, ""aaa"");
         n1.setProperty(propertyName2, 3);
"
0,"jcr ext doesn't compile with jdk 1.4IllegalStateException(String str, Exception e) isn't supported."
0,"Non-standards configuration and trackingA simple strict or setLenient is likely inadequate.  Each particular
non-standard behaviour should be tagged, and settable from the client.  A mask
for particular behavioural features could be provided, with STRICT meaning none
and LENIENT meaning all."
0,"allow unsetting of DEFAULT_PROXY and FORCED_ROUTE parameters in the client stackSince we don't want to delay client alpha3 until HTTPCORE-139 is solved in beta2, we need a parameter specific solution for unsetting these client parameters on the request level.
"
0,"Add SimpleFragListBuilder constructor with margin parameter{{SimpleFragListBuilder}} would benefit from an additional constructor that takes in {{margin}}. Currently, the margin is defined as a constant, so to ""implement"" a {{FragListBuilder}} with a different margin, one has no choice but to copy and paste {{SimpleFragListBuilder}} into a new class that must be placed in the {{org.apache.lucene.search.vectorhighlight}} package due to accesses of package-protected fields in other classes.

If this change were made, the precondition check of the constructor's {{fragCharSize}} should probably be altered to ensure that it's less than {{max(1, margin*3)}} to allow for a margin of 0."
1,"Multipart post is brokenI tried to do HttpPost request with MultipartEntity, this request was encoded to wire with 3 line separators after header and not processed correctly by http server.
MultipartEntry add 1 extra line separator before write itself to wire. I'm not sure about standards, but it is at least not ""browser compatible"".

"
0,"refactoring of docvalues params in Codec.javaWhile working on LUCENE-2621 I am trying to do some cleanup of the Codec APIs, currently Codec.java has a boolean for getDocValuesUseCFS()

I think this is an impl detail that should not be in Codec.java: e.g. i might make a SimpleText impl that uses only 1 file and then the param
is awkward.

So, instead I created Sep impls that dont use CFS (use separate files) and placed them under the sep package, if you don't want to use
CFS you can just use these implementations in your codec."
0,"Introduce 'SecurityConfig' for better extensability.the current repository configuration parser parses the security confguration (inluding appName, AccessManagerConfig and LoginModuleconfig) internally and the passes those 3 values to the repository config. i suggest to add a new 'SecurityConfig' object that encapsulates those 3 values and is parsed in a seperate method, in order to allow for better extensability. this also reduces the size of the alredy bloated repository config constructor."
0, Reduce number of compiler warning by adding @Override and generics where appropriate Same as JCR-2482 for the webdav library and the modules using it.
1,"Jcr-server: DeltaVResource lists MKWORKSPACE in the method constant.RFC 3253 requires REPORT to be supported by all DeltaV compliant resources.
The method constant therefore should list REPORT only."
0,Random access non RAM resident IndexDocValues (CSF)There should be a way to get specific IndexDocValues by going through the Directory rather than loading all of the values into memory.
0,"move contrib/analyzers to modules/analysisThis is a patch to move contrib/analyzers under modules/analyzers

We can then continue consolidating (LUCENE-2413)... in truth this will sorta be 
an ongoing thing anyway, as we try to distance indexing from analysis, etc
"
0,Documentation - jackrabbit features beyond the spec
0,"TestIndexReaderReopen nightly build failureAn interesting failure in last night's build (http://hudson.zones.apache.org/hudson/job/Lucene-trunk/920).

I think the root cause wast he AIOOB exception... all the ""lock obtain timed out"" exceptions look like they cascaded.

{code}
    [junit] Testsuite: org.apache.lucene.index.TestIndexReaderReopen
    [junit] Lock obtain timed out: org.apache.lucene.store.SingleInstanceLock@6ac615: write.lock)
    [junit] Tests run: 15, Failures: 1, Errors: 0, Time elapsed: 31.087 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] java.lang.ArrayIndexOutOfBoundsException: Array index out of range: 148
    [junit] 	at org.apache.lucene.util.BitVector.getAndSet(BitVector.java:74)
    [junit] 	at org.apache.lucene.index.SegmentReader.doDelete(SegmentReader.java:908)
    [junit] 	at org.apache.lucene.index.IndexReader.deleteDocument(IndexReader.java:1122)
    [junit] 	at org.apache.lucene.index.DirectoryReader.doDelete(DirectoryReader.java:521)
    [junit] 	at org.apache.lucene.index.IndexReader.deleteDocument(IndexReader.java:1122)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$8.modifyIndex(TestIndexReaderReopen.java:638)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen.refreshReader(TestIndexReaderReopen.java:840)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen.access$400(TestIndexReaderReopen.java:47)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$9.run(TestIndexReaderReopen.java:681)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$ReaderThread.run(TestIndexReaderReopen.java:822)
    [junit] org.apache.lucene.store.LockObtainFailedException: Lock obtain timed out: org.apache.lucene.store.SingleInstanceLock@88d319: write.lock
    [junit] 	at org.apache.lucene.store.Lock.obtain(Lock.java:85)
    [junit] 	at org.apache.lucene.index.DirectoryReader.acquireWriteLock(DirectoryReader.java:666)
    [junit] 	at org.apache.lucene.index.IndexReader.setNorm(IndexReader.java:994)
    [junit] 	at org.apache.lucene.index.IndexReader.setNorm(IndexReader.java:1020)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$8.modifyIndex(TestIndexReaderReopen.java:634)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen.refreshReader(TestIndexReaderReopen.java:840)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen.access$400(TestIndexReaderReopen.java:47)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$9.run(TestIndexReaderReopen.java:681)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$ReaderThread.run(TestIndexReaderReopen.java:822)
    ...
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testThreadSafety(org.apache.lucene.index.TestIndexReaderReopen):	FAILED
    [junit] Error occurred in thread Thread-36:
    [junit] Lock obtain timed out: org.apache.lucene.store.SingleInstanceLock@6ac615: write.lock
    [junit] junit.framework.AssertionFailedError: Error occurred in thread Thread-36:
    [junit] Lock obtain timed out: org.apache.lucene.store.SingleInstanceLock@6ac615: write.lock
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen.testThreadSafety(TestIndexReaderReopen.java:764)
    [junit] 
    [junit] 
{code}"
0,"LocalTestServer and supporting classes should be available as a separate jarLocalTestServer and it's supporting classes are useful to anyone who wants to easily ""mock""/test simple http calls without having to embed a full jetty or something.
It would be awesome if these were available in a separate http-localtestserver.jar that could be used in projects outside of httpclient."
0,"Cut over numeric docvalues to fixed straight bytesCurrently numeric docvalues types are encoded and stored individually which creates massive duplication of writing / indexing code. Yet, almost all of them (except packed ints) are essentially a fixed straight bytes variant. "
0,"Add possibility to disable automatic authentication header processing.In the application I'm working on I need the possibility to manually get the 
WWW-Authenticate header instead of letting the HttpClient process it 
automatically. Instead of rewriting the execute() method in a subclass I added 
a configuration setting, similar to the followRedirects flag. Diffs below for 
anyone interested."
0,JSR 283: Workspace Management
0,"Clone proxStream lazily in SegmentTermPositionsIn SegmentTermPositions the proxStream should be cloned lazily, i. e. at the first time nextPosition() is called. Then the initialization costs of TermPositions are not higher anymore compared to TermDocs and thus there is no reason anymore for Scorers to use TermDocs instead of TermPositions. In fact, all Scorers should use TermPositions, because custom subclasses of existing scorers might want to access payloads, which is only possible via TermPositions. We could further merge SegmentTermDocs and SegmentTermPositions into one class and deprecate the interface TermDocs.

I'm going to attach a patch once the payloads feature (LUCENE-755) is committed."
0,"Port to Java5For my needs I've updated Lucene so that it uses Java 5 constructs. I know Java 5 migration had been planned for 2.1 someday in the past, but don't know when it is planned now. This patch against the trunk includes :

- most obvious generics usage (there are tons of usages of sets, ... Those which are commonly used have been generified)
- PriorityQueue generification
- replacement of indexed for loops with for each constructs
- removal of unnececessary unboxing

The code is to my opinion much more readable with those features (you actually *know* what is stored in collections reading the code, without the need to lookup for field definitions everytime) and it simplifies many algorithms.

Note that this patch also includes an interface for the Query class. This has been done for my company's needs for building custom Query classes which add some behaviour to the base Lucene queries. It prevents multiple unnnecessary casts. I know this introduction is not wanted by the team, but it really makes our developments easier to maintain. If you don't want to use this, replace all /Queriable/ calls with standard /Query/.

"
0,"Persian AnalyzerA simple persian analyzer.

i measured trec scores with the benchmark package below against http://ece.ut.ac.ir/DBRG/Hamshahri/ :

SimpleAnalyzer:
SUMMARY
  Search Seconds:         0.012
  DocName Seconds:        0.020
  Num Points:           981.015
  Num Good Points:       33.738
  Max Good Points:       36.185
  Average Precision:      0.374
  MRR:                    0.667
  Recall:                 0.905
  Precision At 1:         0.585
  Precision At 2:         0.531
  Precision At 3:         0.513
  Precision At 4:         0.496
  Precision At 5:         0.486
  Precision At 6:         0.487
  Precision At 7:         0.479
  Precision At 8:         0.465
  Precision At 9:         0.458
  Precision At 10:        0.460
  Precision At 11:        0.453
  Precision At 12:        0.453
  Precision At 13:        0.445
  Precision At 14:        0.438
  Precision At 15:        0.438
  Precision At 16:        0.438
  Precision At 17:        0.429
  Precision At 18:        0.429
  Precision At 19:        0.419
  Precision At 20:        0.415

PersianAnalyzer:
SUMMARY
  Search Seconds:         0.004
  DocName Seconds:        0.011
  Num Points:           987.692
  Num Good Points:       36.123
  Max Good Points:       36.185
  Average Precision:      0.481
  MRR:                    0.833
  Recall:                 0.998
  Precision At 1:         0.754
  Precision At 2:         0.715
  Precision At 3:         0.646
  Precision At 4:         0.646
  Precision At 5:         0.631
  Precision At 6:         0.621
  Precision At 7:         0.593
  Precision At 8:         0.577
  Precision At 9:         0.573
  Precision At 10:        0.566
  Precision At 11:        0.572
  Precision At 12:        0.562
  Precision At 13:        0.554
  Precision At 14:        0.549
  Precision At 15:        0.542
  Precision At 16:        0.538
  Precision At 17:        0.533
  Precision At 18:        0.527
  Precision At 19:        0.525
  Precision At 20:        0.518

"
0,EventImpl should implement toStringThis would simplify logging and debugging.
1,Mandatory jcr:activities node missing after upgradeThe mandatory node is only created when the repository is initially empty. The node is missing when an existing repository instance is upgraded. 
1,"IndexWriter memory leak when large docs are indexedSpinoff from the java-user thread ""IndexWriter and memory usage""...

IndexWriter has had a long standing memory leak, since LUCENE-843.

When the byte/char/int blocks are recycled to the common pool, the
per-thread DW classes incorrectly still hold a reference to them.

This normally is not a problem, since these buffers will be re-used
again.

But, if you index a massive document, causing IW to allocate more than
the RAM buffer allocated to it, then the leak happens.  So you could
have a 16 MB RAM buffer set, but if a huge doc required allocation of
200 MB worth of arrays, those 200 MB are never freed (well, until you
close the IW and deref it from the app).

It's even worse if you use multiple threads: if each thread has ever
had to index a massive document, then that thread incorrectly holds
onto the extra arrays.
"
0,"Expose DocValues via FieldsDocValues Reader are currently exposed / accessed directly via IndexReader. To integrate the new feature in a more ""native"" way we should expose the DocValues via Fields on a perSegment level and on MultiFields in the multi reader case. DocValues should be side by side with Fields.terms  enabling access to Source, SortedSource and ValuesEnum something like that:

{code}
public abstract class Fields {
...

  public DocValues values();

}

public abstract class DocValues {
  /** on disk enum based API */
  public abstract ValuesEnum getEnum() throws IOException;
  /** in memory Random Access API - with enum support - first call loads values in ram*/
  public abstract Source getSource() throws IOException;
  /** sorted in memory Random Access API - optional operation */
  public SortedSource getSortedSource(Comparator<BytesRef> comparator) throws IOException, UnsupportedOperationException;
  /** unloads previously loaded source only but keeps the doc values open */
  public abstract unload();
  /** closes the doc values */
  public abstract close();
}
{code}

"
0,"Let users set Similarity for MoreLikeThisLet users set Similarity used for MoreLikeThis

For discussion, see:
http://www.nabble.com/MoreLikeThis-API-changes--tf3838535.html"
0,"TCK: TextNodeTest and jcr:xmltext/jcr:xmlcharactersTest creates jcr:xmltext nodes without jcr:xmlcharacters properties.  Some repositories may require jcr:xmltext nodes to have jcr:xmlcharacters properties, causing this test to fail.

Proposal: add a jcr:xmlcharacters property to each jcr:xmltext node.

--- TextNodeTest.java   (revision 422074)
+++ TextNodeTest.java   (working copy)
@@ -62,6 +62,7 @@
      */
     public void testTextNodeTest() throws RepositoryException {
         Node text1 = testRootNode.addNode(jcrXMLText);
+        text1.setProperty(jcrXMLCharacters, ""foo"");
         testRootNode.save();
         String xpath = ""/"" + jcrRoot + testRoot + ""/text()"";
         executeXPathQuery(superuser, xpath, new Node[]{text1});
@@ -73,7 +74,9 @@
      */
     public void testTextNodeTestMultiNodes() throws RepositoryException {
         Node text1 = testRootNode.addNode(jcrXMLText);
+        text1.setProperty(jcrXMLCharacters, ""foo"");
         Node text2 = testRootNode.addNode(nodeName1, testNodeType).addNode(jcrXMLText);
+        text2.setProperty(jcrXMLCharacters, ""foo"");
         testRootNode.save();
         String xpath = ""/"" + jcrRoot + testRoot + ""//text()"";
         executeXPathQuery(superuser, xpath, new Node[]{text1, text2});
@@ -105,11 +108,13 @@
             throw new NotExecutableException(""Repository does not support position index"");
         }
         Node text1 = testRootNode.addNode(jcrXMLText);
+        text1.setProperty(jcrXMLCharacters, ""foo"");
         if (!text1.getDefinition().allowsSameNameSiblings()) {
             throw new NotExecutableException(""Node at path: "" + testRoot + "" does not allow same name siblings with name: "" + jcrXMLText);
         }
         testRootNode.addNode(nodeName1, testNodeType);
         Node text2 = testRootNode.addNode(jcrXMLText);
+        text1.setProperty(jcrXMLCharacters, ""foo"");
         testRootNode.save();
         String xpath = ""/"" + jcrRoot + testRoot + ""/text()[2]"";
         executeXPathQuery(superuser, xpath, new Node[]{text2});
"
0,"TestIndexModifier.testIndexWithThreads is not valid?I recently started playing with the trunk of SVN, and noticed that intermitently, TestIndexModifier.testIndexWithThreads (revision 292010) would fail.

The basic premise of the test seems to be that 3 pairs of IndexThread instances can be started in parallel, each pair using the same instance of IndexModifier to concurrently and randomly add/delete/optimize a single FSDirectory index.  
The test is considered a success if the sum of additions-deletions recorded by each pair of threads equals the final docCount() for the IndexModifier instance used by that pair of threads.

Now I freely admit that I'm not 100% familiar with the code for IndexModifier, but at a glance, the basic premise seems to be: 
   a) If method for IndexWriter is called, open it if needed, close the IndexReader first if needed.
   b) if method for IndexReader is called, open it if needed, close the IndexWriter first if needed.

If I'm understnading that correctly, I see no reason to assume this test will pass.  
It seems like there could be plenty of scenerios in which the number of additions-deletions != docCount(). The most trivial example I can think of is:
   1) the first IndexThread instance which has a chance to run adds a document, and optimizes before any other IndexThreads ever open the Directory.
   2) a subsequent pair of IndexThread instances open their IndexModifier instance before any documents are deleted.
   3) the IndexThread instances from #2 do nothing but add documents
...that pair of IndexThreads is now garunteed to have recorded a differnet number of additions then the docCount returned by their IndexModifier.

Am I missing something, or should this test be removed?

"
0,"[PATCH] documentation typoJust a small patch that fixes a typo and changes the first sentence, as that 
one is used by Javadoc as a kind of summary so it should be something more 
useful than ""The Jakarta Lucene API is divided into several packages."""
0,"Allow parsing custom elements in workspace configIn RepositoryConfigurationParser, most *Config elements can be extended in a derived class, e.g.

    public LoginModuleConfig parseLoginModuleConfig(Element security)

Unfortunately, parseWorkspaceConfig expects an InputSource. One should add a

    protected WorkspaceConfig parseWorkspaceConfig(Element root)

to allow returning a WorkspaceConfig derived class, without having to copy the entire implementation."
0,Stop creating huge arrays to represent the absense of field normsCreating and keeping around huge arrays that hold a constant value is very inefficient both from a heap usage standpoint and from a localility of reference standpoint. It would be much more efficient to use null to represent a missing norms table.
0,"A number of documentation fixes for the search package summaryImproves readability and clarity, corrects some basic English, makes some example text even more clear, and repairs typos."
0,Make CFS appendable  Currently CFS is created once all files are written during a flush / merge. Once on disk the files are copied into the CFS format which is basically a unnecessary for some of the files. We can at any time write at least one file directly into the CFS which can save a reasonable amount of IO. For instance stored fields could be written directly during indexing and during a Codec Flush one of the written files can be appended directly. This optimization is a nice sideeffect for lucene indexing itself but more important for DocValues and LUCENE-3216 we could transparently pack per field files into a single file only for docvalues without changing any code once LUCENE-3216 is resolved.
0,"CartesianTierPlotter fieldPrefix should be configurableCartesianTierPlotter field prefix is currrently hardcoded to ""_localTier"" -- this should be configurable"
0,"Fix JFlex tokenizer compiler warningsWe get lots of distracting fallthrough warnings running ""ant compile""
in modules/analysis, from the tokenizers generated from JFlex.

Digging a bit, they actually do look spooky.

So I managed to edit the JFlex inputs to insert a bunch of break
statements in our rules, but I have no idea if this is
right/dangerous, and it seems a bit weird having to do such insertions
of ""naked"" breaks.

But, this does fix all the warnings, and all tests pass...
"
0,"QueryParser is not applicable for the arguments (String, String, Analyzer) error in results.jsp when executing search in the browser (demo from Lucene 2.0)When executing search in the browser (as described in demo3.html Lucene demo) I get error, because the demo uses the method (QueryParser with three arguments) which is deleted (it was deprecated).
I checked the demo from Lucene 1.4-final it with Lucene 1.4-final - it works, because those time the method was there.
But demo from Lucene 2.0 does not work with Lucene 2.0

The error stack is here:
TTP Status 500 -

type Exception report

message

description The server encountered an internal error () that prevented it from fulfilling this request.

exception

org.apache.jasper.JasperException: Unable to compile class for JSP

An error occurred at line: 60 in the jsp file: /results.jsp
Generated servlet error:
The method parse(String) in the type QueryParser is not applicable for the arguments (String, String, Analyzer)


org.apache.jasper.servlet.JspServletWrapper.handleJspException(JspServletWrapper.java:510)
org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:375)
org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:314)
org.apache.jasper.servlet.JspServlet.service(JspServlet.java:264)
javax.servlet.http.HttpServlet.service(HttpServlet.java:802)

root cause

org.apache.jasper.JasperException: Unable to compile class for JSP

An error occurred at line: 60 in the jsp file: /results.jsp
Generated servlet error:
The method parse(String) in the type QueryParser is not applicable for the arguments (String, String, Analyzer)


org.apache.jasper.compiler.DefaultErrorHandler.javacError(DefaultErrorHandler.java:84)
org.apache.jasper.compiler.ErrorDispatcher.javacError(ErrorDispatcher.java:328)
org.apache.jasper.compiler.JDTCompiler.generateClass(JDTCompiler.java:409)
org.apache.jasper.compiler.Compiler.compile(Compiler.java:297)
org.apache.jasper.compiler.Compiler.compile(Compiler.java:276)
org.apache.jasper.compiler.Compiler.compile(Compiler.java:264)
org.apache.jasper.JspCompilationContext.compile(JspCompilationContext.java:563)
org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:303)
org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:314)
org.apache.jasper.servlet.JspServlet.service(JspServlet.java:264)
javax.servlet.http.HttpServlet.service(HttpServlet.java:802)

note The full stack trace of the root cause is available in the Apache Tomcat/5.5.15 logs."
0,Improve password hashing
0,"[PATCH] don't delete all files in index directory on index creationMany people use Lucene to index a part of their file system. The chance that  
you some day mix up index directory and document directory isn't that bad.  
Currently Lucene will delete *all* files in the index directory when the  
create paramater passed to IndexWriter is true, thus deleting your documents 
if you mixed up the parameters. I'll attach a patch that fixes  
this. Any objections?"
0,"remove IndexSearcher.docFreq/maxDocAs pointed out by Mark on SOLR-1632, these are no longer used by the scoring system.

We've added new stats to Lucene, so having these methods on indexsearcher makes no sense.
Its confusing to people upgrading if they subclassed IndexSearcher to provide distributed stats,
only to find these are not used (LUCENE-3555 has a correct API for them to do this).

So I think we should remove these in 4.0."
1,"Repository Home locked not released despite RepositoryException being thrown.When an exception is thrown when calling RepositoryImpl.create(...) a .lock file is created in the repository home directory and not removed despite there no longer being an active connection.  If the user attempts to create the repository again (e.g recover from the exception because the url of the repository was temporarily unavailable) a RepositoryException is thrown again indicating that the repository home is locked by another process because there is a .lock file.  If a Repository is not successfully created then the repository home should not be locked.

The lock is only released when the repository is shutdown but in this case the Repository object is never created successfully for that method to be called.

"
0,Remove SegmentReader.document synchronizationThis is probably the last synchronization issue in Lucene.  It is the document method in SegmentReader.  It is avoidable by using a threadlocal for FieldsReader.  
1,"NullPointerException thrown when invalid header encounteredIf a server returns a header with no name but with a value (ie: an invalid line in the headers), HttpClient throws a NullPointerException instead of just skipping that header line or perhaps treating it as a continuation of the previous header (need to consult the RFC to confirm this).

Problem reported by Eduardo Francos on the commons-user list.

A good test URL for this problem is:

http://www.pc.ibm.com/us/accessories/monitors/p_allmodelos.html

which should return a 404 error but throws the NullPointerException instead."
0,"Simple Google style queryIn the Sling project there's a need for a simple query language. See SLING-573.

I've created a parser that translates the simple query into an XPath query statement and executes it on a JCR workspace.

I'll commit it to the jackrabbit-jcr-commons module."
0,"Allow configuration of SO_LINGERThere is currently no way to configure the SO_LINGER option on a socket.

Please change the HttpClient class to allow the configuration of the SO_LINGER
option on a socket, similar to the way the SO_TIMEOUT can be configured.

Suggested extension to the interface of the HttpClient class:
- Add method setSoLinger() to set the current setting for SO_LINGER. The method
could accept one argument. A negative value could indicate that the SO_LINGER
should be disabled.
- Add method getSoLinger() that returns the current setting for SO_LINGER. A
negative value would indicate that the SO_LINGER option is disabled.

See:
http://java.sun.com/j2se/1.4.2/docs/api/java/net/Socket.html#setSoLinger(boolean,%20int)"
1,"SQL query with jcr:path LIKE '/foo/%' only selects childrenA query like: 

SELECT * FROM nt:base WHERE jcr:path LIKE '/foo/%'

only selects the children of /foo instead off all descendants of /foo."
0,"use VersionInfo of coreWith core alpha5, a version detection scheme was introduced.
Replace the preliminary version detection of client alpha1 with that in core.
That means new version.properties files, at least one per JAR, maybe one per potential JAR.
"
0,"Benchmark contrib should allow multiple locations in ext.classpathWhen {{ant run-task}} is invoked with the  {{-Dbenchmark.ext.classpath=...}} option, only a single location may be specified.  If a classpath with more than one location is specified, none of the locations is put on the classpath for the invoked JVM."
1,"{XML|Object}PersistenceManager.destroy(*) may failThe destroy methods of the ObjectPersistenceManager class try to delete their files without checking for their existence. This may result in a FileSystemException being thrown because according to the specification of FileSystem.deleteFile() a FileSystemException is thrown ""if this path does not denote a file or if another error occurs.""

While the Jackrabbit LocalFileSystem implementation silently ignores a request to delete a non-existing file, our internal implementation of the interface throws a FileSystemException in this case, which cause destroy to fail.

I suggest all destroy methods should be extended to first check for the existence of the file to prevent from being thrown.

Note: This not only applies to ObjectPersistenceManager but also to XMLPersistenceManager."
0,"Polish AnalyzerAndrzej Bialecki has written a Polish stemmer and provided stemming tables for it under Apache License.

You can read more about it here: http://www.getopt.org/stempel/

In reality, the stemmer is general code and we could use it for more languages too perhaps."
1,"XML import should not access external entitiesWith current Jackrabbit the following XML document can not be imported:

    <!DOCTYPE foo SYSTEM ""http://invalid.address/""><foo/>

Even if the DTD address (or some other external resource referenced in the XML document) is correct, I don't think importXML() should even try resolving those references."
0,"ProtocolException thrown on slightly broken headersHTTPClient throws an exception when parsing headers returned by GET from the
following URL:

 http://butler.cit.nih.gov/hembase/hembase.taf

The headers returned are as follows:

HTTP/1.0 200 OK\r\nServer: WebSTAR/1.0 ID/ACGI\r\nMIME-Version:
1.0\r\nContent-Type: text/html\r\nSet-Cookie:
Tango_UserReference=ADC5871C57FABEDEC63DD47B; path=/\n\r\r\n\r\n<!DOCTYPE HTML
PUBLIC ""-//W3C//DTD HTML 4.01 Transitional//EN"">...

Please note the superfluous \r in the line separating headers from the body.
IMHO this type of error should generate a warning, but then it should cause a
graceful recovery. Currently a ProtocolException is thrown.

Standard java.net.HttpURLConnection handles this just fine, without giving any
warning."
0,"CheckIndex should verify numUniqueTerms == recomputedNumUniqueTermsJust glancing at the code it seems to sorta do this check, but only in the hasOrd==true case maybe (which seems to be testing something else)?

It would be nice to verify this also for terms dicts that dont support ord.

we should add explicit checks per-field in 4.x, and for-all-fields in 3.x and preflex"
0,"Remove Maven 1 filesNow that we have a working Maven 2 build environment (JCR-332) we should remove the old Maven 1
project files to avoid confusion and misunderstandings. The old Maven 1 build environment doesn't even
work anymore.

Unless anyone objects, I'll proceed to remove the project.xml, maven.xml, and project.properties files in a few days."
1,"ClassDescriptor.hasIdField() fails if id is declared in upper classorg.apache.jackrabbit.ocm.mapper.model.ClassDescriptor.hasIdField() looks up only current class and not the whole hierarchy, so it fails when the id field is declared in a upper class.

hasIdField should use getIdFieldDescriptor and not access idFieldDescriptor field directly, as follows :

    public boolean hasIdField() {
   		return (this.getIdFieldDescriptor() != null && this
    				.getIdFieldDescriptor().isId());
    }

Please find patch enclosed.

Sincerely yours,

Stéphane Landelle"
0,"Improve the use of isDeleted in the indexing codeA spin off from here: http://www.nabble.com/Some-thoughts-around-the-use-of-reader.isDeleted-and-hasDeletions-td23931216.html.
Two changes:
# Optimize SegmentMerger work when a reader has no deletions.
# IndexReader.document() will no longer check if the document is deleted.

Will post a patch shortly"
0,"jcr-commons: Add utility to translate a string to a AuthorizableQuery and execute it on the user manager it would be convenient if jackrabbit-jcr-commons would provide a utility to generate authorizable
queries from a string."
0,"spi2dav: Drop Q*DefinitionImpl implementations and use spi-commons Q*DefinitionBuilderspi2dav provides separate implementations of the Q*Definition interfaces that apart from the construction just duplicate the code
present in spi-commons. Instead the Q*DefinitionBuilder helpers could be used to generate the definition instances."
0,VersionTest.testGetUUID() failsVersionTest.testGetUUID() fails due to inproper invlaidation of the successor properties after checkin.
1,"NTLM Authentication No Longer Working In Latest ReleaseOur application has been working fine using NTLM auth with HttpClient for 3 years.   We were most recently on 4.0.3.    Upon upgrading to 4.1.2, NTLM stopped working.

I tried both the new for 4.1 built-in NTLM and the ""old way"" of using JCIFS: client.getAuthSchemes().register(""ntlm"", new NTLMSchemeFactory()); 

Using wireshark I can see that NTLM auth is not even attempted using 4.1.2.    Rolling back to 4.0.3 immediately resolved this problem."
0,"[patch] javadoc and comment updates for BooleanClause.Javadoc and comment updates for BooleanClause, one minor code simplification."
0,"Too many open files when merging large index segmentsWhen large index segments are merged it may happen that lots of smaller index segments are created but have to wait until the large
index merge has completed. This may lead to a 'too many open files' exception on some system.

We should find a solution where large index merges are better decoupled from regular index operations."
0,"Support multi-selector OR constraints in join queriesOur current join implementation doesn't support OR constraints that refer to more than one selector. For example the following query is not possible:

    SELECT a.* FROM [my:type] AS a INNER JOIN [my:type] as b ON a.foo = b.bar WHERE a.baz = 'x' OR b.baz = 'y'

This limitation is a result of the way the join execution splits the query into per-selector components and merges the result based on the given join condition.

A simple but often inefficient solution would be to process such OR constraints as post-processing filters like we already do for some other more complex constraint types."
0,create test case to verify we support > 2.1B termsI created a test case for this... I'm leaving it as @Ignore because it takes more than four hours on a faaast machine (beast) to run.  I think we should run this before each release.
1,"Bundle binding deserialization problemI'm trying to upgrade from 1.3.x to jackrabbit 1.4.x (branch)  and have problems with existing repostories (probaly the same issue is with 1.5.x)

Caused by: org.apache.jackrabbit.core.state.ItemStateException: failed to read bundle: deadbeef-face-babe-cafe-babecafebabe: java.lang.IllegalArgumentException: invalid namespaceURI specified
 at org.apache.jackrabbit.core.persistence.bundle.BundleDbPersistenceManager.loadBundle(BundleDbPersistenceManager.java:1229)
 at org.apache.jackrabbit.core.persistence.bundle.BundleDbPersistenceManager.loadBundle(BundleDbPersistenceManager.java:1161)

It looks that issue was introduced by resolving JCR-1632"
1,"SQL-2 query returns more than the requested columnIf I do :

SELECT alias.[jcr:title] FROM [jnt:mainContent] as alias

and then iterate through the returned columns of the rows, I get the same result as for :

SELECT * FROM [jnt:mainContent] 

which is ALL the properties defined for jnt:mainContent.

Only if I use

SELECT alias.[jcr:title] as title FROM [jnt:mainContent] as alias

the result is limited to the title column."
0,SQL2: Implement LIKE support for node namesDoing a LIKE constraint on the local name of a node that throws javax.jcr.UnsupportedRepositoryOperationException.
0,"Promotion of SPI from ContribThis has been suggested by Jukka on the dev-list [1] and i would like to start the promotion now. Apart from 
the promotion itself the discussion regarding distribution of common classes [2] and the related issue JCR-996 somehow depends on the promotion and i consider the latter one to be important to follow.

So far nobody objected the promotion. If this is still true, i will start working on that.




[1] http://www.mail-archive.com/dev@jackrabbit.apache.org/msg06433.html
[2] http://www.mail-archive.com/dev@jackrabbit.apache.org/msg06698.html"
0,JCR2SPI: remove dependency to state-package within nodetype package
0,"[PATCH]character encoding handling is invalid at multipartHi,

Commons-Httpclient handle character encoding incorrect at multipart. This is 
significant problem for other than English people like me. Multipart has two 
encoding. First is header encoding which specify header of each part. Second 
is it's body encoding. Body encoding works well but header encoding is fixed 
as 'asc-ii'. This problem user following situation.

* upload file which file name is described by other than ""asc-ii"".
* use parameter which include other than ""asc-ii"" character.

Unfortunately , It seems RFC doesn't define header encoding for multipart but 
a lot of people needs set header encoding for thier own laungage. I attached
the patch. Please fix this problem.

regards,

Takashi Okamoto"
1,"ObjectConverterImpl.getObject(Session, Class, String) may not resolve mapping correctly for incompletely described mappingsWhen a node is mapped by calling the ObjectConverter.getObject(Session, Class, String) method and no discriminator property is configured the ObjectConverterImpl class tries to find a ""best"" mapping for the effective node. This is done by walking the class descriptor hierarchy starting at the descriptor for the selected class until a mapping for the node type is found.

In case the class descriptor hierarchy is incomplete because an improperly defined class descriptor would actually perfectly map the node but is not declared to extend (or implement) its parent classes/interfaces, the hierarchy walk down will not find the mapping and thus in the end, the originally requested class will be instantiated. If the class is abstract or an interface this of course fails.

If an exact class descriptor for the node type would be looked up directly, the mapping might be found immediately and the class of the descriptor can be verified it actually is assignement compatible with the requested class. If this would fail, we could still walk the hierarchy to see, whether we find another classdescriptor.

To clarify the issue consider the following example of an abstract base class and a concrete extension class with their node types

   AbstractBaseClass maps abstractly to AbstractBaseType
   BaseClass (extends AbstractBaseClass) maps to BaseType ( with supertype AbstractBaseType )

Note, that the BaseClass mapping does not declare to extend the AbstractBaseClass.

When calling ObjectConverterImpl.getObject(session, AbstractBaseClass.class, aBaseTypeNode), the descriptor fore the AbstractBaseClass is inspected agains the node and then it is decided to check the class descriptor hierarchy. Node mapping can be found by walking the hierarchy and hence the AbstractBaseClass is instantiated, which of course fails.

If the BaseClass mapping would be declared as extending the AbstractBaseClass mapping, everything would be fine."
0,"TestUTF32ToUTF8 can run foreverStress testing this particular test uncovered that the testRandomRanges testcase can run forever, depending on the random numbers picked..."
0,"move contrib/snowball to contrib/analyzersto fix bugs in some duplicate, handcoded impls of these stemmers (nl, fr, ru, etc) we should simply merge snowball and analyzers, and replace the buggy impls with the proper snowball stemfilters.
"
0,"Add preemptive authenticationWishlist request for preemptive authentication to be included in the API, like HttpClient 3.x had.  There is an example ClientPreemptiveBasicAuthentication.java that uses HttpRequestInterceptor which I had adapted to my application and it works fine."
0,Improved error reporting from JcrUtils.getRepositoryThe service provider mechanism and the null return value used by the RepositoryFactory API makes it a bit difficult to troubleshoot cases where a repository can not be accessed. It would be helpful if the JcrUtils.getRepository methods reported as accurate failure information as possible in case the requested repository is not found.
1,"Parameters 'idleTime' and 'queryClass' cause QueryHandler to failThis issue does not occur in a released jackrabbit-core version. With the changes from JCR-1462 jackrabbit now fails to startup if there is an unknown parameter in a bean configuration.

The parameters 'idleTime' and 'queryClass' are not used by the QueryHandler but by the SearchManager, which instantiates the QueryHandler. Therefore the parameters do not show up in the QueryHandler.

I suggest we introduce them in the common base class AbstractQueryHandler."
1,"FVH: slow performance on very large queriesThe change from HashSet to ArrayList for flatQueries in LUCENE-3019 resulted in very significant slowdown in some of our e-discovery queries after upgrade from 3.4.0 to 3.5.0. Our queries sometime contain tens of thousands of terms. As a result, major portion of execution time for such queries is now spent in the flatQueries.contains( sourceQuery ) method calls."
0,Implementation of a memory file systemI needed a memory file system for my test cases. A patch for a simple implementation the works well in my environment is attached
0,Basic support for fn:name()Add basic support for fn:name() in XPath queries. Jackrabbit should at least support the the fn:name() function within an equals expression.
0,"SPI: change param order with RepositoryService.createBatchall methods on RepositoryService that require a SessionInfo list the info as first parameter, except for 

RepositoryService.createBatch(ItemId, SessionInfo) 

unless someone objects i would refacter the method signature for consistency reasons.

new:

RepositoryService.createBatch(SessionInfo, ItemId)"
0,"Support all of unicode in StandardTokenizerStandardTokenizer currently only supports the BMP.

If it encounters characters outside of the BMP, it just discards them... 
it should instead implement fully implement UAX#29 across all of unicode."
1,"TaxonomyReader.refresh() is broken, replace its logic with reopen(), following IR.reopen patternWhen recreating the taxonomy index, TR's assumption that categories are only added does not hold anymore.
As result, calling TR.refresh() will be incorrect at best, but usually throw an AIOOBE."
0,"The repository-1.5.dtd is not well formedThe repository-1.5.dtd file at http://jackrabbit.apache.org/dtd/repository-1.5.dtd
is not well formed at the time of this writing 200/05/23 19:30GMT

1. It looks like a #REQUIRED is missing at line 173
2. Detected this while trying the 5minutes with ocm tutorial

Hope this helps,
 S."
1,"Finding Newest Segment In Empty IndexWhile extending the index writer, I discovered that its newestSegment method does not check to see if there are any segments before accessing the segment infos vector. Specifically, if you call the IndexWriter#newestSegment method on a brand-new index which is essentially empty, then it throws an java.lang.ArrayIndexOutOfBoundsException exception.

The proposed fix is to return null if no segments exist, as shown below:

--- lucene/src/java/org/apache/lucene/index/IndexWriter.java	(revision 930788)
+++ lucene/src/java/org/apache/lucene/index/IndexWriter.java	(working copy)
@@ -4587,7 +4587,7 @@
 
   // utility routines for tests
   SegmentInfo newestSegment() {
-    return segmentInfos.info(segmentInfos.size()-1);
+    return segmentInfos.size() > 0 ? segmentInfos.info(segmentInfos.size()-1) : null;
   }
"
1,"nonce-count in digest auth should not be quotedIn 3.0rc3 nonce-count (nc) is enclosed in quote marks. According to rfc2617 this
is wrong, nonce-count shouldn't be enclosed in quote marks.

> 3.2.2 The Authorization Request Header
> 
>    The client is expected to retry the request, passing an Authorization
>    header line, which is defined according to the framework above,
>    utilized as follows.
> 
>        credentials      = ""Digest"" digest-response
>        digest-response  = 1#( username | realm | nonce | digest-uri
>                        | response | [ algorithm ] | [cnonce] |
>                        [opaque] | [message-qop] |
>                            [nonce-count]  | [auth-param] )
> 
>        username         = ""username"" ""="" username-value
>        username-value   = quoted-string
>        digest-uri       = ""uri"" ""="" digest-uri-value
>        digest-uri-value = request-uri   ; As specified by HTTP/1.1
>        message-qop      = ""qop"" ""="" qop-value
>        cnonce           = ""cnonce"" ""="" cnonce-value
>        cnonce-value     = nonce-value
>        nonce-count      = ""nc"" ""="" nc-value
>        nc-value         = 8LHEX
>        response         = ""response"" ""="" request-digest
>        request-digest = <""> 32LHEX <"">
>        LHEX             =  ""0"" | ""1"" | ""2"" | ""3"" |
>                            ""4"" | ""5"" | ""6"" | ""7"" |
>                            ""8"" | ""9"" | ""a"" | ""b"" |
>                            ""c"" | ""d"" | ""e"" | ""f"""
0,"Enhance Ingres persistence bundle to handle unicodeTiny change to ingres.ddl for persistent bundles to handle unicode strings.

"
1,"CircularRedirectException encountered when using a proxy, but not when reaching the target directlyA CircularRedirectException is encountered when using a proxy (tinyproxy on a remote machine), whereas everything is fine when using no proxy. The target is a URL such as http://www.seoconsultants.com/w3c/status-codes/301.asp which has a 301 redirection.

The issue can be fixed by using ALLOW_CIRCULAR_REDIRECTS set to true (client params), but I can't consider this a ""real"" fix.

Here is a snippet of code that exemplifies the problem (use your own proxy):

---
String proxyHost = ""xyz.webfactional.com"";
int proxyPort = 7295;

DefaultHttpClient httpclient = new DefaultHttpClient();
// without a proxy it's OK!
httpclient.getParams().setParameter(ConnRoutePNames.DEFAULT_PROXY,
        new HttpHost(proxyHost, proxyPort, ""http""));

HttpParams params = httpclient.getParams();
HttpClientParams.setRedirecting(params, true);
HttpProtocolParams.setUserAgent(params,
        ""Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.0.10) Gecko/2009042315 Firefox/3.0.10"");

// OK, this fixes the problem, but at what cost / other problems ?
//httpclient.getParams().setParameter(ClientPNames.ALLOW_CIRCULAR_REDIRECTS, true);

String url = ""http://www.seoconsultants.com/w3c/status-codes/301.asp"";

HttpUriRequest request;
HttpResponse response;

request = new HttpGet(url);
System.out.println(""request = "" + request.getRequestLine());
response = httpclient.execute(request);
System.out.println(""status = "" + response.getStatusLine());
System.out.println(""headers = "" + Arrays.asList(response.getAllHeaders()));
---"

label,summarydescription
1,"Authentication fails when connecting to server with username and password in non ascii characters. Tried connecting to an exchange server using NTLM authentication
Username : íñëäíñëä
password : íñëäíñëä

I am getting 401 response.
Auth failed
Body: 
Error: Access is Denied."
1,"JCR-RMI problem with large binary values. As reported on the mailing list, a JCR-RMI connection will hang when given a binary value that's larger than the default 64kB buffer size."
1,"Header Connection Close - Closes the Connection. If the connectionHeader equals Close the conection imediateley closes, without 
waiting for the responce back from the server. 

If the client is pulling data from a CGI script which has not sent the Content 
Length - most servers will send a Connection Close header. For example 

-----------------------------------------
HTTP/1.1 200 OK
Date: Fri, 21 Jun 2002 17:08:46 GMT
Server: Apache/1.3.14 (Unix)
Connection: close
Content-Type: text/html
 
<html>
      <head>
            <title>thegumtree.com - London's online community for Aussies, Kiwis
 and South Africans</title>
                           <meta http-equiv=""Content-Type"" content=""text/html; c
harset=iso-8859-1"">
                   </head>

--------------------------------

I do not yet have a work arround apart from commenting out the the following 
code in 

Header connectionHeader = getResponseHeader(""connection""); etc

in HttpMethodBase"
1,"Query does not work after logging into workspace with no indexes. When I login to workspace that does not have indexes, they are created but my queries do not return results unless I relog. Output from running attached code sample is:

Node [node1240842434531] created in workspace [test1240842434312]
Property [name] set to [someValueOfMyProperty]
Asking query: select * from nt:unstructured where nt:name like 'someValueOfMyProperty'
Found: 1 nodes before deleting index.
Asking query: select * from nt:unstructured where nt:name like 'someValueOfMyProperty'
Found: 0 nodes after deleting index.
done"
1,"XA Transaction Recovery. If i add a node to the repository i get a XAException because i run into a Timeout ... 
I see the Warn Message: Transaction rolled back because timeout expired.
The default Timeout is set to 5 sec and i dont know how to set it to a higher value
The Problem is if i restart my server websphere has a RecoveryManager and he try to recover this Transaction
and then i get a NullpointerException in JCAManagedConnectionFactory. createManagedConnection beacuse the given 
ConnectionRequestInfo is null.
So i dont know why the RecoveryManager tries to recover the Transaction ? The only solution for me is to delete the Tran-Log Files wich keep Websphere to recvoer
XA Trasnactions.
"
1,"spi2dav : Query offset not respected. the TCK query test SetOffsetTest fails in the setup jcr2spi - spi2dav(ex) - jcr-server.
not sure whether it is due to missing implementation on client or server part of something completely different...."
1,"Restart of RMI-component fails (because it's not released while shutdown). I just moved setup model for the Jackrabbit repository from a Tomcat-global JNDI-datasouce to a autonomous server connected via RMI to get rid off the problem of a total restart of the tomcat, if e.g. something is changed in the jackrabbit setup.

But the restart of the RMI component of the jackrabbit server package will fail, because on shutdown the rmi binding isn't released. From that, at restart, the socket is still in use and the (just) RMI component fails to start. In the other hand, it isn't possible to connect to the server through the remaining rmi component; you'll get a EOF-exception in RMI communication. Of course, a complete restart of the Tomcat will help, but isn't appropriate. 

It looks to me like just some release on shutdown is missing. May somebody provide a patch?

(log exception at restart)
20080306-093849.086 INFO  [ajp-8009-2] [] [RepositoryStartupServlet] Cannot create Registry
java.rmi.server.ExportException: Port already in use: 1099; nested exception is: 
        java.net.BindException: Address already in use
        at sun.rmi.transport.tcp.TCPTransport.listen(TCPTransport.java:249)
        at sun.rmi.transport.tcp.TCPTransport.exportObject(TCPTransport.java:184)
        at sun.rmi.transport.tcp.TCPEndpoint.exportObject(TCPEndpoint.java:382)
        at sun.rmi.transport.LiveRef.exportObject(LiveRef.java:116)
        at sun.rmi.server.UnicastServerRef.exportObject(UnicastServerRef.java:180)
        at sun.rmi.registry.RegistryImpl.setup(RegistryImpl.java:92)
        at sun.rmi.registry.RegistryImpl.<init>(RegistryImpl.java:68)
        at java.rmi.registry.LocateRegistry.createRegistry(LocateRegistry.java:222)
        at org.apache.jackrabbit.j2ee.RepositoryStartupServlet.registerRMI(RepositoryStartupServlet.
        at org.apache.jackrabbit.j2ee.RepositoryStartupServlet.startup(RepositoryStartupServlet.java
        at org.apache.jackrabbit.j2ee.RepositoryStartupServlet.init(RepositoryStartupServlet.java:21
        at javax.servlet.GenericServlet.init(GenericServlet.java:212)
"
1,"HttpMethod#getResponseBody throws NPE. HttpMethod#getResponseBody throws an NPE if the response from the server was 
204.  Shouldn't getResponseBody return null by contract instead of throwing 
NPE?"
1,"Incorrect node position after import. I have found a behavior that does not seem to be consistent with the
spec:

After replacing a node with importXML using IMPORT_UUID_COLLISION_REPLACE_EXISTING the new node is not at the position of the replaced node (talking about the position among the siblings).

The origininal node is removed, but the new node is created as the last child of the parent node, and not spec-compliant at the position of the replaced node.

Here how I use it:

// assume Session s, Node n, String text (holding XML data)

s.importXML(
	n.getPath(), 
	new ByteArrayInputStream (text.getBytes(""UTF-8"")),
	ImportUUIDBehavior.IMPORT_UUID_COLLISION_REPLACE_EXISTING
);
s.save();
 

And here a quote from the spec section 7.3.6

ImportUUIDBehavior.IMPORT_UUID_COLLISION_REPLACE_EXISTING: 
If an incoming referenceable node has the same UUID as a node already existing in the workspace then the already existing node is replaced by the incoming node in the same position as the existing node.
[note ""same position""]
"
1,Node deleted while query is executed should not affect result size. Currently the QueryResultImpl counts result nodes as invalid when the access check throws a ItemNotFoundException (line 311). This leads to inconsistent total size. IMO it is sufficient to count them as invalid when the client iterates over the nodes (line 555).
1,UserManager.getAuthorizable() may fail with InvalidQueryException. Happens when the principal name contains an apostrophe.
1,"NPE in NearSpansUnordered from PayloadNearQuery. The following query causes a NPE in NearSpansUnordered, and is reproducible with the the attached unit test. The failure occurs on the last document scored.
"
1,"org.apache.lucene.search.BooleanQuery$TooManyClauses when using '>' operator. when using a query with a '>' operator, the query engine does not scale with number of matching properties and a org.apache.lucene.search.BooleanQuery$TooManyClauses exception is thrown"
1,"Deleting docs of all returned Hits during search causes ArrayIndexOutOfBoundsException. For background user discussion:
http://www.nabble.com/document-deletion-problem-to14414351.html

{code}
Hits h = m_indexSearcher.search(q); // Returns 11475 documents 
for(int i = 0; i < h.length(); i++) 
{ 
  int doc = h.id(i); 
  m_indexSearcher.getIndexReader().deleteDocument(doc);  <-- causes ArrayIndexOutOfBoundsException when i = 6400
} 
{code}
"
1,"HttpConnection isOpen flag concurrency problem. The HttpConnection.java class contains an isOpen boolean used to track the state
of the connection (opened or closed).  The problem is that in the
closeSocketAndStreams(), the flag is only flipped at the end of the
unsynchronized method (after resources have been released) which causes a
concurrency issue in flushRequestOutputStream() where the flag is checked first
and the the outputStream is accessed.

I'm providing a patch for this problem."
1,"TestIndexWriter.testBackgroundOptimize fails with too many open files. Recreate with this line:

ant test -Dtestcase=TestIndexWriter -Dtestmethod=testBackgroundOptimize -Dtests.seed=-3981504507637360146:51354004663342240

Might be related to LUCENE-2873 ?"
1,"Contrib queries package Query implementations do not override equals(). Query implementations should override equals() so that Query instances can be cached and that Filters can know if a Query has been used before.  See the discussion in this thread.

http://www.mail-archive.com/java-user@lucene.apache.org/msg13061.html

Following 3 contrib Query implementations do no override equals()

org.apache.lucene.search.BoostingQuery;
org.apache.lucene.search.FuzzyLikeThisQuery;
org.apache.lucene.search.similar.MoreLikeThisQuery;

Test cases below show the problem.

package com.teamware.office.lucene.search;

import static org.junit.Assert.*;

import org.apache.lucene.analysis.standard.StandardAnalyzer;
import org.apache.lucene.index.Term;
import org.apache.lucene.search.BoostingQuery;
import org.apache.lucene.search.FuzzyLikeThisQuery;
import org.apache.lucene.search.TermQuery;
import org.apache.lucene.search.similar.MoreLikeThisQuery;
import org.junit.After;
import org.junit.Before;
import org.junit.Test;
public class ContribQueriesEqualsTest
{
    /**
     * @throws java.lang.Exception
     */
    @Before
    public void setUp() throws Exception
    {
    }

    /**
     * @throws java.lang.Exception
     */
    @After
    public void tearDown() throws Exception
    {
    }
    
    /**
     *  Show that the BoostingQuery in the queries contrib package 
     *  does not implement equals() correctly.
     */
    @Test
    public void testBoostingQueryEquals()
    {
        TermQuery q1 = new TermQuery(new Term(""subject:"", ""java""));
        TermQuery q2 = new TermQuery(new Term(""subject:"", ""java""));
        assertEquals(""Two TermQueries with same attributes should be equal"", q1, q2);
        BoostingQuery bq1 = new BoostingQuery(q1, q2, 0.1f);
        BoostingQuery bq2 = new BoostingQuery(q1, q2, 0.1f);
        assertEquals(""BoostingQuery with same attributes is not equal"", bq1, bq2);
    }

    /**
     *  Show that the MoreLikeThisQuery in the queries contrib package 
     *  does not implement equals() correctly.
     */
    @Test
    public void testMoreLikeThisQueryEquals()
    {
        String moreLikeFields[] = new String[] {""subject"", ""body""};
        
        MoreLikeThisQuery mltq1 = new MoreLikeThisQuery(""java"", moreLikeFields, new StandardAnalyzer());
        MoreLikeThisQuery mltq2 = new MoreLikeThisQuery(""java"", moreLikeFields, new StandardAnalyzer());
        assertEquals(""MoreLikeThisQuery with same attributes is not equal"", mltq1, mltq2);
    }
    /**
     *  Show that the FuzzyLikeThisQuery in the queries contrib package 
     *  does not implement equals() correctly.
     */
    @Test
    public void testFuzzyLikeThisQueryEquals()
    {
        FuzzyLikeThisQuery fltq1 = new FuzzyLikeThisQuery(10, new StandardAnalyzer());
        fltq1.addTerms(""javi"", ""subject"", 0.5f, 2);
        FuzzyLikeThisQuery fltq2 = new FuzzyLikeThisQuery(10, new StandardAnalyzer());
        fltq2.addTerms(""javi"", ""subject"", 0.5f, 2);
        assertEquals(""FuzzyLikeThisQuery with same attributes is not equal"", fltq1, fltq2);
    }
}
"
1,"Background threads should use jackrabbit classloader as thread context classloader. The RepositoryImpl uses a thread executor with a default thread factory for some background threads. These threads should use the jackrabbit class loader (the classloader used for loading jackrabbit)
as thread context classloader. Currently the classloader is used which causes a new thread to be greated.
For example in combination with Sling the following can happen: a jsp in sling initiates a save to jackrabbit, this causes the indexing to start which is done in a background thread. A new thread is taken from the pool and the thread context class loader is set to the thread context classloader of the jsp/sling. If now Sling is undeployed, jackrabbit still holds this class loader. This creates a hugh memory leak.
"
1,"Using WildcardQuery with MultiSearcher, and Boolean MUST_NOT clause. We are searching across multiple indices using a MultiSearcher. There seems to be a problem when we use a WildcardQuery to exclude documents from the result set. I attach a set of unit tests illustrating the problem.

In these tests, we have two indices. Each index contains a set of documents with fields for 'title',  'section' and 'index'. The final aim is to do a keyword search, across both indices, on the title field and be able to exclude documents from certain sections (and their subsections) using a
WildcardQuery on the section field.
 
 e.g. return documents from both indices which have the string 'xyzpqr' in their title but which do not lie
 in the news section or its subsections (section = /news/*).
 
The first unit test (testExcludeSectionsWildCard) fails trying to do this.
 If we relax any of the constraints made above, tests pass:
 
* Don't use WildcardQuery, but pass in the news section and it's child section to exclude explicitly (testExcludeSectionsExplicit)</li>
* Exclude results from just one section, not it's children too i.e. don't use WildcardQuery(testExcludeSingleSection)</li>
* Do use WildcardQuery, and exclude a section and its children, but just use one index thereby using the simple
   IndexReader and IndexSearcher objects (testExcludeSectionsOneIndex).
* Try the boolean MUST clause rather than MUST_NOT using the WildcardQuery i.e. only include results from the /news/ section
   and its children."
1,"Multiple namespace definitions in CND prevent definition of node type without child nodes. The BNF in http://jackrabbit.apache.org/api-1/org/apache/jackrabbit/core/nodetype/compact/CompactNodeTypeDefReader.html
defines:

[...]
cnd ::= {ns_mapping | node_type_def}
[...]

so multiple namespace definitions should not affect the node type definitions.

However, the following CND definition will fail:

<namespace= 'http://www.mynamespace.co.uk/namespace'>
<nt = 'http://www.jcp.org/jcr/nt/1.0'>
[namespace:document] > nt:file
   - namespace:name (string) mandatory

<namespace= 'http://www.mynamespace.co.uk/namespace'>
<nt = 'http://www.jcp.org/jcr/nt/1.0'>
[namespace:document2] > nt:file
   - namespace:name (string) mandatory


Remove the second set of namespace definitions, and all's well:

<namespace= 'http://www.mynamespace.co.uk/namespace'>
<nt = 'http://www.jcp.org/jcr/nt/1.0'>
[namespace:document] > nt:file
   - namespace:name (string) mandatory

[namespace:document2] > nt:file
   - namespace:name (string) mandatory"
1,"Search with Filter does not work!. See attached JUnitTest, self-explanatory


"
1,"Invalid Journal Record appearing when read during sync operation. ERROR: Error while processing revision 3161: Unknown entry type: 
(a) (2007-05-13 19:57:02,258 main_org.apache.jackrabbit.core.cluster.ClusterNode)
ERROR: Unable to start clustered node, forcing shutdown... (2007-05-13 19:57:02,259 main_org.apache.jackrabbit.core.RepositoryImpl)
org.apache.jackrabbit.core.cluster.ClusterException: Unable to read record with revision: 3161
        at org.apache.jackrabbit.core.cluster.ClusterNode.sync(ClusterNode.java:285)
        at org.apache.jackrabbit.core.cluster.ClusterNode.start(ClusterNode.java:229)
        at org.apache.jackrabbit.core.RepositoryImpl.<init>(RepositoryImpl.java:308)
        at org.apache.jackrabbit.core.RepositoryImpl.create(RepositoryImpl.java:584)
        at org.sakaiproject.jcr.jackrabbit.RepositoryBuilder.init(RepositoryBuilder.java:213)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:585)


This is a 2 node cluster, with persistance managers in the DB and journal on the shared filesystem.

Start the first node in the cluster up from a completely clean and empty repo.

Let it add some note types, and create a since workspace (called sakai) and then connect via webdav (using OSX Finder) which creates some Journal Records (due to the finder putting some .xxxx files in)

Dont add any files.

Then start the second node in the cluster up,

It runs through the first 15 or so journal entries and then hits a one where the entry is unknown (stack trace above)

Some analysis to follow
"
1,"NPE in TestNRTThreads. I hit this when while(1)ing this test... I think it's because the logic on when we ask the SegmentReader to load stored fields is off...

{noformat}
*** Thread: Lucene Merge Thread #1 ***
org.apache.lucene.index.MergePolicy$MergeException:
java.lang.NullPointerException
       at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:507)
       at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:472)
Caused by: java.lang.NullPointerException
       at org.apache.lucene.index.SegmentReader$FieldsReaderLocal.initialValue(SegmentReader.java:245)
       at org.apache.lucene.index.SegmentReader$FieldsReaderLocal.initialValue(SegmentReader.java:242)
       at org.apache.lucene.util.CloseableThreadLocal.get(CloseableThreadLocal.java:68)
       at org.apache.lucene.index.SegmentReader.getFieldsReader(SegmentReader.java:749)
       at org.apache.lucene.index.SegmentReader.document(SegmentReader.java:838)
       at org.apache.lucene.index.IndexReader.document(IndexReader.java:951)
       at org.apache.lucene.index.TestNRTThreads$1.warm(TestNRTThreads.java:86)
       at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3311)
       at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2875)
       at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:379)
       at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:447)
{noformat}
"
1,"Group#getMembers may list inherited members multiple times. i just happen to detect the following regression that seems to be introduces quite a while ago:

Group#getMembers is defined to return all group members including those inherited by another group being member of that group.

Example:
User t
Group a : t is declared member
Group b : t is declared member
Group c : a, b are declared members

The expected result of Group.getMembers was: a, b and t.

What is currently happening is that t is included twice in the returned iterator.
Quickly testing on jackrabbit 2.0 revealed that this used to work before...

I didn't carefully check when that bug has been introduced but the the refactoring of the membership
collections seems to be a possible culprit.

"
1,"Concurrent locking operations fail. I prepared simple test which tries to lock/unlock single node from many
threads. I expected only LockExceptions thrown by some threads which can
occur if node is already locked by other thread.

But I get incorrect effect sporadically. It looks like some thread
managed to acquire lock, but then can't release it.

Following exception is thrown then :

javax.jcr.InvalidItemStateException:
7c198c7b-76c8-47c8-96a8-d9dfefd4b387 has been modified externally
    at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1193)
    at org.apache.jackrabbit.core.NodeImpl.unlock(NodeImpl.java:3790)
    at JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:95)

additionally warning appears in log

org.apache.jackrabbit.core.lock.LockManagerImpl$LockInfo.loggingOut(LockManagerImpl.java:892)
- Unable to unlock session-scoped lock on node
'7c198c7b-76c8-47c8-96a8-d9dfefd4b387-W': Unable to unlock node. Node
has pending changes: /folder

In consequence node is left in locked state. It looks like a bug.
If one thread locked node successfully, then none other can modify it,
and the same thread should release lock without any problems.

Shouldn't be lock operation atomic itself ?

Przemo


package com.oyster.mom.contentserver.jcr.transaction;

import javax.jcr.Node;
import javax.jcr.RepositoryException;
import javax.jcr.Session;
import javax.jcr.SimpleCredentials;
import javax.jcr.lock.LockException;

import org.apache.jackrabbit.core.RepositoryImpl;
import org.apache.jackrabbit.core.config.RepositoryConfig;

public class JrTestConcurrentLocks extends Thread {

   private static final org.apache.commons.logging.Log log = org.apache.commons.logging.LogFactory.getLog(JrTestConcurrentLocks.class);

   public static String REPOSITORY_HOME = ""d:/repo/jackrabbit/"";

   public static String REPOSITORY_CONFIG = REPOSITORY_HOME + ""repository.xml"";

   public static void main(String[] args) throws Exception {

       JrTestConcurrentLocks test = new JrTestConcurrentLocks(-1);
       test.startup();

       JrTestConcurrentLocks tests[] = new JrTestConcurrentLocks[3];

       for (int i = 0; i < tests.length; i++) {
           JrTestConcurrentLocks x = new JrTestConcurrentLocks(i);
           x.setSession(repository.login(new SimpleCredentials(""admin"", ""admin"".toCharArray())));
           x.start();
           tests[i] = x;
       }

       for (int i = 0; i < tests.length; i++) {
           tests[i].join();
           tests[i].getSession().logout();
       }

       test.shutdown();
   }

   private static RepositoryImpl repository;

   private int id;

   private Session session;

   public void setSession(Session session) {
       this.session = session;
   }

   public Session getSession() {
       return this.session;
   }

   public JrTestConcurrentLocks(int i) {
       this.id = i;
   }

   public void startup() throws Exception {
       System.setProperty(""java.security.auth.login.config"", ""c:/jaas.config"");

       RepositoryConfig config = RepositoryConfig.create(REPOSITORY_CONFIG, REPOSITORY_HOME);
       repository = RepositoryImpl.create(config);

       Session session = repository.login(new SimpleCredentials(""admin"", ""admin"".toCharArray()));
       Node rootNode = session.getRootNode();
       if (!rootNode.hasNode(""folder"")) {
           Node folder = rootNode.addNode(""folder"");
           folder.addMixin(""mix:versionable"");
           folder.addMixin(""mix:lockable"");
           rootNode.save();
       }
       session.logout();
   }

   public void shutdown() throws RepositoryException {
       repository.shutdown();
   }

   public Node getFolder(Session session) throws RepositoryException {
       return session.getRootNode().getNode(""folder"");
   }

   public void run() {

       for (int i = 0; i < 10; i++) {
           log.info(""START id:"" + id + "", i="" + i);

           try {
               session.refresh(false);

               Node folder = getFolder(session);
               folder.lock(false, true);
               folder.unlock();

               log.info(""SUCCESS id:"" + id + "", i="" + i);
           }
           catch (LockException e) {
               log.info(""FAIL:"" + id + "", i="" + i);
           }
           catch (Exception e) {
               log.warn(""ERROR:"" + id + "", i="" + i, e);
           }


       }

   }
}


15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=0
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=0
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=0
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=0
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=1
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=1
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=0
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=1
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=1
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=2
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=2
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=3
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=3
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=4
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=2
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=2
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=3
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=3
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=4
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=4
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=5
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:97) - SUCCESS id:1, i=4
15:46:17 WARN  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:103) - ERROR:0, i=5
javax.jcr.ItemNotFoundException: 7c198c7b-76c8-47c8-96a8-d9dfefd4b387/{http://www.jcp.org/jcr/1.0}lockOwner
       at org.apache.jackrabbit.core.ItemManager.createItemInstance(ItemManager.java:463)
       at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:319)
       at org.apache.jackrabbit.core.NodeImpl.getProperty(NodeImpl.java:1436)
       at org.apache.jackrabbit.core.NodeImpl.getOrCreateProperty(NodeImpl.java:428)
       at org.apache.jackrabbit.core.NodeImpl.internalSetProperty(NodeImpl.java:1267)
       at org.apache.jackrabbit.core.NodeImpl.lock(NodeImpl.java:3740)
       at JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:94)
15:46:17 WARN  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:103) - ERROR:2, i=0
javax.jcr.InvalidItemStateException: 7c198c7b-76c8-47c8-96a8-d9dfefd4b387 has been modified externally
       at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1193)
       at org.apache.jackrabbit.core.NodeImpl.unlock(NodeImpl.java:3790)
       at JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:95)
15:46:17 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=5
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=6
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=1
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=5
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=6
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=1
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=6
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=7
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=2
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=6
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=7
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=2
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=7
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=8
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=3
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=7
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=8
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=3
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=8
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:0, i=9
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=4
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:1, i=8
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:0, i=9
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=4
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:1, i=9
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=5
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=5
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=6
15:46:18 WARN  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:103) - ERROR:1, i=9
javax.jcr.InvalidItemStateException: /folder: the node cannot be saved because it has been modified externally.
       at org.apache.jackrabbit.core.NodeImpl.makePersistent(NodeImpl.java:908)
       at org.apache.jackrabbit.core.ItemImpl.persistTransientItems(ItemImpl.java:682)
       at org.apache.jackrabbit.core.ItemImpl.save(ItemImpl.java:1173)
       at org.apache.jackrabbit.core.NodeImpl.lock(NodeImpl.java:3744)
       at JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:94)
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=6
15:46:18 WARN  org.apache.jackrabbit.core.lock.LockManagerImpl$LockInfo.loggingOut(LockManagerImpl.java:892) - Unable to unlock session-scoped lock on node '7c198c7b-76c8-47c8-96a8-d9dfefd4b387-W': Unable to unlock node. Node has pending changes: /folder
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=7
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=7
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=8
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=8
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:88) - START id:2, i=9
15:46:18 INFO  JrTestConcurrentLocks.run(JrTestConcurrentLocks.java:100) - FAIL:2, i=9

"
1,"FilterIndexReader does not override all of IndexReader methods. FilterIndexReader does not override all of IndexReader methods. We've hit an error in LUCENE-3573 (and fixed it). So I thought to write a simple test which asserts that FIR overrides all methods of IR (and we can filter our methods that we don't think that it should override). The test is very simple (attached), and it currently fails over these methods:
{code}
getRefCount
incRef
tryIncRef
decRef
reopen
reopen
reopen
reopen
clone
numDeletedDocs
document
setNorm
setNorm
termPositions
deleteDocument
deleteDocuments
undeleteAll
getIndexCommit
getUniqueTermCount
getTermInfosIndexDivisor
{code}

I didn't yet fix anything in FIR -- if you spot a method that you think we should not override and delegate, please comment."
1,"Importing strings with special characters fails. Both Session.importXML and Workspace.importXML don't work correctly in some cases.

Importing very large foreign language (for example, Chinese) text property values could result in incorrect values on some platforms. The reason is, BufferedStringValue (buffers very large string to a temporary file) uses the platform default encoding to read and write the text.

BufferedStringValue is relatively slow on some systems when importing large texts or binary data because of using FD().sync().

If an exported string value contains a carriage return (\r), this character was truncated on some platforms.

If an exported string value contains a characters with code below 32 excluding newline (\n) and tab (\t) - for example form feed (\f) - the imported string value was base64 encoded.
"
1,"cache module should populate Via header to capture upstream and downstream protocols. Because the cache module is currently implemented as a decorator that behaves like a transparent caching proxy, we need it to correctly populate the Via header so that we can preserve the record of which protocol versions were used upstream and downstream from the caching module.

This is a MUST per the RFC:
http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.45"
1,"HttpMultipart doesn't generate Content-Type part header in mode BROWSER_COMPATIBLE. Browsers (tested with Firefox 3.6 and IE6) send a Content-Type header for file parts, what org.apache.http.entity.mime.HttpMultipart doesn't do in BROWSER_COMPATIBLE mode.


Example:

-----------------------------142889018617181602061216500409

Content-Disposition: form-data; name=""myFileFieldName2""; filename=""webtest.png""

Content-Type: image/png


In HtmlUnit we wil subclass HttpEntity and MultipartEntity to fix this problem."
1,"Overlapping index aggregates not updated. When there are overlapping index aggregates in an indexing configuration and an item is updated that belongs to multiple aggregates, then only the first aggregate root is re-indexed."
1,"HttpURL creates wrong authority String when user info is changed. When changing the user info on an existing HttpURL which has additional port
information, the new authority String contains a wrong hostname part: instead of
getting ""hostname:portnumber"" the string is ""hostnameportnumber"", i.e. the "":""
is missing.
Methods which needs to be changed are:

setRawPassword(...)
setRawUser(...)
setRawUserinfo(...)

(look for the line
String hostport = (_port == -1) ? hostname : hostname + _port;
)

Andreas FÃ¤nger
ESIGN Software GmbH"
1,SynFilter doesn't set offsets for outputs that hang off the end of the input tokens. If you have syn rule a -> x y and input a then output is a/x y but... what should y's offsets be?  Right now we set to 0/0.
1,"Upgrade to commons-compress 1.2. Commons Compress bug COMPRESS-127 was fixed in 1.2, so the workaround in benchmark's StreamUtils is no longer required. Compress is also used in solr. Replace with new jar in both benchmark and solr and get rid of that workaround."
1,"Locking bug. In org.apache.lucene.store.Lock, line 57 (lucene_1_4_final branch):

if (++sleepCount == maxSleepCount)

is incorrect, the sleepCount is incremented before the compare causing it
throwing the exception with out waiting for at least 1 interation.

Should be changed instead to:
if (sleepCount++ == maxSleepCount)

As this is a self-contained simple fix, I am not submitting a patch.

Thanks

-John"
1,"DisjunctionSumScorer gives slightly (float iotas) different scores when you .nextDoc vs .advance. Spinoff from LUCENE-1536.

I dug into why we hit a score diff when using luceneutil to benchmark
the patch.

At first I thought it was BS1/BS2 difference, but because of a bug in
the patch it was still using BS2 (but should be BS1) -- Robert's last
patch fixes that.

But it's actually a diff in BS2 itself, whether you next or advance
through the docs.

It's because DisjunctionSumScorer, when summing the float scores for a
given doc that matches multiple sub-scorers, might sum in a different
order, when you had .nextDoc'd to that doc than when you had .advance'd
to it.

This in turn is because the PQ used by that scorer (ScorerDocQueue)
makes no effort to break ties.  So, when the top N scorers are on the
same doc, the PQ doesn't care what order they are in.

Fixing ScorerDocQueue to break ties will likely be a non-trivial perf
hit, though, so I'm not sure whether we should do anything here..."
1,"LuceneDictionary skips first word in enumeration. The current code for LuceneDictionary will always skip the first word of the TermEnum. The reason is that it doesn't initially retrieve TermEnum.term - its first call is to TermEnum.next, which moves it past the first term (line 76).
To see this problem cause a failure, add this test to TestSpellChecker:
similar = spellChecker.suggestSimilar(""eihgt"",2);
      assertEquals(1, similar.length);
      assertEquals(similar[0], ""eight"");

Because ""eight"" is the first word in the index, it will fail.
"
1,"Exception shouldn't be thrown for unsupported authentication method. Currently, Authenticator will throw an UnsupportedOperationException for 
unsupported authentication method (like NTLM). This is correct. However,  
HttpMethodBase.execute only catches HttpException, so this 
UnsupportedOperationException is leaked to the user. This is undesirable, 
because user may want a chance to handle such such authentication themself. The 
correct way is to pass the http status code to the user, just like how it 
treats redirect to a different host.

The simple fix is to catch all exceptions in HttpMethodBase.execute when 
calling Authenticator.authenticate."
1,MSSql and MySQL bunlde PM schemas missing definition for name index. the mssql and mysql ddl files of the respective bundle persistence managers are missing the definitions for the name index.
1,"HttpState cannot differentiate credentials for different hosts with same Realm names. It seems that one needs a separate HttpState per client per host: from the 
javadocs, if (by coincidence or by design) more than one host uses the same 
realm name, such as ""Private"", then there's an unresolvable conflict, as 
HttpState can only store one set of credentials for a given name...

According to Oleg Kalnichevski, it is plausible just to extend the HttpState 
class with additional methods that would require host to be specified along the
authentication realm when dealing with credentials.

See postings on ""Commons HttpClient Project"" mailing list for more info (dated 
21/03/2003)."
1,"OCM:The UUID of the collection elements changes on update.. On ocm.update transaction, the  Current implementation of DefaultCollectionConverterImpl recreates the colleciton-element nodes if there is no id field specificaiton.  This is completely valid for majority of the cases.  But I came across a case where the colleciton element has a uuid field.  In this case also what is happening with the current implementation is that it drops all the elements from the old collection-elements and recreates the new ones.  The major flip side is that now I am left with brand new UUIDs.  I think we should address the uniqueness characteristics specified through UUID also while mapping colleciton elements.

I have a patch and a TestCase to verify the same.  I have implemented it only for the digester.  If people feel the approach is right I will work out an annotation based testcase as well.  I do not think it is going to fail even with annotations.
"
1,"potential infinite loop around InternalVersionImpl.getSuccessors. There's an infinite loop waiting to happen when the underlying persisted version storage is defect:

{noformat}
at
org.apache.jackrabbit.core.version.InternalVersionImpl.getSuccessors(InternalVersionImpl.java:148)

at
org.apache.jackrabbit.core.version.InternalVersionHistoryImpl.init(InternalVersionHistoryImpl.java:165)

at
org.apache.jackrabbit.core.version.InternalVersionHistoryImpl.reload(InternalVersionHistoryImpl.java:180)

at
org.apache.jackrabbit.core.version.InternalVersionHistoryImpl.getVersion(InternalVersionHistoryImpl.java:299) 
{noformat}

(line numbers from 2.2)

What happens here is that when a version can not be instantiated, reload() is called, which in turn calls init(), which, as part of piece of code labeled ""fix legacy"" will call getSuccessors(), which in turn wants to instantiate versions.

"
1,upgrade icu jar to 4.8.1.1 / remove lucenetestcase hack. This bug fix release fixes problems with icu under java7: http://bugs.icu-project.org/trac/ticket/8734
1,"OCM: translate-project goal not found. The jackrabbit-ocm POM doesn't specify the required version of the Retrotranslator plugin it uses. In some cases this causes the build to use an older version of the plugin that doesn't come with the translate-plugin goal.

The goal is included in the latest version (1.0-alpha-4) of the plugin."
1,"SpellChecker not working because of stale IndexSearcher. The SpellChecker unit test did not work, because of a stale IndexReader and IndexSearcher instance after calling indexDictionary(Dictionary)."
1,"Cookie with domain .mydomain.com not sent to host mydomain.com. A cookie with for example 
  .mydomain.com 
as domain property is not sent to the host
  mydomain.com
(without www. or anything else before ""mydomain.com"")

This concern all CookieSpec as the relevant code is located in CookieSpecBase:

    public boolean domainMatch(final String host, final String domain) {
        return host.endsWith(domain);
    }

It should be changed for instance to something like:

    public boolean domainMatch(final String host, final String domain) {
        // take care of host ""myDomain.com"" and domain "".myDomain.com""
        return host.endsWith(domain)
	|| _host.equals(_domain.substring(1));
    }"
1,"spi2dav: Observation's user data not property handled. org.apache.jackrabbit.test.api.observation#GetUserDataTest still fail in the setup jcr2spi - spi2dav(ex) - jcr-server.

"
1,"Some small fixes after the flex merge.... Changes:

  * Re-introduced specialization optimization to FieldCacheRangeQuery;
    also fixed bug (was failing to check deletions in advance)

  * Changes 2 checkIndex methods from protected -> public

  * Add some missing null checks when calling MultiFields.getFields or
    IndexReader.fields()

  * Tweak'd CHANGES a bit

  * Removed some small dead code
"
1,"Avoid exceptions during shutting repository down if several PMs/FSs use same DB. According to docs and forum discussions, it's legal to use same DB for different FileSystems/Persistence Managers. Such configurations seem to work fine, but when repository is stopped, exceptions are produced like following:

SEVERE: Error while closing Version Manager.
java.sql.SQLNonTransientConnectionException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.getMetaData(Unknown Source)
	at org.apache.jackrabbit.core.persistence.db.DerbyPersistenceManager.closeConnection(DerbyPersistenceManager.java:109)
	at org.apache.jackrabbit.core.persistence.db.DatabasePersistenceManager.close(DatabasePersistenceManager.java:261)
	at org.apache.jackrabbit.core.version.VersionManagerImpl.close(VersionManagerImpl.java:201)
	at org.apache.jackrabbit.core.RepositoryImpl.doShutdown(RepositoryImpl.java:1000)
	at org.apache.jackrabbit.core.RepositoryImpl.shutdown(RepositoryImpl.java:948)
	at org.apache.jackrabbit.core.TransientRepository.stopRepository(TransientRepository.java:275)
	at org.apache.jackrabbit.core.TransientRepository.loggedOut(TransientRepository.java:427)
	at org.apache.jackrabbit.core.SessionImpl.notifyLoggedOut(SessionImpl.java:574)
	at org.apache.jackrabbit.core.SessionImpl.logout(SessionImpl.java:1247)
	at org.apache.jackrabbit.core.XASessionImpl.logout(XASessionImpl.java:403)
	at com.blandware.tooling.jcrplugin.ExportMojo.execute(ExportMojo.java:81)
	at org.apache.maven.plugin.DefaultPluginManager.executeMojo(DefaultPluginManager.java:447)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoals(DefaultLifecycleExecutor.java:539)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeStandaloneGoal(DefaultLifecycleExecutor.java:493)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoal(DefaultLifecycleExecutor.java:463)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoalAndHandleFailures(DefaultLifecycleExecutor.java:311)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeTaskSegments(DefaultLifecycleExecutor.java:278)
	at org.apache.maven.lifecycle.DefaultLifecycleExecutor.execute(DefaultLifecycleExecutor.java:143)
	at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:333)
	at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:126)
	at org.apache.maven.cli.MavenCli.main(MavenCli.java:282)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.classworlds.Launcher.launchEnhanced(Launcher.java:315)
	at org.codehaus.classworlds.Launcher.launch(Launcher.java:255)
	at org.codehaus.classworlds.Launcher.mainWithExitCode(Launcher.java:430)
	at org.codehaus.classworlds.Launcher.main(Launcher.java:375)
Caused by: java.sql.SQLException: No current connection.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 35 more
"
1,FastVectorHighlighter: out of alignment when the first value is empty in multiValued field. 
1,"Surround Query doesn't properly handle equals/hashcode. In looking at using the surround queries with Solr, I am hitting issues caused by collisions due to equals/hashcode not being implemented on the anonymous inner classes that are created by things like DistanceQuery (branch 3.x, near line 76)"
1,FreqProxTermsWriter leaks file handles if exceptions are thrown during flush(). FreqProxTermsWriter leaks open file handles if exceptions are thrown during flush. Code needs to be protected by try-finally clauses.
1,"repository lock file not removed without a clean shutdown. actually when a repository is loaded a "".lock"" file is created. This file is removed only after a clean shutdown but, if a jackrabbit instance has been killed, you have to manually delete the file in order to load the repository again, also if there was no live instance of jackrabbit that was using it.

The problem comes from the fact that the simple presence of the .lock file is used to indicate a live instance.
I suggest replacing this behavior using this method (used for example by eclipse when opening workspaces):
- when an instance is loaded create a "".lock"" file and open it with exclusive access
- when a new instance is started try to delete an eventual .lock file. Only if the file can't be deleted because in use assume that another jackrabbit instance is running.
"
1,"InstantiatedIndexReader throws NullPointerException in norms() when used with a MultiReader. 
When using InstantiatedIndexReader under a MultiReader where the other Reader contains documents, a NullPointerException is thrown here;

 public void norms(String field, byte[] bytes, int offset) throws IOException {
    byte[] norms = getIndex().getNormsByFieldNameAndDocumentNumber().get(field);
    System.arraycopy(norms, 0, bytes, offset, norms.length);
  }

the 'norms' variable is null. Performing the copy only when norms is not null does work, though I'm sure it's not the right fix.

java.lang.NullPointerException
	at org.apache.lucene.store.instantiated.InstantiatedIndexReader.norms(InstantiatedIndexReader.java:297)
	at org.apache.lucene.index.MultiReader.norms(MultiReader.java:273)
	at org.apache.lucene.search.TermQuery$TermWeight.scorer(TermQuery.java:70)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:131)
	at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:112)
	at org.apache.lucene.search.Searcher.search(Searcher.java:136)
	at org.apache.lucene.search.Searcher.search(Searcher.java:146)
	at org.apache.lucene.store.instantiated.TestWithMultiReader.test(TestWithMultiReader.java:41)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:164)
	at junit.framework.TestCase.runBare(TestCase.java:130)
	at junit.framework.TestResult$1.protect(TestResult.java:106)
	at junit.framework.TestResult.runProtected(TestResult.java:124)
	at junit.framework.TestResult.run(TestResult.java:109)
	at junit.framework.TestCase.run(TestCase.java:120)
	at junit.framework.TestSuite.runTest(TestSuite.java:230)
	at junit.framework.TestSuite.run(TestSuite.java:225)
	at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)

"
1,"TestDocValuesIndexing.testAddIndexes failures on docvalues branch. doc values branch r1124825, reproducible 
{code}
    [junit] Testsuite: org.apache.lucene.index.values.TestDocValuesIndexing
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.716 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestDocValuesIndexing -Dtestmethod=testAddIndexes -Dtests.seed=5939035003978436534:-6429764582682717131
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=MockRandom, BYTES_VAR_DEREF=MockRandom, INTS=Pulsing(freqCutoff=13)}, locale=da_DK, timezone=Asia/Macao
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDocValuesIndexing]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=88582432,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAddIndexes(org.apache.lucene.index.values.TestDocValuesIndexing):     Caused an ERROR
    [junit] null
    [junit] java.nio.channels.ClosedChannelException
    [junit]     at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:88)
    [junit]     at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:603)
    [junit]     at org.apache.lucene.store.NIOFSDirectory$NIOFSIndexInput.readInternal(NIOFSDirectory.java:161)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:222)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)
    [junit]     at org.apache.lucene.store.DataInput.readInt(DataInput.java:73)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readInt(BufferedIndexInput.java:162)
    [junit]     at org.apache.lucene.store.DataInput.readLong(DataInput.java:115)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readLong(BufferedIndexInput.java:175)
    [junit]     at org.apache.lucene.store.MockIndexInputWrapper.readLong(MockIndexInputWrapper.java:136)
    [junit]     at org.apache.lucene.index.values.PackedIntsImpl$IntsEnumImpl.<init>(PackedIntsImpl.java:263)
    [junit]     at org.apache.lucene.index.values.PackedIntsImpl$IntsEnumImpl.<init>(PackedIntsImpl.java:249)
    [junit]     at org.apache.lucene.index.values.PackedIntsImpl$IntsReader.getEnum(PackedIntsImpl.java:239)
    [junit]     at org.apache.lucene.index.values.DocValues.getEnum(DocValues.java:54)
    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.getValuesEnum(TestDocValuesIndexing.java:484)
    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.testAddIndexes(TestDocValuesIndexing.java:202)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1304)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1233)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.values.TestDocValuesIndexing FAILED
{code}

and

{code}

    [junit] Testsuite: org.apache.lucene.index.values.TestDocValuesIndexing
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.94 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestDocValuesIndexing -Dtestmethod=testAddIndexes -Dtests.seed=-3677966427932339626:-4746638811786223564
    [junit] NOTE: test params are: codec=RandomCodecProvider: {id=Standard, BYTES_FIXED_DEREF=MockSep, FLOAT_64=SimpleText}, locale=ca, timezone=Asia/Novosibirsk
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDocValuesIndexing]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=88596152,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testAddIndexes(org.apache.lucene.index.values.TestDocValuesIndexing):     Caused an ERROR
    [junit] Bad file descriptor
    [junit] java.io.IOException: Bad file descriptor
    [junit]     at java.io.RandomAccessFile.seek(Native Method)
    [junit]     at org.apache.lucene.store.SimpleFSDirectory$SimpleFSIndexInput.readInternal(SimpleFSDirectory.java:101)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.refill(BufferedIndexInput.java:222)
    [junit]     at org.apache.lucene.store.BufferedIndexInput.readByte(BufferedIndexInput.java:39)
    [junit]     at org.apache.lucene.store.MockIndexInputWrapper.readByte(MockIndexInputWrapper.java:105)
    [junit]     at org.apache.lucene.index.values.Floats$FloatsReader.load(Floats.java:281)
    [junit]     at org.apache.lucene.index.values.SourceCache$DirectSourceCache.load(SourceCache.java:101)
    [junit]     at org.apache.lucene.index.values.DocValues.getSource(DocValues.java:101)
    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.getSource(TestDocValuesIndexing.java:472)
    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.getValuesEnum(TestDocValuesIndexing.java:482)
    [junit]     at org.apache.lucene.index.values.TestDocValuesIndexing.testAddIndexes(TestDocValuesIndexing.java:203)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1304)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1233)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.values.TestDocValuesIndexing FAILED
{code}"
1,"Lucene RAM Directory doesn't work for Index Size > 8 GB. from user list - http://www.gossamer-threads.com/lists/lucene/java-user/50982

Problem seems to be casting issues in RAMInputStream.

Line 90:
      bufferStart = BUFFER_SIZE * currentBufferIndex;
both rhs are ints while lhs is long.
so a very large product would first overflow MAX_INT, become negative, and only then (auto) casted to long, but this is too late. 

Line 91: 
     bufferLength = (int) (length - bufferStart);
both rhs are longs while lhs is int.
so the (int) cast result may turn negative and the logic that follows would be wrong.
"
1,"Wildcard query with no wildcard characters in the term throws StringIndexOutOfBounds exception. 
Query q1 = new WildcardQuery(new Term(""Text"", ""a""));
Hits hits = searcher.search(q1);


Caught Exception
java.lang.StringIndexOutOfBoundsException : String index out of range: -1
    at java.lang.String.substring(Unknown Source)
    at org.apache.lucene.search.WildcardTermEnum.<init>(WildcardTermEnum.java:65)
    at org.apache.lucene.search.WildcardQuery.getEnum (WildcardQuery.java:38)
    at org.apache.lucene.search.MultiTermQuery.rewrite(MultiTermQuery.java:54)
    at org.apache.lucene.search.IndexSearcher.rewrite(IndexSearcher.java:137)
    at org.apache.lucene.search.Query.weight (Query.java:92)
    at org.apache.lucene.search.Hits.<init>(Hits.java:41)
    at org.apache.lucene.search.Searcher.search(Searcher.java:44)
    at org.apache.lucene.search.Searcher.search(Searcher.java:36)
    at QuickTest.main(QuickTest.java:45)


From Erik Hatcher

Feel free to log this as a bug report in our JIRA issue tracker.  It
seems like a reasonable change to make, such that a WildcardQuery
without a wildcard character would behave like TermQuery."
1,"DefaultClientRequestDirector doesn't release connections back to ClientConnectionManager on exceptions. See HTTPCLIENT-747 for more info.  Basically the deal is that an entry is always allocated, but currently it's only released if execute(..) completes normally."
1,"After IW.addIndexesNoOptimize, IW.close may hang. Spinoff from here:

  http://mail-archives.apache.org/mod_mbox/lucene-java-user/200804.mbox/%3c43128.192.168.1.71.1208561409.webmail@192.168.1.71%3e

The addIndexesNoOptimize method first merges eligible segments
according to the MergePolicy, and then copies over one by one any
remaining ""external"" segments.

That copy can possibly (rather rarely) result in new merges becoming
eligible because its size can change if the index being added was
created with autoCommit=false.

However, we fail to then invoke the MergeScheduler to run these
merges.  As a result, in close, where we wait until all running and
pending merges complete, we will never return.

The fix is simple: invoke the merge scheduler inside
copyExternalSegments() if any segments were copied.  I also added
defensive invocation of the merge scheduler during close, just in case
other code paths could allow for a merge to be added to the pending
queue but not scheduled.

"
1,"Session returned does not offers transaction support. The javax.jcr.Session instance returned by the repository is an implementation of org.apache.jackrabbit.jca.JCASessionHandle which doesn't implement the interface org.apache.jackrabbit.api.XASession.
"
1,"weird error when adding a node using an abstract/mixin nodetype. when trying to add a node ""files"" with an abstract nodetype, i.e. nt:base, the following error is reported:

javax.jcr.nodetype.ConstraintViolationException: {}files is abstract  be used as primary node type.

the correct wording could be:

javax.jcr.nodetype.ConstraintViolationException: not allowed to add node {}files: {http://www.jcp.org/jcr/nt/1.0}base is abstract and cannot be used as primary node type.
"
1,"System view export truncates carriage return. If a string contains a carriage return (\r), this character was truncated on some platforms. "
1,"IndexWriter.updateDocument is no longer atomic. Spinoff from LUCENE-847.

Ning caught that as of LUCENE-843, we lost the atomicity of the delete
+ add in IndexWriter.updateDocument.

Ning suggested a simple fix: move the buffered deletes into
DocumentsWriter and let it do the delete + add atomically.  This has a
nice side effect of also consolidating the ""time to flush"" logic in
DocumentsWriter.

"
1,"Moving a node while index is merged leads to inconsistent index. The IndexMerger keeps track of nodes that are deleted from the index and applies that change also to the merged index, but if the same node is added again to the index during the merge process the index becomes inconsistent."
1,"Cloning a tree containing shareable nodes into another workspace throws ItemExistsException. There's a problem when trying to clone a tree in another workspace, when this tree contains shareable nodes.

Let ws1 be one workspace, which contains one node A. This node has two sub-nodes B and C. B and C share a shareable sub-node D :

A 
|   \
B  C
|    |
D  D

Let ws2 be a second workspace. Then calling ws2.clone(""ws1"" , ""/A"" , ""/A"" , false) throws an ItemExistsException ( copyNodeState line 1628 ) . This is done when the copyNodeState is checking if the nodeId is already present in the workspace - which is the case when copying the second instance of the shareable node. I can't find in the specification something about this case - but it would be logical to add a share to the node when coming across this situation - at least in the CLONE ( and probable COPY too ) cases. I don't know what would be expected in the CLONE_REMOVE_EXISTING case - we might not want to remove the node if it's shareable, and also add a share here.

I fixed the issue by handling the case the node is shareable in the COPY and CLONE cases of copyNodeState - you'll find attached the corresponding patch. Do you think this solution is ok ?"
1,"Jackrabbit's lucene based query implementation does not check property constraints on the root node.. XPath queries of the kind ""/jcr:root[<any property constraint>]"" apparently always match."
1,"DictionaryCompoundWordTokenFilter does not properly add tokens from the end compound word.. Due to an off-by-one error, a subword placed at the end of a compound word will not get a token added to the token stream.


For example (from the unit test in the attached patch):
Dictionary: {""ab"", ""cd"", ""ef""}
Input: ""abcdef""
Created tokens: {""abcdef"", ""ab"", ""cd""}
Expected tokens: {""abcdef"", ""ab"", ""cd"", ""ef""}


Additionally, it could produce tokens that were shorter than the minSubwordSize due to another off-by-one error. For example (again, from the attached patch):


Dictionary: {""abc"", ""d"", ""efg""}
Minimum subword length: 2
Input: ""abcdefg""
Created tokens: {""abcdef"", ""abc"", ""d"", ""efg""}
Expected tokens: {""abcdef"", ""abc"", ""efg""}
"
1,"session.move() throws ItemExistsException despite same name siblings. code to reproduce:

            Session session = r.login(new SimpleCredentials(""johndoe"", """".toCharArray()), wspName);
            Node root = session.getRootNode();

            // setup test case
            if (!root.hasNode(""foo"")) {
                root.addNode(""foo"");
                root.save();
            }
            if (!root.hasNode(""bar"")) {
                root.addNode(""bar"");
                root.save();
            }

            session.move(""/foo"", ""/bar"");    // ==> ItemExistsException
"
1,"Query dump failed with deep query tree. With a big query (more than 400 OR operands) the query dump failed.
The query dump is made at QueryImpl.execute (line 136)

It failed because of the constant PADDING at QueryTreeDump.visit(line 85).
The constant PADDING is a 255 character array, but in my program it would need it to be bigger.
I think putting it to 65535 would not be a problem : it would only take a little bit more memory.

This is the top of the stacktract for info:
java.lang.ArrayIndexOutOfBoundsException
	at java.lang.System.arraycopy(Native Method)
	at java.lang.StringBuffer.append(StringBuffer.java:499)
	at org.apache.jackrabbit.core.query.QueryTreeDump.visit(QueryTreeDump.java:85)
	at org.apache.jackrabbit.core.query.OrQueryNode.accept(OrQueryNode.java:50)
	at org.apache.jackrabbit.core.query.QueryTreeDump.traverse(QueryTreeDump.java:263)
                     ...

This is not critical because I can avoid the dump by unactivating debug logs.
"
1,"The creation of a spell index from a LuceneDictionary via SpellChecker.indexDictionary (Dictionary dict) fails starting with 1.9.1 (up to current svn version). Two different errors in 1.9.1/2.0.0 and current svn version.

1.9.1/2.0.0:
at the end of indexDictionary (Dictionary dict) 
the IndexReader-instance reader is closed.
This causes a NullpointerException because reader has not been initialized before (neither in that method nor in the constructor).
Uncommenting this line (reader.close()) seems to resolve that issue.

current svn:
the constructor tries to create an IndexSearcher-instance for the specified path;
as there is no index in that path - it is not created yet -  an exception is thrown.

"
1,"DefaultHttpRequestRetryHandler#retryRequest should not retry aborted requests. DefaultHttpRequestRetryHandler#retryRequest incorrectly retries aborted requests; I have seen the following log messages in JMeter:

org.apache.http.impl.client.DefaultHttpClient: I/O exception (java.net.SocketException) caught when processing request: socket closed
org.apache.http.impl.client.DefaultHttpClient: Retrying request

and

org.apache.http.impl.client.DefaultHttpClient: I/O exception (java.net.BindException) caught when connecting to the target host: Address already in use: connect
org.apache.http.impl.client.DefaultHttpClient: Retrying connect

The abort() method sets the isAborted() flag, but the retry handler does not check it."
1,"NodeTypeRegistry.reregister unregisters dependent types. NodeTypeRegistry.reregister allows modifying a registered node type if the difference to the currently registered node type with the same name is TRIVIAL according to NodeTypeDefDiff.

Before registering the new node type definition the old node type is unregistered. The side effect of that first step is that also all NodeTypes, which depend (extend ?) the node type to be re-registered, are removed from the registry.

After the modified node type is then registered, the previously registered dependent node types will not be registered anymore and will not be known any more.

While it makes sense to me, to temporarily unregister dependent node types, those must be registered again after the re-registered node type has been registered. Otherwise the system may become pretty useless."
1,"DefaultRequestDirector converts redirects of PUT/POST to GET for status codes 301, 302, 307. The DefaultRequestDirector treats redirect requests created by all redirect status codes (HttpStatus.SC_MOVED_TEMPORARILY: , HttpStatus.SC_MOVED_PERMANENTLY, HttpStatus.SC_SEE_OTHER, HttpStatus.SC_TEMPORARY_REDIRECT) the same, converting PUT/POST methods to GET.  The HttpClient Tutorial even documents this as being in accordance with the specification, but I don't believe that's true.

Per the RFC (http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html), conversion of PUT/POST to GET is appropriate only for 303 (See Other).  The others do not suggest this behavior.  In fact, the following notes attached to them call it out as incorrect.

301 (Moved Permanently) has this note:

      Note: When automatically redirecting a POST request after
      receiving a 301 status code, some existing HTTP/1.0 user agents
      will erroneously change it into a GET request.

And 302 (Found) say this:

      Note: RFC 1945 and RFC 2068 specify that the client is not allowed
      to change the method on the redirected request.  However, most
      existing user agent implementations treat 302 as if it were a 303
      response, performing a GET on the Location field-value regardless
      of the original request method. The status codes 303 and 307 have
      been added for servers that wish to make unambiguously clear which
      kind of reaction is expected of the client.

The currently implemented behavior is causing problems with interacting with Central Authentication Service protected resources, among other things."
1,"XMLPersistenceManager trims string property values. The XMLPersistenceManager trims the text of property values read in, so what's returned doesn't match the value set if it included whitespace at the start or end."
1,"NullPointerException when accessing the about.jsp page because of missing /META-INF/NOTICE.TXT. Accessing http://localhost:8080/about.jsp produces:

java.lang.NullPointerException
	at org.apache.jsp.about_jsp.output(org.apache.jsp.about_jsp:39)
	at org.apache.jsp.about_jsp._jspService(org.apache.jsp.about_jsp:103)
	at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:109)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:389)
	at org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:486)
	at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:380)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:363)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.handler.RequestLogHandler.handle(RequestLogHandler.java:49)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:324)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:864)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:533)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:207)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:403)
	at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:522)

This is because the jar misses the following file:
/META-INF/NOTICE.TXT
but there is /NOTICE.TXT and /META-INF/NOTICE

This problem is not reproducible with jackrabbit-webapp-2.0-beta1.war.
"
1,"Incorrect ShingleFilter behavior when outputUnigrams == false. ShingleFilter isn't working as expected when outputUnigrams == false. In particular, it is outputting unigrams at least some of the time when outputUnigrams==false.

I'll attach a patch to ShingleFilterTest.java that adds some test cases that demonstrate the problem.

I haven't checked this, but I hypothesize that the behavior for outputUnigrams == false got changed when the class was upgraded to the new TokenStream API?"
1,"TestIndexWriterExceptions reproducible AOOBE in MockVariableIntBlockCodec. {code}
  [junit] Testsuite: org.apache.lucene.index.TestIndexWriterExceptions
    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.739 sec
    [junit]
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testDocumentsWriterAbort -Dtests.seed=4579947455
682149564:-7960989923752018504
    [junit] NOTE: test params are: codec=RandomCodecProvider: {content=MockVariableIntBlock(baseBlockSize=32)}, locale=bg_BG, timezone=Brazil
/Acre
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestIndexWriterExceptions]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=94363216,total=125632512
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testDocumentsWriterAbort(org.apache.lucene.index.TestIndexWriterExceptions):      Caused an ERROR
    [junit] 66
    [junit] java.lang.ArrayIndexOutOfBoundsException: 66
    [junit]     at org.apache.lucene.index.codecs.mockintblock.MockVariableIntBlockCodec$MockIntFactory$2.add(MockVariableIntBlockCodec.java:
114)
    [junit]     at org.apache.lucene.index.codecs.intblock.VariableIntBlockIndexOutput.close(VariableIntBlockIndexOutput.java:118)
    [junit]     at org.apache.lucene.index.codecs.sep.SepPostingsWriterImpl.close(SepPostingsWriterImpl.java:320)
    [junit]     at org.apache.lucene.index.codecs.BlockTermsWriter.close(BlockTermsWriter.java:137)
    [junit]     at org.apache.lucene.index.PerFieldCodecWrapper$FieldsWriter.close(PerFieldCodecWrapper.java:81)
    [junit]     at org.apache.lucene.index.FreqProxTermsWriter.flush(FreqProxTermsWriter.java:103)
    [junit]     at org.apache.lucene.index.TermsHash.flush(TermsHash.java:118)
    [junit]     at org.apache.lucene.index.DocInverter.flush(DocInverter.java:80)
    [junit]     at org.apache.lucene.index.DocFieldProcessor.flush(DocFieldProcessor.java:75)
    [junit]     at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:457)
    [junit]     at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:417)
    [junit]     at org.apache.lucene.index.DocumentsWriter.postUpdate(DocumentsWriter.java:309)
    [junit]     at org.apache.lucene.index.DocumentsWriter.updateDocument(DocumentsWriter.java:381)
    [junit]     at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1469)
    [junit]     at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1229)
    [junit]     at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1210)
    [junit]     at org.apache.lucene.index.TestIndexWriterExceptions.testDocumentsWriterAbort(TestIndexWriterExceptions.java:555)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1333)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1251)
    [junit]
    [junit]
    [junit] Test org.apache.lucene.index.TestIndexWriterExceptions FAILED
{code}

trunk: r1127871"
1,"Using transactions leads to memory leak. There is global static map in XASessionImpl class which stores all Xids and TransactionContexts

    /**
     * Global transactions
     */
    private static final Map txGlobal = new HashMap();

It looks like this map is never cleared, even after end of transaction. It leads to memory leak because TransactionContexts and all nested objects (including XASessionImpl) are still referenced and couldn't be freed.

Proposed solution : Is it posssible to add just single line which will remove TransactionContext from static map at the end of transaction ?

      if (flags == TMSUCCESS || flags == TMFAIL) {
            associate(null);
-->       txGlobal.remove(xid);
        } else  if (flags == TMSUSPEND) {
            associate(null);
        } else {
            throw new XAException(XAException.XAER_INVAL);
        }

If this is not acceptable, then we have to unreference TransactionContext in another way."
1,Deadlock on concurrent commit/checkin operations. Running concurrently jackrabbit transactions including checkin operations leads to deadlock.
1,"Lock.getNode() does not return lock holder. spec: "" N.getLock().getNode() (where N is a locked node) ... If N is in the subtree of the lock holder, H, then this call will return H.""

now N is returned."
1,"IndexReader.clone can leave files open. I hit this in working on LUCENE-1516.

When not using compound file format, if you clone an IndexReader, then close the original, then close the clone, the stored fields files (_X.fdt, _X.fdx) remain incorrectly open.

I have a test showing it; fix is trivial.  Will post patch & commit shortly."
1,"Benchmark's ExtractReuters creates its temp dir wrongly if provided out-dir param ends by slash. See LUCENE-929 for context.
As result, it might fail to create the temp dir at all."
1,"addIndexesNoOptimize intermittantly throws incorrect ""segment exists in external directory..."" exception. Spinoff from here:

    http://mail-archives.apache.org/mod_mbox/lucene-java-user/200809.mbox/%3Cba72f77f0809111418l29cf215dnd45bf679832d7d42%40mail.gmail.com%3E

Here's my response on that thread:

The bug only happens when you call addIndexesNoOptimize, and one
simple workaround would be to use SerialMergeScheduler.

I think this is already fixed in trunk (soonish to be 2.4) as a side
effect of https://issues.apache.org/jira/browse/LUCENE-1335.

In 2.3, merges that involve external segments (which are segments
folded in by addIndexesNoOptimize) are not supposed to run in a BG
thread.  This is to prevent addIndexesNoOptimize from returning until
after all external segments have been carried over (merged or copied)
into the index, so that if there is an exception (eg disk full),
addIndexesNoOptimize is able to rollback to the index to the starting
point.

The primary merge() method of CMS indeed does not BG any external
merges, but the bug is that when a BG merge finishes it then selects a
new merge to kick off and that selection is happy to pick an external
segment."
1,"Query for all node fails after restart. The query handler initially indexes the node type definitions exposed at /jcr:system/jcr:nodetypes. However after a restart or a node type registration the UUIDs of those nodes will change because they consist of VirtualNodeStates. The index will still use the UUIDs of the initial indexing and will return a query result that refers to UUIDs that do not exist in the workspace anymore.

As an short term solution we should disable indexing of VirtualNodeStates.

Please note, this does not only apply to xpath queries but also sql queries of course."
1,"sometimes if a BG merge hits an exception, optimize() will fail to forward the exception. I was seeing an intermittant failure, only on a Windows instance running inside VMWare, of TestIndexWriter.testAddIndexOnDiskFull.

It is happening because the while loop that checks for merge exceptions that had occurred during optimize fails to catch the case where all the BG optimize merges completed (or hit exceptions) before the while loop begins.  IE, all BG threads finished before the FG thread advanced to the while loop.  In that case the code fails to check if there were any exceptions.

The fix is straightforward: change the while loop so that it always checks, at least once, whether there were exceptions."
1,"TopFieldCollector throws AIOOBE if numHits is 0. See solr-user thread ""ArrayIndexOutOfBoundsException for query with rows=0 and sort param"".

I think we should just create a null collector (only tallies up totalHits) if numHits is 0?"
1,"jcr:successors property not persisted correctly within a transaction. During a transaction, if you create a new version then read the version history the ""jcr:successors"" property is not updated. Note that ""jcr:predecessors"" is updated properly.

Also, the version history is sometimes not propertly read. During the transaction, it might appear empty. This behavior in not consistent from one execution to another.

After a restart of the repository, the version history and the ""jcr:successors"" property is read properly.

* Tests cases will follow shortly.

Thanks, 

Nicolas"
1,"Creating AccessControlEntryImpl from a base entry results in wrong restrictions. during creation of a new AccessControlEntryImpl using a base entry the restrictions of the base entry are
not copied to the new instance."
1,"JNDIDatabaseJournal doesn't work with ""oracle"" schema (or: unable to use OracleDatabaseJournal with a jndi datasource). Database journal works fine on oracle when using the OracleDatabaseJournal implementation; but when you need to use a jndi datasource you actually need to use org.apache.jackrabbit.core.journal.JNDIDatabaseJournal which doesn't work fine with the ""oracle"" schema.

With the following configuration:
<Cluster id=""node1"" syncDelay=""10"">
    <Journal class=""org.apache.jackrabbit.core.journal.JNDIDatabaseJournal"">
      <param name=""schema"" value=""oracle"" />

jackrabbit crashes at startup with a not well defined sql error. Investigating on the problem I see that the ""oracle.ddl"" file contains a ""tablespace"" variable that is replaced only by the OracleDatabaseJournal implementation.

As a workaround users can create a different ddl without a tablespace variable, but this should probably work better out of the box.

WDYT about one of the following solutions?
- make the base DatabaseJournal implementation support jndi datasource just like PersistenceManagers do (without a specific configuration property but specifying a jndi location in the url property)
- move the replacement of the tablespace variable (and maybe: add a generic replacement of *any* parameter found in the databaseJournal configuration) to the main DatabaseJournal implementation. This could be handy and it will make the OracleDatabaseJournal extension useless, but I see that at the moment there can be a problem with the MsSql implementation, since it adds ""on "" to the tablespace name only when it's not set to an empty string.





"
1,"jcr2spi: Item.isSame may return wrong result if any ancestor is invalidated. julian detected an issue with jcr2spi that was previously shadowed due to heavy reloading of items upon save.
with the most recent changes however reloading of items is postponed until the next access. this will cause the following test to fail:

        Node n = testRootNode.addNode(""aFile"", ""nt:file"");
        n = n.addNode(""jcr:content"", ""nt:resource"");
        n.setProperty(""jcr:lastModified"", Calendar.getInstance());
        n.setProperty(""jcr:mimeType"", ""text/plain"");
        Property jcrData = n.setProperty(""jcr:data"", ""abc"", PropertyType.BINARY);
        testRootNode.save();

        // access same property through different session
        Session otherSession = helper.getReadOnlySession();
        try {
            Property otherProperty = (Property) otherSession.getItem(jcrData.getPath());
            assertTrue(jcrData.isSame(otherProperty));
        } finally {
            otherSession.logout();
        }

while 
     
       assertTrue(n.isSame(otherSession.getItem(n.getPath()));

would be successful.

the reason: the jcrData property is not reloaded and it's parent is still _invalidated_. consequently the property isn't aware of it's id having changed due to the fact that nt:resource is a node type extending from mix:referenceable.

possible fixes:

1) mark all items _invalid_ after save 
    instead of setting status non-protected/autocreated properties to EXISTING.
    -> forcing jcrData to be reloaded before isSame can be called.
    -> drawback: much more round trip(s) to the server just to make sure the id is up to date.

2) change Item#isSame to make sure the workspaceId is up to date (walking up the
     hierarchy and force reloading of the first invalidated ancestor).
     -> drawback: if referenceable nodes are rare or missing at all, this causes some
          extra round trips.
 
3) change Item.isSame to compare the 'workspacePath' instead of the 'workspaceId'.
     -> drawback: upon persisted move of a referenceable node Item#isSame will return false


after taking a closer look at the code and at some additional tests i would opt for 2).
"
1,occasional MergeException while indexing. TestStressIndexing2.testMultiConfig occasionally hits merge exceptions
1,"AIOOB thrown when length of termText is longer than 16384 characters (ArrayIndexOutOfBoundsException). DocumentsWriter has a max term length of 16384; if you cross that you
get an unfriendly ArrayIndexOutOfBoundsException.  We should fix to raise a clearer exception."
1,"bugs in ByteArrayDataInput. ByteArrayDataInput has a byte[] ctor, but it doesn't actually work (some things like readVint will work, others will fail due to asserts).

The problem is it doesnt set things like limit in the ctor... I think the ctor should call reset()
Most code using this passes null to the ctor to initialize it, then uses reset(), instead they could just call ByteArrayInput(BytesRef.EMPTY_BYTES) if they want to do that.
finally, reset()'s limit looks like it should be offset + len"
1,webapp welcome page shows incorrect port when port is the default port. See summary.
1,"MultiThreadedHttpConnectionManager does not properly respond to thread interrupts. MultiThreadedHttpConnectionManager uses interrupts to notify waiting threads when a connection is ready for them. Issues arise if the threads are interrupted by someone else while they are still waiting on a thread, because doGetConnection does not remove the threads from the queue of waiting threads when they are interrupted:

                        connectionPool.wait(timeToWait);

                        // we have not been interrupted so we need to remove ourselves from the 
                        // wait queue
                        hostPool.waitingThreads.remove(waitingThread);                        connectionPool.waitingThreads.remove(waitingThread);
                    } catch (InterruptedException e) {
                        // do nothing                    } finally {
                        if (useTimeout) {
                            endWait = System.currentTimeMillis();
                            timeToWait -= (endWait - startWait);                        }                    }

Under ordinary circumstances, the queue maintenance is done by the notifyWaitingThread method. However, if the thread is interrupted by any other part of the system, it will (1) not actually be released, since the loop in doGetConnection will force it back to the wait, and (2) will be added the waiting thread to the queue repeatedly, which basically means that the thread will eventually receive the interrupt from notifyWaitingThread at some later point, when it is no longer actually waiting for a connection.

This code could probably be re-architected to make it less error-prone, but the fundamental issue seems to be the use of interrupts to signal waiting threads, as opposed to something like a notify. "
1,CheckIndex overstates how many fields have norms enabled. It simply tells you how many unique fields there are... it should instead only say how many have norms.
1,"RMI-DateValue does not support full ISO8601 format. as mentioned in the javadoc:

 * To convert <code>Calendar</code> instances to and from strings, this class
 * uses a <code>SimpleDateFormat</code> instance with the pattern
 * <code>yyyy-MM-dd'T'HH:mm:ss'Z'</code>. The issue with this pattern is that
 * the era specification as defined in the JCR specification (+/- prefix) as
 * well as full time zone naming are not supported.
"
1,"IndexMerger blocks client threads when obsolete index segments are deleted. When index segments have been merged, the obsolete indexes are replaced with the new one an deleted afterwards. Currently deleting the obsolete segments is inside a MultiIndex synchronized block, which may block other threads from updating the index concurrently."
1,"IOUtils - getCreated(...) - SimpleDateFormat is not threadsafe. SimpleDateFormat is not threadsafe (http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4146524)
See exception in attachment.
IMHO it will be enough to synchronize 'format' method in HttpDateFormat class."
1,"DEFAULT_HEADERS not added to subsequent requests. DEFAULT_HEADERS are added to the original request only, not to subsequent requests for redirects or authentication."
1,"primaryItemName is not inherited. if no primaryItemName is defined for a nodetype definition, it should be inherited from one of the supertypes. the spec is unclear about this, though it seems to be the natural behaviour.

for example when extending nt:resource, the subtype should not be force to redefine the jcr:data as primaryItemName.

"
1,"principalbased ACL editing fails if principalName differs from the authorizableID. this issue has been reported by alexK:

editing the permissions for a principal whose name differs from the id of the corresponding user/group fails with AccessControlException.

i quickly had a look at it and the main problem is caused by the ACEditor that assumes that the last segment of the 
path corresponds to the principal name. this isn't true if the principalName differs from the id.



"
1,"Token authentication parameters are not loaded from JAAS configuration.. token based authentication can be disabled and expiration time set in the login module config.
however, this only works with local auth context but  not when using a jaas configuration."
1,"Problem with child order after restoring of parent. The following sequence leads to swapped child nodes in the mentioned trunk version (this worked in 1.0.1).
Specifically:

Add nodes:
 parent
   childA
   childB

Remove childA:
 parent
   childB

Restore initial version:
 parent
   childB
   childA

The parent node is of type nt:unstructured which carries an
OrderableChildNodes=true so I would assume that upon restoring the order
would be preserved.

Cheers,
Tanju

--------------

TESTCASE used with both Derby & InMemPMs (boiled down beyond sense :)

// Add parent & childA, childB
Node parent = session.getRootNode().addNode(""parent"", ""nt:unstructured"");
parent.addMixin(""mix:versionable"");
Node c1 = parent.addNode(""childA"", ""nt:unstructured"");
c1.addMixin(""mix:versionable"");
Node c2 = parent.addNode(""childB"", ""nt:unstructured"");
c2.addMixin(""mix:versionable"");
session.save();
c1.checkin();
c2.checkin();
Version v1 = parent.checkin();
// OK : parent.getNodes() -> childA, childB

// Remove childA
parent = session.getRootNode().getNode(""parent"");
parent.checkout();
c1 = parent.getNodes().nextNode();
c1.checkout();
c1.remove();
session.save();
Version v2 = parent.checkin();
// OK : parent.getNodes() -> childA, childB

// Remove childA
parent = session.getRootNode().getNode(""parent"");
parent.restore(v1, true);
// Not OK : parent.getNodes() -> childB, childA"
1,"Text Extractor: Image parser throws exception (jpeg). the below exception is thrown over an over while uploading jpeg images:
16.11.2009 17:20:42 *WARN * LazyTextExtractorField: Failed to extract text from a binary property (LazyTextExtractorField.java, line 165)
org.apache.tika.exception.TikaException: TIKA-198: Illegal IOException from org.apache.tika.parser.image.ImageParser@c7bc3
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:125)
	at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:105)
	at org.apache.jackrabbit.core.query.lucene.LazyTextExtractorField$ParsingTask.run(LazyTextExtractorField.java:160)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:417)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:269)
	at java.util.concurrent.FutureTask.run(FutureTask.java:123)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:65)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:168)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:650)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:675)
	at java.lang.Thread.run(Thread.java:613)
Caused by: javax.imageio.IIOException: Not a JPEG file: starts with 0x00 0x05
	at com.sun.imageio.plugins.jpeg.JPEGImageReader.readImageHeader(Native Method)
	at com.sun.imageio.plugins.jpeg.JPEGImageReader.readNativeHeader(JPEGImageReader.java:554)
	at com.sun.imageio.plugins.jpeg.JPEGImageReader.checkTablesOnly(JPEGImageReader.java:309)
	at com.sun.imageio.plugins.jpeg.JPEGImageReader.gotoImage(JPEGImageReader.java:431)
	at com.sun.imageio.plugins.jpeg.JPEGImageReader.readHeader(JPEGImageReader.java:547)
	at com.sun.imageio.plugins.jpeg.JPEGImageReader.getHeight(JPEGImageReader.java:609)
	at org.apache.tika.parser.image.ImageParser.parse(ImageParser.java:47)
	at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:119)
	... 10 more"
1,"Incorrect SegmentInfo.delCount when IndexReader.flush() is used. When deleted documents are flushed using IndexReader.flush() the delCount in SegmentInfo is updated based on the current value and SegmentReader.pendingDeleteCount (introduced by LUCENE-1267). It seems that pendingDeleteCount is not reset after the commit, which means after a second flush() or close() of an index reader the delCount in SegmentInfo is incorrect. A subsequent IndexReader.open() call will fail with an error when assertions are enabled. E.g.:

java.lang.AssertionError: delete count mismatch: info=3 vs BitVector=2
	at org.apache.lucene.index.SegmentReader.loadDeletedDocs(SegmentReader.java:405)
[...]"
1,"MsExcelTextFilter throws Exception. Repository is not startable. If i try to add a Excel File (see attachment) i get this Exception

Caused by: java.lang.NumberFormatException: You cannot get a string value from a numeric cell
	at org.apache.poi.hssf.usermodel.HSSFCell.getStringCellValue(HSSFCell.java:800)
	at org.apache.jackrabbit.core.query.MsExcelTextFilter$1.initializeReader(MsExcelTextFilter.java:97)
	at org.apache.jackrabbit.core.query.LazyReader.read(LazyReader.java:79)

The bad news is that if you add this file the repository is not startabel anymore because the file is in the redo.log and you
get a blocker !

The stack from the restart after NumberFormatException

19.09.2006 08:47:23 *ERROR* RepositoryImpl: Unable to start repository, forcing shutdown... 
19.09.2006 08:47:23 *INFO * RepositoryImpl: Shutting down repository... 
19.09.2006 08:47:23 *INFO * RepositoryImpl: shutting down workspace 'default'... 
19.09.2006 08:47:23 *INFO * ObservationManagerFactory: Notification of EventListeners stopped. 
19.09.2006 08:47:23 *INFO * RepositoryImpl: workspace 'default' has been shutdown 

I think its very important that you can not block a whole repository if the indexer throws a exception.
thanks
claus"
1,"indexing doesn't reset token state. IndexWriter (DocumentsWriter) forgets to reset the token state resulting in incorrect positionIncrements, payloads, and token types."
1,"VarDerefBytesImpl doc values prefix length may fall across two pages. The VarDerefBytesImpl doc values encodes the unique byte[] with prefix (1 or 2 bytes) first, followed by bytes, so that it can use PagedBytes.fillSliceWithPrefix.

It does this itself rather than using PagedBytes.copyUsingLengthPrefix...

The problem is, it can write an invalid 2 byte prefix spanning two blocks (ie, last byte of block N and first byte of block N+1), which fillSliceWithPrefix won't decode correctly.

"
1,SpanScorer does not respect ConstantScoreRangeQuery setting. ConstantScoreRangeQuery is actually on and can't be disabled when it should default to off with the option to turn it on.
1,Can't create NIOFSDirectory w/o setting a system property. NIOFSDirectory.getDirectory() returns a FSDirectory object
1,"Binary field content lost during optimize. Scenario:

* create an index with arbitrary content, and close it
* open IndexWriter again, and add a document with binary field (stored but not compressed)
* close IndexWriter _without_ optimizing, so that the new document is in a separate segment.
* open IndexReader. You can read the last document and its binary field just fine.
* open IndexWriter, optimize the index, close IndexWriter
* open IndexReader. Now the field is still present (not null) and is marked as binary, but the data is not there - Field.getBinaryLength() returns 0.
"
1,"Jackrabbit logs a NullPointerException on shutdown if the version manager wasn't initialized. If opening the repository fails, and the version manager was not initialized, then the shutdown method logs a NullPointerException when trying to close the version manager. This is a nuisance."
1,"Registering a Nodetype based on an existing NodeType fail. If I create a new NodeTypeTemplate using the code show below,

           NodeTypeManagerImpl ntm = (NodeTypeManagerImpl) session.getWorkspace().getNodeTypeManager();
           NodeTypeDefinition nt = (NodeTypeDefinition) ntm.getNodeType(""wr:entity"");
           NodeTypeTemplate ntt = ntm.createNodeTypeTemplate(nt);

the list of declaredSuperType contains the same name of the original nodeType (repeted twice) and not the declaredSuperType of the original nodeType (in this example [nt:base, nt:file])

          ntt.getDeclaredSupertypeNames(); -> [wr:entity, wr:entity]"
1,"NodeTypeRegistry.validateNodeTypeDef causes NullPointerException. NodeTypeRegistry.registerNodeType(NodeTypeDef) checks the given node type definition for circular inheritance (amongst other things, of course). If the the node type definition does not contain a list of super types, the validateNodeTypeDef() (line 442) causes a NullPointerException being thrown in checkForCircularInheritance() because the ""supertypes"" variable is null and is not being checked.

Interestingly the other accesses to the same supertypes object in validNodeTypeDef() are all guarded against null and length==0. Might be an ommission."
1,"ArrayIndexOutOfBoundsException: ConcurrentCache. ArrayIndexOutOfBoundsException after several days of uptime.

I'm experiencing some strange ArrayIndexOutOfBoundsExceptions on
 accessing the jackrabbit ConcurrentCache in 2.2.5. in Line 241 during
 shrinkIfNeeded check.

 Caused by: java.lang.ArrayIndexOutOfBoundsException: -14
        at
 org.apache.jackrabbit.core.cache.ConcurrentCache.shrinkIfNeeded(ConcurrentCache.java:241)


I reviewed jackrabbit-code and I'm sure it's caused by that
 AtomicInteger for realizing accessCounter in AbstractCache, which will
 have become negative during increasing over the Integer.MAX_VALUE constant.

         // Semi-random start index to prevent bias against the first
 segments
         int start = (int) getAccessCount() % segments.length;
         for (int i = start; isTooBig(); i = (i + 1) % segments.length) {
             synchronized (segments[i]) {

 ___________________________

 Uncaught Throwable java.lang.ArrayIndexOutOfBoundsException: -7
         at
 org.apache.jackrabbit.core.cache.ConcurrentCache.shrinkIfNeeded(ConcurrentCache.java:241)
         at
 org.apache.jackrabbit.core.cache.ConcurrentCache.put(ConcurrentCache.java:176)
         at
 org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.getBundle(AbstractBundlePersistenceManager.java:657)
         at
 org.apache.jackrabbit.core.persistence.bundle.AbstractBundlePersistenceManager.load(AbstractBundlePersistenceManager.java:400)
         at
 org.apache.jackrabbit.core.state.SharedItemStateManager.loadItemState(SharedItemStateManager.java:1819)
         at
 org.apache.jackrabbit.core.state.SharedItemStateManager.getNonVirtualItemState(SharedItemStateManager.java:1739)
         at
 org.apache.jackrabbit.core.state.SharedItemStateManager.getItemState(SharedItemStateManager.java:261)
         at
 org.apache.jackrabbit.core.state.LocalItemStateManager.getNodeState(LocalItemStateManager.java:107)
         at
 org.apache.jackrabbit.core.state.LocalItemStateManager.getItemState(LocalItemStateManager.java:172)
         at
 org.apache.jackrabbit.core.state.XAItemStateManager.getItemState(XAItemStateManager.java:260)
         at
 org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:161)
         at
 org.apache.jackrabbit.core.ItemManager.getItemData(ItemManager.java:370)
         at
 org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:316)
         at
 org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:610)
         at
 org.apache.jackrabbit.core.SessionImpl.getNodeById(SessionImpl.java:493)
         at
 org.apache.jackrabbit.core.SessionImpl.getNodeByIdentifier(SessionImpl.java:1045)
         at sun.reflect.GeneratedMethodAccessor31.invoke(Unknown Source)
         at
 sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
         at java.lang.reflect.Method.invoke(Method.java:597)
         at
 org.apache.sling.jcr.base.SessionProxyHandler$SessionProxyInvocationHandler.invoke(SessionProxyHandler.java:109)
         at $Proxy2.getNodeByIdentifier(Unknown Source)
         at
 de.dig.cms.frontend.servlet.helper.ResourceUtil.findResourceById(ResourceUtil.java:44)
         at
 de.dig.cms.frontend.servlet.CMSContentEnrichServletFilter.doFilter(CMSContentEnrichServletFilter.java:194)
         at
 org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60)
         at
 de.dig.cms.frontend.servlet.CacheControlFilter.doFilter(CacheControlFilter.java:120)
         at
 org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60)
         at
 de.dig.cms.cache.impl.WallCacheServletFilter.processCacheableRequest(WallCacheServletFilter.java:244)
         at
 de.dig.cms.cache.impl.WallCacheServletFilter.processCacheableRequestWithLatch(WallCacheServletFilter.java:185)
         at
 de.dig.cms.cache.impl.WallCacheServletFilter.doFilter(WallCacheServletFilter.java:154)
         at
 org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60)
         at
 de.dig.cms.frontend.servletapi.CMSSlingHttpServletRequestFilter.doFilter(CMSSlingHttpServletRequestFilter.java:52)
         at
 org.apache.sling.engine.impl.filter.AbstractSlingFilterChain.doFilter(AbstractSlingFilterChain.java:60)
         at
 org.apache.sling.engine.impl.SlingMainServlet.service(SlingMainServlet.java:313)
         at
 org.apache.sling.engine.impl.SlingMainServlet.service(SlingMainServlet.java:207)
         at
 org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)
         at
 org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:389)
         at
 org.ops4j.pax.web.service.internal.HttpServiceServletHandler.handle(HttpServiceServletHandler.java:64)
         at
 org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
         at
 org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)
         at
 org.ops4j.pax.web.service.internal.HttpServiceContext.handle(HttpServiceContext.java:111)
         at
 org.ops4j.pax.web.service.internal.JettyServerHandlerCollection.handle(JettyServerHandlerCollection.java:64)
         at
 org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
         at org.mortbay.jetty.Server.handle(Server.java:324)
         at
 org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:535)
         at
 org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:865)
         at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:539)
         at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
         at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
         at
 org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)
         at
 org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:520)"
1,"MS Proxy with NTLM authentication set up does not work. When I try to go via a MS Proxy which is set up with NTLM authentication I
always get a ""407"" error, no matter which credentials used."
1,"Under Heavy load in a Cluster HTTP Threads Block and stall requests. Under Heavy load created by mounting both nodes in the cluster in OSX Finder and then uploading large numebers of files to each node at the same time ( a few 1000), eventually one of the nodes stops responding and the Finder mount timesout and disconnects.

Once that happens that node becomes unusable.
More mount attempts will prompt for a password indicating HTTP is still running, but will timeout once the connection is authenticated.
Access by the Web Browser will prompt for a password, conenct and provide a once only listing of any collection in the workspace. If you try to refresh that collection, the HTTP request hangs forever."
1,"RequestWrapper does not use the headers of the request it wraps.. The RequestWrapper does not use the headers of the request it wraps. Therefore the wrapper appears as having no header, while the wrapped request may have some.

To work-around that behavior, I have to call resetHeaders() on the wrapper just after having created it.
This method does the following:
    public void resetHeaders()
    {
        headergroup.clear();
        setHeaders(original.getAllHeaders());
    }

I suggest calling setHeaders directly in the constructor. Or at least highlight in the Javadoc that we should call resetHeaders()."
1,"wordnet parsing bug. A user reported that wordnet parses the prolog file incorrectly.

Also need to check the wordnet parser in the memory contrib for this problem.

If this is a false alarm, i'm not worried, because the test will be the first unit test wordnet package ever had.

{noformat}
For example, looking up the synsets for the
word ""king"", we get:

java SynLookup wnindex king
baron
magnate
mogul
power
queen
rex
scrofula
struma
tycoon

Here, ""scrofula"" and ""struma"" are extraneous. This happens because, the line
parser code in Syns2Index.java interpretes the two consecutive single quotes
in entry s(114144247,3,'king''s evil',n,1,1) in  wn_s.pl file, as
termination
of the string and separates into ""king"". This entry concerns
synset of words ""scrofula"" and ""struma"", and thus they get inserted in the
synset of ""king"". *There 1382 such entries, in wn_s.pl* and more in other
WordNet
Prolog data-base files, where such use of two consecutive single quotes
appears.

We have resolved this by adding a statement in the line parsing portion of
Syns2Index.java, as follows:

           // parse line
           line = line.substring(2);
          * line = line.replaceAll(""\'\'"", ""`""); // added statement*
           int comma = line.indexOf(',');
           String num = line.substring(0, comma);  ... ... etc.
In short we replace ""''"" by ""`"" (a back-quote). Then on recreating the
index, we get:

java SynLookup zwnindex king
baron
magnate
mogul
power
queen
rex
tycoon
{noformat}"
1,"ExportSysViewTest fails with: System property org.xml.sax.driver not specified. The ExportSysViewTest class uses the XMLReaderFactory.createXMLReader() method that depends on the system property ""org.xml.sax.driver"" being specified. Apparently using a TransformerFactory works around this issue in some way, as the problem only appeared once we changed the XML export feature to use a custom serializer class instead of a JAXP Transformer for serialization (see JCR-1952).

The current workaround is to explicitly force a Transformer to be loaded, but we really should fix the cause of this issue for example by replacing the XMLReader instance with a SAXParser."
1,"RegexCapabilities is not Serializable. The class RegexQuery is marked Serializable by its super class, but it contains a RegexCapabilities which is not Serializable. Thus attempting to serialize the query results in an exception. 

Making RegexCapabilities serializable should be no problem since its subclasses contain only serializable classes (java.util.regex.Pattern and org.apache.regexp.RE)."
1,"[PATCH] TermInfosReader, SegmentTermEnum Out Of Memory Exception. We've been experiencing terrible memory problems on our production search server, running lucene (1.4.3).

Our live app regularly opens new indexes and, in doing so, releases old IndexReaders for garbage collection.

But...there appears to be a memory leak in org.apache.lucene.index.TermInfosReader.java.
Under certain conditions (possibly related to JVM version, although I've personally observed it under both linux JVM 1.4.2_06, and 1.5.0_03, and SUNOS JVM 1.4.1) the ThreadLocal member variable, ""enumerators"" doesn't get garbage-collected when the TermInfosReader object is gc-ed.

Looking at the code in TermInfosReader.java, there's no reason why it _shouldn't_ be gc-ed, so I can only presume (and I've seen this suggested elsewhere) that there could be a bug in the garbage collector of some JVMs.

I've seen this problem briefly discussed; in particular at the following URL:
  http://java2.5341.com/msg/85821.html
The patch that Doug recommended, which is included in lucene-1.4.3 doesn't work in our particular circumstances. Doug's patch only clears the ThreadLocal variable for the thread running the finalizer (my knowledge of java breaks down here - I'm not sure which thread actually runs the finalizer). In our situation, the TermInfosReader is (potentially) used by more than one thread, meaning that Doug's patch _doesn't_ allow the affected JVMs to correctly collect garbage.

So...I've devised a simple patch which, from my observations on linux JVMs 1.4.2_06, and 1.5.0_03, fixes this problem.

Kieran
PS Thanks to daniel naber for pointing me to jira/lucene

@@ -19,6 +19,7 @@
 import java.io.IOException;

 import org.apache.lucene.store.Directory;
+import java.util.Hashtable;

 /** This stores a monotonically increasing set of <Term, TermInfo> pairs in a
  * Directory.  Pairs are accessed either by Term or by ordinal position the
@@ -29,7 +30,7 @@
   private String segment;
   private FieldInfos fieldInfos;

-  private ThreadLocal enumerators = new ThreadLocal();
+  private final Hashtable enumeratorsByThread = new Hashtable();
   private SegmentTermEnum origEnum;
   private long size;

@@ -60,10 +61,10 @@
   }

   private SegmentTermEnum getEnum() {
-    SegmentTermEnum termEnum = (SegmentTermEnum)enumerators.get();
+    SegmentTermEnum termEnum = (SegmentTermEnum)enumeratorsByThread.get(Thread.currentThread());
     if (termEnum == null) {
       termEnum = terms();
-      enumerators.set(termEnum);
+      enumeratorsByThread.put(Thread.currentThread(), termEnum);
     }
     return termEnum;
   }
@@ -195,5 +196,15 @@
   public SegmentTermEnum terms(Term term) throws IOException {
     get(term);
     return (SegmentTermEnum)getEnum().clone();
+  }
+
+  /* some jvms might have trouble gc-ing enumeratorsByThread */
+  protected void finalize() throws Throwable {
+    try {
+        // make sure gc can clear up.
+        enumeratorsByThread.clear();
+    } finally {
+        super.finalize();
+    }
   }
 }



TermInfosReader.java, full source:
======================================
package org.apache.lucene.index;

/**
 * Copyright 2004 The Apache Software Foundation
 *
 * Licensed under the Apache License, Version 2.0 (the ""License"");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

import java.io.IOException;

import org.apache.lucene.store.Directory;
import java.util.Hashtable;

/** This stores a monotonically increasing set of <Term, TermInfo> pairs in a
 * Directory.  Pairs are accessed either by Term or by ordinal position the
 * set.  */

final class TermInfosReader {
  private Directory directory;
  private String segment;
  private FieldInfos fieldInfos;

  private final Hashtable enumeratorsByThread = new Hashtable();
  private SegmentTermEnum origEnum;
  private long size;

  TermInfosReader(Directory dir, String seg, FieldInfos fis)
       throws IOException {
    directory = dir;
    segment = seg;
    fieldInfos = fis;

    origEnum = new SegmentTermEnum(directory.openFile(segment + "".tis""),
                                   fieldInfos, false);
    size = origEnum.size;
    readIndex();
  }

  public int getSkipInterval() {
    return origEnum.skipInterval;
  }

  final void close() throws IOException {
    if (origEnum != null)
      origEnum.close();
  }

  /** Returns the number of term/value pairs in the set. */
  final long size() {
    return size;
  }

  private SegmentTermEnum getEnum() {
    SegmentTermEnum termEnum = (SegmentTermEnum)enumeratorsByThread.get(Thread.currentThread());
    if (termEnum == null) {
      termEnum = terms();
      enumeratorsByThread.put(Thread.currentThread(), termEnum);
    }
    return termEnum;
  }

  Term[] indexTerms = null;
  TermInfo[] indexInfos;
  long[] indexPointers;

  private final void readIndex() throws IOException {
    SegmentTermEnum indexEnum =
      new SegmentTermEnum(directory.openFile(segment + "".tii""),
			  fieldInfos, true);
    try {
      int indexSize = (int)indexEnum.size;

      indexTerms = new Term[indexSize];
      indexInfos = new TermInfo[indexSize];
      indexPointers = new long[indexSize];

      for (int i = 0; indexEnum.next(); i++) {
	indexTerms[i] = indexEnum.term();
	indexInfos[i] = indexEnum.termInfo();
	indexPointers[i] = indexEnum.indexPointer;
      }
    } finally {
      indexEnum.close();
    }
  }

  /** Returns the offset of the greatest index entry which is less than or equal to term.*/
  private final int getIndexOffset(Term term) throws IOException {
    int lo = 0;					  // binary search indexTerms[]
    int hi = indexTerms.length - 1;

    while (hi >= lo) {
      int mid = (lo + hi) >> 1;
      int delta = term.compareTo(indexTerms[mid]);
      if (delta < 0)
	hi = mid - 1;
      else if (delta > 0)
	lo = mid + 1;
      else
	return mid;
    }
    return hi;
  }

  private final void seekEnum(int indexOffset) throws IOException {
    getEnum().seek(indexPointers[indexOffset],
	      (indexOffset * getEnum().indexInterval) - 1,
	      indexTerms[indexOffset], indexInfos[indexOffset]);
  }

  /** Returns the TermInfo for a Term in the set, or null. */
  TermInfo get(Term term) throws IOException {
    if (size == 0) return null;

    // optimize sequential access: first try scanning cached enum w/o seeking
    SegmentTermEnum enumerator = getEnum();
    if (enumerator.term() != null                 // term is at or past current
	&& ((enumerator.prev != null && term.compareTo(enumerator.prev) > 0)
	    || term.compareTo(enumerator.term()) >= 0)) {
      int enumOffset = (int)(enumerator.position/enumerator.indexInterval)+1;
      if (indexTerms.length == enumOffset	  // but before end of block
	  || term.compareTo(indexTerms[enumOffset]) < 0)
	return scanEnum(term);			  // no need to seek
    }

    // random-access: must seek
    seekEnum(getIndexOffset(term));
    return scanEnum(term);
  }

  /** Scans within block for matching term. */
  private final TermInfo scanEnum(Term term) throws IOException {
    SegmentTermEnum enumerator = getEnum();
    while (term.compareTo(enumerator.term()) > 0 && enumerator.next()) {}
    if (enumerator.term() != null && term.compareTo(enumerator.term()) == 0)
      return enumerator.termInfo();
    else
      return null;
  }

  /** Returns the nth term in the set. */
  final Term get(int position) throws IOException {
    if (size == 0) return null;

    SegmentTermEnum enumerator = getEnum();
    if (enumerator != null && enumerator.term() != null &&
        position >= enumerator.position &&
	position < (enumerator.position + enumerator.indexInterval))
      return scanEnum(position);		  // can avoid seek

    seekEnum(position / enumerator.indexInterval); // must seek
    return scanEnum(position);
  }

  private final Term scanEnum(int position) throws IOException {
    SegmentTermEnum enumerator = getEnum();
    while(enumerator.position < position)
      if (!enumerator.next())
	return null;

    return enumerator.term();
  }

  /** Returns the position of a Term in the set or -1. */
  final long getPosition(Term term) throws IOException {
    if (size == 0) return -1;

    int indexOffset = getIndexOffset(term);
    seekEnum(indexOffset);

    SegmentTermEnum enumerator = getEnum();
    while(term.compareTo(enumerator.term()) > 0 && enumerator.next()) {}

    if (term.compareTo(enumerator.term()) == 0)
      return enumerator.position;
    else
      return -1;
  }

  /** Returns an enumeration of all the Terms and TermInfos in the set. */
  public SegmentTermEnum terms() {
    return (SegmentTermEnum)origEnum.clone();
  }

  /** Returns an enumeration of terms starting at or after the named term. */
  public SegmentTermEnum terms(Term term) throws IOException {
    get(term);
    return (SegmentTermEnum)getEnum().clone();
  }

  /* some jvms might have trouble gc-ing enumeratorsByThread */ 
  protected void finalize() throws Throwable {
    try {
        // make sure gc can clear up.
        enumeratorsByThread.clear();
    } finally {
        super.finalize();
    }
  }
}
"
1,"ConnPoolByRoute driving RouteSpecificPool to IllegalState. Hi all,

I encountered an issue on ConnPoolByRoute / RouteSpecificPool on HTTPClient 4.0.1, akin to HTTPCLIENT-747 (it also leads to a java.lang.IllegalStateException: No entry created for this pool. HttpRoute[{}XXX] ), but it is not a concurrency issue (no race condition, just a logic error if I understood it correctly).

From my understanding, the error lies in ConnPoolByRoute#getEntryBlocking
Quoting from the code (line 309-314) :
RouteSpecificPool rospl = getRoutePool(route, true);
... 
} else if (hasCapacity && !freeConnections.isEmpty()) {

deleteLeastUsedEntry();
entry = createEntry(rospl, operator);

} else { ...

The short version of the issue is : under certain circumstances, #deleteLeastUsedEntry can remove rospl from the map of known RootSpecificPool. But as this code still holds on to the rospl instance, it will modify its state in a way the pool will never recover from later, not having any other way to access this instance when the connection gets released.

A Step by Step guide to what's going wrong.
0) You have to be in a condition that leads to the execution of said code extract (i.e. no free entry on the current route - but the route already is registered to the global pool -, current Route has capacity, max connections reached for the global pool, but there are free connections to destroy).
2) We arrive in deleteLeastUsedEntry(). We get the last entry from a queue. It can be that this entry is bound to the same (hashCode() wise) Route that the one we are getting a connection to (i.e. rospl instance held in the #getEntryBlocking context)
3) this entry can be the last of its pool, thus at this point, rospl.isUnused() == true
4) As a consequence, deleteEntry() will remove rospl from the routeToPool map
5) Back in the getEntryBlocking method, we do entry = createEntry(rospl, operator), which will do createdEntry() on the ""locally-scoped"" rospl instance that has just been removed from routeToPool 
6) When the connexion from this new entry is released at some point in the future, the rospl instance that got the createdEntry() does not exist anymore, and it is a new one that gets the freeEntry() call
7) App breaks : this newly created RouteSpecificPool throws IllegalStateException.

Step 0, though, is a rare condition that I only reached during stress tests, and on a SSL client-auth server. This is so because this is the only condition that I know of in HTTPClient, where there is a keep-alive connection in the RouteSpecificPool that can not be reused (when the State is set to the X500 principals of the client cert in the pool, but not in the request).

Possible fix (from what I understand) :
The rospl instance variable in the context of getEntryBlocking() should be protected against the consequences of #deleteLeastUsedEntry().
Not being confortable with all issues at hand, nor with the code base, the simplest thing I can think of would be to preemptively reset the rospl variable after deleteLeastUsedEntry(), thus writing the previous code extract as :

} else if (hasCapacity && !freeConnections.isEmpty()) {

deleteLeastUsedEntry();
// delete may have made deprecated the RouteSpecificPool instance
rospl = getRoutePool(route, true);
entry = createEntry(rospl, operator);

} else { ...


I have a test case that I will attach to this issue ASAP.
It is a simple example that triggers the above conditions with 3 HttpGet calls, in a serial fashion. As stated previsouly, these calls need nothing particular, except that one of these calls must go to a HTTPS server with client-side certificate authentication (I guess NTLM would be OK, anything that will place a non null state along with the route in BasicEntryPool).

I hope code is self-explainatory. I get 100% failure in my setup. Just configure your 2 URLS, configure classpath, set your keystores system properties, and launch.

Workaround :
Best workaround I found is : do not get to step 0.
The most robust way I found to do that (i.e. a way that does not involve things like setting max pool size to a gigantic number that can never be reached, ...) is to actively set the ClientContext.USER_TOKEN attribute in an exec context while submitting the request to the client.
Step 0 triggers when there is an idle connection that waits, and when this idle connection can not be reused, which can only happen if the request's ""USER_TOKEN"" does not match the BasicPoolEntry#getState(). As, in the SSL case, the state is the SSL Cert's X500PrincipalName, and I know it in advance, it's easy to set up front.

By the way, this taught me that I never could benefit from connection reuse strategies in this SSL case, as connections would always get into the pool with a USER_TOKEN that my requests never had. Don't know if it's mentionned somewhere in the documentation, but this is a noteworthy fact to me.

Please feel free to comment / correct any mistakes."
1,"When node is created and locked in same transaction, exception is thrown. Following code fails when executed inside an XA transaction:

Node n = session.getRootNode().addNode(""n"");
n.addMixin(""mix:lockable"");
session.save();
Lock lock = n.lock(false, false);

Stacktrace is

Caused by: javax.transaction.xa.XAException
	at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:155)
	at org.apache.jackrabbit.core.XASessionImpl.commit(XASessionImpl.java:337)
	at org.apache.jackrabbit.jca.TransactionBoundXAResource.commit(TransactionBoundXAResource.java:39)
	at org.apache.geronimo.transaction.manager.WrapperNamedXAResource.commit(WrapperNamedXAResource.java:47)
	at org.apache.geronimo.transaction.manager.TransactionImpl.commit(TransactionImpl.java:301)
	... 32 more
Caused by: org.apache.jackrabbit.core.TransactionException: Unable to update.
	at org.apache.jackrabbit.core.lock.XAEnvironment.prepare(XAEnvironment.java:275)
	at org.apache.jackrabbit.core.lock.XALockManager.prepare(XALockManager.java:245)
	at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:138)
	... 36 more
Caused by: javax.jcr.ItemNotFoundException: failed to build path of 48fb59d8-ac77-4b9f-8b53-9f2492dca5e5: 48fb59d8-ac77-4b9f-8b53-9f2492dca5e5: 48fb59d8-ac77-4b9f-8b53-9f2492dca5e5
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:407)
	at org.apache.jackrabbit.core.CachingHierarchyManager.getPath(CachingHierarchyManager.java:272)
	at org.apache.jackrabbit.core.lock.LockManagerImpl.getPath(LockManagerImpl.java:651)
	at org.apache.jackrabbit.core.lock.LockManagerImpl.internalLock(LockManagerImpl.java:276)
	at org.apache.jackrabbit.core.lock.XAEnvironment$LockInfo.update(XAEnvironment.java:409)
	at org.apache.jackrabbit.core.lock.XAEnvironment.prepare(XAEnvironment.java:273)
	... 38 more
Caused by: org.apache.jackrabbit.core.state.NoSuchItemStateException: 48fb59d8-ac77-4b9f-8b53-9f2492dca5e5
	at org.apache.jackrabbit.core.state.SessionItemStateManager.getItemState(SessionItemStateManager.java:189)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getItemState(HierarchyManagerImpl.java:188)
	at org.apache.jackrabbit.core.HierarchyManagerImpl.getPath(HierarchyManagerImpl.java:402)
	... 43 more

"
1,"CachingHttpClient returns a 411 respones when executing a POST (HttpPost) request . The CachingHttpClient validates requests prior executing them, by calling RequestProtocolCompliance.requestIsFatallyNonCompliant(..).

When executing an HttpPost, this method considers the request is invalid because it does not contain (yet) a content-length header. Indeed, I observed that this header is generated at the time the DefaultHttpClient fires the request.

NB: i'm using the Cache 4.1-alpha2 plugged over the HttpClient 4.0.1-final. I can't use the latest version for both because I need to rely on a stable version if there's any. I would be curious to know if we get the same behaviour in 4.1...

Anyway, I would see two fixes for that issue:
- make HttpPost set the content-length at the time the entity is set,
- or remove the validation step on the CachingHttpClient side.
"
1,"jcr2spi: Unprocessed ItemInfos call to RepositoryService#getItemInfos. stefan reported the following problem:

- batchread config reads with depths infinity
- invalidate tree by calling Node.refresh(false)
- force loading of the tree (e.g. Node.getPath())

afterwards, there may still be invalidated item states indicating that not all ItemInfos were processed.
consequently, there are additional calls to getItemInfos that should have been covered by the loading of the tree.
the problem occuring is not related to limitation of the item-cache size.

problem analysis:

there is a bug in WorkspaceItemStateFactory#createItemStates.
there is a wrapper built around the ItemInfo-Iterator but later on the ItemInfo-Iterator is used instead of the wrapper, which pre-fetches items from the underlying iterator and process them upon hasNext()/next()."
1,"There are a few binary search implmentations in lucene that suffer from a now well known overflow bug. http://googleresearch.blogspot.com/2006/06/extra-extra-read-all-about-it-nearly.html

The places I see it are:

MultiSearcher.subSearcher(int)
TermInfosReader.getIndexOffset(Term)
MultiSegmentReader.readerIndex(int, int[], int)
MergeDocIDRemapper.remap(int)

I havn't taken much time to consider how likely any of these are to overflow. The values being averaged would have to be very large. That would rule out possible problems for at least a couple of these, but how about something like the MergeDocIDRemapper? Is there a document number that could be reached that has a chance of triggering this bug? If not we can close this and have a record of looking into it."
1,"CachingIndexReader: NullPointerException initializing parents cache. Using the jackrabbit-core-1.4.9 (after upgrading from jackrabbot-core-1.4.6), the following exception is logged. The code where the exception happens was introduced in JCR-1884 and is first included in the 1.4.9 core release.

10.03.2009 18:56:25 *WARN * CachingIndexReader: Error initializing parents cache. (CachingIndexReader.java, line 310)
java.lang.NullPointerException
    at org.apache.jackrabbit.core.query.lucene.CachingIndexReader$CacheInitializer$2.collect(CachingIndexReader.java:362)
    at org.apache.jackrabbit.core.query.lucene.CachingIndexReader$CacheInitializer.collectTermDocs(CachingIndexReader.java:426)
    at org.apache.jackrabbit.core.query.lucene.CachingIndexReader$CacheInitializer.initializeParents(CachingIndexReader.java:356)
    at org.apache.jackrabbit.core.query.lucene.CachingIndexReader$CacheInitializer.run(CachingIndexReader.java:306)
    at org.apache.jackrabbit.core.query.lucene.CachingIndexReader.<init>(CachingIndexReader.java:109)
    at org.apache.jackrabbit.core.query.lucene.AbstractIndex.getReadOnlyIndexReader(AbstractIndex.java:276)
    at org.apache.jackrabbit.core.query.lucene.MultiIndex.getIndexReader(MultiIndex.java:731)
    at org.apache.jackrabbit.core.query.lucene.MultiIndex.<init>(MultiIndex.java:303)
    at org.apache.jackrabbit.core.query.lucene.SearchIndex.doInit(SearchIndex.java:454)
    at com.day.crx.query.lucene.LuceneHandler.doInit(LuceneHandler.java:93)
    at org.apache.jackrabbit.core.query.AbstractQueryHandler.init(AbstractQueryHandler.java:53)
    at org.apache.jackrabbit.core.SearchManager.initializeQueryHandler(SearchManager.java:583)
    at org.apache.jackrabbit.core.SearchManager.<init>(SearchManager.java:265)
    at org.apache.jackrabbit.core.RepositoryImpl$WorkspaceInfo.getSearchManager(RepositoryImpl.java:1600)
    at org.apache.jackrabbit.core.RepositoryImpl.initWorkspace(RepositoryImpl.java:606)
    at org.apache.jackrabbit.core.RepositoryImpl.getWorkspaceInfo(RepositoryImpl.java:718)
    at com.day.crx.core.CRXRepositoryImpl.login(CRXRepositoryImpl.java:964)
    at org.apache.sling.jcr.base.internal.SessionPool.acquireSession(SessionPool.java:268)
    at org.apache.sling.jcr.base.internal.SessionPoolManager.login(SessionPoolManager.java:99)
    at org.apache.sling.jcr.base.AbstractSlingRepository.login(AbstractSlingRepository.java:240)
    at org.apache.sling.jcr.base.AbstractSlingRepository.loginAdministrative(AbstractSlingRepository.java:206)
    at org.apache.sling.jcr.base.AbstractSlingRepository.pingAndCheck(AbstractSlingRepository.java:506)
    at org.apache.sling.jcr.base.AbstractSlingRepository.startRepository(AbstractSlingRepository.java:810)
    at org.apache.sling.jcr.base.AbstractSlingRepository.activate(AbstractSlingRepository.java:629)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.felix.scr.impl.ImmediateComponentManager.createImplementationObject(ImmediateComponentManager.java:226)
    at org.apache.felix.scr.impl.ImmediateComponentManager.createComponent(ImmediateComponentManager.java:133)
    at org.apache.felix.scr.impl.AbstractComponentManager.activateInternal(AbstractComponentManager.java:476)
    at org.apache.felix.scr.impl.AbstractComponentManager.enableInternal(AbstractComponentManager.java:398)
    at org.apache.felix.scr.impl.AbstractComponentManager.access$000(AbstractComponentManager.java:36)
    at org.apache.felix.scr.impl.AbstractComponentManager$1.run(AbstractComponentManager.java:99)
    at org.apache.felix.scr.impl.ComponentActorThread.run(ComponentActorThread.java:85)
10.03.2009 18:56:31 *INFO * SearchIndex: Index initialized: /u01/media/u01/crxlocal/workspaces/dailymail-prod/index Version: 2 (SearchIndex.java, line 492)
"
1,"TestIndexWriter.testThreadInterruptDeadlock failed (can't reproduce). trunk: r1134163 

ran it a few times with tests.iter=200 and couldn't reproduce, but i believe you like an issue anyway.

{code}
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter
    [junit] Testcase: testThreadInterruptDeadlock(org.apache.lucene.index.TestIndexWriter):     FAILED
    [junit]
    [junit] junit.framework.AssertionFailedError:
    [junit]     at org.apache.lucene.index.TestIndexWriter.testThreadInterruptDeadlock(TestIndexWriter.java:1203)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1403)
    [junit]     at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1321)
    [junit]
    [junit]
    [junit] Tests run: 40, Failures: 1, Errors: 0, Time elapsed: 23.79 sec
    [junit]
    [junit] ------------- Standard Output ---------------
    [junit] CheckIndex failed
    [junit] ERROR: could not read any segments file in directory
    [junit] java.io.FileNotFoundException: segments_2w
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:407)
    [junit]     at org.apache.lucene.index.codecs.DefaultSegmentInfosReader.openInput(DefaultSegmentInfosReader.java:112)
    [junit]     at org.apache.lucene.index.codecs.DefaultSegmentInfosReader.read(DefaultSegmentInfosReader.java:45)
    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:257)
    [junit]     at org.apache.lucene.index.SegmentInfos$1.doBody(SegmentInfos.java:287)
    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:698)
    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:533)
    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:283)
    [junit]     at org.apache.lucene.index.CheckIndex.checkIndex(CheckIndex.java:311)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:154)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:144)
    [junit]     at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:1154)
    [junit]
    [junit] CheckIndex FAILED: unexpected exception
    [junit] java.lang.RuntimeException: CheckIndex failed
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:158)
    [junit]     at org.apache.lucene.util._TestUtil.checkIndex(_TestUtil.java:144)
    [junit]     at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:1154)
    [junit] IndexReader.open FAILED: unexpected exception
    [junit] java.io.FileNotFoundException: segments_2w
    [junit]     at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:407)
    [junit]     at org.apache.lucene.index.codecs.DefaultSegmentInfosReader.openInput(DefaultSegmentInfosReader.java:112)
    [junit]     at org.apache.lucene.index.codecs.DefaultSegmentInfosReader.read(DefaultSegmentInfosReader.java:45)
    [junit]     at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:257)
    [junit]     at org.apache.lucene.index.DirectoryReader$1.doBody(DirectoryReader.java:88)
    [junit]     at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:698)
    [junit]     at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:84)
    [junit]     at org.apache.lucene.index.IndexReader.open(IndexReader.java:500)
    [junit]     at org.apache.lucene.index.IndexReader.open(IndexReader.java:293)
    [junit]     at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:1161)

    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testThreadInterruptDeadlock -Dtests.seed=6733070832417768606:3130345095020099096
    [junit] NOTE: test params are: codec=RandomCodecProvider: {=MockRandom, f6=SimpleText, f7=MockRandom, f8=MockSep, f9=Standard, f1=SimpleText, f0=Standard, f3=Standard, f2=MockSep, f5=Pulsing(freqCutoff=12),
 f4=MockFixedIntBlock(blockSize=552), c=MockVariableIntBlock(baseBlockSize=43), d9=MockVariableIntBlock(baseBlockSize=43), d8=MockRandom, d5=MockSep, d4=Pulsing(freqCutoff=12), d7=MockFixedIntBlock(blockSize=55
2), d6=MockVariableIntBlock(baseBlockSize=43), d25=MockSep, d0=MockVariableIntBlock(baseBlockSize=43), c29=MockFixedIntBlock(blockSize=552), d24=Pulsing(freqCutoff=12), d1=MockFixedIntBlock(blockSize=552), c28=
Standard, d23=SimpleText, d2=SimpleText, c27=MockSep, d22=Standard, d3=MockRandom, d21=MockRandom, d20=SimpleText, c22=MockSep, c21=Pulsing(freqCutoff=12), c20=SimpleText, d29=SimpleText, c26=MockVariableIntBlo
ck(baseBlockSize=43), d28=Standard, c25=MockRandom, d27=MockVariableIntBlock(baseBlockSize=43), c24=Pulsing(freqCutoff=12), d26=MockRandom, c23=MockFixedIntBlock(blockSize=552), e9=Pulsing(freqCutoff=12), e8=St
andard, e7=MockSep, e6=MockRandom, e5=SimpleText, c17=MockFixedIntBlock(blockSize=552), e3=MockFixedIntBlock(blockSize=552), d12=Pulsing(freqCutoff=12), c16=MockVariableIntBlock(baseBlockSize=43), e4=Pulsing(fr
eqCutoff=12), d11=MockFixedIntBlock(blockSize=552), c19=MockRandom, e1=MockSep, d14=MockVariableIntBlock(baseBlockSize=43), c18=SimpleText, e2=Standard, d13=MockRandom, e0=SimpleText, d10=MockSep, d19=MockVaria
bleIntBlock(baseBlockSize=43), c11=MockVariableIntBlock(baseBlockSize=43), c10=MockRandom, d16=Standard, c13=MockRandom, c12=SimpleText, d15=MockSep, d18=Pulsing(freqCutoff=12), c15=Standard, d17=MockFixedIntBl
ock(blockSize=552), c14=MockSep, b3=Standard, b2=MockSep, b5=Pulsing(freqCutoff=12), b4=MockFixedIntBlock(blockSize=552), b7=MockFixedIntBlock(blockSize=552), b6=MockVariableIntBlock(baseBlockSize=43), d50=Mock
Random, b9=MockRandom, b8=SimpleText, d43=SimpleText, d42=Standard, d41=MockVariableIntBlock(baseBlockSize=43), d40=MockRandom, d47=Pulsing(freqCutoff=12), d46=MockFixedIntBlock(blockSize=552), b0=MockRandom, d
45=Standard, b1=MockVariableIntBlock(baseBlockSize=43), d44=MockSep, d49=MockRandom, d48=SimpleText, c6=SimpleText, c5=Standard, c4=MockVariableIntBlock(baseBlockSize=43), c3=MockRandom, c9=MockFixedIntBlock(bl
ockSize=552), c8=Standard, c7=MockSep, d30=Standard, d32=Pulsing(freqCutoff=12), d31=MockFixedIntBlock(blockSize=552), c1=Pulsing(freqCutoff=12), d34=MockFixedIntBlock(blockSize=552), c2=MockSep, d33=MockVariab
leIntBlock(baseBlockSize=43), d36=MockRandom, c0=SimpleText, d35=SimpleText, d38=MockSep, d37=Pulsing(freqCutoff=12), d39=MockVariableIntBlock(baseBlockSize=43), e92=MockFixedIntBlock(blockSize=552), e93=Pulsin
g(freqCutoff=12), e90=MockSep, e91=Standard, e89=Standard, e88=MockVariableIntBlock(baseBlockSize=43), e87=MockRandom, e86=MockFixedIntBlock(blockSize=552), e85=MockVariableIntBlock(baseBlockSize=43), e84=MockS
ep, e83=Pulsing(freqCutoff=12), e80=MockFixedIntBlock(blockSize=552), e81=SimpleText, e82=MockRandom, e77=Standard, e76=MockSep, e79=Pulsing(freqCutoff=12), e78=MockFixedIntBlock(blockSize=552), e73=MockVariabl
eIntBlock(baseBlockSize=43), e72=MockRandom, e75=SimpleText, e74=Standard, binary=MockSep, f98=MockRandom, f97=SimpleText, f99=MockSep, f94=Pulsing(freqCutoff=12), f93=MockFixedIntBlock(blockSize=552), f96=Mock
VariableIntBlock(baseBlockSize=43), f95=MockRandom, e95=MockRandom, e94=SimpleText, e97=Standard, e96=MockSep, e99=MockSep, e98=Pulsing(freqCutoff=12), id=Standard, f34=SimpleText, f33=Standard, f32=MockVariabl
eIntBlock(baseBlockSize=43), f31=MockRandom, f30=MockFixedIntBlock(blockSize=552), f39=SimpleText, f38=MockVariableIntBlock(baseBlockSize=43), f37=MockRandom, f36=Pulsing(freqCutoff=12), f35=MockFixedIntBlock(b
lockSize=552), f43=MockSep, f42=Pulsing(freqCutoff=12), f45=MockFixedIntBlock(blockSize=552), f44=MockVariableIntBlock(baseBlockSize=43), f41=Standard, f40=MockSep, f47=SimpleText, f46=Standard, f49=MockSep, f4
8=Pulsing(freqCutoff=12), content=Standard, e19=Standard, e18=MockSep, e17=SimpleText, f12=MockRandom, e16=Standard, f11=SimpleText, f10=MockFixedIntBlock(blockSize=552), e15=MockVariableIntBlock(baseBlockSize=
43), e14=MockRandom, f16=MockFixedIntBlock(blockSize=552), e13=MockSep, e12=Pulsing(freqCutoff=12), f15=MockVariableIntBlock(baseBlockSize=43), e11=SimpleText, f14=MockSep, e10=Standard, f13=Pulsing(freqCutoff=
12), f19=Standard, f18=MockVariableIntBlock(baseBlockSize=43), f17=MockRandom, e29=MockRandom, e26=MockSep, f21=Standard, e25=Pulsing(freqCutoff=12), f20=MockSep, e28=MockFixedIntBlock(blockSize=552), f23=Pulsi
ng(freqCutoff=12), e27=MockVariableIntBlock(baseBlockSize=43), f22=MockFixedIntBlock(blockSize=552), f25=MockRandom, e22=MockFixedIntBlock(blockSize=552), f24=SimpleText, e21=MockVariableIntBlock(baseBlockSize=
43), f27=Standard, e24=MockRandom, f26=MockSep, e23=SimpleText, f29=MockSep, f28=Pulsing(freqCutoff=12), e20=Pulsing(freqCutoff=12), field=MockSep, string=MockVariableIntBlock(baseBlockSize=43), e30=MockFixedIn
tBlock(blockSize=552), e31=Pulsing(freqCutoff=12), a98=MockSep, e34=SimpleText, a99=Standard, e35=MockRandom, f79=MockSep, e32=MockVariableIntBlock(baseBlockSize=43), e33=MockFixedIntBlock(blockSize=552), b97=M
ockRandom, f77=MockRandom, e38=MockVariableIntBlock(baseBlockSize=43), b98=MockVariableIntBlock(baseBlockSize=43), f78=MockVariableIntBlock(baseBlockSize=43), e39=MockFixedIntBlock(blockSize=552), b99=Standard,
 f75=MockFixedIntBlock(blockSize=552), e36=Pulsing(freqCutoff=12), f76=Pulsing(freqCutoff=12), e37=MockSep, f73=Pulsing(freqCutoff=12), f74=MockSep, f71=Standard, f72=SimpleText, f81=Standard, f80=MockSep, e40=
MockVariableIntBlock(baseBlockSize=43), e41=Standard, e42=SimpleText, e43=MockSep, e44=Standard, e45=MockFixedIntBlock(blockSize=552), e46=Pulsing(freqCutoff=12), f86=Standard, e47=SimpleText, f87=SimpleText, e
48=MockRandom, f88=Pulsing(freqCutoff=12), e49=MockSep, f89=MockSep, f82=MockVariableIntBlock(baseBlockSize=43), f83=MockFixedIntBlock(blockSize=552), f84=SimpleText, f85=MockRandom, f90=Pulsing(freqCutoff=12),
 f92=MockVariableIntBlock(baseBlockSize=43), f91=MockRandom, str=MockRandom, a76=Standard, e56=Standard, f59=Pulsing(freqCutoff=12), a77=SimpleText, e57=SimpleText, a78=Pulsing(freqCutoff=12), e54=MockRandom, f
57=Standard, a79=MockSep, e55=MockVariableIntBlock(baseBlockSize=43), f58=SimpleText, e52=MockVariableIntBlock(baseBlockSize=43), e53=MockFixedIntBlock(blockSize=552), e50=Pulsing(freqCutoff=12), e51=MockSep, f
51=MockSep, f52=Standard, f50=MockRandom, f55=MockVariableIntBlock(baseBlockSize=43), f56=MockFixedIntBlock(blockSize=552), f53=Pulsing(freqCutoff=12), e58=MockFixedIntBlock(blockSize=552), f54=MockSep, e59=Pul
sing(freqCutoff=12), a80=Pulsing(freqCutoff=12), e60=Pulsing(freqCutoff=12), a82=MockVariableIntBlock(baseBlockSize=43), a81=MockRandom, a84=MockRandom, a83=SimpleText, a86=Standard, a85=MockSep, a89=SimpleText
, f68=MockVariableIntBlock(baseBlockSize=43), e65=Pulsing(freqCutoff=12), f69=MockFixedIntBlock(blockSize=552), e66=MockSep, a87=MockVariableIntBlock(baseBlockSize=43), e67=MockVariableIntBlock(baseBlockSize=43
), a88=MockFixedIntBlock(blockSize=552), e68=MockFixedIntBlock(blockSize=552), e61=SimpleText, e62=MockRandom, e63=MockSep, e64=Standard, f60=MockFixedIntBlock(blockSize=552), f61=Pulsing(freq

Cutoff=12), f62=MockRandom, f63=MockVariableIntBlock(baseBlockSize=43), e69=Standard, f64=SimpleText, f65=MockRandom, f66=MockSep, f67=Standard, f70=MockFixedIntBlock(blockSize=552), a93=MockSep, a92=Pulsing(freqCutoff=12), a91=SimpleText, e71=SimpleText, a90=Standard, e70=Standard, a97=MockVariableIntBlock(baseBlockSize=43), a96=MockRandom, a95=Pulsing(freqCutoff=12), a94=MockFixedIntBlock(blockSize=552), c58=MockRandom, a63=MockFixedIntBlock(blockSize=552), a64=Pulsing(freqCutoff=12), c59=MockVariableIntBlock(baseBlockSize=43), c56=MockFixedIntBlock(blockSize=552), d59=MockRandom, a61=MockSep, c57=Pulsing(freqCutoff=12), a62=Standard, c54=Pulsing(freqCutoff=12), c55=MockSep, a60=SimpleText, c52=Standard, c53=SimpleText, d53=SimpleText, d54=MockRandom, d51=MockVariableIntBlock(baseBlockSize=43), d52=MockFixedIntBlock(blockSize=552), d57=Pulsing(freqCutoff=12), b62=Standard, d58=MockSep, b63=SimpleText, d55=Standard, b60=MockRandom, d56=SimpleText, b61=MockVariableIntBlock(baseBlockSize=43), b56=Standard, b55=MockSep, b54=MockRandom, b53=SimpleText, d61=MockVariableIntBlock(baseBlockSize=43), b59=MockVariableIntBlock(baseBlockSize=43), d60=MockRandom, b58=MockSep, b57=Pulsing(freqCutoff=12), c62=Standard, c61=MockSep, a59=MockVariableIntBlock(baseBlockSize=43), c60=MockRandom, a58=MockRandom, a57=MockFixedIntBlock(blockSize=552), a56=MockVariableIntBlock(baseBlockSize=43), a55=MockSep, a54=Pulsing(freqCutoff=12), a72=MockRandom, c67=Standard, a73=MockVariableIntBlock(baseBlockSize=43), c68=SimpleText, a74=Standard, c69=Pulsing(freqCutoff=12), a75=SimpleText, c63=MockVariableIntBlock(baseBlockSize=43), c64=MockFixedIntBlock(blockSize=552), a70=MockVariableIntBlock(baseBlockSize=43), c65=SimpleText, a71=MockFixedIntBlock(blockSize=552), c66=MockRandom, d62=MockSep, d63=Standard, d64=MockFixedIntBlock(blockSize=552), b70=Standard, d65=Pulsing(freqCutoff=12), b71=Pulsing(freqCutoff=12), d66=MockVariableIntBlock(baseBlockSize=43), b72=MockSep, d67=MockFixedIntBlock(blockSize=552), b73=MockVariableIntBlock(baseBlockSize=43), d68=SimpleText, b74=MockFixedIntBlock(blockSize=552), d69=MockRandom, b65=Pulsing(freqCutoff=12), b64=MockFixedIntBlock(blockSize=552), b67=MockVariableIntBlock(baseBlockSize=43), b66=MockRandom, d70=SimpleText, b69=MockRandom, b68=SimpleText, d72=MockSep, d71=Pulsing(freqCutoff=12), c71=Pulsing(freqCutoff=12), c70=MockFixedIntBlock(blockSize=552), a69=Pulsing(freqCutoff=12), c73=MockVariableIntBlock(baseBlockSize=43), c72=MockRandom, a66=MockRandom, a65=SimpleText, a68=Standard, a67=MockSep, c32=MockSep, c33=Standard, c30=SimpleText, c31=MockRandom, c36=MockVariableIntBlock(baseBlockSize=43), a41=Pulsing(freqCutoff=12), c37=MockFixedIntBlock(blockSize=552), a42=MockSep, a0=MockRandom, c34=Pulsing(freqCutoff=12), c35=MockSep, a40=SimpleText, b84=MockSep, d79=MockFixedIntBlock(blockSize=552), b85=Standard, b82=SimpleText, d77=MockSep, c38=Standard, b83=MockRandom, d78=Standard, c39=SimpleText, b80=MockRandom, d75=Standard, b81=MockVariableIntBlock(baseBlockSize=43), d76=SimpleText, d73=MockRandom, d74=MockVariableIntBlock(baseBlockSize=43), d83=MockRandom, a9=MockFixedIntBlock(blockSize=552), d82=SimpleText, d81=MockFixedIntBlock(blockSize=552), d80=MockVariableIntBlock(baseBlockSize=43), b79=MockFixedIntBlock(blockSize=552), b78=MockSep, b77=Pulsing(freqCutoff=12), b76=SimpleText, b75=Standard, a1=Pulsing(freqCutoff=12), a35=Pulsing(freqCutoff=12), a2=MockSep, a34=MockFixedIntBlock(blockSize=552), a3=MockVariableIntBlock(baseBlockSize=43), a33=Standard, a4=MockFixedIntBlock(blockSize=552), a32=MockSep, a5=MockRandom, a39=MockRandom, c40=SimpleText, a6=MockVariableIntBlock(baseBlockSize=43), a38=SimpleText, a7=Standard, a37=MockFixedIntBlock(blockSize=552), a8=SimpleText, a36=MockVariableIntBlock(baseBlockSize=43), c41=MockFixedIntBlock(blockSize=552), c42=Pulsing(freqCutoff=12), c43=MockRandom, c44=MockVariableIntBlock(baseBlockSize=43), c45=SimpleText, a50=MockVariableIntBlock(baseBlockSize=43), c46=MockRandom, a51=MockFixedIntBlock(blockSize=552), c47=MockSep, a52=SimpleText, c48=Standard, a53=MockRandom, b93=MockFixedIntBlock(blockSize=552), d88=MockRandom, c49=MockVariableIntBlock(baseBlockSize=43), b94=Pulsing(freqCutoff=12), d89=MockVariableIntBlock(baseBlockSize=43), b95=MockRandom, b96=MockVariableIntBlock(baseBlockSize=43), d84=Pulsing(freqCutoff=12), b90=SimpleText, d85=MockSep, b91=Pulsing(freqCutoff=12), d86=MockVariableIntBlock(baseBlockSize=43), b92=MockSep, d87=MockFixedIntBlock(blockSize=552), d92=Standard, d91=MockSep, d94=Pulsing(freqCutoff=12), d93=MockFixedIntBlock(blockSize=552), b87=MockFixedIntBlock(blockSize=552), b86=MockVariableIntBlock(baseBlockSize=43), d90=SimpleText, b89=MockRandom, b88=SimpleText, a44=MockVariableIntBlock(baseBlockSize=43), a43=MockRandom, a46=SimpleText, a45=Standard, a48=Standard, a47=MockSep, c51=MockFixedIntBlock(blockSize=552), a49=MockFixedIntBlock(blockSize=552), c50=MockVariableIntBlock(baseBlockSize=43), d98=MockFixedIntBlock(blockSize=552), d97=MockVariableIntBlock(baseBlockSize=43), d96=MockSep, d95=Pulsing(freqCutoff=12), d99=MockRandom, a20=MockVariableIntBlock(baseBlockSize=43), c99=SimpleText, c98=Standard, c97=MockVariableIntBlock(baseBlockSize=43), c96=MockRandom, b19=MockVariableIntBlock(baseBlockSize=43), a16=Pulsing(freqCutoff=12), a17=MockSep, b17=Pulsing(freqCutoff=12), a14=Standard, b18=MockSep, a15=SimpleText, a12=SimpleText, a13=MockRandom, a10=MockVariableIntBlock(baseBlockSize=43), a11=MockFixedIntBlock(blockSize=552), b11=MockFixedIntBlock(blockSize=552), b12=Pulsing(freqCutoff=12), b10=Standard, b15=SimpleText, b16=MockRandom, a18=MockRandom, b13=MockVariableIntBlock(baseBlockSize=43), a19=MockVariableIntBlock(baseBlockSize=43), b14=MockFixedIntBlock(blockSize=552), b30=MockRandom, a31=MockSep, a30=Pulsing(freqCutoff=12), b28=SimpleText, a25=MockVariableIntBlock(baseBlockSize=43), b29=MockRandom, a26=MockFixedIntBlock(blockSize=552), a27=SimpleText, a28=MockRandom, a21=MockSep, a22=Standard, a23=MockFixedIntBlock(blockSize=552), a24=Pulsing(freqCutoff=12), b20=MockRandom, b21=MockVariableIntBlock(baseBlockSize=43), b22=Standard, b23=SimpleText, a29=Pulsing(freqCutoff=12), b24=MockSep, b25=Standard, b26=MockFixedIntBlock(blockSize=552), b27=Pulsing(freqCutoff=12), b41=Pulsing(freqCutoff=12), b40=MockFixedIntBlock(blockSize=552), c77=MockRandom, c76=SimpleText, c75=MockFixedIntBlock(blockSize=552), c74=MockVariableIntBlock(baseBlockSize=43), c79=SimpleText, c78=Standard, c80=MockSep, c83=MockRandom, c84=MockVariableIntBlock(baseBlockSize=43), c81=MockFixedIntBlock(blockSize=552), b39=MockFixedIntBlock(blockSize=552), c82=Pulsing(freqCutoff=12), b37=Standard, b38=SimpleText, b35=MockRandom, b36=MockVariableIntBlock(baseBlockSize=43), b33=MockVariableIntBlock(baseBlockSize=43), b34=MockFixedIntBlock(blockSize=552), b31=Pulsing(freqCutoff=12), b32=MockSep, str2=MockSep, b50=MockVariableIntBlock(baseBlockSize=43), b52=SimpleText, str3=MockVariableIntBlock(baseBlockSize=43), b51=Standard, c86=Standard, tvtest=MockSep, c85=MockSep, c88=Pulsing(freqCutoff=12), c87=MockFixedIntBlock(blockSize=552), c89=MockVariableIntBlock(baseBlockSize=43), c90=SimpleText, c91=MockRandom, c92=Standard, c93=SimpleText, c94=Pulsing(freqCutoff=12), c95=MockSep, content1=MockRandom, b46=Pulsing(freqCutoff=12), b47=MockSep, content3=MockFixedIntBlock(blockSize=552), b48=MockVariableIntBlock(baseBlockSize=43), content4=MockVariableIntBlock(baseBlockSize=43), b49=MockFixedIntBlock(blockSize=552), content5=Pulsing(freqCutoff=12), b42=SimpleText, b43=MockRandom, b44=MockSep, b45=Standard}, locale=sk, timezone=America/Rainy_River
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestDateTools, TestDeletionPolicy, TestDocsAndPositions, TestFlex, TestIndexReaderCloneNorms, TestIndexWriter]
    [junit] NOTE: Linux 2.6.39-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=86065896,total=127401984
    [junit] ------------- ---------------- ---------------
    [junit] TEST org.apache.lucene.index.TestIndexWriter FAILED
{code}"
1,"Cookie.java hashCode method violates contract. org.apache.commons.httpclient.Cookie hashCode() does not meet object.hashCode
() contract.  Cookie.hashCode() returns different values even though data used 
in equals() comparison is the same.

Contract:**Whenever it is invoked on the same object more than once during an 
execution of a Java application, the hashCode method must consistently return 
the same integer, provided no information used in equals comparisons on the 
object is modified.**

Breaks use of cookie within collections such as when using contains().

Traced problem back to parent class NameValuePair.  Cookie.hashCode() calls 
NameValuePair.hashCode() which relies on name/value hashes.  Cookie does not 
rely on value to determine equality."
1,"no-cache directives with field names are transmitted downstream. ""Field names MUST NOT be included with the no-cache directive in a request.""

http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.4

Currently, the cache implementation allows a request containing something like:
    Cache-Control: no-cache=""Content-Location""
to be passed downstream towards the origin.

This is another one of those tricky situations where our client has passed us a non-compliant request.
"
1,"minimizeHopcroft OOMEs on smallish (2096 states, finite) automaton. Not sure what's up w/ this... if you check out the blocktree branch (LUCENE-3030) and comment out the @Ignore in TestTermsEnum2.testFiniteVersusInfinite then this should hit OOME: {[ant test-core -Dtestcase=TestTermsEnum2 -Dtestmethod=testFiniteVersusInfinite -Dtests.seed=-2577608857970454726:-2463580050179334504}}"
1,"Http Authentication with invalid credentials causes infinite loop. At HttpMethodBase(460), a break statement is executed only if
log.isInfoEnabled(). The break statement needs to be moved outside of the if
statement so that it breaks if realms already contains foo. Patch submitted on
mailing list as per Apache site guidelines."
1,"NodeIdImpl is not really serializable . I've been trying to get jcr2spi - rmi - spi2jcr to work. 

The error I'm seeing is reported as:
java.io.NotSerializableException: org.apache.jackrabbit.spi.commons.identifier.IdFactoryImpl

I believe I tracked this down.  It is because NodeIdImpl is implicitly referencing its containing instance IdFactoryImpl which is not serializable.

NodeIdImpl is attempted to be serialized, in my case, with the following stack:

at org.apache.jackrabbit.spi.rmi.client.ClientRepositoryService.getItemInfos(ClientRepositoryService.java:258)
at org.apache.jackrabbit.jcr2spi.state.WorkspaceItemStateFactory.createNodeState(WorkspaceItemStateFactory.java:94)
at org.apache.jackrabbit.jcr2spi.state.TransientISFactory.createNodeState(TransientISFactory.java:99)
at org.apache.jackrabbit.jcr2spi.hierarchy.NodeEntryImpl.doResolve(NodeEntryImpl.java:972)
at org.apache.jackrabbit.jcr2spi.hierarchy.HierarchyEntryImpl.resolve(HierarchyEntryImpl.java:95)
at org.apache.jackrabbit.jcr2spi.hierarchy.HierarchyEntryImpl.getItemState(HierarchyEntryImpl.java:212)
at org.apache.jackrabbit.jcr2spi.ItemManagerImpl.getItem(ItemManagerImpl.java:170)
at org.apache.jackrabbit.jcr2spi.SessionImpl.getRootNode(SessionImpl.java:216)

I think I must be doing something wrong, because it seems like this is a fundamental problem with doing jcr2spi - rmi - spi2jcr, and looking at the SVN history I don't see how this ever could have worked.  
So either session.getRootNode() has never been tested using jcr2spi - rmi - spi2jcr, or I've got something setup wrong."
1,"We should never open an IndexInput when an IndexOutput is still open. I modified MockDirWrapper to assert this (except for
segments_N/segments.gen, where it's expected), and, it uncovered a
couple of places involving NRT readers where we open a shared doc
store file that's still open for writing.

First, if you install a merged segment warmer, we were failing to
force the merge of the doc stores in this case, thus potentially
opening the same doc stores that are also still open for writing.

Second, if you're actively adding docs in other threads when you call
IW.getReader(), the other threads could sneak in and flush new
segments sharing the doc stores.  The returned reader then opens the
doc store files that are still open for writing.
"
1,"Parametrizing H1 and H2. The DFR normalizations {{H1}} and {{H2}} are parameter-free. This is in line with the [original article|http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.101.742], but not with the [thesis|http://theses.gla.ac.uk/1570/], where H2 accepts a {{c}} parameter, nor with [information-based models|http://dl.acm.org/citation.cfm?id=1835490], where H1 also accepts a {{c}} parameter."
1,"PostMethod#setParameter. [HttpClient2.0-rc1]

-------- code fragment 1 -------------------------
PostMethod method = new PostMethod(uriString);
method.addParameter(""tel"", ""1111-1111"");
method.addParameter(""tel"", ""2222-2222"");
method.setParameter(""tel"", ""3333-3333"");

(post data sent)
tel=1111-1111&tel=2222-2222&tel=3333-3333

(post data i hope)
tel=3333-3333
-------------------------------------------------

---------------- code fragment 2 -----------------
PostMethod method = new PostMethod(uriString);
method.addParameter(""tel"", ""1111-1111"");
method.addParameter(""tel"", ""2222-2222"");
method.addParameter(""tel"", ""3333-3333"");

(post data sent)
tel=1111-1111&tel=2222-2222&tel=3333-3333
--------------------------------------------------

what difference between code 1 and code2 ?

sorry for my poor english."
1,"Index recovery may fail with IllegalArgumentException. When repeatedly killed and started up again, jackrabbit may throw an IllegalArgumentException on index recovery:

Caused by: java.lang.IllegalArgumentException: already contains: _c
   at org.apache.jackrabbit.core.query.lucene.IndexInfos.addName(IndexInfos.java:170)
   at org.apache.jackrabbit.core.query.lucene.MultiIndex.deleteIndex(MultiIndex.java:716)
   at org.apache.jackrabbit.core.query.lucene.MultiIndex$DeleteIndex.execute(MultiIndex.java:1553)
   at org.apache.jackrabbit.core.query.lucene.MultiIndex.executeAndLog(MultiIndex.java:809)
   at org.apache.jackrabbit.core.query.lucene.MultiIndex.flush(MultiIndex.java:740)
   at org.apache.jackrabbit.core.query.lucene.Recovery.run(Recovery.java:160)
"
1,"XercesImpl is missing in WebDav contrib project. $ /usr/local/maven/bin/maven
 __  __
|  \/  |__ _Apache__ ___
| |\/| / _` \ V / -_) ' \  ~ intelligent projects ~
|_|  |_\__,_|\_/\___|_||_|  v. 1.0.2

build:start:

multiproject:install:
multiproject:projects-init:
    [echo] Gathering project list
Starting the reactor...
Our processing order:
JCRWebdavServer Webdav Library
JCRWebdavServer Server Library
JCRWebdavServer Client Library
JCRWebdavServer WebApplication
+----------------------------------------
| Gathering project list JCRWebdavServer Webdav Library
| Memory: 3M/4M
+----------------------------------------
+----------------------------------------
| Gathering project list JCRWebdavServer Server Library
| Memory: 3M/4M
+----------------------------------------
+----------------------------------------
| Gathering project list JCRWebdavServer Client Library
| Memory: 3M/4M
+----------------------------------------
+----------------------------------------
| Gathering project list JCRWebdavServer WebApplication
| Memory: 3M/4M
+----------------------------------------
Starting the reactor...
Our processing order:
JCRWebdavServer Webdav Library
JCRWebdavServer Server Library
JCRWebdavServer Client Library
JCRWebdavServer WebApplication
+----------------------------------------
| Executing multiproject:install-callback JCRWebdavServer Webdav Library
| Memory: 3M/4M
+----------------------------------------
Attempting to download jackrabbit-commons-1.0-SNAPSHOT.jar.
Response content length is not known
Artifact /org.apache.jackrabbit/jars/jackrabbit-commons-1.0-SNAPSHOT.jar doesn't exists in remote repository, but it exists locally

multiproject:goal:
build:start:

multiproject:install-callback:
    [echo] Running jar:install for JCRWebdavServer Webdav Library
java:prepare-filesystem:

java:compile:
    [echo] Compiling to /home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/target/classes
    [javac] Compiling 109 source files to /home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/target/classes
/home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/src/java/org/apache/jackrabbit/webdav/WebdavResponseImpl.java:26: package org.apache.xml.serialize does not exist
import org.apache.xml.serialize.OutputFormat;
                                ^
/home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/src/java/org/apache/jackrabbit/webdav/WebdavResponseImpl.java:27: package org.apache.xml.serialize does not exist
import org.apache.xml.serialize.XMLSerializer;
                                ^
/home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/src/java/org/apache/jackrabbit/webdav/WebdavResponseImpl.java:149: cannot resolve symbol
symbol  : class OutputFormat 
location: class org.apache.jackrabbit.webdav.WebdavResponseImpl
                OutputFormat format = new OutputFormat(""xml"", ""UTF-8"", true);
                ^
/home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/src/java/org/apache/jackrabbit/webdav/WebdavResponseImpl.java:149: cannot resolve symbol
symbol  : class OutputFormat 
location: class org.apache.jackrabbit.webdav.WebdavResponseImpl
                OutputFormat format = new OutputFormat(""xml"", ""UTF-8"", true);
                                          ^
/home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/src/java/org/apache/jackrabbit/webdav/WebdavResponseImpl.java:150: cannot resolve symbol
symbol  : class XMLSerializer 
location: class org.apache.jackrabbit.webdav.WebdavResponseImpl
                XMLSerializer serializer = new XMLSerializer(out, format);
                ^
/home/jeremi/src/jackrabbit/trunk/contrib/jcr-server/webdav/src/java/org/apache/jackrabbit/webdav/WebdavResponseImpl.java:150: cannot resolve symbol
symbol  : class XMLSerializer 
location: class org.apache.jackrabbit.webdav.WebdavResponseImpl
                XMLSerializer serializer = new XMLSerializer(out, format);
                                               ^
Note: Some input files use or override a deprecated API.
Note: Recompile with -deprecation for details.
6 errors

BUILD FAILED
File...... /home/jeremi/.maven/cache/maven-multiproject-plugin-1.3.1/plugin.jelly
Element... maven:reactor
Line...... 217
Column.... -1
Unable to obtain goal [multiproject:install-callback] -- /home/jeremi/.maven/cache/maven-java-plugin-1.5/plugin.jelly:63:-1: <ant:javac> Compile failed; see the compiler error output for details.
Total time: 8 seconds
Finished at: Fri Jan 27 07:14:37 CET 2006

$"
1,"cache getting out of sync with transientstore causes pathnotfoundexception. Done some further debugging and think the problem is in the synchronization between cache and transientstore. When I retrieve a childnode when I just made its parent node transient (by removing a prop or something), it will not be added to the cache. When I then remove this node, its nodeid is not removed from cache since its stateId wasn't saved in the cache.  After that I add the same node node again with the same name. When I now try to retrieve this node, I get a path not found exception. I see that by retrieving it, its nodeit is resolved from the cache using its path. Only since the removed node was not removed from cache it returns the nodeid of the already removed node. There is no node present with this id in the transientstore and therefor it throws a pathnotfoundexception.

provided a failing junit test and repository.xml"
1,"Destination header not containing URI scheme causes NPE. In WebDAVRequestImpl. getDestinationLocator assumes that URI.getAuthority is always non-null.

In RFC2518, a full URI is indeed required, but the NPE causes a status of 500, instead of 400 as expected.

In RFC4918, an absolute path is allowed.

Proposal: delegate to gethrefLocator, which already does the right thing.
"
1,"non-contiguous LogMergePolicy should be careful to not select merges already running. Now that LogMP can do non-contiguous merges, the fact that it disregards which segments are already being merged is more problematic since it could result in it returning conflicting merges and thus failing to run multiple merges concurrently.
"
1,"NPE in PhraseQuery.toString(String f). the section

public String toString(String f) {
    StringBuffer buffer = new StringBuffer();
    if (!field.equals(f)) {
      buffer.append(field);
      buffer.append("":"");
    }
    <snip>


should be

public String toString(String f) {
    StringBuffer buffer = new StringBuffer();
    if (field != null && !field.equals(f)) {
      buffer.append(field);
      buffer.append("":"");
    }
    <snip>


The issue arises if a phrase query is created, no terms are added, then the phrase query is added to a boolean query. Calling toString on the boolean query will result in a NPE insdie of the PhraseQuery.
"
1,"FSDirectory.copy() impl is unsafe. There are a couple of issues with it:

# FileChannel.transferFrom documents that it may not copy the number of bytes requested, however we don't check the return value. So need to fix the code to read in a loop until all bytes were copied..
# When calling addIndexes() w/ very large segments (few hundred MBs in size), I ran into the following exception (Java 1.6 -- Java 1.5's exception was cryptic):
{code}
Exception in thread ""main"" java.io.IOException: Map failed
    at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:770)
    at sun.nio.ch.FileChannelImpl.transferToTrustedChannel(FileChannelImpl.java:450)
    at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:523)
    at org.apache.lucene.store.FSDirectory.copy(FSDirectory.java:450)
    at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:3019)
Caused by: java.lang.OutOfMemoryError: Map failed
    at sun.nio.ch.FileChannelImpl.map0(Native Method)
    at sun.nio.ch.FileChannelImpl.map(FileChannelImpl.java:767)
    ... 7 more
{code}

I changed the impl to something like this:
{code}
long numWritten = 0;
long numToWrite = input.size();
long bufSize = 1 << 26;
while (numWritten < numToWrite) {
  numWritten += output.transferFrom(input, numWritten, bufSize);
}
{code}

And the code successfully adds the indexes. This code uses chunks of 64MB, however that might be too large for some applications, so we definitely need a smaller one. The question is how small so that performance won't be affected, and it'd be great if we can let it be configurable, however since that API is called by other API, such as addIndexes, not sure it's easily controllable.

Also, I read somewhere (can't remember now where) that on Linux the native impl is better and does copy in chunks. So perhaps we should make a Linux specific impl?"
1,"HttpMethodDirector fails when redirecting to a encoded URL location. When HttpMethodDirector handles the case of redirecting the incoming connection to the location specified in the header of the http caller method, if this location has any ""special"" charset encoding (extended charsets like ISO 8859-1,etc.) the redirection fails this way:

dd-MMM-YYYY hh:mm:ss org.apache.commons.httpclient.HttpMethodDirector processRedirectResponse
WARNING: Redirected location 'http://www.anyCharsetEncodedUrl.ko' is malformed


You can test it using this class:


public class SimpleHttpTestNotWorking {

	public static int urlStatus(String pUrl) throws org.apache.commons.httpclient.HttpException,java.io.IOException{
		org.apache.commons.httpclient.HttpClient client = new org.apache.commons.httpclient.HttpClient();
		org.apache.commons.httpclient.HttpMethod method = new org.apache.commons.httpclient.methods.GetMethod(pUrl);
		return client.executeMethod(method);
	}
		
	public static void main(String[] args) {
		try{
			String url = ""http://www.dipualba.es/municipios/F%E9rez""; //known problematic URL
			System.out.println(""Return code for [""+url+""]: ""+SimpleHttpTestWorking.urlStatus(url));
		}catch(Exception e){
			e.printStackTrace();
		}
	}
}


What I've done to solve it for my particular case has been:


1) In the requester side, I've modified the calling:


public class SimpleHttpTestWorking {

	public static int urlStatus(String pUrl) throws org.apache.commons.httpclient.HttpException,java.io.IOException{
		org.apache.commons.httpclient.HttpClient client = new org.apache.commons.httpclient.HttpClient();
		org.apache.commons.httpclient.HttpMethod method;
	    String encoding = (String)client.getParams().getParameter(""http.protocol.content-charset"");
	    client.getParams().setParameter(""http.protocol.element-charset"", encoding);
	    try{
	    	method = new org.apache.commons.httpclient.methods.GetMethod(pUrl);
	    }catch(IllegalArgumentException iae){
		    try{
		    	org.apache.commons.httpclient.URI uri = new org.apache.commons.httpclient.URI(pUrl,true);
		    	method = new org.apache.commons.httpclient.methods.GetMethod(uri.getURI());
		    }catch(org.apache.commons.httpclient.URIException ue){
		    	org.apache.commons.httpclient.URI uri = new org.apache.commons.httpclient.URI(pUrl,false,encoding);
			    method = new org.apache.commons.httpclient.methods.GetMethod(uri.getEscapedURI());
		    }		    	
	    }		
		return client.executeMethod(method);
	}
			
	public static void main(String[] args) {
		try{
			String url = ""http://www.dipualba.es/municipios/Férez""; //the same problematic URL
			System.out.println(""Return code for [""+url+""]: ""+SimpleHttpTestWorking.urlStatus(url));
		}catch(Exception e){
			e.printStackTrace();
		}
	}
}


2) In org.apache.commons.httpclient.HttpMethodDirector.processRedirectResponse(HttpMethod method) , I've replaced


...
redirectUri = new URI(location, true);
...


for the following code:


...
/*
 * [2006-11-14] 
 * Handles redirections to encoded URI locations 
 * (only if URI and Connection encoding charset has been properly setted)
 * */ 
try{
	redirectUri = new URI(location, true);
}catch(URIException ue){
	Object encoding = this.conn.getParams().getParameter(""http.protocol.element-charset"");
	if(encoding != null){
		redirectUri = new URI(location, false, (String)encoding);
	}else{
		throw ue;
	}
}
...



Hope it helps!"
1,"NullPointerException in IndexModifier.close(). We upgraded from Lucene 2.0.0. to 2.3.1 hoping this would resolve this issue.

http://jira.codehaus.org/browse/MRM-715

Trace is as below for Lucene 2.3.1:
java.lang.NullPointerException
at org.apache.lucene.index.IndexModifier.close(IndexModifier.java:576)
at org.apache.maven.archiva.indexer.lucene.LuceneRepositoryContentIndex.closeQuietly(LuceneRepositoryContentIndex.java:416)
at org.apache.maven.archiva.indexer.lucene.LuceneRepositoryContentIndex.modifyRecord(LuceneRepositoryContentIndex.java:152)
at org.apache.maven.archiva.consumers.lucene.IndexContentConsumer.processFile(IndexContentConsumer.java:169)
at org.apache.maven.archiva.repository.scanner.functors.ConsumerProcessFileClosure.execute(ConsumerProcessFileClosure.java:51)
at org.apache.commons.collections.functors.IfClosure.execute(IfClosure.java:117)
at org.apache.commons.collections.CollectionUtils.forAllDo(CollectionUtils.java:388)
at org.apache.maven.archiva.repository.scanner.RepositoryContentConsumers.executeConsumers(RepositoryContentConsumers.java:283)
at org.apache.maven.archiva.proxy.DefaultRepositoryProxyConnectors.transferFile(DefaultRepositoryProxyConnectors.java:597)
at org.apache.maven.archiva.proxy.DefaultRepositoryProxyConnectors.fetchFromProxies(DefaultRepositoryProxyConnectors.java:157)
at org.apache.maven.archiva.web.repository.ProxiedDavServer.applyServerSideRelocation(ProxiedDavServer.java:447)
at org.apache.maven.archiva.web.repository.ProxiedDavServer.fetchContentFromProxies(ProxiedDavServer.java:354)
at org.apache.maven.archiva.web.repository.ProxiedDavServer.process(ProxiedDavServer.java:189)
at org.codehaus.plexus.webdav.servlet.multiplexed.MultiplexedWebDavServlet.service(MultiplexedWebDavServlet.java:119)
at org.apache.maven.archiva.web.repository.RepositoryServlet.service(RepositoryServlet.java:155)
at javax.servlet.http.HttpServlet.service(HttpServlet.java:803)"
1,"org.apache.lucene.analysis.cn.ChineseTokenizer missing offset decrement. Apparently, in ChineseTokenizer, offset should be decremented like bufferIndex
when Character is OTHER_LETTER.  This directly affects startOffset and endOffset
values.

This is critical to have Highlighter working correctly because Highlighter marks
matching text based on these offset values."
1,JCR2SPI: incomplete changelog when combining move with removal of new destination parent. 
1,"Intermitted failure on DocValues branch. I lately ran into two random failures on the CSF branch that seem not to be related to docValues but I can't reproduce them neither on docvalues branch nor on trunk.

{code}
jError Message

IndexFileDeleter doesn't know about file _1e.tvx
Stacktrace

junit.framework.AssertionFailedError: IndexFileDeleter doesn't know about file _1e.tvx
	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:979)
	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:917)
	at org.apache.lucene.index.IndexWriter.filesExist(IndexWriter.java:3633)
	at org.apache.lucene.index.IndexWriter.startCommit(IndexWriter.java:3699)
	at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2407)
	at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2478)
	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2460)
	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2444)
	at org.apache.lucene.index.TestIndexWriterExceptions.testRandomExceptionsThreads(TestIndexWriterExceptions.java:213)
Standard Output

NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterExceptions -Dtestmethod=testRandomExceptionsThreads -Dtests.seed=-6528669668419768890:4860241142852689334 -Dtests.codec=randomPerField -Dtests.multiplier=3
NOTE: test params are: codec=PreFlex, locale=sv, timezone=Atlantic/South_Georgia
Standard Error

NOTE: all tests run in this JVM:
[TestDemo, TestToken, TestBinaryDocument, TestCodecs, TestDirectoryReader, TestIndexInput, TestIndexWriterExceptions]
{code}

and

{code}

[junit] Testsuite: org.apache.lucene.index.TestIndexReaderReopen
    [junit] Testcase: testThreadSafety(org.apache.lucene.index.TestIndexReaderReopen):	Caused an ERROR
    [junit] MockDirectoryWrapper: cannot close: there are still open files: {_4_3.frq=1, _4_3.pos=1, _4_0.frq=1, _4_0.prx=1, _4.pst=1, _4_3.pyl=1, _4_3.skp=1, _4_0.tis=1, _4_3.doc=1, _4_3.tis=1}
    [junit] java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_4_3.frq=1, _4_3.pos=1, _4_0.frq=1, _4_0.prx=1, _4.pst=1, _4_3.pyl=1, _4_3.skp=1, _4_0.tis=1, _4_3.doc=1, _4_3.tis=1}
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:387)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen.testThreadSafety(TestIndexReaderReopen.java:859)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:979)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:917)
    [junit] Caused by: java.lang.RuntimeException: unclosed IndexInput
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.openInput(MockDirectoryWrapper.java:342)
    [junit] 	at org.apache.lucene.store.Directory.openInput(Directory.java:122)
    [junit] 	at org.apache.lucene.index.codecs.standard.StandardPostingsReader.<init>(StandardPostingsReader.java:49)
    [junit] 	at org.apache.lucene.index.codecs.standard.StandardCodec.fieldsProducer(StandardCodec.java:87)
    [junit] 	at org.apache.lucene.index.PerFieldCodecWrapper$FieldsReader.<init>(PerFieldCodecWrapper.java:119)
    [junit] 	at org.apache.lucene.index.PerFieldCodecWrapper.fieldsProducer(PerFieldCodecWrapper.java:211)
    [junit] 	at org.apache.lucene.index.SegmentReader$CoreReaders.<init>(SegmentReader.java:137)
    [junit] 	at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:532)
    [junit] 	at org.apache.lucene.index.SegmentReader.get(SegmentReader.java:509)
    [junit] 	at org.apache.lucene.index.DirectoryReader.<init>(DirectoryReader.java:238)
    [junit] 	at org.apache.lucene.index.DirectoryReader.doReopen(DirectoryReader.java:500)
    [junit] 	at org.apache.lucene.index.DirectoryReader.access$000(DirectoryReader.java:48)
    [junit] 	at org.apache.lucene.index.DirectoryReader$2.doBody(DirectoryReader.java:493)
    [junit] 	at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:623)
    [junit] 	at org.apache.lucene.index.DirectoryReader.doReopenNoWriter(DirectoryReader.java:488)
    [junit] 	at org.apache.lucene.index.DirectoryReader.doReopen(DirectoryReader.java:446)
    [junit] 	at org.apache.lucene.index.DirectoryReader.reopen(DirectoryReader.java:406)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$9.run(TestIndexReaderReopen.java:770)
    [junit] 	at org.apache.lucene.index.TestIndexReaderReopen$ReaderThread.run(TestIndexReaderReopen.java:897)
    [junit] 
    [junit] 
    [junit] Tests run: 17, Failures: 0, Errors: 1, Time elapsed: 13.766 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexReaderReopen -Dtestmethod=testThreadSafety -Dtests.seed=-5455993123574190959:-1935535300313439968 -Dtests.codec=randomPerField -Dtests.multiplier=3
    [junit] NOTE: test params are: codec=RandomCodecProvider: {field5=MockVariableIntBlock(baseBlockSize=29), id=Standard, fielda=Standard, field4=MockFixedIntBlock(blockSize=924), field3=Standard, field2=SimpleText, id2=Standard, field6=MockSep, field1=Pulsing(freqCutoff=8)}, locale=zh_CN, timezone=Asia/Hovd
{code}

I haven't seen those before - let me know if you have!"
1,"Benchmark alg line -  {[AddDoc(4000)]: 4} : * - causes an infinite loop. Background in http://www.mail-archive.com/java-dev@lucene.apache.org/msg10831.html 
The line  
   {[AddDoc(4000)]: 4} : * 
causes an infinite loop because the parallel sequence would mask the exhaustion from the outer sequential sequence.

To fix this the DocMaker exhaustion check should be modified to rely  on the doc maker instance only, and to be reset when the inputs are being reset. "
1,"BooleanQuery can not find all matches in special condition. query: (name:tang*)
doc=5137 score=1.0  doc:Document<stored,indexed<name:tangfulin>>
doc=11377 score=1.0  doc:Document<stored,indexed<name:tangfulin>>
query: name:tang* name:notexistnames
doc=5137 score=0.048133932  doc:Document<stored,indexed<name:tangfulin>>

It is two queries on the same index, one is just a prefix query in a
boolean query, and the other is a prefix query plus a term query in a
boolean query, all with Occur.SHOULD .

what I wonder is why the later query can not find the doc=11377 doc ?

the problem can be repreduced by the code in the attachment ."
1,"Node.getPath() will corrupt the session. When calling Node.getPath() anytime, no mather if its before or after save, and when deleting nodes, the internal reference points to the wrong nodes. 
The attached test will always fail with a javax.jcr.RepositoryException: /: cannot remove root node. 
We have seen other configurations where a node suddenly behaves as the another node that has references and throw a reference exception, and yet other configurations where the node we though we deleted still exists, and another node has now disappeared.

I do not know what causes the bug,a good bet is perhaps the CachingHierarchyManager?. It was not present in Jackrabbit 1.0.1, but was introduced in 1.1.

Have also tested the latest release: 1.2.2, and the bug is still present there.
"
1,"Large Lucene index can hit false OOM due to Sun JRE issue. This is not a Lucene issue, but I want to open this so future google
diggers can more easily find it.

There's this nasty bug in Sun's JRE:

  http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6478546

The gist seems to be, if you try to read a large (eg 200 MB) number of
bytes during a single RandomAccessFile.read call, you can incorrectly
hit OOM.  Lucene does this, with norms, since we read in one byte per
doc per field with norms, as a contiguous array of length maxDoc().

The workaround was a custom patch to do large file reads as several
smaller reads.

Background here:

  http://www.nabble.com/problems-with-large-Lucene-index-td22347854.html
"
1,"Wire produces invalid log skipping zero bytes in certain cases. WireLogInputStream class line 82 check if the byte returned is not -1 meaning end of stream. But the condition is wrong in case if this byte is 0, it should look like

if (l != -1) {
//...
}
"
1,"[PATCH] Loosing first matching document in BooleanQuery. This patch fixes loosing of first matching document when BooleanQuery
with BooleanClause.Occur.SHOULD is added to another BooleanQuery."
1,"SQL Parser fails with SQL 92 timestamp format. The SQL query parser fails with an exception if the SQL 92 timestamp format is used.

E.g:
... WHERE my:date > TIMESTAMP '1976-01-01 00:00:00.000+01:00'

does not work, but the following will succeed using ISO8601:

... WHERE my:date > TIMESTAMP '1976-01-01T00:00:00.000+01:00'"
1,"failing Node.lock() might leave inconsistent transient state. When I try to node.lock(true, false) a node and the lock fails due to lak of user privilegies, the lock stay in the user transient session. If a perform a node.refresh(false) the node still is locked in the transient session."
1,"AbstractJournal doesn't create deep paths for revision files. AbstractJournal throws when trying to create the revision file if the directory the revision file is in doesn't already exist. When initializing a repository during its startup, the create fails is you use a revision param like <param name=""revision"" value=""${rep.home}/repository/revision"" /> because the repository directory hasn't been created yet. Attached is a repository.xml that demonstrates. It uses Oracle for FS and PMs."
1,"BooleanQuery.hashCode and equals ignore isCoordDisabled. BooleanQuery.isCoordDisabled() is not considered by BooleanQuery's hashCode() or equals() methods ... this can cause serious badness to happen when caching BooleanQueries.

bug traces back to at least 1.9"
1,"highlighting exact phrase with overlapping tokens fails.. Fields with overlapping token are not highlighted in search results when searching exact phrases, when using TermVector.WITH_OFFSET.

The document builded in MemoryIndex for highlight does not preserve positions of tokens in this case. Overlapping tokens get ""flattened"" (position increment always set to 1), the spanquery used for searching relevant fragment will fail to identify the correct token sequence because the position shift.

I corrected this by adding a position increment calculation in sub class StoredTokenStream. I added junit test covering this case.

I used the eclipse codestyle from trunk, but style add quite a few format differences between repository and working copy files. I tried to reduce them, but some linewrapping rules still doesn't match.

Correction patch joined"
1,"NPE Thrown when two Cluster Nodes are hitting the same underlying database.. I've created a test that creates two repositories with clustering enabled that are backed by the same database.  Using the following workflow causes a NullPointerException to be thrown.

The workflow I'm using is:
The root node is versioned.
ClusterNode1 creates a versioned child node named ""foo"".
The test waits to make sure the syncDelay has passed so ClusterNode2 will notice the newly created node.
ClusterNode2 retrieves the ""foo"" child node and removes it.
The test waits for the change ClusterNode1 to sync with that change.
ClusterNode1 tries to create another new node however a NullPointerException is thrown when the it tries to checkout the rootNode.

java.lang.NullPointerException: null values not allowed
	at org.apache.commons.collections.map.AbstractReferenceMap.put(AbstractReferenceMap.java:251)
	at org.apache.jackrabbit.core.version.VersionManagerImpl.getItem(VersionManagerImpl.java:280)
	at org.apache.jackrabbit.core.version.XAVersionManager.getItem(XAVersionManager.java:334)
	at org.apache.jackrabbit.core.version.AbstractVersionManager.getVersion(AbstractVersionManager.java:87)
	at org.apache.jackrabbit.core.NodeImpl.getBaseVersion(NodeImpl.java:3198)
	at org.apache.jackrabbit.core.NodeImpl.checkout(NodeImpl.java:2991)
	at com.cerner.system.configuration.repository.jcr.SimpleJackrabbitConflictTest.testNullPointerExceptionThrown(SimpleJackrabbitConflictTest.java:96)"
1,"StaleItemStateException with distributed transactions. There seams to be a serious bug in jackrabbit when used in distributed transactions. It does not occur with local transactions! And it seams to be related to JCR-566.

There are 2 scenarios where a StaleItemStateException occurs reproducible that causes transactions to fail. All my operations (implemented in a custom ServiceBean) such as setProperty() or deleteNode() run in separate transactions. The transactions are configured through Spring Annotations (@Transactional).

Scenario A (setProperty):
(1) multiple setProperty() with same property name on the same node (newly created or already existent)
=> With the 3. setProperty() (and sometimes also the 5.), a StaleItemStateException for the property state is raised when the transaction is commited. Following setProperty invocations will not fail!

Scenario B (deleteNode):
(1) iterate 10 times:
(1.1) create new node n and a subnode for n
(1.2) delete node n
=> Deletion of node n raises a StaleItemStateException for node n in iteration 1, 3 and (6 or 7), when the related transaction is commited. Following deletions of node n will also fail with a predictable pattern.

The Exception trace for scenario A (it's the same for scenario B, with one difference: StaleItemStateException is raised for the node and not for the property):

org.springframework.transaction.UnexpectedRollbackException: JTA transaction unexpectedly rolled back (maybe due to a timeout); nested exception is javax.transaction.RollbackException: Error during one-phase commit
	at org.springframework.transaction.jta.JtaTransactionManager.doCommit(JtaTransactionManager.java:1031)
	at org.springframework.transaction.support.AbstractPlatformTransactionManager.processCommit(AbstractPlatformTransactionManager.java:709)
	at org.springframework.transaction.support.AbstractPlatformTransactionManager.commit(AbstractPlatformTransactionManager.java:678)
	at org.springframework.transaction.interceptor.TransactionAspectSupport.commitTransactionAfterReturning(TransactionAspectSupport.java:321)
	at org.springframework.transaction.interceptor.TransactionInterceptor.invoke(TransactionInterceptor.java:116)
	at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:171)
	at org.springframework.aop.framework.JdkDynamicAopProxy.invoke(JdkDynamicAopProxy.java:204)
	at $Proxy9.setNodeProperty(Unknown Source)
	at de.zeb.control.prototype.jrTxBug.test.TestJackrabbitTxBug.testTransactionBug001(TestJackrabbitTxBug.java:97)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:585)
	at org.testng.internal.MethodHelper.invokeMethod(MethodHelper.java:580)
	at org.testng.internal.Invoker.invokeMethod(Invoker.java:478)
	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:607)
	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:874)
	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:125)
	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109)
	at org.testng.TestRunner.runWorkers(TestRunner.java:689)
	at org.testng.TestRunner.privateRun(TestRunner.java:566)
	at org.testng.TestRunner.run(TestRunner.java:466)
	at org.testng.SuiteRunner.runTest(SuiteRunner.java:301)
	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:296)
	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:276)
	at org.testng.SuiteRunner.run(SuiteRunner.java:191)
	at org.testng.TestNG.createAndRunSuiteRunners(TestNG.java:808)
	at org.testng.TestNG.runSuitesLocally(TestNG.java:776)
	at org.testng.TestNG.run(TestNG.java:701)
	at org.testng.remote.RemoteTestNG.run(RemoteTestNG.java:73)
	at org.testng.remote.RemoteTestNG.main(RemoteTestNG.java:124)
Caused by: javax.transaction.RollbackException: Error during one-phase commit
	at org.apache.geronimo.transaction.manager.TransactionImpl.commit(TransactionImpl.java:281)
	at org.apache.geronimo.transaction.manager.TransactionManagerImpl.commit(TransactionManagerImpl.java:143)
	at org.apache.geronimo.transaction.context.InheritableTransactionContext.complete(InheritableTransactionContext.java:196)
	at org.apache.geronimo.transaction.context.InheritableTransactionContext.commit(InheritableTransactionContext.java:146)
	at org.apache.geronimo.transaction.context.OnlineUserTransaction.commit(OnlineUserTransaction.java:80)
	at org.jencks.factory.UserTransactionFactoryBean$GeronimoUserTransaction.commit(UserTransactionFactoryBean.java:118)
	at org.springframework.transaction.jta.JtaTransactionManager.doCommit(JtaTransactionManager.java:1028)
	... 30 more
Caused by: javax.transaction.xa.XAException
	at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:155)
	at org.apache.jackrabbit.core.XASessionImpl.commit(XASessionImpl.java:337)
	at org.apache.jackrabbit.jca.TransactionBoundXAResource.commit(TransactionBoundXAResource.java:39)
	at org.apache.geronimo.transaction.manager.WrapperNamedXAResource.commit(WrapperNamedXAResource.java:47)
	at org.apache.geronimo.transaction.manager.TransactionImpl.commit(TransactionImpl.java:272)
	... 36 more
Caused by: org.apache.jackrabbit.core.TransactionException: Unable to prepare transaction.
	at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:150)
	at org.apache.jackrabbit.core.TransactionContext.prepare(TransactionContext.java:138)
	... 40 more
Caused by: org.apache.jackrabbit.core.state.StaleItemStateException: bef3c056-bc91-4195-a35c-aa184182b5ad/{}TEST_PROPERTY has been modified externally
	at org.apache.jackrabbit.core.state.SharedItemStateManager$Update.begin(SharedItemStateManager.java:620)
	at org.apache.jackrabbit.core.state.SharedItemStateManager.beginUpdate(SharedItemStateManager.java:843)
	at org.apache.jackrabbit.core.state.XAItemStateManager.prepare(XAItemStateManager.java:144)
	... 41 more


When debugging into jackrabbit you will see, that the cause of the StaleItemStateException is, that the local state und the overlayed state differ in the value of the 'modCount' attribute: modCount of local state is lower than modCount of overlayed state. Perhaps its a state caching problem...
	
I'm attaching a simple java application configured with maven and ready to run standalone. The JCA container of JBoss is therefore replaced with jencks in order to support distributed transactions. The configured repository uses the InMemPersistenceManager. Both scenarios are implemented in a TestNG - test, that catches the occuring TransactionExceptions and prints out the stacktrace. Therefore you will see the exceptions, but the tests will not fail."
1,"TransientFileFactory may throw ConcurrentModificationException on shutdown. When Jackrabbit is stopped the shutdown hook of the TransientFileFactory iterates over all tracked temp files and deletes them. At the same time the reaper thread may still remove file references from the list of tracked temp files. This may lead to a ConcurrentModificationException in the shutdown hook:

java.util.ConcurrentModificationException
	at java.util.AbstractList$Itr.checkForComodification(Unknown Source)
	at java.util.AbstractList$Itr.next(Unknown Source)
	at org.apache.jackrabbit.util.TransientFileFactory$1.run(TransientFileFactory.java:86)
"
1,"NullPointerException on startup if IndexingQueue has pending nodes. This happens because of the newly introduced index version, which is not yet set when the IndexingQueue is instanciated."
1,"Proxy authentication error: Unexpected state: MSG_TYPE3_GENERATED when using NTLM authentication. Trying to connect to a website that requires basic authentication through a proxy that requires NTLM authentication.

Proxy authentication fails with ""Proxy authentication error: Unexpected state: MSG_TYPE3_GENERATED"".

Full wire log attached.  Code to replicate problem follows:

    private void execute() throws HttpException, IOException {
    	
    	URL targetUrl = new URL(TARGET_URL);
    	
        DefaultHttpClient httpclient = new DefaultHttpClient();

        HttpHost targetHost = new HttpHost(targetUrl.getHost()); 
        HttpHost proxyHost = new HttpHost(PROXY_HOST, PROXY_PORT); 
        
        httpclient.getParams().setParameter(ConnRoutePNames.DEFAULT_PROXY, 
        		proxyHost);

        CredentialsProvider credProvider = httpclient.getCredentialsProvider();
        
        Credentials proxyCredentials = new NTCredentials(PROXY_USER, 
        		PROXY_PASSWORD, PROXY_MACHINE, PROXY_DOMAIN);
        AuthScope proxyAuthScope = new AuthScope(proxyHost.getHostName(),
        		proxyHost.getPort());
        
        credProvider.setCredentials(proxyAuthScope, proxyCredentials);
        
        Credentials targetCredentials = new UsernamePasswordCredentials(
        		TARGET_USER, TARGET_PASSWORD);
        AuthScope targetAuthScope = new AuthScope(targetHost.getHostName(),
        		targetHost.getPort());
        
        credProvider.setCredentials(targetAuthScope, targetCredentials);
      
        HttpGet httpget = new HttpGet(targetUrl.getPath());

        HttpResponse response = httpclient.execute(targetHost, httpget);
        
        System.out.println(""response = "" + response);
        
       
    }
"
1,"empty host header with ip address. file: HttpMethodBase.java method: addHostRequestHeader

HttpClient writes an empty Host header if the host is referred using IP address.
HTTP 1.1 RFC is not too clear what should be used in this case. However, other
HTTP 1.1 implementations (e.g. Java 1.4.0) uses IP address instead of dns name
in the header.

Furthermore, some HTTP server implementations (e.g. Jetty) will return ""400 bad
request"" if it encounters an empty Host header. That may be a bug in Jetty, but
it might be a good idea to use IP address in Host header to increase compability."
1,"SegmentReader.doCommit should be sync'd; norms methods need not be sync'd. I fixed the failure in TestNRTThreads, but in the process tripped an assert because SegmentReader.doCommit isn't sync'd.

So I sync'd it, but I don't think the norms APIs need to be sync'd -- we populate norms up front and then never change them.  Un-sync'ing them is important so that in the NRT case calling IW.commit doesn't block searches trying to pull norms.

Also some small code refactoring."
1,"Two or more writers over NFS can cause index corruption. When an index is used over NFS, and, more than one machine can be a
writer such that they swap roles quickly, it's possible for the index
to become corrupt if the NFS client directory cache is stale.

Not all NFS clients will show this.  Very recent versions of Linux's
NFS client do not seem to show the issue, yet, slightly older ones do,
and the latest Mac OS X one does as well.

I've been working with Patrick Kimber, who provided a standalone test
showing the problem (thank you Patrick!).  This came out of this
thread:

  http://www.gossamer-threads.com/lists/engine?do=post_view_flat;post=50680;page=1;sb=post_latest_reply;so=ASC;mh=25;list=lucene

Note that the first issue in that discussion has been resolved
(LUCENE-948).  This is a new issue.
"
1,"WebDAV Library: VersionControlledResource constant lists wrong method. VersionControlledResource .methods_checkedIn constant lists UNCHECKOUT which is obviously
wrong.

RFC3253 lists the UNCHECKOUT method as mandatory method for checked-out vc-resource
for the 'checkout-in-place'  feature."
1,"Occasional NullPointerException in ItemManager. From time to time I see a NullPointerException in ItemManager when running ConcurrentReadWriteTest. The exception is probably caused by another session that removes the property, which has the effect that the ItemState in ItemData is set to null.

Exception in thread ""Thread-11"" java.lang.NullPointerException
	at org.apache.jackrabbit.core.ItemManager.canRead(ItemManager.java:313)
	at org.apache.jackrabbit.core.ItemManager.getItemData(ItemManager.java:293)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:226)
	at org.apache.jackrabbit.core.ItemManager.getItem(ItemManager.java:486)
	at org.apache.jackrabbit.core.LazyItemIterator.prefetchNext(LazyItemIterator.java:111)
	at org.apache.jackrabbit.core.LazyItemIterator.<init>(LazyItemIterator.java:93)
	at org.apache.jackrabbit.core.LazyItemIterator.<init>(LazyItemIterator.java:75)
	at org.apache.jackrabbit.core.ItemManager.getChildProperties(ItemManager.java:658)
	at org.apache.jackrabbit.core.NodeImpl.getProperties(NodeImpl.java:2663)
	at org.apache.jackrabbit.core.ConcurrentReadWriteTest$1$1.execute(ConcurrentReadWriteTest.java:65)
	at org.apache.jackrabbit.core.AbstractConcurrencyTest$Executor.run(AbstractConcurrencyTest.java:206)
	at java.lang.Thread.run(Thread.java:595)

This issue does not occur in a release but only in trunk."
1,"[PATCH]multiple wildcards ? at the end of search pattern return incorrect hits. The problem is if you search on ""ca??"", the hit includes 'cat', 'CA', 
etc, while the user only wants 4 letter words start with CA, such as 
'card', 'cash', to be returned. This happens only when multiple '?' at 
the end of search pattern. The solution is to check if the word that is 
matching against search pattern ends while there is still '?' left. If 
this is the case, match should return false.

Attached is the patch code I generated use 'diff'
********************************************************************

--- WildcardTermEnum.org	2004-05-11 11:42:10.000000000 -0400
+++ WildcardTermEnum.java	2004-11-08 14:35:14.823610500 -0500
@@ -132,6 +132,10 @@
             }
             else
             {
+	      //to prevent ""cat"" matches ""ca??""
+	      if(wildchar == WILDCARD_CHAR){
+		return false;
+	      }	      
               // Look at the next character
               wildcardSearchPos++;
             }
**********************************************************************"
1,"HTTPS Post Does Not Work. Using Java 1.4.1_01 on Windows 2000. An HTTPS Post results in HTTP/100-Continue 
messages. The same code posting to a non HTTPS URL works. The code populates 
the request body using a NameValuePair array."
1,"BundleDbPersistenceManager consistencyFix doesn't fix missing non system childnode  entries of the root node. The bundle check/fix mechanism completely skips the checks on the root node, but the root node can also have non system child node entries which can be broken/missing. The attached patch makes the check only check the non system child node entries of the root node. It would be nice if this patch (if/when accepted) could also be backported to the 1.5 and 1.6 branches.
"
1,"Query with document order fails when result set size > caching hierarchy manager size. When a query returns a lot of nodes in the query result and document order is enabled (which is the default) then the query will fail with error messages in the log:

*ERROR* [main] DocOrderNodeIteratorImpl: Internal error: unable to determine document order of nodes: (DocOrderNodeIteratorImpl.java, line 241)
*ERROR* [main] DocOrderNodeIteratorImpl:    Node1: /stuff/node[2]/node[13]/node9 (DocOrderNodeIteratorImpl.java, line 242)
*ERROR* [main] DocOrderNodeIteratorImpl:    Node2: /stuff/node[2]/node[13]/node5 (DocOrderNodeIteratorImpl.java, line 243)

The critical size seems to be equivalent to the cache size of the caching hierarchy manager. Attached are two test cases. The first one simply creates test nodes and the second one executes a query for those nodes. Using the cache size of 10'000 in the CachingHierarchyManager#DEFAULT_UPPER_LIMIT everything works fine, but when this value is set to 1000 (you need to re-compile the class CachingHierarchyManager) the test fails with the mentioned errors."
1,"search.jsp doesn't handle utf-8 parameters correctly. 
1.  I  cannot use WebDav client to uploaded a file whose name is in Chinese.  The file name I had is ''郭可为.txt'  and the uploaded command by the WebDav client did something like:

  ========= Outbound Message =========
PUT /op/%ED%EF%3A.txt HTTP/1.1
Host: localhost:8080
-----

 The server didn't decode it correctly -- the result is the file name got screwed and the file content was not uploaded.

2. In the default web.war module,  there is search.jsp for rendering the search page. If I type Chinese text in the search box,  search.jsp does not decode the input parameter from ISO-8859-1 to utf-8 and in turn the search engine searches wrong string.

3. The search engine does do search correctly if I hardcode the query  variable in search.jsp or do decoding the query parameter from ISO-885901 to utf-8.

"
1,Connections are not release when a recoverable exception occurs.. Please see the url for discussion details.
1,"No equals operation for Credentials implementations. I tripped across a scenario where I wanted to compare credentials, so I could
know to discard connection state (and thus any associated cookies).

Patch to follow shortly."
1,"(Parallel-)MultiSearcher: using Sort object changes the scores. Example: 
Hits hits=multiSearcher.search(query);
returns different scores for some documents than
Hits hits=multiSearcher.search(query, Sort.RELEVANCE);
(both for MultiSearcher and ParallelMultiSearcher)

The documents returned will be the same and in the same order, but the scores in the second case will seem out of order.

Inspecting the Explanation objects shows that the scores themselves are ok, but there's a bug in the normalization of the scores.

The document with the highest score should have score 1.0, so all document scores are divided by the highest score.  (Assuming the highest score was>1.0)

However, for MultiSearcher and ParallelMultiSearcher, this normalization factor is applied *per index*, before merging the results together (the merge itself is ok though).

An example: if you use
Hits hits=multiSearcher.search(query, Sort.RELEVANCE);
for a MultiSearcher with two subsearchers, the first document will have score 1.0.
The next documents from the same subsearcher will have decreasing scores.
The first document from the other subsearcher will however have score 1.0 again !

The same applies for other Sort objects, but it is less visible.

I will post a TestCase demonstrating the problem and suggested patches to solve it in a moment..."
1,"[PATCH] FilePart fails to send data on second call to send. When using a FilePart with the MultipartPostMethod and a server that requires 
authentication, the first call to FilePart.send() sends the data correctly and 
HttpClient receives an unauthorized response from the server.  If HttpClient is 
set to automatically handle authentication attempts it then attempts to send 
the FilePart again at which time the InputStream FilePart reads from is empty 
so it doesn't send any data.

Due to this, the data actually sent by HttpClient doesn't match the content 
length specified so the server continues to wait for the data and doesn't 
respond, leaving HttpClient to timeout while waiting for a response.

This occurs with the latest source from CVS as of 16 October 2002."
1,"Deadlock when concurrently committing and reading versioning states. there is a rear occation when one thread commits a transaction and another thread reads versioing related information, so that a deadlock can occurr. 

example:

Thread1:
                        ut.begin();
                        session.getWorkspace().clone(""default"", ""/content"", ""/content"", true);
                        ut.commit();

Thread2:
                        VersionHistory vh = folder.getVersionHistory();
                        VersionIterator iter = vh.getAllVersions();
                        while (iter.hasNext()) {
                            Version v = iter.nextVersion();
                        }


to fix this issue we must ensure, that methods below the shareditemstatemgr do not call higher instances (like itemmgr) again."
1,"Locking two same-name siblings and unlocking first apparently unlocks second instead.. Executing the following test that unlocks the first of two locked same-name siblings:

public void testLocking() throws RepositoryException {
       Session jcrSession = ((S1SessionImpl) session).getSession();
       Node rootNode = jcrSession.getRootNode();

       Node n1 = rootNode.addNode(""path"");
       n1.addMixin(""mix:lockable"");
       Node n2 = rootNode.addNode(""path"");
       n2.addMixin(""mix:lockable"");

       jcrSession.save();

       n1.lock(true, true);
       n2.lock(true, true);

       System.out.println(""n1.isLocked() = "" + n1.isLocked());
       System.out.println(""n2.isLocked() = "" + n2.isLocked());
       assertTrue(n1.isLocked());
       assertTrue(n2.isLocked());

       n1.save();
       n1.unlock();

       System.out.println(""n1.isLocked() = "" + n1.isLocked());
       System.out.println(""n2.isLocked() = "" + n2.isLocked());
       assertFalse(n1.isLocked());
       assertTrue(n2.isLocked());
   }

Results in:

n1.isLocked() = true
n2.isLocked() = true
n1.isLocked() = true
n2.isLocked() = false

which is wrong."
1,SQL2 ISDESCENDANTNODE BooleanQuery#TooManyClauses returns. The initial fix is not generic enough. It still fails after adding twice the max clauses count.
1,MatchAllQuery does not implement extractTerms(). This is required for rep:excerpt() functionality.
1,"Occasional ""Host connection pool not found"". I'm using HttpClient with MultiThreadedHttpConnectionManager in a crawler
application. The application issues requests to many hosts, in 10-20 parallel
request threads. Each thread creates a new GetMethod, but all threads use the
same instance of HttpClient, created once with a multi-threaded manager.

The code in each thread looks like this:

GetMethod get = new GetMethod(url);
try {
  int code = getSharedHttpClient().executeMethod(get);
  // ... read response, do stuff
} finally {
  get.releaseConnection();
}


From time to time I get an error like this:

Host connection pool not found, hostConfig=HostConfiguration[host=http://a.b.c]

where the url is a random url from my fetch list. I looked into the source code
of the nightly release (MultiThreadedHttpConnectionManager.java:979), but the
comment there is not enlightening... ;-) Any help or suggestions for further
debugging would be appreciated."
1,"Database connection leak with DBCP, MySQL, and Observers. When using DBCP and MySQL with an observer that modifies the content repository, we are seeing abandoned connections in our connection pool."
1,"SharedItemStateManager not properly synchronized. Some time ago we removed synchronized modifiers from the methods store() hasItemState() and getItemState(). While some care has been taken to ensure the cache integrity, I think the contract for the SharedItemStateManager (SISM) is now broken. The JavaDoc does not clearly document this, but I think all relevant methods of the SISM working on ItemStates should be atomic.

E.g. a call to hasItemState() should not return true for an ItemState that another thread is currently adding in store(). Similarly a getItemState() should not return an ItemState that is currenly added or modified in a store() operation.

Currently I see two options:
- Change the methods to synchronized again. This will actually serialize all calls to the SISM.
- Implement a more sophisticated synchronisation. E.g. multiple store operations can still be allowed, as long as their ChangeLogs do not intersect. Retrieving ItemStates might still be allowed while a ChangeLog is stored, as long as the ItemState to retrieve is not part of the ChangeLog.

Comments and suggestions are very welcome."
1,"MultiPhraseQuery throws AIOOBE. See thread ""MultiPhraseQuery throws ArrayIndexOutOfBounds Exception"" on dev@ by Jayendra Patil."
1,"CachingHierarchyManager synchronization problem. The method CachingHierarchyManager.resolveNodePath(..) does not synchronize on the cacheMonitor object. This can result in an endless loop in cache(), in NullPointerException or in other unexpected behavior in CachingHierarchyManager."
1,"Node.checkin() throws ArrayIndexOutOfBoundsException. I get an ArrayIndexOutOfBoundsException for index 0 when checking-in a node. After drilling into the code I found, that during checkin, the jcr:uuid property (defined as OPV INITIALIZE) is not copied from the node to the frozen node.

After checkin though the implementation tries to access the string value of the jcr:uuid property, which is not existing, hence the internal property implementation throws the exception when accessing the first element in the empty value array.

As a workaround I currently the set jcr:uuid property to OPV=COPY in the mixin:referenceable node type. But I could imagine, that this might be incorrect according to the spec, yet it works in my use case."
1,"Session.save() and Session.refresh(boolean) rely on accessibility of the root node. follow-up issue to JCR-2418:

an editing session that is only allowed to write in a subtree but isn't allowed to access the root node will not be
able to save or revert changes made in the transient space within that subtree.

the reason for this is, that both SessionImpl.save() and SessionImpl.refresh(boolean) access the root node
in order to execute the call. since it's the regular call READ permissions are checked, although the user
made no attempt to *look* at the root.

A workaround would be to call Item.save() on the modified tree itself that obviously was visible for the 
user... unfortunately that method is deprecated as of JCR 2.0. Therefore, I have the impression that we
should fix the methods mentioned above.

"
1,"jcr2spi: transient removal of mandatory item throws ConstraintViolationException. reported by tobi:

the transient removal of a mandatory (non-protected) item immediately fails. 
instead the check should be postponed until the save() call, since it would be perfectly legal to remove the mandatory item and then re-add it.

suggested fix:
ItemStateValidator#checkRemoveConstraints should only check for protection and ignore mandatory definitions."
1,"MultipartPostMethod Holding File Stream Open?. From: ""Daniel Walsh"" <daniel.walsh13@verizon.net>
Date: Tue Feb 25, 2003  8:05:49 PM US/Eastern
To: ""Commons HttpClient Project"" <commons-httpclient-dev@jakarta.apache.org>
Subject: MultipartPostMethod Holding File Stream Open?
Reply-To: ""Commons HttpClient Project"" <commons-httpclient-dev@jakarta.apache.org>

I'm using a MultipartPostMethod to upload a file to a servlet:

File file = new File(strUrl);

HttpClient client = new HttpClient();
HostConfiguration hostConfig = new HostConfiguration();
MultipartPostMethod mpPost = new MultipartPostMethod();

 hostConfig.setHost(someURL.getHost(), someURL.getPort(), someURL.getProtocol());
client.setConnectionTimeout(30000);
client.setHostConfiguration(hostConfig);

mpPost.addParameter(""someName"", ""someValue"");
mpPost.addParameter(file.getName(), file);

mpPost.setPath(strPath);
client.executeMethod(mpPost);

String confirmUpload = tpPost.getResponseBodyAsString();
mpPost.releaseConnection();

file.delete();  // this is being blocked.

After the upload, I would like to delete the file off of my disk.  Using other
methods of uploading the file (in particular a PutMethod), I was able to then
delete the file after the upload.  Now that I am using the MultipartPostMethod
obj for the upload, I am unable to delete the file (the return value is false,
and there is no SecurityException being thrown - no SecurityManager even set as
of this point either).

So, I guess my question is whether there is a call to the MultipartPostMethod
obj that I'm overlooking that would release it's connection (I'm sure that it is
opening an InputStream of some sort to read the file contents, in order to form
the HTTP message) to the file - so that I can then have unimpeded access to it
for other operations?"
1,"DatabaseJournal commits twice inside a transaction, causing an error with MySQL. When committing a transaction in a clustered setup, multiple records may be appended to the DatabaseJournal. After having appended a record, commit() is called on the connection and auto-commit mode is again enabled. Apart from not being semantically correct, committing a connection that is already in auto-commit mode throws an error when using MySQL as backend."
1,"document with no term vector fields after documents with term vector fields corrupts the index. If a document with no term-vector-enabled fields is added after
document(s) that did have term vectors, as part of a single set of
buffered docs, then the term-vector documents file is corrupted
because we fail to write a ""0"" vInt.

Thanks to Grant for spotting this!

Spinoff from this thread:

    http://www.gossamer-threads.com/lists/lucene/java-dev/53306
"
1,"BasicClientConnectionManager.releaseConnection accesses poolEntry using non-standard lock. According to the annotation, poolEntry is @GuardedBy(""this"").

However, in at least one place, it is accessed without holding a lock on this: 

BasicClientConnectionManager.releaseConnection synchronizes on managedConn, and then accesses poolEntry without synchronising on this.

[Synch. only works if all parties use the same lock.]"
1,"XPath parser ignores parent axis. The query handler in Jackrabbit does not support the parent axis and should throw an invalid query exception in that case.

Example query:

//foo/.."
1,"Catch SocketTimeoutException not InterruptedIOException. There are a couple of places where you're catching InterruptedIOException 
that should catch SocketTimeoutException instead.  For example, from 
HttpConnection:

    protected boolean isStale() {
        boolean isStale = true;
        if (isOpen) {
            // the connection is open, but now we have to see if we can 
read it
            // assume the connection is not stale.
            isStale = false;
            try {
                if (inputStream.available() == 0) {
                    try {
                        socket.setSoTimeout(1);
                        inputStream.mark(1);
                        int byteRead = inputStream.read();
                        if (byteRead == -1) {
                            // again - if the socket is reporting all data 
read,
                            // probably stale
                            isStale = true;
                        } else {
                            inputStream.reset();
                        }
                    } finally {
                        socket.setSoTimeout(this.params.getSoTimeout());
                    }
                }
            } catch (InterruptedIOException e) {
                // aha - the connection is NOT stale - continue on!

Here the catch of InterruptedIOException is intended to happen when 
inputStream.read() terminates due to the socket.setSoTimeout() time being 
reached.  However, it could also occur because Thread.interrupt() has been 
called, in which case ""continue on"" is not what should happen, instead, the 
request should terminate.

There are legitimate reasons why someone might want to interrupt the 
httpclient code, for example, httpclient does not provide a hard timeout on 
the total length of time a request may take, including connecting, sending 
the request, and receiving the complete response, so to enforce a hard 
timeout it is necessary to run the request in a worker thread and interrupt 
it if it hasn't completed before the timeout expires (the technique used in 
your TimeoutController class).

Note that SocketTimeoutException was added in 1.4.  For compatibility with 
older jdk versions, the code can catch InterruptedIOException and use 
getClass() to see whether it is a SocketTimeoutException.

There are probably other places in the code where InterruptedIOException is 
caught and interpreted as a socket timeout, and where Thread.interrupt() 
will not have the proper effect of causing the request to terminate ASAP, 
but I'm not familiar enough with the code to find them all."
1,"AutoCloseInputStream.available() throws IOException when auto-closed. ACIS auto-close itself as soon as EOF is detected. That leads to IOExceptions being throw in response to calls that are valid for a stream that has reached EOF.
ACIS should instead close the _underlying_ stream and switch itself into an EOF mode that does not throw exceptions until it is closed explicitly.

reported by Tom Lipkis on the developer mailing list
http://mail-archives.apache.org/mod_mbox/jakarta-httpcomponents-dev/200702.mbox/%3c200702101905.l1AJ5MeK027997@fw3.pss.com%3e"
1,"InternalValue.createCopy for binary properties (jcr:data) causes problems. Running 1.4 with no data store configured, and option org.jackrabbit.useDataStore not set (i.e true), the following code gives 0 for the property length.

Node n = root.getNode(relPath);
session.getWorkspace().copy(n.getPath(), destPath);
Node contentNode = n.getNode(JcrConstants.JCR_CONTENT);
Property p = contentNode.getProperty(JcrConstants.JCR_DATA);
System.out.println(""length = ""+p.getLength());

InternalValue.createCopy checks USE_DATA_STORE and returns the same value for the source node's state. BundleBinding.writeState() calls BLOBInMemory.discard() when persisting the new node. This has now changed the value of the existing nodes property. Setting the option org.jackrabbit.useDataStore to false works fine. Possibly the check for binary property type in InternalValue.createCopy should be done first?"
1,"finalize()-methods of FSDirectory.FSIndexInput and FSDirectory.FSIndexOutput try to close already closed file. Hi all,

I found a small problem in FSDirectory: The finalize()-methods of FSDirectory.FSIndexInput and FSDirectory.FSIndexOutput try to close the underlying file. This is not a problem unless the file has been closed before by calling the close() method. If it has been closed before, the finalize method throws an IOException saying that the file is already closed. Usually this IOException would go unnoticed, because the GarbageCollector, which calls finalize(), just eats it. However, if I use the Eclipse debugger the execution of my code will always be suspended when this exception is thrown.

Even though this exception probably won't cause problems during normal execution of Lucene, the code becomes cleaner if we apply this small patch. Might this IOException also have a performance impact, if it is thrown very frequently?

I attached the patch which applies cleanly on the current svn HEAD. All testcases pass and I verfied with the Eclipse debugger that the IOException is not longer thrown."
1,"XPath query with negative numbers incorrect. A XPath query that contains a negative number does not return correct results.

E.g. the query:

//*[@number = -10]

does not return nodes with number properties containing a value of -10 but will return nodes with number values equal to 10. Similarly the query returns wrong results for double values."
1,"Preemptive authentication causes NTLM auth scheme to fail. The NTLM authentication scheme does not work when the preemptive authentication
is enabled.

Reported by Dave Seidel <dave at mindreef.com>"
1,"DbInputStream does not support mark()/reset() when exhausted.. The DbDataStore implementation uses a DbInputStream to read binary properties from the database. When a new binary property is created, Jackrabbit attempts to index it. Tika's CharsetDetector is used in the process, which marks the input stream, reads the first 8000 bytes and then resets the stream.

This results in the stacktrace shown at the end of the issue, if the following two conditions hold true:
* the property is larger than the minRecordLength configuration of the Datastore and
* the property is smaller than 8000 bytes

The DbInputStream needs to have the following properties:
1. lazy instantiation of the underlying stream
2. auto-close underlying stream when EOF is reached
3. fully support mark()/reset() even if  the underlying stream is auto-closed due to 2.


12.03.2010 15:53:28 *WARN * LazyTextExtractorField: Failed to extract text from a binary property (LazyTextExtractorField.java, line 165)
java.io.EOFException
        at org.apache.jackrabbit.core.data.db.DbInputStream.reset(DbInputStream.java:180)
        at org.apache.tika.io.ProxyInputStream.reset(ProxyInputStream.java:156)
        at org.apache.tika.io.ProxyInputStream.reset(ProxyInputStream.java:156)
        at org.apache.tika.parser.txt.CharsetDetector.setText(CharsetDetector.java:131)
        at org.apache.tika.parser.txt.TXTParser.parse(TXTParser.java:77)
        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:120)
        at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:101)
        at org.apache.tika.parser.AutoDetectParser.parse(AutoDetectParser.java:114)
        at org.apache.jackrabbit.core.query.lucene.LazyTextExtractorField$ParsingTask.run(LazyTextExtractorField.java:160)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:207)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
"
1,"Test dependency on Jackrabbit core (from o.a.j.c.security.user.XPathQueryEvaluator). o.a.j.c.security.user.XPathQueryEvaluator wrongly imports 

    import org.apache.jackrabbit.test.api.util.Text

instead of

    import org.apache.jackrabbit.util.Text



"
1,Versioning might no be thread safe. check versioning for thread safeness
1,"If you hit the ""max term prefix"" warning when indexing, it never goes away. Silly bug.

If IW's infoStream is on, we warn whenever we hit a ridiculously long term (> 16 KB in length).  The problem is, we never reset this warning, so, once one doc contains such a massive term, we then keep warning over and over about that same term for future docs."
1,"Test failures with spi2jcr in AddEventListenerTest. Two tests fail:

- AddEventListenerTest.testUUID
- AddEventListenerTest.testNodeType"
1,"Virtual host setting does not apply when parsing and matching cookies. Virtual host setting does not apply when parsing and matching cookies.

Problem has been reported on the httpclient-dev list by Dan Levine"
1,"QueryNodeImpl throws ConcurrentModificationException on add(List<QueryNode>). on adding a List of children to a QueryNodeImplemention a ConcurrentModificationException is thrown.
This is due to the fact that QueryNodeImpl instead of iteration over the supplied list, iterates over its internal clauses List.

Patch:
Index: QueryNodeImpl.java
===================================================================
--- QueryNodeImpl.java    (revision 911642)
+++ QueryNodeImpl.java    (working copy)
@@ -74,7 +74,7 @@
           .getLocalizedMessage(QueryParserMessages.NODE_ACTION_NOT_SUPPORTED));
     }
 
-    for (QueryNode child : getChildren()) {
+    for (QueryNode child : children) {
       add(child);
     }
 "
1,"Generated cluster node id should be persisted. If no cluster node id is specified in the configuration, a cluster node id is automatically generated. This id is never persisted, so after another startup a new, probably different cluster node id is used. Instead, an automatically generated cluster id should be persisted inside the repository home."
1,"CacheEntryUpdater does not properly update cache entry resource. CacheEntryUpdater#updateCacheEntry() copies the old cache entry's resource, though I believe it should only do so if the response is a 304.  Otherwise it should take the response from the server to update the entry.  This method gets called when validating a cache entry and the server returns a 200 or 304."
1,"select query fails on 'like xxx322' statement. In class org.apache.jackrabbit.core.search.lucene.WildcardTermEnum, there's a condition around line 77 like

...
if (Character.isLetter(likePattern.charAt(i))) {
...

which fails on query like
SELECT * FROM my:type LOCATION /news// WHERE my:text LIKE ""asd2""

it should be changed to 

if (Character.isLetter(likePattern.charAt(i)) || Character.isDigit(likePattern.charAt(i))) {

"
1,"MultiSearcher.rewrite() incorrectly rewrites queries. This was reported on the userlist, in the context of range queries.

Its also easy to make our existing tests fail with my patch on LUCENE-2751:
{noformat}
ant test-core -Dtestcase=TestBoolean2 -Dtestmethod=testRandomQueries -Dtests.seed=7679849347282878725:-903778383189134045
{noformat}

The fundamental problem is that MultiSearcher first rewrites against individual subs, 
then uses Query.combine() which simply OR's these sub-clauses.

This is incorrect for expanded MUST_NOT queries (e.g. from wildcard), as it violates demorgan's law.
"
1,"TestIndexWriter#testThreadInterruptDeadlock fails with OOM . Selckin reported a repeatedly failing test that throws OOM Exceptions. According to the heapdump the MockDirectoryWrapper#createdFiles HashSet takes about 400MB heapspace containing 4194304 entries. Seems kind of way too many though :)

{noformat}
 [junit] java.lang.OutOfMemoryError: Java heap space
    [junit] Dumping heap to /tmp/java_pid25990.hprof ...
    [junit] Heap dump file created [520807744 bytes in 4.250 secs]
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriter
    [junit] Testcase: testThreadInterruptDeadlock(org.apache.lucene.index.TestIndexWriter):	FAILED
    [junit] 
    [junit] junit.framework.AssertionFailedError: 
    [junit] 	at org.apache.lucene.index.TestIndexWriter.testThreadInterruptDeadlock(TestIndexWriter.java:2249)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1282)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1211)
    [junit] 
    [junit] 
    [junit] Testcase: testThreadInterruptDeadlock(org.apache.lucene.index.TestIndexWriter):	FAILED
    [junit] Some threads threw uncaught exceptions!
    [junit] junit.framework.AssertionFailedError: Some threads threw uncaught exceptions!
    [junit] 	at org.apache.lucene.util.LuceneTestCase.tearDown(LuceneTestCase.java:557)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1282)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$LuceneTestCaseRunner.runChild(LuceneTestCase.java:1211)
    [junit] 
    [junit] 
    [junit] Tests run: 67, Failures: 2, Errors: 0, Time elapsed: 3,254.884 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] FAILED; unexpected exception
    [junit] java.lang.OutOfMemoryError: Java heap space
    [junit] 	at org.apache.lucene.store.RAMFile.newBuffer(RAMFile.java:85)
    [junit] 	at org.apache.lucene.store.RAMFile.addBuffer(RAMFile.java:58)
    [junit] 	at org.apache.lucene.store.RAMOutputStream.switchCurrentBuffer(RAMOutputStream.java:132)
    [junit] 	at org.apache.lucene.store.RAMOutputStream.copyBytes(RAMOutputStream.java:171)
    [junit] 	at org.apache.lucene.store.MockIndexOutputWrapper.copyBytes(MockIndexOutputWrapper.java:155)
    [junit] 	at org.apache.lucene.index.CompoundFileWriter.copyFile(CompoundFileWriter.java:223)
    [junit] 	at org.apache.lucene.index.CompoundFileWriter.close(CompoundFileWriter.java:189)
    [junit] 	at org.apache.lucene.index.SegmentMerger.createCompoundFile(SegmentMerger.java:138)
    [junit] 	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3344)
    [junit] 	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:2959)
    [junit] 	at org.apache.lucene.index.SerialMergeScheduler.merge(SerialMergeScheduler.java:37)
    [junit] 	at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1763)
    [junit] 	at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1758)
    [junit] 	at org.apache.lucene.index.IndexWriter.maybeMerge(IndexWriter.java:1754)
    [junit] 	at org.apache.lucene.index.IndexWriter.updateDocument(IndexWriter.java:1373)
    [junit] 	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1230)
    [junit] 	at org.apache.lucene.index.IndexWriter.addDocument(IndexWriter.java:1211)
    [junit] 	at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:2154)
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testThreadInterruptDeadlock -Dtests.seed=7183538093651149:3431510331342554160
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriter -Dtestmethod=testThreadInterruptDeadlock -Dtests.seed=7183538093651149:3431510331342554160
    [junit] The following exceptions were thrown by threads:
    [junit] *** Thread: Thread-379 ***
    [junit] java.lang.RuntimeException: MockDirectoryWrapper: cannot close: there are still open files: {_3r1n_0.tib=1, _3r1n_0.frq=1, _3r1n_0.pos=1, _3r1m.cfs=1, _3r1n_0.doc=1, _3r1n.tvf=1, _3r1n.tvd=1, _3r1n.tvx=1, _3r1n.fdx=1, _3r1n.fdt=1, _3r1q.cfs=1, _3r1o.cfs=1, _3r1n_0.skp=1, _3r1n_0.pyl=1}
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.close(MockDirectoryWrapper.java:448)
    [junit] 	at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:2217)
    [junit] Caused by: java.lang.RuntimeException: unclosed IndexOutput
    [junit] 	at org.apache.lucene.store.MockDirectoryWrapper.createOutput(MockDirectoryWrapper.java:367)
    [junit] 	at org.apache.lucene.index.FieldInfos.write(FieldInfos.java:563)
    [junit] 	at org.apache.lucene.index.DocFieldProcessor.flush(DocFieldProcessor.java:82)
    [junit] 	at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:381)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:378)
    [junit] 	at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:505)
    [junit] 	at org.apache.lucene.index.IndexWriter.doFlush(IndexWriter.java:2621)
    [junit] 	at org.apache.lucene.index.IndexWriter.flush(IndexWriter.java:2598)
    [junit] 	at org.apache.lucene.index.IndexWriter.prepareCommit(IndexWriter.java:2464)
    [junit] 	at org.apache.lucene.index.IndexWriter.commitInternal(IndexWriter.java:2537)
    [junit] 	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2519)
    [junit] 	at org.apache.lucene.index.IndexWriter.commit(IndexWriter.java:2503)
    [junit] 	at org.apache.lucene.index.TestIndexWriter$IndexerThreadInterrupt.run(TestIndexWriter.java:2156)
    [junit] NOTE: test params are: codec=RandomCodecProvider: {=MockVariableIntBlock(baseBlockSize=105), f6=MockFixedIntBlock(blockSize=1372), f7=Pulsing(freqCutoff=11), f8=MockRandom, f9=MockVariableIntBlock(baseBlockSize=105), f1=MockSep, f0=Pulsing(freqCutoff=11), f3=Pulsing(freqCutoff=11), f2=MockFixedIntBlock(blockSize=1372), f5=MockVariableIntBlock(baseBlockSize=105), f4=MockRandom, f=Standard, c=Pulsing(freqCutoff=11), termVector=MockFixedIntBlock(blockSize=1372), d9=MockVariableIntBlock(baseBlockSize=105), d8=MockRandom, d5=MockSep, d4=Pulsing(freqCutoff=11), d7=MockFixedIntBlock(blockSize=1372), d6=MockVariableIntBlock(baseBlockSize=105), d25=SimpleText, d0=Pulsing(freqCutoff=11), c29=SimpleText, d24=MockSep, d1=MockSep, c28=MockVariableIntBlock(baseBlockSize=105), d23=MockRandom, d2=MockVariableIntBlock(baseBlockSize=105), c27=MockRandom, d22=Standard, d3=MockFixedIntBlock(blockSize=1372), d21=MockVariableIntBlock(baseBlockSize=105), d20=MockRandom, c22=SimpleText, c21=MockSep, c20=MockRandom, d29=MockFixedIntBlock(blockSize=1372), c26=MockFixedIntBlock(blockSize=1372), d28=MockVariableIntBlock(baseBlockSize=105), c25=MockVariableIntBlock(baseBlockSize=105), d27=MockSep, c24=MockSep, d26=Pulsing(freqCutoff=11), c23=Pulsing(freqCutoff=11), e9=MockSep, e8=Standard, e7=SimpleText, e6=MockVariableIntBlock(baseBlockSize=105), e5=MockRandom, c17=MockSep, e3=SimpleText, d12=Pulsing(freqCutoff=11), c16=Pulsing(freqCutoff=11), e4=Standard, d11=MockFixedIntBlock(blockSize=1372), c19=MockFixedIntBlock(blockSize=1372), e1=MockRandom, d14=MockVariableIntBlock(baseBlockSize=105), c18=MockVariableIntBlock(baseBlockSize=105), e2=MockVariableIntBlock(baseBlockSize=105), d13=MockRandom, e0=MockFixedIntBlock(blockSize=1372), d10=MockSep, d19=Pulsing(freqCutoff=11), c11=MockVariableIntBlock(baseBlockSize=105), c10=MockRandom, d16=MockRandom, c13=MockRandom, c12=Standard, d15=Standard, d18=SimpleText, c15=SimpleText, d17=MockSep, c14=MockSep, b3=SimpleText, b2=MockSep, b5=Pulsing(freqCutoff=11), b4=MockFixedIntBlock(blockSize=1372), b7=MockFixedIntBlock(blockSize=1372), b6=MockVariableIntBlock(baseBlockSize=105), d50=Pulsing(freqCutoff=11), b9=MockRandom, b8=Standard, d43=MockRandom, d42=Standard, d41=MockFixedIntBlock(blockSize=1372), d40=MockVariableIntBlock(baseBlockSize=105), d47=MockSep, d46=Pulsing(freqCutoff=11), b0=MockFixedIntBlock(blockSize=1372), d45=Standard, b1=Pulsing(freqCutoff=11), d44=SimpleText, d49=Pulsing(freqCutoff=11), d48=MockFixedIntBlock(blockSize=1372), c6=MockRandom, c5=Standard, c4=MockFixedIntBlock(blockSize=1372), c3=MockVariableIntBlock(baseBlockSize=105), c9=Pulsing(freqCutoff=11), c8=Standard, c7=SimpleText, d30=SimpleText, d32=Pulsing(freqCutoff=11), d31=MockFixedIntBlock(blockSize=1372), c1=Standard, d34=MockFixedIntBlock(blockSize=1372), c2=MockRandom, d33=MockVariableIntBlock(baseBlockSize=105), d36=MockRandom, c0=MockFixedIntBlock(blockSize=1372), d35=Standard, d38=Standard, d37=SimpleText, d39=Pulsing(freqCutoff=11), e92=MockFixedIntBlock(blockSize=1372), e93=Pulsing(freqCutoff=11), e90=MockSep, e91=SimpleText, e89=Pulsing(freqCutoff=11), e88=Standard, e87=SimpleText, e86=MockRandom, e85=Standard, e84=MockFixedIntBlock(blockSize=1372), e83=MockVariableIntBlock(baseBlockSize=105), e80=MockVariableIntBlock(baseBlockSize=105), e81=SimpleText, e82=Standard, e77=MockFixedIntBlock(blockSize=1372), e76=MockVariableIntBlock(baseBlockSize=105), e79=MockRandom, e78=Standard, e73=SimpleText, e72=MockSep, e75=Pulsing(freqCutoff=11), e74=MockFixedIntBlock(blockSize=1372), binary=Pulsing(freqCutoff=11), f98=MockSep, f97=Pulsing(freqCutoff=11), f99=MockVariableIntBlock(baseBlockSize=105), f94=MockRandom, f93=Standard, f96=SimpleText, f95=MockSep, e95=MockSep, e94=Pulsing(freqCutoff=11), e97=MockFixedIntBlock(blockSize=1372), e96=MockVariableIntBlock(baseBlockSize=105), e99=MockVariableIntBlock(baseBlockSize=105), e98=MockRandom, id=MockRandom, f34=SimpleText, f33=MockSep, f32=MockRandom, f31=Standard, f30=MockVariableIntBlock(baseBlockSize=105), f39=MockRandom, f38=MockFixedIntBlock(blockSize=1372), f37=MockVariableIntBlock(baseBlockSize=105), f36=MockSep, f35=Pulsing(freqCutoff=11), f43=MockSep, f42=Pulsing(freqCutoff=11), f45=MockFixedIntBlock(blockSize=1372), f44=MockVariableIntBlock(baseBlockSize=105), f41=SimpleText, f40=MockSep, f47=MockVariableIntBlock(baseBlockSize=105), f46=MockRandom, f49=Standard, f48=SimpleText, content=MockFixedIntBlock(blockSize=1372), e19=Standard, e18=SimpleText, e17=MockRandom, f12=Standard, e16=Standard, f11=SimpleText, f10=MockVariableIntBlock(baseBlockSize=105), e15=MockFixedIntBlock(blockSize=1372), e14=MockVariableIntBlock(baseBlockSize=105), f16=Pulsing(freqCutoff=11), e13=Pulsing(freqCutoff=11), f15=MockFixedIntBlock(blockSize=1372), e12=MockFixedIntBlock(blockSize=1372), e11=SimpleText, f14=SimpleText, e10=MockSep, f13=MockSep, f19=Standard, f18=MockFixedIntBlock(blockSize=1372), f17=MockVariableIntBlock(baseBlockSize=105), e29=MockFixedIntBlock(blockSize=1372), e26=Standard, f21=SimpleText, e25=SimpleText, f20=MockSep, e28=MockSep, f23=Pulsing(freqCutoff=11), e27=Pulsing(freqCutoff=11), f22=MockFixedIntBlock(blockSize=1372), f25=MockFixedIntBlock(blockSize=1372), e22=MockFixedIntBlock(blockSize=1372), f24=MockVariableIntBlock(baseBlockSize=105), e21=MockVariableIntBlock(baseBlockSize=105), f27=MockRandom, e24=MockRandom, f26=Standard, e23=Standard, f29=Standard, f28=SimpleText, e20=Pulsing(freqCutoff=11), field=MockRandom, string=Pulsing(freqCutoff=11), e30=MockSep, e31=SimpleText, a98=MockRandom, e34=MockVariableIntBlock(baseBlockSize=105), a99=MockVariableIntBlock(baseBlockSize=105), e35=MockFixedIntBlock(blockSize=1372), f79=MockVariableIntBlock(baseBlockSize=105), e32=Pulsing(freqCutoff=11), e33=MockSep, b97=Pulsing(freqCutoff=11), f77=MockFixedIntBlock(blockSize=1372), e38=SimpleText, b98=MockSep, f78=Pulsing(freqCutoff=11), e39=Standard, b99=MockVariableIntBlock(baseBlockSize=105), f75=MockSep, e36=MockRandom, f76=SimpleText, e37=MockVariableIntBlock(baseBlockSize=105), f73=SimpleText, f74=Standard, f71=MockRandom, f72=MockVariableIntBlock(baseBlockSize=105), f81=MockFixedIntBlock(blockSize=1372), f80=MockVariableIntBlock(baseBlockSize=105), e40=MockSep, e41=MockVariableIntBlock(baseBlockSize=105), e42=MockFixedIntBlock(blockSize=1372), e43=MockRandom, e44=MockVariableIntBlock(baseBlockSize=105), e45=SimpleText, e46=Standard, f86=MockVariableIntBlock(baseBlockSize=105), e47=MockSep, f87=MockFixedIntBlock(blockSize=1372), e48=SimpleText, f88=Standard, e49=MockFixedIntBlock(blockSize=1372), f89=MockRandom, f82=MockSep, f83=SimpleText, f84=MockFixedIntBlock(blockSize=1372), f85=Pulsing(freqCutoff=11), f90=MockVariableIntBlock(baseBlockSize=105), f92=Standard, f91=SimpleText, str=MockFixedIntBlock(blockSize=1372), a76=MockVariableIntBlock(baseBlockSize=105), e56=MockRandom, f59=MockRandom, a77=MockFixedIntBlock(blockSize=1372), e57=MockVariableIntBlock(baseBlockSize=105), a78=Standard, e54=MockFixedIntBlock(blockSize=1372), f57=MockFixedIntBlock(blockSize=1372), a79=MockRandom, e55=Pulsing(freqCutoff=11), f58=Pulsing(freqCutoff=11), e52=Pulsing(freqCutoff=11), e53=MockSep, e50=SimpleText, e51=Standard, f51=Standard, f52=MockRandom, f50=MockFixedIntBlock(blockSize=1372), f55=Pulsing(freqCutoff=11), f56=MockSep, f53=SimpleText, e58=Standard, f54=Standard, e59=MockRandom, a80=MockVariableIntBlock(baseBlockSize=105), e60=MockRandom, a82=Standard, a81=SimpleText, a84=SimpleText, a83=MockSep, a86=Pulsing(freqCutoff=11), a85=MockFixedIntBlock(blockSize=1372), a89=Pulsing(freqCutoff=11), f68=Standard, e65=Standard, f69=MockRandom, e66=MockRandom, a87=SimpleText, e67=MockSep, a88=Standard, e68=SimpleText, e61=MockFixedIntBlock(blockSize=1372), e62=Pulsing(freqCutoff=11), e63=MockRandom, e64=MockVariableIntBlock(baseBlockSize=105), f60=SimpleText, f61=Standard, f62=Pulsing(freqCutoff=11), f63=MockSep, e69=Pulsing(freqCutoff=11), f64=MockFixedIntBlock(blockSize=1372), f65=Pulsing(freqCutoff=11), f66=MockRandom, f67=MockVariableIntBlock(baseBlockSize=105), f70=MockRandom, a93=Pulsing(freqCutoff=11), a92=MockFixedIntBlock(blockSize=1372), a91=SimpleText, e71=MockSep, a90=MockSep, e70=Pulsing(freqCutoff=11), a97=MockRandom, a96=Standard, a95=MockFixedIntBlock(blockSize=1372), a94=MockVariableIntBlock(baseBlockSize=105), c58=MockFixedIntBlock(blockSize=1372), a63=Pulsing(freqCutoff=11), a64=MockSep, c59=Pulsing(freqCutoff=11), c56=MockSep, d59=MockSep, a61=SimpleText, c57=SimpleText, a62=Standard, c54=SimpleText, c55=Standard, a60=MockRandom, c52=MockRandom, c53=MockVariableIntBlock(baseBlockSize=105), d53=MockVariableIntBlock(baseBlockSize=105), d54=MockFixedIntBlock(blockSize=1372), d51=Pulsing(freqCutoff=11), d52=MockSep, d57=SimpleText, b62=Standard, d58=Standard, b63=MockRandom, d55=MockRandom, b60=MockVariableIntBlock(baseBlockSize=105), d56=MockVariableIntBlock(baseBlockSize=105), b61=MockFixedIntBlock(blockSize=1372), b56=MockSep, b55=Pulsing(freqCutoff=11), b54=Standard, b53=SimpleText, d61=SimpleText, b59=MockRandom, d60=MockSep, b58=Pulsing(freqCutoff=11), b57=MockFixedIntBlock(blockSize=1372), c62=MockFixedIntBlock(blockSize=1372), c61=MockVariableIntBlock(baseBlockSize=105), a59=MockRandom, c60=MockSep, a58=Standard, a57=MockVariableIntBlock(baseBlockSize=105), a56=MockRandom, a55=Pulsing(freqCutoff=11), a54=MockFixedIntBlock(blockSize=1372), a72=MockFixedIntBlock(blockSize=1372), c67=MockVariableIntBlock(baseBlockSize=105), a73=Pulsing(freqCutoff=11), c68=MockFixedIntBlock(blockSize=1372), a74=MockRandom, c69=Standard, a75=MockVariableIntBlock(baseBlockSize=105), c63=MockSep, c64=SimpleText, a70=Pulsing(freqCutoff=11), c65=MockFixedIntBlock(blockSize=1372), a71=MockSep, c66=Pulsing(freqCutoff=11), d62=MockRandom, d63=MockVariableIntBlock(baseBlockSize=105), d64=SimpleText, b70=MockRandom, d65=Standard, b71=SimpleText, d66=MockSep, b72=Standard, d67=SimpleText, b73=Pulsing(freqCutoff=11), d68=MockFixedIntBlock(blockSize=1372), b74=MockSep, d69=Pulsing(freqCutoff=11), b65=Pulsing(freqCutoff=11), b64=MockFixedIntBlock(blockSize=1372), b67=MockVariableIntBlock(baseBlockSize=105), b66=MockRandom, d70=MockSep, b69=MockRandom, b68=Standard, d72=MockFixedIntBlock(blockSize=1372), d71=MockVariableIntBlock(baseBlockSize=105), c71=MockVariableIntBlock(baseBlockSize=105), c70=MockRandom, a69=Pulsing(freqCutoff=11), c73=Standard, c72=SimpleText, a66=MockRandom, a65=Standard, a68=SimpleText, a67=MockSep, c32=Standard, c33=MockRandom, c30=MockVariableIntBlock(baseBlockSize=105), c31=MockFixedIntBlock(blockSize=1372), c36=Pulsing(freqCutoff=11), a41=MockSep, c37=MockSep, a42=SimpleText, a0=MockSep, c34=SimpleText, c35=Standard, a40=MockRandom, b84=SimpleText, d79=MockSep, b85=Standard, b82=MockRandom, d77=Standard, c38=MockFixedIntBlock(blockSize=1372), b83=MockVariableIntBlock(baseBlockSize=105), d78=MockRandom, c39=Pulsing(freqCutoff=11), b80=MockVariableIntBlock(baseBlockSize=105), d75=MockRandom, b81=MockFixedIntBlock(blockSize=1372), d76=MockVariableIntBlock(baseBlockSize=105), d73=MockFixedIntBlock(blockSize=1372), d74=Pulsing(freqCutoff=11), d83=MockSep, a9=Standard, d82=Pulsing(freqCutoff=11), d81=Standard, d80=SimpleText, b79=MockVariableIntBlock(baseBlockSize=105), b78=Pulsing(freqCutoff=11), b77=MockFixedIntBlock(blockSize=1372), b76=SimpleText, b75=MockSep, a1=SimpleText, a35=MockFixedIntBlock(blockSize=1372), a2=Standard, a34=MockVariableIntBlock(baseBlockSize=105), a3=Pulsing(freqCutoff=11), a33=MockSep, a4=MockSep, a32=Pulsing(freqCutoff=11), a5=MockFixedIntBlock(blockSize=1372), a39=Standard, c40=Pulsing(freqCutoff=11), a6=Pulsing(freqCutoff=11), a38=SimpleText, a7=MockRandom, a37=MockVariableIntBlock(baseBlockSize=105), a8=MockVariableIntBlock(baseBlockSize=105), a36=MockRandom, c41=SimpleText, c42=Standard, c43=Pulsing(freqCutoff=11), c44=MockSep, c45=MockFixedIntBlock(blockSize=1372), a50=Pulsing(freqCutoff=11), c46=Pulsing(freqCutoff=11), a51=MockSep, c47=MockRandom, a52=MockVariableIntBlock(baseBlockSize=105), c48=MockVariableIntBlock(baseBlockSize=105), a53=MockFixedIntBlock(blockSize=1372), b93=MockSep, d88=Pulsing(freqCutoff=11), c49=Standard, b94=SimpleText, d89=MockSep, b95=MockFixedIntBlock(blockSize=1372), b96=Pulsing(freqCutoff=11), d84=Standard, b90=MockVariableIntBlock(baseBlockSize=105), d85=MockRandom, b91=SimpleText, d86=MockSep, b92=Standard, d87=SimpleText, d92=Pulsing(freqCutoff=11), d91=MockFixedIntBlock(blockSize=1372), d94=MockVariableIntBlock(baseBlockSize=105), d93=MockRandom, b87=MockFixedIntBlock(blockSize=1372), b86=MockVariableIntBlock(baseBlockSize=105), d90=MockSep, b89=MockRandom, b88=Standard, a44=MockVariableIntBlock(baseBlockSize=105), a43=MockRandom, a46=Standard, a45=SimpleText, a48=SimpleText, a47=MockSep, c51=MockRandom, a49=MockFixedIntBlock(blockSize=1372), c50=Standard, d98=MockRandom, d97=Standard, d96=MockFixedIntBlock(blockSize=1372), d95=MockVariableIntBlock(baseBlockSize=105), d99=SimpleText, a20=Standard, c99=MockSep, c98=Pulsing(freqCutoff=11), c97=Standard, c96=SimpleText, b19=Standard, a16=Standard, a17=MockRandom, b17=MockVariableIntBlock(baseBlockSize=105), a14=MockVariableIntBlock(baseBlockSize=105), b18=MockFixedIntBlock(blockSize=1372), a15=MockFixedIntBlock(blockSize=1372), a12=MockFixedIntBlock(blockSize=1372), a13=Pulsing(freqCutoff=11), a10=MockSep, a11=SimpleText, b11=SimpleText, b12=Standard, b10=MockVariableIntBlock(baseBlockSize=105), b15=MockFixedIntBlock(blockSize=1372), b16=Pulsing(freqCutoff=11), a18=SimpleText, b13=MockSep, a19=Standard, b14=SimpleText, b30=Standard, a31=Pulsing(freqCutoff=11), a30=MockFixedIntBlock(blockSize=1372), b28=SimpleText, a25=SimpleText, b29=Standard, a26=Standard, a27=Pulsing(freqCutoff=11), a28=MockSep, a21=MockVariableIntBlock(baseBlockSize=105), a22=MockFixedIntBlock(blockSize=1372), a23=Standard, a24=MockRandom, b20=MockSep, b21=SimpleText, b22=MockFixedIntBlock(blockSize=1372), b23=Pulsing(freqCutoff=11), a29=MockFixedIntBlock(blockSize=1372), b24=MockVariableIntBlock(baseBlockSize=105), b25=MockFixedIntBlock(blockSize=1372), b26=Standard, b27=MockRandom, b41=MockVariableIntBlock(baseBlockSize=105), b40=MockRandom, c77=SimpleText, c76=MockSep, c75=MockRandom, c74=Standard, c79=MockSep, c78=Pulsing(freqCutoff=11), c80=MockSep, c83=MockRandom, c84=MockVariableIntBlock(baseBlockSize=105), c81=MockFixedIntBlock(blockSize=1372), b39=MockRandom, c82=Pulsing(freqCutoff=11), b37=MockVariableIntBlock(baseBlockSize=105), b38=MockFixedIntBlock(blockSize=1372), b35=Pulsing(freqCutoff=11), b36=MockSep, b33=MockSep, b34=SimpleText, b31=Standard, b32=MockRandom, str2=Standard, b50=MockRandom, b52=SimpleText, str3=Pulsing(freqCutoff=11), b51=MockSep, c86=MockSep, tvtest=Pulsing(freqCutoff=11), c85=Pulsing(freqCutoff=11), c88=MockFixedIntBlock(blockSize=1372), c87=MockVariableIntBlock(baseBlockSize=105), c89=MockRandom, c90=MockRandom, c91=MockVariableIntBlock(baseBlockSize=105), c92=Standard, c93=MockRandom, c94=MockSep, c95=SimpleText, content1=SimpleText, b46=MockRandom, b47=MockVariableIntBlock(baseBlockSize=105), content3=MockRandom, b48=SimpleText, content4=Standard, b49=Standard, content5=MockVariableIntBlock(baseBlockSize=105), b42=Pulsing(freqCutoff=11), b43=MockSep, b44=MockVariableIntBlock(baseBlockSize=105), b45=MockFixedIntBlock(blockSize=1372)}, locale=it_CH, timezone=Europe/Chisinau
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestMergeSchedulerExternal, TestToken, TestCodecs, TestFieldInfos, TestFlushByRamOrCountsPolicy, TestIndexReaderReopen, TestIndexWriter]
    [junit] NOTE: Linux 2.6.37-gentoo amd64/Sun Microsystems Inc. 1.6.0_25 (64-bit)/cpus=8,threads=1,free=275548088,total=309395456
{noformat}"
1,"DocValues infinite loop caused by - a call to getMinValue | getMaxValue | getAverageValue. org.apache.lucene.search.function.DocValues offers 3 public (optional) methods to access value statistics like min, max and average values of the internal values. A call to one of the methods will result in an infinite loop. The internal counter is not incremented. 
I added a testcase, javadoc and a slightly different implementation to it. I guess this is not breaking any back compat. as a call to those methodes would have caused an infinite loop anyway.
I changed the return value of all of those methods to Float.NaN if the DocValues implementation does not contain any values.

It might be considerable to fix this in 2.4.2 and 2.3.3

"
1,"IndexWriter.unlock does does nothing if NativeFSLockFactory is used. If NativeFSLockFactory is used, IndexWriter.unlock will return, silently doing nothing. The reason is that NativeFSLockFactory's makeLock always creates a new NativeFSLock. NativeFSLock's release first checks if its lock is not null. However, only if obtain() is called, that lock is not null. So release actually does nothing, and so IndexWriter.unlock does not delete the lock, or fail w/ exception.
This is only a problem in NativeFSLock, and not in other Lock implementations, at least as I was able to see.

Need to think first how to reproduce in a test, and then fix it. I'll work on it."
1,"WeightedSpanTermExtractor incorrectly treats the same terms occurring in different query types. Given a BooleanQuery with multiple clauses, if a term occurs both in a Span / Phrase query, and in a TermQuery, the results of term extraction are unpredictable and depend on the order of clauses. Concequently, the result of highlighting are incorrect.

Example text: t1 t2 t3 t4 t2
Example query: t2 t3 ""t1 t2""
Current highlighting: [t1 t2] [t3] t4 t2
Correct highlighting: [t1 t2] [t3] t4 [t2]

The problem comes from the fact that we keep a Map<termText, WeightedSpanTerm>, and if the same term occurs in a Phrase or Span query the resulting WeightedSpanTerm will have a positionSensitive=true, whereas terms added from TermQuery have positionSensitive=false. The end result for this particular term will depend on the order in which the clauses are processed.

My fix is to use a subclass of Map, which on put() always sets the result to the most lax setting, i.e. if we already have a term with positionSensitive=true, and we try to put() a term with positionSensitive=false, we set the result positionSensitive=false, as it will match both cases."
1,"[PATCH] Wirelog corrections. This patch 

* fixes the problem reported by Geir H. Pettersen <geir at cellus.no>. See
http://marc.theaimsgroup.com/?t=108072355300004&r=1&w=2 for details

* increases the priority of HTTP request/status line & HTTP headers output from
DEBUG to INFO. Quite often request/response content generate excessive amount of
output in the wirelog and produce no valuable debug information of what so ever.
By setting wirelog verbosity to INFO one can turn off the logging of 
request/response content.

I believe the patch should be applied to both CVS HEAD and 2.0 branch. Please
let me know if you agree

Oleg"
1,HttpClient treats URI fragments in redirect URIs incosistently. HttpClient treats URI fragments in redirect URIs incosistently. It strips fragments from relative URIs but leaves absolute ones unchanges.  
1,"Open-scoped locks may be lost on restart and might not be transferrable. Two issues with open-scoped locks were reported by Cédric Damioli:

(1) When open-scoped locks are being reapplied on repository startup, locks on non-referenceable nodes are lost.
(2) When a session holding an open-scoped lock logs out, the lock token is not automatically removed from the session and other sessions are not able to take responsibility for the lock, even when having the correct lock token.
"
1,"NullPointerException in NegotiateScheme. - server is configured to allow client to authenticate with kerberos with principal foobar
- client, using httpclient with a registered authscheme SPNEGO set as a NegotiateSchemeFactory

- when the client authenticate with the (correct) principal foobar, it works !
- when the client authenticate with the (wrong) principal fooba, it fails with a NPE below.


Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.commons.codec.binary.Base64.encodeBase64(Base64.java:233)
	at org.apache.commons.codec.binary.Base64.encode(Base64.java:521)
	at org.apache.http.impl.auth.NegotiateScheme.authenticate(NegotiateScheme.java:240)
	at org.apache.http.client.protocol.RequestTargetAuthentication.process(RequestTargetAuthentication.java:99)
	at org.apache.http.protocol.ImmutableHttpProcessor.process(ImmutableHttpProcessor.java:108)
	at org.apache.http.protocol.HttpRequestExecutor.preProcess(HttpRequestExecutor.java:167)
	at org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:460)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:689)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:624)
	at org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:602)
"
1,"GData-server storage fix activation depth. Fixed nullpointer exception while rendering feeds with big amount of extensions. DB4O context.

"
1,"IndexMerger: Synchronization issue on repository shutdown. After inserting a large number of nodes (~200000) into a repository and then closing the session, I get the following exception:

19:42:40.556 [jackrabbit-pool-5] DEBUG o.a.j.core.query.lucene.IndexMerger - # of busy merge workers: 2
19:42:40.556 [jackrabbit-pool-3] DEBUG o.a.j.core.query.lucene.IndexMerger - accepted merge request
19:42:40.556 [jackrabbit-pool-5] DEBUG o.a.j.core.query.lucene.IndexMerger - Worker finished
19:42:40.556 [jackrabbit-pool-3] DEBUG o.a.j.core.query.lucene.IndexMerger - create new index
19:42:40.557 [jackrabbit-pool-3] DEBUG o.a.j.core.query.lucene.IndexMerger - get index readers from MultiIndex
19:42:40.640 [main] INFO  c.a.kmp.generator.JpaToJcrImporter - end JCR save
19:42:40.849 [main] INFO  o.a.j.core.TransientRepository - Session closed
19:42:40.849 [main] INFO  o.a.jackrabbit.core.RepositoryImpl - Shutting down repository...
19:42:40.849 [main] DEBUG o.a.j.core.query.lucene.IndexMerger - dispose IndexMerger
19:42:40.849 [main] DEBUG o.a.j.core.query.lucene.IndexMerger - quit flag set
19:42:40.849 [main] INFO  o.a.j.core.query.lucene.SearchIndex - Index closed: repository/repository/index
19:42:40.850 [main] INFO  o.a.jackrabbit.core.RepositoryImpl - shutting down workspace 'default'...
19:42:40.850 [main] INFO  o.a.j.c.o.ObservationDispatcher - Notification of EventListeners stopped.
19:42:40.850 [main] DEBUG o.a.j.core.query.lucene.IndexMerger - dispose IndexMerger
19:42:40.850 [main] DEBUG o.a.j.core.query.lucene.IndexMerger - quit flag set
19:42:40.850 [main] DEBUG o.a.j.core.query.lucene.IndexMerger - IndexMerger.Worker thread stopped
19:42:40.855 [main] DEBUG o.a.j.core.query.lucene.IndexMerger - index added: name=_6h, numDocs=890
19:42:41.367 [jackrabbit-pool-3] DEBUG o.a.j.core.query.lucene.IndexMerger - deleting index _6g
19:42:41.393 [main] INFO  o.a.j.core.query.lucene.SearchIndex - Index closed: repository/workspaces/default/index
19:42:41.410 [jackrabbit-pool-3] ERROR o.a.j.core.query.lucene.IndexMerger - Error while merging indexes: 
org.apache.lucene.store.AlreadyClosedException: this IndexWriter is closed
	at org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:412) ~[lucene-core-2.4.1.jar:2.4.1 750176 - 2009-03-04 21:56:52]
	at org.apache.lucene.index.IndexWriter.ensureOpen(IndexWriter.java:417) ~[lucene-core-2.4.1.jar:2.4.1 750176 - 2009-03-04 21:56:52]
	at org.apache.lucene.index.IndexWriter.startTransaction(IndexWriter.java:2511) ~[lucene-core-2.4.1.jar:2.4.1 750176 - 2009-03-04 21:56:52]
	at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:3273) ~[lucene-core-2.4.1.jar:2.4.1 750176 - 2009-03-04 21:56:52]
	at org.apache.jackrabbit.core.query.lucene.PersistentIndex.addIndexes(PersistentIndex.java:114) ~[jackrabbit-core-2.1.1.jar:2.1.1]
	at org.apache.jackrabbit.core.query.lucene.IndexMerger$Worker.run(IndexMerger.java:525) ~[jackrabbit-core-2.1.1.jar:2.1.1]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441) [na:1.6.0_20]
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) [na:1.6.0_20]
	at java.util.concurrent.FutureTask.run(FutureTask.java:138) [na:1.6.0_20]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98) [na:1.6.0_20]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:207) [na:1.6.0_20]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_20]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_20]
	at java.lang.Thread.run(Thread.java:619) [na:1.6.0_20]
19:42:41.420 [jackrabbit-pool-3] DEBUG o.a.j.core.query.lucene.IndexMerger - Worker finished
19:42:41.839 [main] INFO  o.a.j.c.p.b.DerbyPersistenceManager - Database 'repository/workspaces/default/db' shutdown.


The problem is reproducible. Apparently, the Lucene index is closed before all IndexMerger worker threads are terminated. The root cause seems to be the AtomicBoolean IndexMerger.Worker.terminated which is always true. The enclosed patch solves the problem in my use case.

"
1,"IndexWriter.addIndexes(IndexReader[] readers) doesn't correctly handle exception success flag.. After this bit of code in addIndexes(IndexReader[] readers)

 try {
        flush(true, false, true);
        optimize();					  // start with zero or 1 seg
        success = true;
      } finally {
        // Take care to release the write lock if we hit an
        // exception before starting the transaction
        if (!success)
          releaseWrite();
      }

The success flag should be reset to ""false"" because it's used again in another try/catch/finally block.  

TestIndexWriter.testAddIndexOnDiskFull() sometimes will hit this bug; but it's infrequent.


"
1,"if you use setNorm, lucene writes a headerless separate norms file. In this case SR.reWrite just writes the bytes with no header...
we should write it always.

we can detect in these cases (segment written <= 3.1) with a 
sketchy length == maxDoc check.
"
1,"In DatabasePersistenceManager.store(), if the exception is null or its cause is not an SQLException, then the PM keeps looping forever. In the line
if (ise != null && ise.getCause() instanceof SQLException && --trials > 0) {
if one of the first two checks fails, the shortcircuit doesn't decrement trials."
1,"AbstractHttpClient.determineTarget(HttpUriRequest). Issue with 4.1 beta1 fails to parse the right host from the URL, eg. http://my.site.com/search/?for=http://other.site.com
This fails request for eg. REST that has a param value with ':' or '?' or '/'.

AbstractHttpClient.determineTarget(HttpUriRequest)
httpcomponents-client-4.0.3:
    private HttpHost determineTarget(HttpUriRequest request) {
        // A null target may be acceptable if there is a default target.
        // Otherwise, the null target is detected in the director.
        HttpHost target = null;

        URI requestURI = request.getURI();
        if (requestURI.isAbsolute()) {
            target = new HttpHost(
                    requestURI.getHost(),
                    requestURI.getPort(),
                    requestURI.getScheme());
        }
        return target;
    }


httpcomponents-client-4.1-beta1:
    private HttpHost determineTarget(HttpUriRequest request) throws ClientProtocolException {
        // A null target may be acceptable if there is a default target.
        // Otherwise, the null target is detected in the director.
        HttpHost target = null;

        URI requestURI = request.getURI();
        if (requestURI.isAbsolute()) {
            String ssp = requestURI.getSchemeSpecificPart();
            ssp = ssp.substring(2, ssp.length()); //remove ""//"" prefix
            int end = ssp.indexOf(':') > 0 ? ssp.indexOf(':') :
                    ssp.indexOf('/') > 0 ? ssp.indexOf('/') :
                    ssp.indexOf('?') > 0 ? ssp.indexOf('?') : ssp.length();
            String host = ssp.substring(0, end);

            int port = requestURI.getPort();
            String scheme = requestURI.getScheme();
            if (host == null || """".equals(host)) {
                throw new ClientProtocolException(
                        ""URI does not specify a valid host name: "" + requestURI);
            }
            target = new HttpHost(host, port, scheme);
        }
        return target;
    }
"
1,"[PATCH] RussianAnalyzer's tokenizer skips numbers from input text,. RussianAnalyzer's tokenizer skips numbers from input text, so that resulting token stream miss numbers. Problem can be solved by adding numbers to RussianCharsets.UnicodeRussian. See test case below  for details.

{code:title=TestRussianAnalyzer.java|borderStyle=solid}

public class TestRussianAnalyzer extends TestCase {

  Reader reader = new StringReader(""text 1000"");

  // test FAILS
  public void testStemmer() {
    testAnalyzer(new RussianAnalyzer());
  }

  // test PASSES
  public void testFixedRussianAnalyzer() {
    testAnalyzer(new RussianAnalyzer(getRussianCharSet()));
  }

  private void testAnalyzer(RussianAnalyzer analyzer) {
    try {
      TokenStream stream = analyzer.tokenStream(""text"", reader);
      assertEquals(""text"", stream.next().termText());
      assertNotNull(stream.next());
    } catch (IOException e) {
      fail(e.getMessage());
    }
  }

  private char[] getRussianCharSet() {
    int length = RussianCharsets.UnicodeRussian.length;
    final char[] russianChars = new char[length + 10];

    System
        .arraycopy(RussianCharsets.UnicodeRussian, 0, russianChars, 0, length);
    russianChars[length++] = '0';
    russianChars[length++] = '1';
    russianChars[length++] = '2';
    russianChars[length++] = '3';
    russianChars[length++] = '4';
    russianChars[length++] = '5';
    russianChars[length++] = '6';
    russianChars[length++] = '7';
    russianChars[length++] = '8';
    russianChars[length] = '9';
    return russianChars;
  }
}

{code} "
1,"Document View Import: ISO 9075-encoded element/attribute names may lead to illegal node/property names . reported by sridhar raman on the users-list:

importing the following xml document leads to a node of name ""abc [1]"" which is illegal:

<?xml version=""1.0""?>
<abc_x0020__x005B_1_x005D_ foo=""bar""/>
"
1,"Wrong cookie matching port number reported when using a proxy. Following the example given in https://issues.apache.org/jira/browse/HTTPCLIENT-852 and the route HttpRoute[{}->http://xyz.webfactional.com:7295->http://www.seoconsultants.com]:

one of the new cookies is reported to be added as:

[java] 2009/05/28 19:58:23:398 CEST [DEBUG] RequestAddCookies - Cookie [version: 0][name: ASPSESSIONIDCSARBQBA][value: MAMPAMKCBDJJFKNAAPKPMDAA][domain: www.seoconsultants.com][path: /][expiry: null] match [www.seoconsultants.com:7295/]

whereas it should be:

[java] 2009/05/28 19:57:46:667 CEST [DEBUG] RequestAddCookies - Cookie [version: 0][name: ASPSESSIONIDCSARBQBA][value: AAMPAMKCMBINHNEHPFEBFADA][domain: www.seoconsultants.com][path: /][expiry: null] match [www.seoconsultants.com:80/]

i.e. the same as without using a proxy. 7295 is the port number used to access the proxy. The target domain www.seoconsultants.com is accessed through the regular HTTP port number 80, thus the cookie matching should also refer to port 80 and not the proxy's port."
1,"CharArraySet.contains(char[] text, int off, int len) does not work. I try to use the CharArraySet for a filter I am writing. I heavily use char-arrays in my code to speed up things. I stumbled upon a bug in CharArraySet while doing that.

The method _public boolean contains(char[] text, int off, int len)_ seems not to work.

When I do 

{code}
if (set.contains(buffer,offset,length) {
  ...
}
{code}

my code fails.

But when I do

{code}
if (set.contains(new String(buffer,offset,length)) {
   ...
}
{code}

everything works as expected.

Both variants should behave the same. I attach a small piece of code to show the problem."
1,"RamUsageEstimator.NUM_BYTES_ARRAY_HEADER and other constants are incorrect. RamUsageEstimator.NUM_BYTES_ARRAY_HEADER is computed like that: NUM_BYTES_OBJECT_HEADER + NUM_BYTES_INT + NUM_BYTES_OBJECT_REF. The NUM_BYTES_OBJECT_REF part should not be included, at least not according to this page: http://www.javamex.com/tutorials/memory/array_memory_usage.shtml

{quote}
A single-dimension array is a single object. As expected, the array has the usual object header. However, this object head is 12 bytes to accommodate a four-byte array length. Then comes the actual array data which, as you might expect, consists of the number of elements multiplied by the number of bytes required for one element, depending on its type. The memory usage for one element is 4 bytes for an object reference ...
{quote}

While on it, I wrote a sizeOf(String) impl, and I wonder how do people feel about including such helper methods in RUE, as static, stateless, methods? It's not perfect, there's some room for improvement I'm sure, here it is:

{code}
	/**
	 * Computes the approximate size of a String object. Note that if this object
	 * is also referenced by another object, you should add
	 * {@link RamUsageEstimator#NUM_BYTES_OBJECT_REF} to the result of this
	 * method.
	 */
	public static int sizeOf(String str) {
		return 2 * str.length() + 6 // chars + additional safeness for arrays alignment
				+ 3 * RamUsageEstimator.NUM_BYTES_INT // String maintains 3 integers
				+ RamUsageEstimator.NUM_BYTES_ARRAY_HEADER // char[] array
				+ RamUsageEstimator.NUM_BYTES_OBJECT_HEADER; // String object
	}
{code}

If people are not against it, I'd like to also add sizeOf(int[] / byte[] / long[] / double[] ... and String[])."
1,"Lock.refresh(): throws if lock is alive. The spec says:

Lock.refresh():
If this lock's time-to-live is governed by a timer, this method resets that timer so that the lock does not timeout and expire. If this lock's time-to-live is not governed by a timer, then this method has no effect.A LockException is thrown if this Session does not hold the correct lock token for this lock.A RepositoryException is thrown if another error occurs.


The jackrabbit impl does:

/**
     * {@inheritDoc}
     */
    public void refresh() throws LockException, RepositoryException {
        if (isLive()) {
            throw new LockException(""Lock still alive."");
        }
        [...]
    }


Isn't this a leftover from a very old version of the spec?
There was ones a misunderstanding about the refresh (bringing locks back to live) that has been discussed (mail by david to g. clemm, tobi, peeter and myself, 25.4.2005). as far is i know everybody agreed that this does not make sense and the spec has been adjusted accordingly.

The usage of the refresh is to prevent the lock from being timeouted. That was the original meaning of the refresh, when i suggested it for the JCR locking. If the lock is still alive and there is no timeout to reset, then the method should simply not do anything.

am i missing something? 

regards
angela

"
1,IndexWriter.doAfterFlush not being called when there are no deletions flushed. It should be called when flushing either added docs or deletions.  The fix is trivial.  I'll commit shortly to trunk & 2.3.2.
1,"cache module does not completely handle upstream Warning headers correctly. There are a couple of MUST requirements from the RFC for Warning headers that aren't correctly handled by the current implementation:

http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.46

1. ""However, if a cache successfully validates a cache entry, it SHOULD remove any Warning headers previously attached to that entry except as specified for specific Warning codes. It MUST then add any Warning headers received in the validating response.""

2. ""If an implementation receives a message with a warning-value that includes a warn-date, and that warn-date is different from the Date value in the response, then that warning-value MUST be deleted from the message before storing, forwarding, or using it. (This prevents bad consequences of naive caching of Warning header fields.) If all of the warning-values are deleted for this reason, the Warning header MUST be deleted as well."" "
1,"o.a.j.core.state.ChildNodeEntries does not override equals(Object) and hashCode() methods. o.a.j.c.state.NodeStateMerger calls ChildNodeEntries.equals(ChildNodeEntries) to compare two child node entries collections.
ChildNodeEntries however doesn't override the equals(Object) method."
1,"InstantiatedIndexReader does not handle #termDocs(null) correct (AllTermDocs). This patch contains core changes so someone else needs to commit it.

Due to the incompatible #termDocs(null) behaviour at least MatchAllDocsQuery, FieldCacheRangeFilter and ValueSourceQuery fails using II since 2.9.

AllTermDocs now has a superclass, AbstractAllTermDocs that also InstantiatedAllTermDocs extend.

Also:

 * II-tests made less plausable to pass on future incompatible changes to TermDocs and TermEnum
 * IITermDocs#skipTo and #next mimics the behaviour of document posisioning from SegmentTermDocs#dito when returning false
 * II now uses BitVector rather than sets for deleted documents
"
1,"ClassCastException bei unregisterNodeType. I have a NodeType with various childnodes which I want to unregister. If I call:

    
      NodeTypeManager ndmg = session.getWorkspace().getNodeTypeManager();
      NodeTypeRegistry ntReg = ((NodeTypeManagerImpl) ndmg).getNodeTypeRegistry();
      ntReg.unregisterNodeType(new QName(""testURI"",""Page""));


I get a 

java.lang.ClassCastException
 at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.getDependentNodeTypes(NodeTypeRegistry.java:1242)
 at org.apache.jackrabbit.core.nodetype.NodeTypeRegistry.unregisterNodeType(NodeTypeRegistry.java:1120)
 at de.freaquac.test.JCRTest.main(JCRTest.java:80)

It looks to me like there are QNames in the Iterator but NodeTyeDefs are expected.
"
1,"NullPointerException may be thrown when trying to enumerate observation event listeners. When calling the ObservationManager.getRegisteredEventListeners() a NullPointerException may be thrown if no event listener has been registered (yet). The reason for this is, that in the ClientObservationManager.getRegisteredEventListener method an internal field is access directly, which is created on demand and thus may be null."
1,"SSL connections cannot be established using resolvable IP address. HttpClient 4.1 introduced a regression in establishing SSL connections to remote peers (it seems this is a common regression for major httpclient updates, see HTTPCLIENT-803).
The new SSLSocketFactory.connectSocket method calls the X509HostnameVerifier with InetSocketAddress.getHostName() parameter. When the selected IP address has a reverse lookup name, the verifier is called with the resolved name, and so the IP check fails.
4.0 release checked for original ip/hostname, but this cannot be done with the new connectSocket() method. 
The TestHostnameVerifier.java only checks 127.0.0.1/.2 and so masked the issue, because the matching certificate has both ""localhost"" and ""127.0.0.1"", but actually only ""localhost"" is matched. A test case with 8.8.8.8 would be better."
1,"RepositoryCopier does not copy open-scoped Locks. If you use the RepositoryCopier to make a backup of your repository and you have open-scoped (not session scoped) locks, these locks will not be copied. If you try to restore your copy of the repository all locks are gone."
1,"WebDAV server should treat non-wellformed XML in request bodies as error. The WebDAV server should treat non-wellformed XML request bodies as errors (instead of treating the request as if the request body was missing).

(causes Litmus test suite failure in test case propfind_invalid)"
1,"IndexReader.getCurrentVersion() and isCurrent should use commit lock.. There is a race condition if one machine is checking the current version of an index while another wants to update the segments file in IndexWriter.close().

java.io.IOException: Cannot delete segments
	at org.apache.lucene.store.FSDirectory.renameFile(FSDirectory.java:213)
	at org.apache.lucene.index.SegmentInfos.write(SegmentInfos.java:90)
	at org.apache.lucene.index.IndexWriter$3.doBody(IndexWriter.java:503)
	at org.apache.lucene.store.Lock$With.run(Lock.java:109)
	at org.apache.lucene.index.IndexWriter.mergeSegments(IndexWriter.java:501)
	at org.apache.lucene.index.IndexWriter.flushRamSegments(IndexWriter.java:440)
	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:242)

On the windows platform reading the contents of a file disallows deleting the file.

I use Lucene to maintain an index of +-700.000 documents, one server adds documents, while other servers handle the searches.
The search servers poll the index version regularly to check if they have to reopen their IndexSearcher.
Once in a while (about once every two days on average), IndexWriter.close() fails because it cannot delete the previous segments file, even though it hold the commit lock.
The reason is probably that search servers are reading the segments file to check the version without using the commit lock.
"
1,"Unable to create repository using jackrabbit-webapp because a directory called ""jackrabbit"" already exists. I mount the jackrabbit-webapp.war in a Jetty installation
* at startup i have the following exception:
ERROR RepositoryStartupServlet: Either create thejackrabbit/bootstrap.properties file or
ERROR RepositoryStartupServlet: use the '/config/index.jsp' for easy configuration.
ERROR RepositoryStartupServlet: RepositoryStartupServlet initializing failed: javax.servlet.ServletException: Repository startup configuration is not valid.
* then when i access http://localhost:8080/ i am forwarded to the page:
 http://localhost:8080/bootstrap/missing.jsp
* creating the repository by clicking on ""Create Content Repository"" button fails complaining that the jackrabbit directory already exists

Indeed, i find a jackrabbit directory in my JETTY_HOME (from where is started Jetty).

A workaround is to delete this ""jackrabbit"" directory and then i can create the repository by clicking on the previous button and therefore access the newly created repository."
1,SearcherManager misses to close IR if manager is closed during reopen. if we close SM while there is a thread calling maybReopen() and swapSearcher throws already closed exception we miss to close the searcher / reader.
1,"HttpOptions.getAllowedMethods expects single Allow header. In client.methods.HttpOptions.getAllowMethods(), a single Allow header is parsed to obtain the result. Since the value is a comma-separated list, servers can optionally return the values in multiple headers. HttpMethod.getHeaders(name) should be used instead of .getFirstHeader(name).
"
1,"CompactNodeTypeDefReader adds nt:base as declared supertype even if already extending. (reported to the list by michael singer)

I wrote a simple program which uses the nt-ns-util contribution to
register custom node types written in CND language.

I defined the following (very simple) custom node types:

<test = 'http://foo.bar/test'>
[test:firstnodetype]
+ test:secondnodetype mandatory

<test = 'http://foo.bar/test'>
[test:secondnodetype] > test:firstnodetype
+ test:thirdnodetype

<test = 'http://foo.bar/test'>
[test:thirdnodetype] > test:secondnodetype
- test:catalog (string)  < 'URI', 'URN', 'DOI', 'ISBN', 'ISSN'
- test:entry (string) m


In the resulting custom_nodetypes.xml each of the custom nodes has a
supertype of ""nt:base"" but I didn't explicitely define a supertype of
""nt:base"" for [test:secondnodetype] and [test:thirdnodetype].

I think this behavior is wrong since the method getDeclaredSupertypes()
of class NodeType always returns ""nt:base"" plus the explicitely declared
Supertype (which it e.g. does not for ""nt:folder"").
"
1,"OutOfMemory problem: HandleMonitor does not release closed input streams. The class o.a.j.core.fs.local.HandleMonitor does not release closed MonitoredInputStream. There is a close method, but it is never called. The input streams are kept in a hash set / map of HandleMonitor. Eventually, this leads to an OutOfMemory exception after opening / closing many files."
0,"allow subclassing of NodeTypeReader. in working towards an offline tool to import custom namespaces and node types, i found that i needed to make some small changes to NodeTypeReader and DOMWalker so that i could subclass NodeTypeReader and access the namespaces specified in the node type definition file. see the attached patch.
"
0,Improvements to user management (2). follow up issue as JCR-2199 is already closed.
0,"Speed up Top-K sampling tests. speed up the top-k sampling tests (but make sure they are thorough on nightly etc still)

usually we would do this with use of atLeast(), but these tests are somewhat tricky,
so maybe a different approach is needed."
0,"allow different strategies when checking CN of x509 cert. We're now doing a decent job for checking the CN of the x509 cert with https:

http://issues.apache.org/jira/browse/HTTPCLIENT-613

I think the patch for HTTPCLIENT-613 should cover 99.9% of the users out there.  But there are some more esoteric possibilities, so I think Oleg is right.  We need to let the user change the strategy, or provide their own strategy if they want to. 

Some additional things to think about:

- http://wiki.cacert.org/wiki/VhostTaskForce !!!   CN is depreciated?!?!   (I am not able to find a popular website on HTTPS that isn't using CN!)

- [*.example.com] matches subdomains [a.b.example.com] on Firefox, but not IE6.  The patch for HTTPCLIENT-613 allows subdomains.

- Should we support multiple CN's in the subject?

- Should we support ""subjectAltName=DNS:www.example.com"" ?  Should we support lots of them in a single cert?

- Should we support a mix of CN and subjectAltName?


If we do create some alternate strategies for people to try, I'd probably lean towards something like this:

X509NameCheckingStrategy.SUN_JAVA_6  (default)
X509NameCheckingStrategy.FIREFOX2
X509NameCheckingStrategy.IE7
X509NameCheckingStrategy.FIRST_CN_AND_NO_WILDCARDS   (aka ""STRICT"")

"
0,Allow separate control over whether body is stored or analyzed. Simple enhancement to DocMaker.
0,"Make tests using java.util.Random reproducible on failure. This is a patch for LuceneTestCase to support logging of the Random seed used in randomized tests. The patch also includes an example implementation in TestTrieRangeQuery.

It overrides the protected method runTest() and inserts a try-catch around the super.runTest() call. Two new methods newRandom() and newRandom(long) are available for the test case. As each test case is run in an own TestCase object instance (so 5 test methods in a class instantiate 5 instances each method working in separate), the random seed is saved on newRandom() and when the test fails with any Throwable, a message with the seed (if not null) is printed out. If newRandom was never called no message will be printed.

This patch has only one problem: If a single test method calls newRandom() more than once, only the last seed is saved and printed out. But each test method in a Testcase should call newRandom() exactly once for usage during the execution of this test method. And it is not thread save (no sync, no volatile), but for tests it's unimportant.

I forgot to mention: If a test fails, the message using the seed is printed to stdout. The developer can then change the test temporarily:

{code}LuceneTestCase.newRandom() -> LuceneTestCase.newRandom(long){code}

using the seed from the failed test printout.

*Reference:*
{quote}
: By allowing Random to randomly seed itself, we effectively test a much
: much larger space, ie every time we all run the test, it's different.  We can
: potentially cast a much larger net than a fixed seed.

i guess i'm just in favor of less randomness and more iterations.

: Fixing the bug is the ""easy"" part; discovering a bug is present is where
: we need all the help we can get ;)

yes, but knowing a bug is there w/o having any idea what it is or how to 
trigger it can be very frustrating.

it would be enough for tests to pick a random number, log it, and then use 
it as the seed ... that way if you get a failure you at least know what 
seed was used and you can then hardcode it temporarily to reproduce/debug

-Hoss
{quote}"
0,"activity storage path. JSR-283 states in 15.12.3:

""15.12.3 Activity Storage
Activities are persisted as nodes of type nt:activity under system-generated
node names in activity storage below /jcr:system/jcr:activities.""

As far as I can tell, this is currently not the case.

A related test case, org.apache.jackrabbit.test.api.version.ActivitiesTest#testActivitiesPath, apparently was taken out accidentally.

If Jackrabbit can't implement this JCR requirement, we either need to document it, or raise it as issue in the JCR Expert Group."
0,"Order of stored Fields not maintained. As noted in these threads...

http://www.nabble.com/Order-of-fields-returned-by-Document.getFields%28%29-to21034652.html
http://www.nabble.com/Order-of-fields-within-a-Document-in-Lucene-2.4%2B-to24210597.html

somewhere prior to Lucene 2.4.1 a change was introduced that prevents the Stored fields of a Document from being returned in same order that they were originally added in.  This can cause serious performance problems for people attempting to use LoadFirstFieldSelector or a custom FieldSelector with the LOAD_AND_BREAK, or the SIZE_AND_BREAK options (since the fields don't come back in the order they expect)

Speculation in the email threads is that the origin of this bug is code introduced by LUCENE-1301 -- but the purpose of that issue was refactoring, so if it really is the cause of the change this would seem to be a bug, and not a side affect of a conscious implementation change.

Someone who understands indexing internals should investigate this.  At a minimum, if it's decided that this is not actual a bug, then prior to resolving this bug the wiki docs and some of the FIeldSelector javadocs should be updated to make it clear what order Fields will be returned in.

"
0,Move FilterIterator and SizedIterator from package flat to package iterator. I suggest to move said classes from package org.apache.jackrabbit.commons.flat to package org.apache.jackrabbit.commons.iterator. 
0,Include to jackrabbit-jcr-rmi and jackrabbit-jcr-servlet in main trunk. Jackrabbit 2.0 should include the 2.0 version of the RMI component and the related jcr-servlet updates.
0,"factor out a shared spellchecking module. In lucene's contrib we have spellchecking support (index-based spellchecker, directspellchecker, etc). 
we also have some things like pluggable comparators.

In solr we have auto-suggest support (with two implementations it looks like), some good utilities like HighFrequencyDictionary, etc.

I think spellchecking is really important... google has upped the ante to what users expect.
So I propose we combine all this stuff into a shared modules/spellchecker, which will make it easier
to refactor and improve the quality.
"
0,Versioning operations should be done on the workspace. currently all versioning operations modify the transient states of the items where the operation is executed although all operations are workspace operations.
0,"TCK does not clean 2nd workspace during AbstractJCRTest.setUp(). the XATest.testXAVersionsThoroughly fails if run 2 times, since the 2nd workspace is not cleaned on startup. will provide provisonairy fix."
0,"Remove ""System Properties"" page from release specific docs. We no longer use system properties to configure Lucene in version 3.0, the page is obsolete and should be removed before release."
0,"Cosmetic JavaDoc updates. I've taken the liberty of making a few cosmetic updates to various JavaDocs:

* MergePolicy (minor cosmetic change)
* LogMergePolicy (minor cosmetic change)
* IndexWriter (major cleanup in class description, changed anchors to JavaDoc links [now works in Eclipse], no content change)

Attached diff from SVN r780545.

I would appreciate if whomever goes over this can let me know if my issue parameter choices were correct (yeah, blame my OCD), and if there's a more practical/convenient way to send these in, please let me know :-)
"
0,"Changes.html generation improvements. Bug fixes for and improvements to changes2html.pl, which generates Changes.html from CHANGES.txt:

# When the current location has a fragment identifier, expand parent sections, so that the linked-to section is visible.
# Properly handle beginning-of-release comments that don't fall under a section heading (previously: some content in release ""1.9 final"" was invisible).
# Auto-linkify SOLR-XXX and INFRA-XXX JIRA issues (previously: only LUCENE-XXX issues).
# Auto-linkify Bugzilla bugs prefaced with ""Issue"" (previously: only ""Bug"" and ""Patch"").
# Auto-linkify Bugzilla bugs in the form ""bugs XXXXX and YYYYY"".
# Auto-linkify issues that follow attributions.
"
0,Get rid of (another) hard coded path. 
0,"Move FuzzyQuery rewrite as separate RewriteMode into MTQ, was: Highlighter fails to highlight FuzzyQuery. As FuzzyQuery does not allow to change the rewrite mode, highlighter fails with UOE in flex since LUCENE-2110, because it changes the rewrite mode to Boolean query. The fix is: Allow MTQ to change rewrite method and make FUZZY_REWRITE public for that.

The rewrite mode will live in MTQ as TOP_TERMS_SCORING_BOOLEAN_REWRITE. Also the code will be refactored to make heavy reuse of term enumeration code and only plug in the PQ for filtering the top terms."
0,"Allow to pass an instance of RateLimiter to FSDirectory allowing to rate limit merge IO across several directories / instances. This can come in handy when running several Lucene indices in the same VM, and wishing to rate limit merge across all of them."
0,"contrib/benchmark config does not play nice with doubles with the flush.by.ram value. In the o.a.l.benchmark.byTask.utils.Config.java file, the nextRound and various other methods do not handle doubles in the ""round"" property configuration syntax.

To replicate this, copy the micro-standard.alg and replace 
merge.factor=mrg:10:100:10:100
max.buffered=buf:10:10:100:100

with

ram.flush.mb=ram:32:40:48:56

and you will get various ClassCastExceptions in Config (one in newRound() and, when that is fixed, in getColsValuesForValsByRound.

The fix seems to be to just to mirror the handling of int[].

The fix seems relatively minor.  Patch shortly and will plan to commit tomorrow evening."
0,"TestIndexModifier.testIndexWithThreads is not valid?. I recently started playing with the trunk of SVN, and noticed that intermitently, TestIndexModifier.testIndexWithThreads (revision 292010) would fail.

The basic premise of the test seems to be that 3 pairs of IndexThread instances can be started in parallel, each pair using the same instance of IndexModifier to concurrently and randomly add/delete/optimize a single FSDirectory index.  
The test is considered a success if the sum of additions-deletions recorded by each pair of threads equals the final docCount() for the IndexModifier instance used by that pair of threads.

Now I freely admit that I'm not 100% familiar with the code for IndexModifier, but at a glance, the basic premise seems to be: 
   a) If method for IndexWriter is called, open it if needed, close the IndexReader first if needed.
   b) if method for IndexReader is called, open it if needed, close the IndexWriter first if needed.

If I'm understnading that correctly, I see no reason to assume this test will pass.  
It seems like there could be plenty of scenerios in which the number of additions-deletions != docCount(). The most trivial example I can think of is:
   1) the first IndexThread instance which has a chance to run adds a document, and optimizes before any other IndexThreads ever open the Directory.
   2) a subsequent pair of IndexThread instances open their IndexModifier instance before any documents are deleted.
   3) the IndexThread instances from #2 do nothing but add documents
...that pair of IndexThreads is now garunteed to have recorded a differnet number of additions then the docCount returned by their IndexModifier.

Am I missing something, or should this test be removed?

"
0,"Analysis Package Level Javadocs. Analysis package level javadocs need improving.  An overview of what an Analyzer does, and maybe some sample code showing how to write you own Analyzer, Tokenizer and TokenFilter would be really helpful.  Bonus would be some discussion on best practices for achieving performance during analysis. "
0,bug form doesn't list latest version.  
0,"[PATCH] IndexWriter.maybeMergeSegments() takes lots of CPU resources. Note: I believe this to be the same situation with 1.4.3 as with SVN HEAD.

Analysis using hprof utility shows that during index creation with many
documents highlights that the CPU spends a large portion of it's time in
IndexWriter.maybeMergeSegments(), which seems to be a 'waste' compared with
other valuable CPU intensive operations such as tokenization etc.

Using the following test snippet to retrieve some rows from the db and create an
index:

        Analyzer a = new StandardAnalyzer();
        writer = new IndexWriter(indexDir, a, true);
        writer.setMergeFactor(1000);
        writer.setMaxBufferedDocs(10000);
        writer.setUseCompoundFile(false);
        connection = DriverManager.getConnection(
                ""jdbc:inetdae7:tower.aconex.com?database=<somedb>"", ""secret"",
                ""squirrel"");
        String sql = ""select userid, userfirstname, userlastname, email from userx"";
        LOG.info(""sql="" + sql);
        Statement statement = connection.createStatement();
        statement.setFetchSize(5000);
        LOG.info(""Executing sql"");
        ResultSet rs = statement.executeQuery(sql);
        LOG.info(""ResultSet retrieved"");
        int row = 0;

        LOG.info(""Indexing users"");
        long begin = System.currentTimeMillis();
        while (rs.next()) {
            int userid = rs.getInt(1);
            String firstname = rs.getString(2);
            String lastname = rs.getString(3);
            String email = rs.getString(4);
            String fullName = firstname + "" "" + lastname;
            Document doc = new Document();
            doc.add(Field.Keyword(""userid"", userid+""""));
            doc.add(Field.Keyword(""firstname"", firstname.toLowerCase()));
            doc.add(Field.Keyword(""lastname"", lastname.toLowerCase()));
            doc.add(Field.Text(""name"", fullName.toLowerCase()));
            doc.add(Field.Keyword(""email"", email.toLowerCase()));
            writer.addDocument(doc);
            row++;
            if((row % 100)==0){
                LOG.info(row + "" indexed"");
            }
        }
        double end = System.currentTimeMillis();
        double diff = (end-begin)/1000;
        double rate = row/diff;
        LOG.info(""rate:"" +rate);

On my 1.5GHz PowerBook with 1.5Gb RAM and a 5400 RPM drive, my CPU is maxed out,
and I end up getting a rate of indexing between 490-515 documents/second run
over 10 times in succession.  

By applying a simple patch to IndexWriter (see attached shortly), which defers
the calling of maybeMergeSegments() so that it is only called every 2000
times(an arbitrary figure), I appear to get a new rate of between 945-970
documents/second.  Using Luke to look inside each index created between these 2
there does not appear to be any difference.  Same number of Documents, same
number of Terms.

I'm not suggesting one should apply this patch, I'm just highlighting the
difference in performance that this sort of change gives you.  

We are about to use Lucene to index 4 million construction document records, and
so speeding up the indexing process is in our best interest! :)  If one
considers the amount of CPU time spent in maybeMergeSegments over the initial
index creation of 4 million documents, I think one could see how it would be
ideal to try to speed this area up (at least move the bottleneck to IO). 

I woul appreciate anyone taking a moment to comment on this."
0,"UserImporter should trigger execution AuthorizableActions in case of user/group creation. in accordance to the new implementation specific extensions made to user mangement in JCR-3118 the user-importer
should be adjusted as well."
0,"Make ""boolean readOnly"" a required arg to IndexReader.open. Most apps don't need read/write IndexReader, and, a readOnly
IndexReader has better concurrent performance.

I'd love to simply default readOnly to true, and you'd have to specify
""false"" if you want a read/write reader (I think that's the natural
default), but I think that'd break too many back-compat cases.

So the workaround is to make the parameter explicit, in 2.9.

I think even for IndexSearcher's methods that open an IndexReader
under the hood, we should also make the parameter explicit.
"
0,"Add RepositoryException to JackrabbitAccessControlList#getRestrictionNames and #getRestrictionType. Throughout the JCR and Jackrabbit API methods that include name processing are define to throw RepositoryException in case of 
an error related to name processing (due to lack of a more specific exception in the javax.jcr package space).

Unfortunately, I forgot those in JackrabbitAccessControlList and JackrabbitAccessControlEntry. While the latter has already been addressed for the 2.2 release,
I would like to fix the JackrabbitAccessControlList interface as well (and subsequently also fix the implementations that currently do not properly cope with 
names).
"
0,"Remove dependency on  EDU.oswego.cs.dl.util.concurrent. EDU.oswego.cs.dl.util.concurrent is in maintenance mode, and http://g.oswego.edu/dl/classes/EDU/oswego/cs/dl/util/concurrent/package-summary.html advises to migrate to the JDK5 java.util.concurrent package.
"
0,"Add ability to specify compilation/matching flags to RegexCapabiltiies implementations. The Jakarta Regexp and Java Util Regex packages both support the ability to provides flags that alter the matching behavior of a given regular expression. While the java.util.regex.Pattern implementation supports providing these flags as part of the regular expression string, the Jakarta Regexp implementation does not.  Therefore, this improvement request is to add the capability to provide those modification flags to either implementation. 

I've developed a working implementation that makes minor additions to the existing code. The default constructor is explicitly defined with no arguments, and then a new constructor with an additional ""int flags"" argument is provided. This provides complete backwards compatibility. For each RegexCapabilties implementation, the appropriate flags from the regular expression package is defined as  FLAGS_XXX static fields. These are pass through to the underlying implementation. They are re-defined to avoid bleeding the actual implementation classes into the caller namespace.

Proposed changes:

For the JavaUtilRegexCapabilities.java, the following is the changes made.

  private int flags = 0;
  
  // Define the optional flags from Pattern that can be used.
  // Do this here to keep Pattern contained within this class.
  
  public final int FLAG_CANON_EQ = Pattern.CANON_EQ;
  public final int FLAG_CASE_INSENSATIVE = Pattern.CASE_INSENSATIVE;
  public final int FLAG_COMMENTS = Pattern.COMMENTS;
  public final int FLAG_DOTALL = Pattern.DOTALL;
  public final int FLAG_LITERAL = Pattern.LITERAL;
  public final int FLAG_MULTILINE = Pattern.MULTILINE;
  public final int FLAG_UNICODE_CASE = Pattern.UNICODE_CASE;
  public final int FLAG_UNIX_LINES = Pattern.UNIX_LINES;
  
  /**
   * Default constructor that uses java.util.regex.Pattern 
   * with its default flags.
   */
  public JavaUtilRegexCapabilities()  {
    this.flags = 0;
  }
  
  /**
   * Constructor that allows for the modification of the flags that
   * the java.util.regex.Pattern will use to compile the regular expression.
   * This gives the user the ability to fine-tune how the regular expression 
   * to match the functionlity that they need. 
   * The {@link java.util.regex.Pattern Pattern} class supports specifying 
   * these fields via the regular expression text itself, but this gives the caller
   * another option to modify the behavior. Useful in cases where the regular expression text
   * cannot be modified, or if doing so is undesired.
   * 
   * @flags The flags that are ORed together.
   */
  public JavaUtilRegexCapabilities(int flags) {
    this.flags = flags;
  }
  
  public void compile(String pattern) {
    this.pattern = Pattern.compile(pattern, this.flags);
  }


For the JakartaRegexpCapabilties.java, the following is changed:

  private int flags = RE.MATCH_NORMAL;

  /**
   * Flag to specify normal, case-sensitive matching behaviour. This is the default.
   */
  public static final int FLAG_MATCH_NORMAL = RE.MATCH_NORMAL;
  
  /**
   * Flag to specify that matching should be case-independent (folded)
   */
  public static final int FLAG_MATCH_CASEINDEPENDENT = RE.MATCH_CASEINDEPENDENT;
 
  /**
   * Contructs a RegexCapabilities with the default MATCH_NORMAL match style.
   */
  public JakartaRegexpCapabilities() {}
  
  /**
   * Constructs a RegexCapabilities with the provided match flags.
   * Multiple flags should be ORed together.
   * 
   * @param flags The matching style
   */
  public JakartaRegexpCapabilities(int flags)
  {
    this.flags = flags;
  }
  
  public void compile(String pattern) {
    regexp = new RE(pattern, this.flags);
  }
"
0,"NodeEntryImpl.getWorkspaceId() very inefficient . NodeEntryImpl.getWorkspaceId() calculates its path on each call by calling itself recursively. Further each call to getWorkspaceId() results in various calls to the path and item factories which might be somewhat expensive by themselves. 

In my test scenario I have a RepositoryService.getItemInfos() call returning ~1000 items. Processing these items results in about 2700000 (!) calls to getWorkspaceId(). Profiler data shows, that 98% of the time to process the 1000 items is spent in getWorkspaceId()  and related calls. "
0,"QueryParser is not applicable for the arguments (String, String, Analyzer) error in results.jsp when executing search in the browser (demo from Lucene 2.0). When executing search in the browser (as described in demo3.html Lucene demo) I get error, because the demo uses the method (QueryParser with three arguments) which is deleted (it was deprecated).
I checked the demo from Lucene 1.4-final it with Lucene 1.4-final - it works, because those time the method was there.
But demo from Lucene 2.0 does not work with Lucene 2.0

The error stack is here:
TTP Status 500 -

type Exception report

message

description The server encountered an internal error () that prevented it from fulfilling this request.

exception

org.apache.jasper.JasperException: Unable to compile class for JSP

An error occurred at line: 60 in the jsp file: /results.jsp
Generated servlet error:
The method parse(String) in the type QueryParser is not applicable for the arguments (String, String, Analyzer)


org.apache.jasper.servlet.JspServletWrapper.handleJspException(JspServletWrapper.java:510)
org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:375)
org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:314)
org.apache.jasper.servlet.JspServlet.service(JspServlet.java:264)
javax.servlet.http.HttpServlet.service(HttpServlet.java:802)

root cause

org.apache.jasper.JasperException: Unable to compile class for JSP

An error occurred at line: 60 in the jsp file: /results.jsp
Generated servlet error:
The method parse(String) in the type QueryParser is not applicable for the arguments (String, String, Analyzer)


org.apache.jasper.compiler.DefaultErrorHandler.javacError(DefaultErrorHandler.java:84)
org.apache.jasper.compiler.ErrorDispatcher.javacError(ErrorDispatcher.java:328)
org.apache.jasper.compiler.JDTCompiler.generateClass(JDTCompiler.java:409)
org.apache.jasper.compiler.Compiler.compile(Compiler.java:297)
org.apache.jasper.compiler.Compiler.compile(Compiler.java:276)
org.apache.jasper.compiler.Compiler.compile(Compiler.java:264)
org.apache.jasper.JspCompilationContext.compile(JspCompilationContext.java:563)
org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:303)
org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:314)
org.apache.jasper.servlet.JspServlet.service(JspServlet.java:264)
javax.servlet.http.HttpServlet.service(HttpServlet.java:802)

note The full stack trace of the root cause is available in the Apache Tomcat/5.5.15 logs."
0,"Add missing getter methods to NumericField, NumericTokenStream, NumericRangeQuery, NumericRangeFilter. These classes are missing bean-style getter methods for some basic properties. This is inconsistent and should be fixed."
0,"Add ability to run backwards-compatibility tests automatically. This is an idea Doug mentioned on LUCENE-1422.

This patch adds new targets to build.xml to automatically download the junit tests from a previous Lucene release and run them against the current core.
Execute tests like this:
ant -Dtag=lucene_2_4_0 test-tag

It will create a new directory tags/lucene_2_4_0 and fetch the tests from the svn repository and run them."
0,"Plug-in authentication modules. Currently only basic authentication is supported.  A Authentication interface
should be provided to allow for plug-in support for other authenticaiton
schemes, some of which may be application specific and therefore have no place
in httpclient itself, but would be required by some users."
0,Use configured credentials in RepositoryFactoryImplTest . The test currently uses hard coded credentials. It should rather use configured credentials like all other tests do.
0,"Default lock timeouts should have static setter/getters. 
We recently stopped using Java system properties to derive defaults for things like the write/commit lock timeout, and switched to getter/setter's across all classes.  See here:

    http://www.gossamer-threads.com/lists/lucene/java-dev/27447

But, in the case at least of the write lock timeout, because it's marked ""public final static"", a consumer of this API can no longer change this value before instantiating the IndexWriter.  This is because the getter/setter for this is not static, which generally makes sense so you can change the timeout for each instance of IndexWriter.  But because IndexWriter on construction uses the timeout value, some uses cases need to change the value before getting an instance of IndexWriter.

This was actually a regression, in that Lucene users lost functionality they previously had, on upgrading.

I would propose that that we add getter/setter for the default value of this timeout, which would be static.  I'll attach a patch file.

See this thread for context that led to this issue:

   http://www.gossamer-threads.com/lists/lucene/java-dev/37421"
0,"Better integration of the TestWebApp-HowTo into the documentation. The excellent webapp howto written by Olegolas needs to be integrated better
into httpclient documentation.  Currently it is in the docs directory as a html
file, but it would be better if it was in the xdocs directory as an xml file."
0,"add suggester that uses shortest path/wFST instead of buckets. Currently the FST suggester (really an FSA) quantizes weights into buckets (e.g. single byte) and puts them in front of the word.
This makes it fast, but you lose granularity in your suggestions.

Lately the question was raised, if you build lucene's FST with positiveintoutputs, does it behave the same as a tropical semiring wFST?

In other words, after completing the word, we instead traverse min(output) at each node to find the 'shortest path' to the 
best suggestion (with the highest score).

This means we wouldnt need to quantize weights at all and it might make some operations (e.g. adding fuzzy matching etc) a lot easier."
0,"[PATCH] MultiFieldQueryParser and BooleanQuery do not provide adequate support for queries across multiple fields. The attached test case demonstrates this problem and provides a fix:
  1.  Use a custom similarity to eliminate all tf and idf effects, just to 
isolate what is being tested.
  2.  Create two documents doc1 and doc2, each with two fields title and 
description.  doc1 has ""elephant"" in title and ""elephant"" in description.  
doc2 has ""elephant"" in title and ""albino"" in description.
  3.  Express query for ""albino elephant"" against both fields.
Problems:
      a.  MultiFieldQueryParser won't recognize either document as containing 
both terms, due to the way it expands the query across fields.
      b.  Expressing query as ""title:albino description:albino title:elephant 
description:elephant"" will score both documents equivalently, since each 
matches two query terms.
  4.  Comparison to MaxDisjunctionQuery and my method for expanding queries 
across fields.  Using notation that () represents a BooleanQuery and ( | ) 
represents a MaxDisjunctionQuery, ""albino elephant"" expands to:
        ( (title:albino | description:albino)
          (title:elephant | description:elephant) )
This will recognize that doc2 has both terms matched while doc1 only has 1 
term matched, score doc2 over doc1.

Refinement note:  the actual expansion for ""albino query"" that I use is:
        ( (title:albino | description:albino)~0.1
          (title:elephant | description:elephant)~0.1 )
This causes the score of each MaxDisjunctionQuery to be the score of highest 
scoring MDQ subclause plus 0.1 times the sum of the scores of the other MDQ 
subclauses.  Thus, doc1 gets some credit for also having ""elephant"" in the 
description but only 1/10 as much as doc2 gets for covering another query term 
in its description.  If doc3 has ""elephant"" in title and both ""albino"" 
and ""elephant"" in the description, then with the actual refined expansion, it 
gets the highest score of all (whereas with pure max, without the 0.1, it 
would get the same score as doc2).

In real apps, tf's and idf's also come into play of course, but can affect 
these either way (i.e., mitigate this fundamental problem or exacerbate it)."
0,"[PATCH] Make a getter for SortField[] fields in org.apache.lucene.search.Sort. I'm have my own Collector and I would like to use the Sort object within my
collector, but SortField[] fields; is not accessible outside Lucene's package.
Can you please consider making a public getFields() method in the Sort object so
we can use it in our implementation?"
0,"JCR2SPI: Remove validation check for same-named Node and Property. JSR 170 disallowed a parent node to have a property and a child node with the same name.

This limitation has been removed with JSR 283 and the RI (jackrabbit-core) already removed the check.
I would suggest to change Jcr2Spi accordingly and leave this validation to the underlying SPI impl.

If I'm not mistaken this JSR 170 requirement is asserted in a single place (ItemStateValidator)."
0,Cookie docs are outdated.. The cookie docs do not reflect the latest code changes.
0,"Minor improvement to JavaDoc for ScoreDocComparator. About to attach a very small patch for ScoreDocComparator which broadens the contract of compare(ScoreDoc, ScoreDoc) to follow the same semantics as java.util.Comparator.compare() -- allow any integer to be returned, rather than specifically -1/0/-1.

Note that this behaviour must already be acceptable; the anonymous ScoreDocComparators returned by FieldSortedHitQueue.comparatorStringLocale() already return the result of Collator.compare(), which is not tied to this -1/0/1 restriction."
0,"HttpMethodBase logger uses wrong class.. I just noticed a minor error in HttpMethodBase: it initializes its Log object
with HttpMethod.class instead of HttpMethodBase.class.  No big deal, but it
probably ought to be fixed at some point.  I'll attach the patch."
0,"Kerberos Authentication Scheme. HttpClient 4.1.2 has a SPNEGO authentication that uses the Negotiate keyword.  But the MS IIS that I must connect to does not send back an WWW-Authenticate: Negotiate, but, instead, does send an WWW-Authenticate: Kerberos

So I used the NegotiateScheme.java and NegotiateSchemeFactory.java as a base to create a ""new"" scheme, called, KerberosScheme.java and KerberosSchemeFactory.java to make it work.

Essentially I replaced every ""Negotiate"" scheme by ""Kerberos"", in the KerberosScheme.java, and removed the part of the code that tried, first, SPNEGO_OID, using KERBEROS_OID directly, instead.

It works fine for me, but took me a while to figure this out.

That why I think it could come on the new versions.

I'll attach my version but it has no package - it was made only for a test project.  It's trivial to put it in the right place/package.
"
0,"Invert IR.getDelDocs -> IR.getLiveDocs. Spinoff from LUCENE-1536, where we need to fix the low level filtering
we do for deleted docs to ""match"" Filters (ie, a set bit means the doc
is accepted) so that filters can be pushed all the way down to the
enums when possible/appropriate.

This change also inverts the meaning first arg to
TermsEnum.docs/AndPositions (renames from skipDocs to liveDocs).
"
0,"Fully decouple IndexWriter from analyzers. IndexWriter only needs an AttributeSource to do indexing.

Yet, today, it interacts with Field instances, holds a private
analyzers, invokes analyzer.reusableTokenStream, has to deal with a
wide variety (it's not analyzed; it is analyzed but it's a Reader,
String; it's pre-analyzed).

I'd like to have IW only interact with attr sources that already
arrived with the fields.  This would be a powerful decoupling -- it
means others are free to make their own attr sources.

They need not even use any of Lucene's analysis impls; eg they can
integrate to other things like [OpenPipeline|http://www.openpipeline.org].
Or make something completely custom.

LUCENE-2302 is already a big step towards this: it makes IW agnostic
about which attr is ""the term"", and only requires that it provide a
BytesRef (for flex).

Then I think LUCENE-2308 would get us most of the remaining way -- ie, if the
FieldType knows the analyzer to use, then we could simply create a
getAttrSource() method (say) on it and move all the logic IW has today
onto there.  (We'd still need existing IW code for back-compat).
"
0,"Allow importing of ACL with unknown principals. It should be possible to import ACLs with principals that are not known to the principal provider, yet."
0,"DocValues type should be recored in FNX file to early fail if user specifies incompatible type. Currently segment merger fails if the docvalues type is not compatible across segments. We already catch this problem if somebody changes the values type for a field within one segment but not across segments. in order to do that we should record the type in the fnx fiel alone with the field numbers.

I marked this 4.0 since it should not block the landing on trunk"
0,Change log level for text extractor timeout. Currently an info message is written to the log when a text extractor job times out. Because this may happen quite frequently this should be changed to log level debug.
0,"Incorrect slf4j-log4j12 dependency scope in spi-commons. The slf4j-log4j12 dependency scope in jackrabbit-spi-commons is ""runtime"", when it should be ""test"". We don't want to impose a specific logging solution to downstream projects."
0,"deprecate Directory.renameFile(). Copied from my mailing list post so this issue can be tracked (if necessary). I will commit a patch.

I see that Directory.renameFile() isn't used anymore. I assume it has only 
been public for technical reasons, not because we expect this to be used 
from outside of Lucene? Should we deprecate this method? Its 
implementation e.g. in FSDirectory looks a bit scary anyway (the comment 
correctly says ""This is not atomic"" while the abstract class says ""This 
replacement should be atomic"").
"
0,"move intblock/sep codecs into test. The intblock and sep codecs in core exist to make it easy for people to try different low-level algos for encoding ints.

Sep breaks docs, freqs, pos, skip data, payloads into 5 separate files (vs 2 files that standard codec uses).

Intblock further enables the docs, freqs, pos files to encode fixed-sized blocks of ints at a time.

So an app can easily ""subclass"" these codecs, using their own int encoder.

But these codecs are now concrete, and they use dummy low-level block int encoder (eg encoding 128 ints as separate vints).

I'd like to change these to be abstract, and move these dummy codecs into test.

The tests would still test these dummy codecs, by rotating them in randomly for all tests.

I'd also like to rename IntBlock -> FixedIntBlock, because I'm trying to get a VariableIntBlock working well (for int encoders like Simple9, Simple16, whose block size varies depending on the particular values).
"
0,"Contrib queryparser should not use CharSequence as Map key. Today, contrib query parser uses Map<CharSequence,...> in many different places, which may lead to problems, since CharSequence interface does not enforce the implementation of hashcode and equals methods. Today, it's causing a problem with QueryTreeBuilder.setBuilder(CharSequence,QueryBuilder) method, that does not works as expected."
0,"Prevent excessive Path.Element instances. Even when a CachingHierarchyManager is used jackrabbit creates a lot of Path.Element instances. The internally used PathMap (spi-commons) creates new Path.Element instances whenever a path is constructed, even when the path is constructed from cached PathMap.Elements.

Running a test with 10k nodes results in 250k Path.Element instances being created and held in memory (mostly for events)."
0,Remove deprecated methods in PriorityQueue. 
0,"New flexible query parser. From ""New flexible query parser"" thread by Micheal Busch

in my team at IBM we have used a different query parser than Lucene's in
our products for quite a while. Recently we spent a significant amount
of time in refactoring the code and designing a very generic
architecture, so that this query parser can be easily used for different
products with varying query syntaxes.

This work was originally driven by Andreas Neumann (who, however, left
our team); most of the code was written by Luis Alves, who has been a
bit active in Lucene in the past, and Adriano Campos, who joined our
team at IBM half a year ago. Adriano is Apache committer and PMC member
on the Tuscany project and getting familiar with Lucene now too.

We think this code is much more flexible and extensible than the current
Lucene query parser, and would therefore like to contribute it to
Lucene. I'd like to give a very brief architecture overview here,
Adriano and Luis can then answer more detailed questions as they're much
more familiar with the code than I am.
The goal was it to separate syntax and semantics of a query. E.g. 'a AND
b', '+a +b', 'AND(a,b)' could be different syntaxes for the same query.
We distinguish the semantics of the different query components, e.g.
whether and how to tokenize/lemmatize/normalize the different terms or
which Query objects to create for the terms. We wanted to be able to
write a parser with a new syntax, while reusing the underlying
semantics, as quickly as possible.
In fact, Adriano is currently working on a 100% Lucene-syntax compatible
implementation to make it easy for people who are using Lucene's query
parser to switch.

The query parser has three layers and its core is what we call the
QueryNodeTree. It is a tree that initially represents the syntax of the
original query, e.g. for 'a AND b':
  AND
 /   \
A     B

The three layers are:
1. QueryParser
2. QueryNodeProcessor
3. QueryBuilder

1. The upper layer is the parsing layer which simply transforms the
query text string into a QueryNodeTree. Currently our implementations of
this layer use javacc.
2. The query node processors do most of the work. It is in fact a
configurable chain of processors. Each processors can walk the tree and
modify nodes or even the tree's structure. That makes it possible to
e.g. do query optimization before the query is executed or to tokenize
terms.
3. The third layer is also a configurable chain of builders, which
transform the QueryNodeTree into Lucene Query objects.

Furthermore the query parser uses flexible configuration objects, which
are based on AttributeSource/Attribute. It also uses message classes that
allow to attach resource bundles. This makes it possible to translate
messages, which is an important feature of a query parser.

This design allows us to develop different query syntaxes very quickly.
Adriano wrote the Lucene-compatible syntax in a matter of hours, and the
underlying processors and builders in a few days. We now have a 100%
compatible Lucene query parser, which means the syntax is identical and
all query parser test cases pass on the new one too using a wrapper.


Recent posts show that there is demand for query syntax improvements,
e.g improved range query syntax or operator precedence. There are
already different QP implementations in Lucene+contrib, however I think
we did not keep them all up to date and in sync. This is not too
surprising, because usually when fixes and changes are made to the main
query parser, people don't make the corresponding changes in the contrib
parsers. (I'm guilty here too)
With this new architecture it will be much easier to maintain different
query syntaxes, as the actual code for the first layer is not very much.
All syntaxes would benefit from patches and improvements we make to the
underlying layers, which will make supporting different syntaxes much
more manageable.
"
0,"Add support for distributed stats. (its a bug in a way, since we broke this, temporarily).

There is no way to do this now (distributed IDF, etc) with the new API.

But we should do it right:
* having the sim ask the searcher for docfreq of a term is wasteful and dangerous, 
  usually we have already seek'd to the term and already collected the 'raw' stuff.
* the situation is more than just docfreq, because you should be able to implement
  distributed scoring for all of the new sim models (or your own), that use any
  of Lucene's stats.
"
0,"Wiki reference a missing CND resource. Wiki page ""http://wiki.apache.org/jackrabbit/nt%3aexample"" references a missing resource:  ""http://jackrabbit.apache.org/doc/nodetype/cnd.html"""
0,"rename KeywordMarkerTokenFilter. I would like to rename KeywordMarkerTokenFilter to KeywordMarkerFilter.
We havent released it yet, so its a good time to keep the name brief and consistent."
0,"FieldsInfo uses deprecated API. The class FieldsInfo.java uses deprecated API in method ""public void add(Document doc)""
I rused the replacement and created the patch -> see attachment"
0,"Small improvements to ArrayUtil.getNextSize. Spinoff from java-dev thread ""Dynamic array reallocation algorithms"" started on Jan 12, 2010.

Here's what I did:

  * Keep the +3 for small sizes

  * Added 2nd arg = number of bytes per element.

  * Round up to 4 or 8 byte boundary (if it's 32 or 64 bit JRE respectively)

  * Still grow by 1/8th

  * If 0 is passed in, return 0 back

I also had to remove some asserts in tests that were checking the actual values returned by this method -- I don't think we should test that (it's an impl. detail)."
0,"Reset zzBuffer in StandardTokenizerImpl* when lexer is reset.. When indexing large documents, the lexer buffer may stay large forever. This sub-issue resets the lexer buffer back to the default on reset(Reader).

This is done on the enclosing issue."
0,"Remove JakarteRegExCapabilities shim to access package protected field. To access the prefix in Jakarta RegExes we use a shim class in the same package as jakarta. I will remove this and replace by reflection like Robert does in his ICUTokenizer rule compiler.

Shim classes have the problem wth signed artifacts, as you cannot insert a new class into a foreign package if you sign regex classes.

This shim-removal also allows users to use later jakarta regex versions, if they are in classpath and cannot be removed (even if they have bugs). Performance is no problem, as the prefix is only get once per TermEnum."
0,"Remove Serializable on ItemState classes. ItemStates are never directly serialized, which means they don't have to implement Serializable anymore.

See also: http://markmail.org/message/wsqnih2lembkcrdf"
0,"more performance improvements for snowball. i took a more serious look at snowball after LUCENE-2194.

This gives greatly improved performance, but note it has some minor breaks to snowball internals:
* Among.s becomes a char[] instead of a string
* SnowballProgram.current becomes a char[] instead of a StringBuilder
* SnowballProgram.eq_s(int, String) becomes eq_s(int, CharSequence), so that eq_v(StringBuilder) doesnt need to create an extra string.
* same as the above with eq_s_b and eq_v_b
* replace_s(int, int, String) becomes replace_s(int, int, CharSequence), so that StringBuilder-based slice and insertion methods don't need to create an extra string.

all of these ""breaks"" imho are only theoretical, the problem is just that pretty much everything is public or protected in the snowball internals.

the performance improvement here depends heavily upon the snowball language in use, but its way more significant than LUCENE-2194.
"
0,"Add <a name=""""> anchors to documentation sections. In all docs, sections are missing <a name> anchors. I see that the xdocs stylesheet in the repository is supposed to generate them, yet the site is missing them at this moment.

See https://svn.apache.org/repos/asf/jakarta/site/xdocs/stylesheets/site.xsl 
template match=""section"""
0,"JCR Test for Adding Node Type Tests That Abstract Nodes Can Be Added as Children, contrary to JCR 2.0 specification. When the TCK test method testLegalAndResidualType in the CanAddChildNodeCallWithNodeTypeTest class picks a node with a residual type, it does not filter out abstract nodes.  For example, in my local test, nt:hierarchyNode is selected for the local variable 'type'.

Since abstract node types ""cannot be directly assigned to a node,""[1] canAddChildNode(anyPropertyName, ""nt:hierarchyNode"") must return false.  However, since the test assumes that a non-abstract node type was chosen, it expects canAddChildNode(String, String) to return true.

This could be fixed if NodeTypeUtil.locateChildNodeDef(...) were extended to add an extra argument allowing or disallowing abstract types and that extra argument was used to filter the type used in testLegalAndResidualType (or if locateChildNodeDef(...) automatically excluded abstract types in the same manner that it automatically excludes protected types).

[1] - Section 3.7.1.3 of the JCR2 specification"
0,"Excessive Arrays.fill(0) in DocumentsWriter drastically slows down small docs (3.9X slowdown!). I've been doing some ""final"" performance testing of 2.3RC1 and
uncovered a fairly serious bug that adds a large fixed CPU cost when
documents have any term vector enabled fields.

The bug does not affect correctness, just performance.

Basically, for every document, we were calling Arrays.fill(0) on a
large (32 KB) byte array when in fact we only needed to zero a small
part of it.  This only happens if term vectors are turned on, and is
especially devastating for small documents."
0,"Initialize the cause of a login exception in the repository. ===================================================================
--- RepositoryImpl.java8(revision 379871)
+++ RepositoryImpl.java (working copy)
@@ -1056,7 +1056,9 @@
             }
             authCtx.login();
         } catch (javax.security.auth.login.LoginException le) {
-            throw new LoginException(le.getMessage());
+           LoginException nle = new LoginException(le.getMessage());
+           nle.initCause(le);
+           throw nle;
         }

         // create session"
0,"Some tests fail due to common use of java.io.tmpdir. Some tests use java.io.tmpdir, while others use tempDir (which is defined in common-build.xml).  Those that rely on java.io.tmpdir can fail when being run on the same machine as someone else who is running tests (this came up in testing the new nightly build scripts on lucene.zones.a.o)

Proposed fix is to map java.io.tmpdir in the ANT Junit task to be the same value as tempDir."
0,"Separate SegmentReaders (and other atomic readers) from composite IndexReaders. With current trunk, whenever you open an IndexReader on a directory you get back a DirectoryReader which is a composite reader. The interface of IndexReader has now lots of methods that simply throw UOE (in fact more than 50% of all methods that are commonly used ones are unuseable now). This confuses users and makes the API hard to understand.

This issue should split ""atomic readers"" from ""reader collections"" with a separate API. After that, you are no longer able, to get TermsEnum without wrapping from those composite readers. We currently have helper classes for wrapping (SlowMultiReaderWrapper - please rename, the name is really ugly; or Multi*), those should be retrofitted to implement the correct classes (SlowMultiReaderWrapper would be an atomic reader but takes a composite reader as ctor param, maybe it could also simply take a List<AtomicReader>). In my opinion, maybe composite readers could implement some collection APIs and also have the ReaderUtil method directly built in (possibly as a ""view"" in the util.Collection sense). In general composite readers do not really need to look like the previous IndexReaders, they could simply be a ""collection"" of SegmentReaders with some functionality like reopen.

On the other side, atomic readers do not need reopen logic anymore? When a segment changes, you need a new atomic reader? - maybe because of deletions thats not the best idea, but we should investigate. Maybe make the whole reopen logic simplier to use (ast least on the collection reader level).

We should decide about good names, i have no preference at the moment."
0,"RepositoryStatistics should be more flexible. Right now, Jackrabbit reports TimeSeries for things like BUNDLE_READ_COUNTER, BUNDLE_WRITE_COUNTER, etc. but there is no way to extend Jackrabbit and report TimeSeries for additional properties. That's because the type of TimeSeries are defined in RepositoryStatistics class as Type enum. Enums in Java cannot be extended which limits to TimeSeries to the Types defined in RepositoryStatistics. 

I suggest that RepositoryStatistics is improved to allow additional TimeSeries. One approach is to define an additional RepositoryStatistics#getType(String) method. "
0,"Benchmark contrib should allow multiple locations in ext.classpath. When {{ant run-task}} is invoked with the  {{-Dbenchmark.ext.classpath=...}} option, only a single location may be specified.  If a classpath with more than one location is specified, none of the locations is put on the classpath for the invoked JVM."
0,"Provide overridables for lock checking. Currently, checking whether a session is allowed to write to some locked node or whether it is allowed to unlock it is quite spread throughout the code. This should be collected to allow a custom lock manager overriding just a few methods to alter the default behavior."
0,"make collection element names configurable. - add jcrElementName to CollectionDescriptor and Collection annotation
- make COLLECTION_ELEMENT_NAME protected instead of private
"
0,"Use GrowingLRUMap in CachingEntryCollector. in order to allow for more flexibility of the cache size in caching entry collection, i would like to use the GrowingLRUMap."
0,Promote the classloader component from contrib. This is a dummy issue for a change I already made (revisions 529068 and 529137) to promote the classloader component from contrib. I'm including this here in the issue tracker to complete the release notes for Jackrabbit 1.3.
0,"isOpen needs to be accessible by subclasses of Directory. The Directory abstract class has a member variable named isOpen which is package accessible. The usage of the variable is such that it should be readable and must be writable (in order to implement close())  by any concrete implementation of directory. Because of the current accessibility of this variable is is not possible to create a Directory implementation that is not also in the org.apache.lucene.store.

I propose that either the isOpen variable either needs to be declared protected or that there should be getter/setter methods that are protected."
0,"Decouple packages in core.query. The packages o.a.j.c.query has cyclic dependencies to sub packages lucene, sql, xpath.

Decoupling the packages will allow to better extend the query implementation with additional query languages."
0,add calendar mime types to jcr-server's mime type registry. attached is a patch that adds mime types for .ics and .ifb files to jcr-server's mime type registry.
0,"Extended javadocs in spellchecker. Added some javadocs that explains why the spellchecker does not work as one might expect it to.

http://www.nabble.com/SpellChecker%3A%3AsuggestSimilar%28%29-Question-tf3118660.html#a8640395

> Without having looked at the code for a long time, I think the problem is what the
> lucene scoring consider to be best. First the grams are searched, resulting in a number
> of hits. Then the edit-distance is calculated on each hit. ""Genetics"" is appearently the
> third most similar hit according to Lucene, but the best according to Levenshtein.
>
> I.e. Lucene does not use edit-distance as similarity. You need to get a bunch of best hits
> in order to find the one with the smallest edit-distance.

I took a look at the code, and my assessment seems to be right."
0,"rename jcr-browser contrib project. There's a project called jcr-browser at sourceforge, it's a desktop browser mantained by sandro boehme. I'll rename the contrib project to jcr-navigator unless someone proposes a better name :). "
0,"UserManagement: Add Membership Cache. due to weakreference nature of the group members, retrieving the groups a given authorizable is member is expensive as the corresponding
API call (Node#getWeakReferences) executes a query [fallback if search is disabled: traversing the complete group tree, which isn't for free either].

i would therefore suggest to add a cache (authorizable nodeId -> group nodeids) that is gets cleared upon any modification to group membership or
group removal and doesn't need any extra observation. this cache may potentially obsolete the principal-based cache in DefaultPrincipalProvider... "
0,"Add deprecated 'transition' api for Document/Field. I think for 4.0 we should have a deprecated transition api for Field so you can do new Field(..., Field.Store.xxx, Field.Index.yyy) like before.

These combinations would just be some predefined fieldtypes that are used behind the scenes if you use these deprecated ctors

Sure it wouldn't be 'totally' backwards binary compat for Field.java, but why must it be all or nothing? I think this would eliminate a big
hurdle for people that want to check out 4.x"
0,"TCK: SessionReadMethodsTest#testIsLive calls logout() more than once. SessionReadMethodsTest#testIsLive calls logout more than once in a session (once in the test, once in tearDown).  JSR-170 doesn't prohibit an implementation from throwing an unchecked exception (such as IllegalStateException) if logout is called more than once.

Proposal: change tearDown to test isLive before calling logout.

--- SessionReadMethodsTest.java (revision 422074)
+++ SessionReadMethodsTest.java (working copy)
@@ -57,7 +57,7 @@
      * Releases the session aquired in {@link #setUp()}.
      */
     protected void tearDown() throws Exception {
-        if (session != null) {
+        if (session != null && session.isLive()) {
             session.logout();
         }
         super.tearDown();
"
0,"Reduce usage of String.intern(), performance is terrible. I profiled a simple MatchAllDocsQuery() against ~1.5 million documents (8 fields of short text, Field.Store.YES,Field.Index.NOT_ANALYZED_NO_NORMS), then retrieved all documents via searcher.doc(i, fs). String.intern() showed up as a top hotspot (see attached screenshot), so i implemented a small optimization to not intern() for every new Field(), instead forcing the intern in the FieldInfos class and adding a optional ""internName"" constructor to Field. This reduced execution time for searching and iterating through all documents by 35%. Results were similar for -server and -client.


TRUNK (2.9) w/out patch: matched 1435563 in 8884 ms/search
TRUNK (2.9) w/patch: matched 1435563 in 5786 ms/search"
0,"Document.fields() only returns stored fields. Document.fields() only returns stored fields, not those which are indexed but not 
stored. This is confusing, as there's a isStored() Method which doesn't make much 
sense then. 
 
Actually fields() returns all fields only just after Document.add(new Field(...)), even the 
ones which are not stored. Sounds confusing? :-) I'll attach a small program that 
demonstrates this. 
 
This should either be fixed so that all fields are always returned or it should be 
documented."
0,"Cleanup 'good' queries code. Before moving some of the classes from the queries contrib to the queries module, I want to just pass over them and clean them up, since we want code in modules to be of the same calibre as core code."
0,"Improved reusability of the JCA package. The jackrabbit-jca package currently has hardcoded references to jackrabbit-core, which makes it difficult to reuse the packaging and related code with other JCR implementations. With the RepositoryFactory interface from JCR 2.0 we can avoid this hard dependency."
0,"Rename BaseMultiReader class to BaseCompositeReader and make public. Currently the abstract DirectoryReader and MultiReader and ParallelCompositeReader extend a package private class. Users that want to implement a composite reader, should be able to subclass this pkg-private class, as it implements lots of abstract methods, useful for own implementations. In fact MultiReader is a shallow subclass only implementing correct closing&refCounting.

By making it public after the rename, the generics problems (type parameter R is not correctly displayed) in the JavaDocs are solved, too."
0,"allow cache to be configured as a non-shared (private) cache. Currently the CachingHttpClient only behaves as a shared cache, which is a safe and conservative assumption. However, in some settings, it would be appropriate to be able to configure the CachingHttpClient as a non-shared cache, which would make more responses cacheable, including:
* responses to requests with Authorization headers
* responses with 'Cache-Control: private'
* ability to serve stale responses when invalidation fails for 'Cache-Control: proxy-revalidate'
"
0,"TCK: AddNodeTest requires implementation to support one-parameter addNode method on test node. This test requires a repository to support addNode(String) [one argument] on the test node.  However, JSR-170 does not require an implementation to have at least one child node definition with a default primary type.  For such repositories, this test will fail, regardless of configuration.

Proposal: introduce a configuration property which, if set, causes calls to addNode(String) to be replaced with addNode(String, String)."
0,"clean up build files so contrib tests are run more easily. Per mailing list discussion...

http://www.nabble.com/Tests%2C-Contribs%2C-and-Releases-tf3768924.html#a10655448

Tests for contribs should be run when ""ant test"" is used,  existing ""test"" target renamed to ""test-core""
"
0,"Things to be done now that Filter is independent from BitSet. (Aside: where is the documentation on how to mark up text in jira comments?)

The following things are left over after LUCENE-584 :

For Lucene 3.0  Filter.bits() will have to be removed.

There is a CHECKME in IndexSearcher about using ConjunctionScorer to have the boolean behaviour of a Filter.

I have not looked into Filter caching yet, but I suppose there will be some room for improvement there.
Iirc the current core has moved to use OpenBitSetFilter and that is probably what is being cached.
In some cases it might be better to cache a SortedVIntList instead.

Boolean logic on DocIdSetIterator is already available for Scorers (that inherit from DocIdSetIterator) in the search package. This is currently implemented by ConjunctionScorer, DisjunctionSumScorer,
ReqOptSumScorer and ReqExclScorer.
Boolean logic on BitSets is available in contrib/misc and contrib/queries

DisjunctionSumScorer calls score() on its subscorers before the score value actually needed.
This could be a reason to introduce a DisjunctionDocIdSetIterator, perhaps as a superclass of DisjunctionSumScorer.

To fully implement non scoring queries a TermDocIdSetIterator will be needed, perhaps as a superclass of TermScorer.

The javadocs in org.apache.lucene.search using matching vs non-zero score:
I'll investigate this soon, and provide a patch when necessary.

An early version of the patches of LUCENE-584 contained a class Matcher,
that differs from the current DocIdSet in that Matcher has an explain() method.
It remains to be seen whether such a Matcher could be useful between
DocIdSet and Scorer.

The semantics of scorer.skipTo(scorer.doc()) was discussed briefly.
This was also discussed at another issue recently, so perhaps it is wortwhile to open a separate issue for this.

Skipping on a SortedVIntList is done using linear search, this could be improved by adding multilevel skiplist info much like in the Lucene index for documents containing a term.

One comment by me of 3 Dec 2008:

A few complete (test) classes are deprecated, it might be good to add the target release for removal there.
"
0,"MergePolicy should require an IndexWriter upon construction. MergePolicy does not require an IW upon construction, but requires one to be passed as method arg to various methods. This gives the impression as if a single MP instance can be shared across various IW instances, which is not true for all MPs (if at all). In addition, LogMergePolicy uses the IW instance passed to these methods incosistently, and is currently exposed to potential NPEs.

This issue will change MP to require an IW instance, however for back-compat reasons the following changes will be made:
# A new MP ctor w/ IW as arg will be introduced. Additionally, for back-compat a default ctor will also be declared which will assign null to the member IW.
# Methods that require IW will be deprecated, and new ones will be declared.
#* For back-compat, the new ones will not be made abstract, but will throw UOE, with a comment that they will become abstract in 3.0.
# All current MP impls will move to use the member instance.
# The code which calls MP methods will continue to use the deprecated methods, passing an IW even that it won't be necessary --> this is strictly for back-compat.

In 3.0, we'll remove the deprecated default ctor and methods, and change the code to not call the IW method variants anymore.

I hope that I didn't leave anything out. I'm sure I'll find out when I work on the patch :)."
0,"[PATCH] Extension to binary Fields that allows fixed byte buffer. This is a very simple patch that supports storing binary values in the index
more efficiently.  A new Field constructor accepts a length argument, allowing a
fixed byte[] to be reused acrossed multiple calls with arguments of different
sizes.  A companion change to FieldsWriter uses this length when storing and/or
compressing the field.

There is one remaining case in Document.  Intentionally, no direct accessor to
the length of a binary field is provided from Document, only from Field.  This
is because Field's created by FieldReader will never have a specified length and
this is usual case for Field's read from Document.  It seems less confusing for
most users.

I don't believe any upward incompatibility is introduced here (e.g., from the
possibility of getting a larger byte[] than actually holds the value from
Document), since no such byte[] values are possible without this patch anyway.

The compression case is still inefficient (much copying), but it is hard to see
how Lucene can do too much better.  However, the application can do the
compression externally and pass in the reused compression-output buffer as a
binary value (which is what I'm doing).  This represents a substantialy
allocation savings for storing large documents bodies (compressed) into the
Lucene index.

Two patch files are attached, both created by svn on 3/17/05."
0,"CachingIndexReader.initializeParents() does not scale well with large indexes. On a 40+ GB index that I'm testing with, the time to initialize the parents cache is 40 minutes.

This is way to much, needs optimization and should be done in a background thread."
0,"Adding same IndexDocValuesField twice trips assert. Doc values fields are single-valued by design, ie a given field name can only occur once in the document.

But if you accidentally add it more than once, you get an assert error, which is spooky because if you run w/o asserts maybe something eviler happens.

I think we should explicitly check for this and throw clear exc since user could easily do this by accident."
0,"should allow receiving secure cookies from non-secure chanel. Currently, httpclient will throw an exception if a secure cookie is received 
from a non-secure chanel. Although RFC doesn't specify explicitly on if the 
client should allow receiving secure cookie from non-secure channel, the 
default setting in browser seems to allow it.

Try the following link in IE:

http://www.snapfish.com

The default cookie policy in httpclient should be the same."
0,"IndexWriter should let you optionally enable reader pooling. For apps using a large index and frequently need to commit and resolve deletes, the cost of opening the SegmentReaders on demand for every commit can be prohibitive.

We an already pool readers (NRT does so), but, we only turn it on if NRT readers are in use.

We should allow separate control.

We should do this after LUCENE-2294."
0,"Massive Code Duplication in Contrib Analyzers - unifly the analyzer ctors. Due to the variouse tokenStream APIs we had in lucene analyzer subclasses need to implement at least one of the methodes returning a tokenStream. When you look at the code it appears to be almost identical if both are implemented in the same analyzer.  Each analyzer defnes the same inner class (SavedStreams) which is unnecessary.
In contrib almost every analyzer uses stopwords and each of them creates his own way of loading them or defines a large number of ctors to load stopwords from a file, set, arrays etc.. those ctors should be removed / deprecated and eventually removed.


"
0,Set svn:eol-style on ddl files. Most of the ddl files in jackrabbit-core/src/main/resources don't have the svn:eol-style property set. It should be set to native.
0,"SPI: improve description of locking methods on RepositoryService. in detail:

1) getLockInfo

- intended behavior if no lock is present?
- intended behavior if locking is not supported?

2) lock

- currently InvalidItemStateException is listed. i don't think this make too much sense.

3) refreshLock

- intended behavior if locking is not supported?

4) unlock

- currently InvalidItemStateException is listed. i don't think this make too much sense."
0,"Enhance SnapshotDeletionPolicy to allow taking multiple snapshots. A spin off from here: http://www.gossamer-threads.com/lists/lucene/java-dev/99161?do=post_view_threaded#99161

I will:
# Replace snapshot() with snapshot(String), so that one can name/identify the snapshot
# Add some supporting methods, like release(String), getSnapshots() etc.
# Some unit tests of course.

This is mostly written already - I want to contribute it. I've also written a PersistentSDP, which persists the snapshots on stable storage (a Lucene index in this case) to support opening an IW with existing snapshots already, so they don't get deleted. If it's interesting, I can contribute it as well.

Porting my patch to the new API. Should post it soon."
0,[PATCH] No need to call toString on a String. code calls toString on a String
0,"Add support for type whitelist in TypeTokenFilter. A usual use case for TypeTokenFilter is allowing only a set of token types. That is, listing allowed types, instead of filtered ones. I'm attaching a patch to add a useWhitelist option for that."
0,"http.connection-manager.timeout is a LONG not an INTEGER. Documentation is wrong.

Table in Preference Architecture page states http.connection-manager.timeout 
is an Integer.

Doing:

setParameter(""http.connection-manager.timeout"", new Integer(n));

Causes:

java.lang.ClassCastException
	at 
org.apache.commons.httpclient.params.DefaultHttpParams.getLongParameter
(DefaultHttpParams.java:171)
	at 
org.apache.commons.httpclient.params.HttpClientParams.getConnectionManagerTimeo
ut(HttpClientParams.java:143)
	at org.apache.commons.httpclient.HttpMethodDirector.executeMethod
(HttpMethodDirector.java:161)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:437)
	at org.apache.commons.httpclient.HttpClient.executeMethod
(HttpClient.java:324)"
0,"Token type as BitSet: typeBits(). It is sometimes useful to have a more compact, easy to parse, type representation for Token than the current type() String.  This patch adds a BitSet onto Token, defaulting to null, with accessors for setting bit flags on a Token.  This is useful for communicating information about a token to TokenFilters further down the chain.  

For example, in the WikipediaTokenizer, the possibility exists that a token could be both a category and bold (or many other variations), yet it is difficult to communicate this without adding in a lot of different Strings for type.  Unlike using the payload information (which could serve this purpose), the BitSet does not get added to the index (although one could easily convert it to a payload.)"
0,"Improve StandardTokenizer's understanding of non ASCII punctuation and quotes. In the vein of LUCENE-1126 and LUCENE-1390, StandardTokenizerImpl.jflex should do a better job at understanding non-ASCII punctuation characters.

For example, its understanding of the single-quote character ""'"" is currently limited to that character only. It will set a token's type to APOSTROPHE only if the ""'"" was used.
In the patch attached, I added all the characters that ASCIIFoldingFilter would change into ""'"".

I'm not sure that this is the right approach so I didn't write a complete patch for all the other hardcoded characters used in jflex rules such as ""."", ""-"" which have some variants in ASCIIFoldingFilter that could be used as well.

Maybe a better approach would be to make it possible to have an ASCIIFoldingFilter-like reader as a character filter that could be in inserted in front of StandardTokenizer ?"
0,"Speed up junit tests. As Lucene grows, so does the number of JUnit tests. This is obviously a good thing, but it comes with longer and longer test times. Now that we also run back compat tests in a standard test run, this problem is essentially doubled.

There are some ways this may get better, including running parallel tests. You will need the hardware to fully take advantage, but it should be a nice gain. There is already an issue for this, and Junit 4.6, 4.7 have the beginnings of something we might be able to count on soon. 4.6 was buggy, and 4.7 still doesn't come with nice ant integration. Parallel tests will come though.

Beyond parallel testing, I think we also need to concentrate on keeping our tests lean. We don't want to sacrifice coverage or quality, but I'm sure there is plenty of fat to skim.

I've started making a list of some of the longer tests - I think with some work we can make our tests much faster - and then with parallelization, I think we could see some really great gains."
0,"EdgeNGram* documentation improvement. To clarify what ""edge"" means, I added some description. That edge means the beggining edge of a term or ending edge of a term."
0,"DirectoryTaxonomyWriter extensions should be able to set internal index writer config attributes such as info stream. Current protected openIndexWriter(Directory directory, OpenMode openMode) does not provide access to the IWC it creates.
So extensions must reimplement this method completely in order to set e.f. info stream for the internal index writer.
This came up in [user question: Taxonomy indexer debug |http://lucene.472066.n3.nabble.com/Taxonomy-indexer-debug-td3533341.html]"
0,"Fix JFlex tokenizer compiler warnings. We get lots of distracting fallthrough warnings running ""ant compile""
in modules/analysis, from the tokenizers generated from JFlex.

Digging a bit, they actually do look spooky.

So I managed to edit the JFlex inputs to insert a bunch of break
statements in our rules, but I have no idea if this is
right/dangerous, and it seems a bit weird having to do such insertions
of ""naked"" breaks.

But, this does fix all the warnings, and all tests pass...
"
0,Factor out SearcherManager from NRTManager. Currently we have NRTManager and SearcherManager while NRTManager contains a big piece of the code that is already in SearcherManager. Users are kind of forced to use NRTManager if they want to have SearcherManager goodness with NRT. The integration into NRTManager also forces you to maintain two instances even if you know you always want deletes. To me NRTManager tries to do more than necessary and mixes lots of responsibilities ie. handling searchers and handling indexing generations. NRTManager should use a SearcherManager by aggregation rather than duplicate a lot of logic. SearcherManager should have a NRT and Directory based implementation users can simply choose from.
0,JSR 283 support. 
0,"Improve LuceneTestCase javadocs. Now that the Lucene test-framework javadocs will be published, they should get some attention."
0,"Use separate index for jcr:system tree. Currently each workspace index also includes index data of repository wide data (e.g. version nodes under jcr:system). There are several drawbacks with this approach:

- indexing is duplicated and does not scale when using a lot of workspaces
- workspaces cannot be 'put to sleep' when they are not actively used.

The repository should have an additional index for system data, which includes: versioning and nodetype representation in content. Basically data under /jcr:system.

Queries issued on a workspace will then use two index to execute the query: the workspace index and the system index."
0,"Transfer-Encoding: identity not supported + possible patch. In HttpMethodBase.readResponseBody only chunked transfer encoding is 
supported.  Some proxy servers like Privoxy, etc send a Transfer-Encoding: 
identity header and HttpClient fails quietly and returns a null result input 
stream.  At line 2037 in HttpMethodBase.java revision 1.160 I inserted the 
following two lines and it appeared to work fine:

} else if (""identity"".equalsIgnoreCase(transferEncodingHeader.getValue())) {
   result = is;

I think it should at least throw an exception or do something when it 
encounters an unsupported Transfer-Encoding instead of returning a null input 
stream."
0,"orm-persistence package doesn't compile against cvs head.. Corrected QName import (which I guess was moved). 
This patch has no meaning besides making the src/java compile and to update the dependencies in the project.xml.
"
0,"Invalid redirects are not corrected. If a get is made to a page with a query argument containing a space, many web
servers, notably including Tomcat 5 can generate a redirect in which the space
in the query argument is not escaped correctly.  Most browsers including IE and
Firefox compensate for this by quoting any included spaces in the redirect
location.  Http client does not.  When this broken URL is presented to a
subsequent server, the GET command is interprted incorrectly resulting (usually)
in a 505.

The fix is to replace spaces in redirect locations with +'s.  This doesn't
entirely fix the problem but that is the job of the web server developers."
0,"Add utitily class to manage NRT reopening. I created a simple class, NRTManager, that tries to abstract away some
of the reopen logic when using NRT readers.

You give it your IW, tell it min and max nanoseconds staleness you can
tolerate, and it privately runs a reopen thread to periodically reopen
the searcher.

It subsumes the SearcherManager from LIA2.  Besides running the reopen
thread, it also adds the notion of a ""generation"" containing changes
you've made.  So eg it has addDocument, returning a long.  You can
then take that long value and pass it back to the getSearcher method
and getSearcher will return a searcher that reflects the changes made
in that generation.

This gives your app the freedom to force ""immediate"" consistency (ie
wait for the reopen) only for those searches that require it, like a
verifier that adds a doc and then immediately searches for it, but
also use ""eventual consistency"" for other searches.

I want to also add support for the new ""applyDeletions"" option when
pulling an NRT reader.

Also, this is very new and I'm sure buggy -- the concurrency is either
wrong over overly-locking.  But it's a start...
"
0,XMLIndexFilter should index the attributes. text/xml indexer extracts the elements text but not the attribute values.
0,"need to add a default constructor for Cookie. The Cookie class doesn't have a default (no argument) constructor. This is 
causing problem for some framework which supports marshalling and unmarshalling 
of data types. e.g. a SOAP implementation may need to do this to transfer it 
between the SOAP server and SOAP client. It would be nice to add a default 
constructor, as it won't break anything, follows JavaBean convention, and 
potentially save user some trouble in the future."
0,"Cannot use Lucene in an unsigned applet due to Java security restrictions. A few of the Lucene source files call System.getProperty and perform a couple of
other operations within static initializers that assume full Java 2 permissions.
This prevents Lucene from being used from an unsigned applet embedded in a
browser page, since the default security permissions for an applet will prohibit
reading of most properties.

I would suggest wrapping the initialization of the properties in try/catch
blocks. This does mean that a couple properties would need to be made non-final,
and in some cases, getter and setter methods might be desirable to allow the
applet programmer to change the property values at runtime (some variables are
public static and could be changed directly without accessors).

This problem occurs with the shipping 1.4.3 version as well as the latest (as of
 07-apr-2005) source code fetched from CVS.

Currently, the files that are affected are org.apache.lucene.index.IndexWriter,
org.apache.lucene.index.SegmentReader, org.apache.lucene.search.BooleanQuery,
and org.apache.lucene.store.FSDirectory.

I have modified versions of these files with some suggested changes, plus a
simple test applet and associated files that demonstrate the situation. The
sample applet can be launched in a browser either by double-clicking the file
locally or by putting it on a web server and launching it from an http URL. As
soon as I can figure out how to attach to a bug report, I'll do that.

P.S. This topic came up in August, 2004 in lucene dev mailing list but as far as
I can tell, has not yet been resolved."
0,"[PATCH] Add StopFilter ignoreCase option. Wanted to have the ability to ignore case in the stop filter.  In some cases, I
don't want to have to lower case before passing through the stop filter, b/c I
may need case preserved for other analysis further down the stream, yet I don't
need the stopwords and I don't want to have to apply stopword filters twice."
0,"Drop Maven 1 compatibility. We migrated from Maven 1 to Maven 2 as the build system in Jackrabbit 1.2, but we kept compatibility with related Maven 1 build with the maven-one-plugin that deployed all builds also to the local Maven 1 repository.

Hardly any downstream project uses Maven 1 anymore, so it's safe for us to simplify our build now by dropping the use of the maven-one-plugin."
0,"Remove Scorer.getSimilarity(). Originally this was part of the patch for per-field Similarity (LUCENE-2236), but I pulled it 
out here as its own issue as its really mostly unrelated. I also like it as a separate issue 
to apply the deprecation to branch_3x to just make less surprises/migration hassles for 4.0 users.

Currently Scorer takes a confusing number of ctors, either a Similarity, or a Weight + Similarity.
Also, lots of scorers don't use the Similarity at all, and its not really needed in Scorer itself.
Additionally, the Weight argument is often null. The Weight makes sense to be here in Scorer, 
its the parent that created the scorer, and used by Scorer itself to support LUCENE-2590's features.
But I dont think all queries work with this feature correctly right now, because they pass null.

Finally the situation gets confusing if you start to consider delegators like ScoreCachingWrapperScorer,
which arent really delegating correctly so I'm unsure features like LUCENE-2590 aren't working with this.

So I think we should remove the getSimilarity, if your scorer uses a Similarity its already coming
to you via your ctor from your Weight and you can manage this yourself.

Also, all scorers should pass the Weight (parent) that created them, and this should be Scorer's only ctor.
I fixed all core/contrib/solr Scorers (even the internal ones) to pass their parent Weight, just for consistency
of this visitor interface. The only one that passes null is Solr's ValueSourceScorer.

I set fix-for 3.1, not because i want to backport anything, only to mark the getSimilarity deprecated there.
"
0,"support protected words in Stemming TokenFilters. This is from LUCENE-1515

I propose that all stemming TokenFilters have an 'exclusion set' that bypasses any stemming for words in this set.
Some stemming tokenfilters have this, some do not.

This would be one way for Karl to implement his new swedish stemmer (as a text file of ignore words).
Additionally, it would remove duplication between lucene and solr, as they reimplement snowballfilter since it does not have this functionality.
Finally, I think this is a pretty common use case, where people want to ignore things like proper nouns in the stemming.

As an alternative design I considered a case where we generalized this to CharArrayMap (and ignoring words would mean mapping them to themselves), which would also provide a mechanism to override the stemming algorithm. But I think this is too expert, could be its own filter, and the only example of this i can find is in the Dutch stemmer.

So I think we should just provide ignore with CharArraySet, but if you feel otherwise please comment.
"
0,"nuke IndexSearcher(directory). IndexSearcher is supposed to be a cheap wrapper around a reader,
but sometimes it is, sometimes it isn't.

I think its confusing tangling of a heavyweight and lightweight
object that it sometimes 'houses' a reader and must close it in that case.
"
0,"Sep codec should store less in terms dict. I'm working on improving Lucene's performance with int block codecs
(FOR/PFOR), but in early perf testing I found that these codecs cause
a big perf hit to those MTQs that need to scan many terms but don't
end up accepting many of those terms (eg fuzzy, wildcard, regexp).

This is because sep codec stores much more in the terms dict, since
each file is separate, ie seek points for each of doc, frq, pos, pyl,
skp files.

So I'd like to shift these seek points to instead be stored in the doc
file, except for the doc seek point itself.  Since a given query will
always need to seek to the doc file, this does not add an extra seek.
But it saves tons of vInt decodes for the next/seke intensive MTQs...
"
0,"Make ItemIds more stable. The ItemIds returned by spi2dav are currently not stable in the sense that they are sometimes uuid based and sometimes not: If a node is referenceable some of its properties will receive fully path based ids while others will receive ids based on the uuid of its parent node. 

The efficiency of caching introduced with JCR-2498 depends on stable ids. I therefore suggest to improve spi2dav such that property ids are always uuid based if the parent's node has a uuid. "
0,spi2davex: some value factory tests from the SPI test suite are failing. 
0,"IndexReader.document always return a doc with all the stored fields loaded. And this can be slow for the indexed document contain huge fields. when generating digest for some documents with huge fields, it should be unnecessary to load the field but just interesting part of the field with the offset information. but indexreader always return the whole field content. afterward, the customized storedfieldsreader will got a repeated loading"
0,"Optimization for IndexWriter.addIndexes(). One big performance problem with IndexWriter.addIndexes() is that it has to optimize the index both before and after adding the segments.  When you have a very large index, to which you are adding batches of small updates, these calls to optimize make using addIndexes() impossible.  It makes parallel updates very frustrating.

Here is an optimized function that helps out by calling mergeSegments only on the newly added documents.  It will try to avoid calling mergeSegments until the end, unless you're adding a lot of documents at once.

I also have an extensive unit test that verifies that this function works correctly if people are interested.  I gave it a different name because it has very different performance characteristics which can make querying take longer."
0,"deprecate Scorer.explain. Spinoff from LUCENE-1749.

We already have QueryWeight.explain, which is directly invoked by IndexSearcher.explain.  Some queries in turn will defer to their Scorer impl's explain, but many do not (and their Scorer.explain simply throw UOE).  So we should deprecate & remove Scorer.explain, leaving it up to individual queries to define that method if they need it."
0,"Links Pointing to Javadocs Are Incorrect and Return 404. There are various links on the ""Configuring Jackrabbit""  page (http://jackrabbit.apache.org/doc/config.html) that are invalid.  For example, I wanted to read the ""PersistenceManager javadocs"" link, but it returns a 404.  This will decrease the confidence in the project and hinder its adoption."
0,"handle multivalue headers correctly. Some times, web servers send back multiple headers with the same key. e.g.

WWW-Authenticate: Negotiate
WWW-Authenticate: NTLM
WWW-Authenticate: Basic realm=""kmdc5""

To handle this correctly, we should add a method 

public java.util.Iterator getResponseHeaders(java.lang.String name)

just as in javax.servlet.http.HttpServletRequest."
0,"Inline nested jars in OSGi bundles. Eclipse doesn't support bundles with nested jars (https://bugs.eclipse.org/bugs/show_bug.cgi?id=111238). The workaround is to inline the contents of the nested jars. This is a simple fix that shouldn't impact non-Eclipse users:

pom.xml
===================================================================
- <Embed-Dependency>*;scope=compile|runtime;inline=false</Embed-Dependency>
+ <Embed-Dependency>*;scope=compile|runtime;inline=true</Embed-Dependency>
"
0,"Remove old byte[] norms api from IndexReader. Followup to LUCENE-3628.

We should remove this api and just use docvalues everywhere, to allow for norms of arbitrary size in the future (not just byte[])"
0,"AbstractConnPool constructor calls thread.Start(). AbstractConnPool constructor calls thread.Start()

Findbugs says:

Constructor invokes Thread.start()

The constructor starts a thread. This is likely to be wrong if the class is ever extended/subclassed, since the thread will be started before the subclass constructor is started.

The class is not final (and the constructor is protected) which suggests that the class is intended to be extended..."
0,Add a Lucene3x private SegmentInfosFormat implemenation. we still don't have a Lucene3x & preflex version of segment infos format. we need this before we release 4.0
0,Index update overhead on cluster slave due to JCR-905. JCR-905 is a quick and dirty fix and causes overhead on a cluster slave node when it processes revisions.
0,"Generalize SearcherManager. I'd like to generalize SearcherManager to a class which can manage instances of a certain type of interfaces. The reason is that today SearcherManager knows how to handle IndexSearcher instances. I have a SearcherManager which manages a pair of IndexSearcher and TaxonomyReader pair.

Recently, few concurrency bugs were fixed in SearcherManager, and I realized that I need to apply them to my version as well. Which led me to think why can't we have an SM version which is generic enough so that both my version and Lucene's can benefit from?

The way I see SearcherManager, it can be divided into two parts: (1) the part that manages the logic of acquire/release/maybeReopen (i.e., ensureOpen, protect from concurrency stuff etc.), and (2) the part which handles IndexSearcher, or my SearcherTaxoPair. I'm thinking that if we'll have an interface with incRef/decRef/tryIncRef/maybeRefresh, we can make SearcherManager a generic class which handles this interface.

I will post a patch with the initial idea, and we can continue from there."
0,"PayloadNearQuery has hardwired explanation for 'AveragePayloadFunction'. The 'explain' method in PayloadNearSpanScorer assumes the AveragePayloadFunction was used. This patch adds the 'explain' method to the 'PayloadFunction' interface, where the Scorer can call it. Added unit tests for 'explain' and for {Min,Max}PayloadFunction."
0,"update tests so that both Query.XPATH and Query:SQL are treated as optional features. In JCR 2.0, both Query.XPATH and Query.SQL are optional (or, actually, deprecated).

We either need to modify the tests so that they pass on a repository that doesn't support them (-> NotExecutableException), or remove them altogether."
0,ID Field Descriptor is not inherited as is the case with UUID Field Descriptor. ID Field descriptor when defined in the base class in jcr-mapping is not inherited. The child class also has to define it again. A patch for the same is attached herewith. Patch is on similar lines of UUID Field Descriptor
0,"jcr-rmi maven ""site"" target fails. the target ""site"" in jcr rmi fails because org.apache.jackrabbit.rmi.remote.SerialValue.java is empty"
0,"Extend the IndexingConfiguration to allow configuration of reuseable analyzers. To the indexing_configuration.xml a xml block of analyzers should be configurable. In each <index-rule> to a property an analyzer can be assigned. This means, that property will be analyzed with that specific analyzer. In the first place, it enables multilingual indexing. 

Documentation needs to be added explaining the difference in searching in the node scope [jcr:contains(.,'foo')] and in some property [jcr:contains(@myprop,'foo')]. The node scope will always be searched and indexed with the default analyzer, which can be configured in the workspace.xml in  the  <SearchIndex> element.

Below a possible indexing_configuration.xml snippet is shown. Also node the possible enhancement (not sure wether this implementation will have it, because it requires a lot of filter Factories and is probably out of scope). Adding custom filters which do not need a factory might be easier.

<analyzers>
	<analyzer name=""fr"" class=""org.apache.lucene.analysis.fr.FrenchAnalyzer""/>
	<analyzer name=""de"" class=""org.apache.lucene.analysis.de.GermanAnalyzer""/>
        <analyzer name=""compound"" class=""org.apache.lucene.analysis.SimpleAnalyzer"">
             <filter class=""jr.StopFilterFactory"" words=""stopwords.txt""/>
             <filter class=""jr.EdgeNGramTokenizerFactory"" side=""front"" minGram=""1"" maxGram=""2""/>
        </analyzer>
</analyzers>

<index-rule nodeType=""nt:unstructured"">
       <property analyzer=""fr"">bode_fr</property>
       <property analyzer=""de"">bode_de</property>
</index-rule>"
0,"improve how MTQs interact with the terms dict cache. Some small improvements:

  * Adds a TermsEnum.cacheCurrentTerm ""hint"" (codec can make this a no-op)

  * Removes the FTE.useTermsCache

  * Changes MTQ's TermCollector API to accept the TermsEnum so collectors can eg call .docFreq directly

  * Adds expert ctor to TermQuery allowing you to pass in the docFreq"
0,Allow random seed to be set in DeleteByPercentTask. Need this to make index identical on multiple runs.  
0,"Use NativeFSLockFactory as default for new API (direct ctors & FSDir.open). A user requested we add a note in IndexWriter alerting the availability of NativeFSLockFactory (allowing you to avoid retaining locks on abnormal jvm exit). Seems reasonable to me - we want users to be able to easily stumble upon this class. The below code looks like a good spot to add a note - could also improve whats there a bit - opening an IndexWriter does not necessarily create a lock file - that would depend on the LockFactory used.


{code}  <p>Opening an <code>IndexWriter</code> creates a lock file for the directory in use. Trying to open
  another <code>IndexWriter</code> on the same directory will lead to a
  {@link LockObtainFailedException}. The {@link LockObtainFailedException}
  is also thrown if an IndexReader on the same directory is used to delete documents
  from the index.</p>{code}

Anyone remember why NativeFSLockFactory is not the default over SimpleFSLockFactory?"
0,Use jackrabbit-jcr-commons in jackrabbit-jcr-rmi. The jackrabbit-jcr-rmi component should leverage the general-purpose classes in jackrabbit-jcr-commons even at the expense of introducing an extra dependency.
0,"Initialization error of Junit tests with solr-test-framework with IDEs and Maven. I'm currently developping a new component for Solr. And in my Netbeans project, I have created two Test classes for this component: one class for simple unit tests (derived from  SolrTestCaseJ4 class) and a second one for tests with sharding (derived from  BaseDistributedSearchTestCase).
When I launch a test with these two classes, I have an error in the initialization of the second class of tests (no matter the class is, this is always the second executed class which fails). The error comes from an ""assert"" which failed in the begining of the function ""initRandom()"" of LuceneTestCase class :

assert !random.initialized;

But, if I launch each test class separatly, all the tests succeed!

After a discussion with Mr. Muir, the problems seems to be related to the incompatibility of the class LuceneTestCase with the functioning of Maven projects in IDEs.

According to mister Muir:

""
The problem is that via ant, tests work like this (e.g. for 3 test classes):
computeTestMethods
beforeClass
afterClass
computeTestMethods
beforeClass
AfterClass
computeTestMethods
beforeClass
afterClass

but via an IDE, if you run it from a folder like you did, then it does this:
computeTestMethods
computeTestMethods
computeTestMethods
beforeClass
afterClass
beforeClass
afterClass
beforeClass
afterClass 
"""
0,"Improve Spatial Utility like classes. - DistanceUnits can be improved by giving functionality to the enum, such as being able to convert between different units, and adding tests.  

- GeoHashUtils can be improved through some code tidying, documentation, and tests.

- SpatialConstants allows us to move all constants, such as the radii and circumferences of Earth, to a single consistent location that we can then use throughout the contrib.  This also allows us to improve the transparency of calculations done in the contrib, as users of the contrib can easily see the values being used.  Currently this issues does not migrate classes to use these constants, that will happen in issues related to the appropriate classes."
0,"Response handlers. Perhaps plugin handlers should be used to handle various ranges of http
responses.  Could be used to respond, auto-forward, resubmit ... Could solve the
difficulty in handling a 303 response."
0,"Additional excerpt provider implementation. The current DefaultHTMLExcerpt implementation is very simple. It basically picks the first three fragments, regardless of how many matches it contains. There should be an alternative implementation that weights the fragments based on the number of matching terms and the whether phrases have matched."
0,"Remove Unnecessary NULL check in FindSegmentsFile - cleanup. FindSegmentsFile accesses the member ""directory"" in line 579 while performing a null check in 592. The null check is unnecessary as if directory is null line 579 would throw a NPE.
I removed the null check and made the member ""directory"" final. In addition I added a null check in the constructor as If the value is null we should catch it asap. 

"
0,"Jenkins trunk tests (nightly only) fail quite often with OOM in Automaton/FST tests. The nightly Job Lucene-trunk quite often fails with OOM (in several methods, not always in the same test):

Example from last night (this time a huge Automaton):

{noformat}
[junit] java.lang.OutOfMemoryError: Java heap space
[junit] Dumping heap to /home/hudson/hudson-slave/workspace/Lucene-trunk/heapdumps/java_pid38855.hprof ...
[junit] Heap dump file created [86965954 bytes in 1.186 secs]
[junit] Testsuite: org.apache.lucene.index.TestTermsEnum
[junit] Testcase: testIntersectRandom(org.apache.lucene.index.TestTermsEnum):	Caused an ERROR
[junit] Java heap space
[junit] java.lang.OutOfMemoryError: Java heap space
[junit] 	at org.apache.lucene.util.automaton.RunAutomaton.<init>(RunAutomaton.java:128)
[junit] 	at org.apache.lucene.util.automaton.ByteRunAutomaton.<init>(ByteRunAutomaton.java:28)
[junit] 	at org.apache.lucene.util.automaton.CompiledAutomaton.<init>(CompiledAutomaton.java:134)
[junit] 	at org.apache.lucene.index.TestTermsEnum.testIntersectRandom(TestTermsEnum.java:266)
[junit] 	at org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)
[junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)
[junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)
[junit] 
[junit] 
[junit] Tests run: 6, Failures: 0, Errors: 1, Time elapsed: 11.699 sec
{noformat}

Other traces:

{noformat}
[junit] Testsuite: org.apache.lucene.util.fst.TestFSTs
[junit] Testcase: testRealTerms(org.apache.lucene.util.fst.TestFSTs):	Caused an ERROR
[junit] Java heap space
[junit] java.lang.OutOfMemoryError: Java heap space
[junit] 	at org.apache.lucene.util.ArrayUtil.grow(ArrayUtil.java:338)
[junit] 	at org.apache.lucene.util.fst.FST$BytesWriter.writeBytes(FST.java:927)
[junit] 	at org.apache.lucene.util.fst.ByteSequenceOutputs.write(ByteSequenceOutputs.java:113)
[junit] 	at org.apache.lucene.util.fst.ByteSequenceOutputs.write(ByteSequenceOutputs.java:32)
[junit] 	at org.apache.lucene.util.fst.FST.addNode(FST.java:451)
[junit] 	at org.apache.lucene.util.fst.NodeHash.add(NodeHash.java:122)
[junit] 	at org.apache.lucene.util.fst.Builder.compileNode(Builder.java:180)
[junit] 	at org.apache.lucene.util.fst.Builder.finish(Builder.java:495)
[junit] 	at org.apache.lucene.index.codecs.memory.MemoryCodec$TermsWriter.finish(MemoryCodec.java:232)
[junit] 	at org.apache.lucene.index.FreqProxTermsWriterPerField.flush(FreqProxTermsWriterPerField.java:414)
[junit] 	at org.apache.lucene.index.FreqProxTermsWriter.flush(FreqProxTermsWriter.java:92)
[junit] 	at org.apache.lucene.index.TermsHash.flush(TermsHash.java:117)
[junit] 	at org.apache.lucene.index.DocInverter.flush(DocInverter.java:80)
[junit] 	at org.apache.lucene.index.DocFieldProcessor.flush(DocFieldProcessor.java:78)
[junit] 	at org.apache.lucene.index.DocumentsWriterPerThread.flush(DocumentsWriterPerThread.java:472)
[junit] 	at org.apache.lucene.index.DocumentsWriter.doFlush(DocumentsWriter.java:420)
[junit] 	at org.apache.lucene.index.DocumentsWriter.flushAllThreads(DocumentsWriter.java:568)
[junit] 	at org.apache.lucene.index.IndexWriter.getReader(IndexWriter.java:366)
[junit] 	at org.apache.lucene.index.IndexReader.open(IndexReader.java:317)
[junit] 	at org.apache.lucene.util.fst.TestFSTs.testRealTerms(TestFSTs.java:1034)
[junit] 	at org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)
{noformat}

or:

{noformat}
[junit] Testsuite: org.apache.lucene.util.automaton.TestCompiledAutomaton
[junit] Testcase: testRandom(org.apache.lucene.util.automaton.TestCompiledAutomaton):	Caused an ERROR
[junit] Java heap space
[junit] java.lang.OutOfMemoryError: Java heap space
[junit] 	at org.apache.lucene.util.automaton.RunAutomaton.<init>(RunAutomaton.java:128)
[junit] 	at org.apache.lucene.util.automaton.ByteRunAutomaton.<init>(ByteRunAutomaton.java:28)
[junit] 	at org.apache.lucene.util.automaton.CompiledAutomaton.<init>(CompiledAutomaton.java:134)
[junit] 	at org.apache.lucene.util.automaton.TestCompiledAutomaton.build(TestCompiledAutomaton.java:39)
[junit] 	at org.apache.lucene.util.automaton.TestCompiledAutomaton.testTerms(TestCompiledAutomaton.java:55)
[junit] 	at org.apache.lucene.util.automaton.TestCompiledAutomaton.testRandom(TestCompiledAutomaton.java:101)
[junit] 	at org.apache.lucene.util.LuceneTestCase$2$1.evaluate(LuceneTestCase.java:611)
[junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:148)
[junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:50)
{noformat}

Almost every nightly test fails, history: [https://builds.apache.org/job/Lucene-trunk]

We should maybe raise the max heap space or reduce doc counts/..."
0,"Add preemptive authentication. Wishlist request for preemptive authentication to be included in the API, like HttpClient 3.x had.  There is an example ClientPreemptiveBasicAuthentication.java that uses HttpRequestInterceptor which I had adapted to my application and it works fine."
0,MultiPhraseQuery should allow access to terms. 
0,"improved internal representation of DATE values. DATE values are currently internally represented as java.util.Calendar objects.

Calendar objects have a huge memory footprint (approx 200bytes per instance) 
and are mutable.

i suggest to replace the internal DATE representation with a ISO8601 format string
(immutable and approx. 85-90% smaller footprint)."
0,JCR taglib. JCR Taglib. Maven generated site at http://cablemodem.fibertel.com.ar/edgarpoce/index.html
0,"JSR 283: Create RepositoryFactory implementation. JSR 283 specifies a RepositoryFactory to retrieve a repository instance based on a map of parameters. We should have the following implementations:

- local repository with repository home and repository configuration parameters
- repository obtained via JNDI
- repository obtained via RMI
"
0,"factor CharTokenizer/CharacterUtils into analyzers module. Currently these analysis components are in the lucene core, but should really
be .util in the analyzers module.

Also, with MockTokenizer extending Tokenizer directly, we can add some additional
checks in the future to try to ensure our consumers are being good consumers (e.g. calling reset).

This is mentioned in http://wiki.apache.org/lucene-java/TestIdeas, I didn't implement it here yet,
this is just the factoring. I think we should try to do this before LUCENE-3040.
"
0,"3.0 not compile-time compatible with 2.0 library usage. To my surprise Oleg says this was the intent, yet the Jakarta-Slide webdavclient
libraries do not compile out of the box.  Patch for that issue to follow."
0,"need a way to set request body in PostMethod. Currently, there is no way for user to set the request body in PostMethod 
directly. The only way to do that is by adding parameters to PostMethod. This 
makes sense in most cases. However, there are situations that the user actually 
knows the request body and want to set it directly. adding the following method 
fixes this:

    public void setRequestBody(String requestBody)
    {
        this.requestBody = requestBody;
    }"
0,"Remove Maven 1 files. Now that we have a working Maven 2 build environment (JCR-332) we should remove the old Maven 1
project files to avoid confusion and misunderstandings. The old Maven 1 build environment doesn't even
work anymore.

Unless anyone objects, I'll proceed to remove the project.xml, maven.xml, and project.properties files in a few days."
0,"Name and Path interfaces in SPI. The SPI interface currently has a dependency to QName and Path classes in jackrabbit-jcr-commons. Architecturally it would be better to have Name and Path interfaces in the SPI package, and have the implementing classes in commons."
0,Interface TermFreqVector has incomplete Javadocs. We should improve the Javadocs of org.apache.lucene.index.TermFreqVector
0,"Expose registered node types below /jcr:system/jcr:nodeTypes. spec says:

6.8 System Node
[...]
For example, if a repository exposes node type definitions in content, then those node type definitions should be located at /jcr:system/jcr:nodeTypes.
"
0,"Poor performance of ISDESCENDANTNODE on SQL 2 queries. Using the latest source code, I have noticed very bad performance on SQL-2 queries that use the ISDESCENDANTNODE constraint on a large sub-tree. For example, the query : 

select * from [jnt:news] as news where ISDESCENDANTNODE(news,'/root/site') order by news.[date] desc 

executes in 600ms 

select * from [jnt:news] as news order by news.[date] desc

executes in 4ms

From looking at the problem in the Yourkit profiler, it seems that the culprit is the constraint building, that uses recursive Lucene searches to build the list of descendant node IDs : 

    private Query getDescendantNodeQuery(
            DescendantNode dn, JackrabbitIndexSearcher searcher)
            throws RepositoryException, IOException {
        BooleanQuery query = new BooleanQuery();

        try {
            LinkedList<NodeId> ids = new LinkedList<NodeId>();
            NodeImpl ancestor = (NodeImpl) session.getNode(dn.getAncestorPath());
            ids.add(ancestor.getNodeId());
            while (!ids.isEmpty()) {
                String id = ids.removeFirst().toString();
                Query q = new JackrabbitTermQuery(new Term(FieldNames.PARENT, id));
                QueryHits hits = searcher.evaluate(q);
                ScoreNode sn = hits.nextScoreNode();
                if (sn != null) {
                    query.add(q, SHOULD);
                    do {
                        ids.add(sn.getNodeId());
                        sn = hits.nextScoreNode();
                    } while (sn != null);
                }
            }
        } catch (PathNotFoundException e) {
            query.add(new JackrabbitTermQuery(new Term(
                    FieldNames.UUID, ""invalid-node-id"")), // never matches
                    SHOULD);
        }

        return query;
    }

In the above example this generates over 2800 Lucene queries, which is the culprit. I wonder if it wouldn't be faster to retrieve the IDs by using the JCR to retrieve the list of child IDs ?

This was probably also missed because I didn't seem to find any performance tests on this constraint."
0,Bundle cache is not cleared when *BundlePersistenceManager is closed. Close method of persistence managers is responsible for releasing all acquired resources. In case of BundlePersistenceManager it should also free memory by clearing the bundle cache.
0,Prefer PathFactory.createElement() over createPath().getNameElement(). The latter creates an unnecessary Path instance.
0,"missing blob.remove in Berkeley DB persistance manager. org/apache/jackrabbit/core/state/bdb/BerkeleyDBPersistenceManager.destroy(PropertyState state) does not remove binary file from
the BLOBStore (filesystem impl)



"
0,"Javadoc - Field constructor with Reader needs comment about retained reference. If you don't dig into the Lucene internals, it isn't obvious the Field constructor http://lucene.apache.org/java/docs/api/org/apache/lucene/document/Field.html#Field%28java.lang.String,%20java.io.Reader%29 retains a reference to the reader for use later on. It would be useful to have a comment added to the Javadoc saying something like:

Note: A reference to java.io.Reader is retained by the field. Reader is read from when the Document which this field is added to is itself added to the index.

Without this, the caller is liable to do silly things like closing the stream after constructing the org.apache.lucene.document.Field."
0,"Unreferenced VersionHistory should be deleted automatically.. since the creation of a VersionHistory is triggered by the creation of a mix:versionable node, the removal should happen automatically, as soon as no references to that version histroy exist anymore. this is the case, when all mix:versionable nodes (in all workspaces) belonging to that VH are deleted, and all the versions in the VH are removed i.e. only the jcr:rootVersion is left. At this point, the VH should be deleted aswell."
0,"Improve InfoStream class in trunk to be more consistent with logging-frameworks like slf4j/log4j/commons-logging. Followup on a [thread by Shai Erea on java-dev@lao|http://lucene.472066.n3.nabble.com/IndexWriter-infoStream-is-final-td3537485.html]: I already discussed with Robert about that, that there is one thing missing. Currently the IW only checks if the infoStream!=null and then passes the message to the method, and that *may* ignore it. For your requirement it is the case that this is enabled or disabled dynamically. Unfortunately if the construction of the message is heavy, then this wastes resources.

I would like to add another method to this class: abstract boolean isEnabled() that can also be implemented. I would then replace all null checks in IW by this method. The default config in IW would be changed to use a NoOutputInfoStream that returns false here and ignores the message.

A simple logger wrapper for e.g. log4j / slf4j then could look like (ignoring component, could be enabled):

{code:java}
Loger log = YourLoggingFramework.getLogger(IndexWriter.class);

public void message(String component, String message) {
  log.debug(component + "": "" + message);
}

public boolean isEnabled(String component) {
  return log.isDebugEnabled();
}
{code}

Using this you could enable/disable logging live by e.g. the log4j management console of your app server by enabling/disabling IndexWriter.class logging.

The changes are really simple:
- PrintStreamInfoStream returns true, always, mabye make it dynamically enable/disable to allow Shai's request
- infoStream.getDefault() is never null and can never be set to null. Instead the default is a singleton NoOutputInfoStream that returns false of isEnabled(component).
- All null checks on infoStream should be replaced by infoStream.isEanbled(component), this is possible as always != null. There are no slowdowns by this - it's like Collections.emptyList() instead stupid null checks."
0,"DataStore: gc.stopScan() should be optional. Data Store garbage collection currently works like this:

gc.scan();
gc.stopScan();
gc.deleteUnused();

Currently, if stopScan() is not called, an exception is thrown. This is not user friendly. Instead, stopScan() should be optional, and should be allowed any time. It may be used to indicate garbage collection has finished:

try {
  gc.scan();
  ...
  gc.deleteUnused();
} finally {
  gc.stopScan();
}

Or when sharing the repository:

gc1.scan();
gc2.scan();
gc1.deleteUnused();
gc2.stopScan();
"
0,"Extend the consistency check in BundleDbPersistenceManager's to fix child-parent relations. It could happen that a child node is not in the ChildNodeEntries of its parent node.
You will get something like (javax.jcr.ItemNotFoundException: failed to build path of node1: parentNode has no child entry for node1) if you try to retrieve the path from node1.
We should handle such cases and fix it on consistency check"
0,Highlighter dist jar includes memory binary class files. Mark Harwood sent me a note about this issue noticed by a colleague. Previous releases have the memory class files in the Highlighter distribution jar. The Highlighter uses the same contrib dependency method that the xml query parser does - the problem doesn't manifest there because of the alphabetical order of build though. Fix is to not inheritAll when launching the ant task to build memory contrib.
0,[PATCH] Mention RangeFilter in javadoc for maxClauseCount.  
0,"client cache does not respect 'Cache-Control: no-store' on requests. ""The purpose of the no-store directive is to prevent the inadvertent release or retention of sensitive information (for example, on backup tapes). The no-store directive applies to the entire message, and MAY be sent either in a response or in a request. If sent in a request, a cache MUST NOT store any part of either this request or any response to it.""

http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.2

The current implementation will incorrectly cache responses to requests containing 'Cache-Control: no-store'."
0,"haversine() is broken / misdocumented. DistanceUtils.haversine() is coded in a way that is erroneous based on the documented order of the parameters.  The parameters are defined as (x1,y1,x2,y2,radius)  -- i.e. lon,lat order.  The code implementing the algorithm, however, is as if the meaning of x and y are transposed, which means that if you supply the arguments in lat,lon (y,x) order, you will get the correct behavior.  It turns out that all callers of this method do this!

FYI I found out about this bug since it is inherited code in LSP (lucene-spatial-playground) and I have been supplying parameters according to its documented order.  Apparently I shouldn't do that ;-)"
0,"add @experimental javadocs tag. There are a lot of things marked experimental, api subject to change, etc. in lucene.

this patch simply adds a @experimental tag to common-build.xml so that we can use it, for more consistency.
"
0,"IndexReader.clone. Based on discussion http://www.nabble.com/IndexReader.reopen-issue-td18070256.html.  The problem is reopen returns the same reader if there are no changes, so if docs are deleted from the new reader, they are also reflected in the previous reader which is not always desired behavior."
0,Wrap SegmentInfos in public class . Wrap SegmentInfos in a public class so that subclasses of MergePolicy do not need to be in the org.apache.lucene.index package.  
0,"PayloadTermQuery refers to a deprecated documentation for redirection . When PayloadTermQuery refers to the function for scoring Similarity - it refers to override the deprecated method - 

Similarity#scorePayload(String, byte[],int,int) . 

That method has been deprecated by  Similarity#scorePayload(int, String, int, int, byte[],int,int) . 


This javadoc patch addresses the class level javadoc for the class to provide the right signature in Similarity to be overridden. "
0,Add accessor for parent to NodeInfoBuilder/PropertyInfoBuilder. NodeInfoBuilder and PropertyInfoBuilder should allow access to its respective parents. I suggest to add a getParent() method to both classes. 
0,Introduce spellchecker functionality based on content in the workspace. Provide a way to spell check a fulltext statement based on the content present in the workspace.
0,Keep WebDAV exception causes. The DavMethodBase and ExceptionConverter classes in jackrabbit-webdav and jackrabbit-spi2dav don't include the cause when throwing an exception based on some caught cause. This makes it harder to identify what is causing a  particular problem. The attached patch fixes that.
0,"FieldCache rewrite method for MultiTermQueries. For some MultiTermQueries, like RangeQuery we have a FieldCacheRangeFilter etc (in this case its particularly optimized).

But in the general case, since LUCENE-2784 we can now have a rewrite method to rewrite any MultiTermQuery 
using the FieldCache, because MultiTermQuery's getEnum no longer takes IndexReader but Terms, and all the 
FilteredTermsEnums are now just real TermsEnum decorators.

In cases like low frequency queries this is actually slower (I think this has been shown for numeric ranges before too),
but for the really high-frequency cases like especially ugly wildcards, regexes, fuzzies, etc, this can be several times faster 
using the FieldCache instead, since all the terms are in RAM and automaton can blast through them quicker.
"
0,"NodeTest.testAddNodeConstraintViolationExceptionUndefinedNodeType relies on addNode(name, ""nt:base""). NodeTest.testAddNodeConstraintViolationExceptionUndefinedNodeType() relies on the ability to create a new node of type ""nt:base"", which isn't something repositories are required to support.

Proposal: make the node type name for this test case configurable.
"
0,"Exclude tests instead skipping them. jcr2spi tests run with the spi2jcr module by default so they are configured to be skipped when jcr2spi is built. Manually running a jcr2spi test like this

mvn -Dtest=MyTest -Dmaven.test.skip=false test

does not work however. The pom configuration seems to take precedence here. 

To fix this I propose to exclude all test instead of skipping them making it possible to manually execute tests like this

mvn -Dtest=MyTest test
"
0,"Use POIExtractor wherever possible. POI scratchpad comes with a couple of text extractor utilities, which makes it easier to extract text. We should rather use those utilities than writing our own extractor code. This helps avoid issues like JCR-1530."
0,"SweetSpotSimiliarity. This is a new Similarity implimention for the contrib/miscellaneous/ package, it provides a Similiarty designed for people who know the ""sweetspot"" of their data.  three major pieces of functionality are included:

1) a lengthNorm which creates a ""platuea"" of values.
2) a baseline tf that provides a fixed value for tf's up to a minimum, at which point it becomes a sqrt curve (this is used by the tf(int) function.
3) a hyperbolic tf function which is best explained by graphing the equation.  this isn't used by default, but is available for subclasses to call from their own tf functions.

All constants used in all functions are configurable.  In the case of lengthNorm, the constants are configurable per field, as well as allowing for defaults for unspecified fields."
0,counter field in segments file is not documented in fileformats.xml. The counter field in the current segments file format is not documented.
0,"maven2 repository. Could somebody care to upload jackrabbit to maven2 repo's at ibiblio? (ideally libraries, javadocs, source, pom's - but at least the lib's)"
0,"Wrong schemaObjectPrefix parameter in default repository.xml. The object schema prefix is hard-coded in the default configuration file (I think this taken from the jackrabbit-core.jar):

        <PersistenceManager class=""org.apache.jackrabbit.core.persistence.bundle.DerbyPersistenceManager"">
          <param name=""url"" value=""jdbc:derby:${wsp.home}/db;create=true""/>
          <param name=""schemaObjectPrefix"" value=""Jackrabbit Core_""/>
        </PersistenceManager>

This is probably caused by JCR-945, though I've no idea why ${wsp.name} is replaced with the name of the module...

I have marked this issue as minor because it still works with the DerbyPersistenceManager. There are separate database instances for each workspace, but it will become a problem if a data base persistence manager on a dedicated server is used."
0,"improve efficiency of snowballfilter. snowball stemming currently creates 2 new strings and 1 new stringbuilder for every word.

all of this is unnecessary, so don't do it.
"
0,"Improve performance of DescendantSelfAxisQuery. Instead of calculating the full result of the sub query, the DescendantSelfAxisQuery should make use of the skipTo() method on the sub scorer."
0,"Performance improvement: Lazy skipping on proximity file. Hello,

I'm proposing a patch here that changes org.apache.lucene.index.SegmentTermPositions to avoid unnecessary skips and reads on the proximity stream. Currently a call of next() or seek(), which causes a movement to a document in the freq file also moves the prox pointer to the posting list of that document.  But this is only necessary if actual positions have to be retrieved for that particular document. 

Consider for example a phrase query with two terms: the freq pointer for term 1 has to move to document x to answer the question if the term occurs in that document. But *only* if term 2 also matches document x, the positions have to be read to figure out if term 1 and term 2 appear next to each other in document x and thus satisfy the query. 

A move to the posting list of a document can be quite expensive. It has to be skipped to the last skip point before that document and then the documents between the skip point and the desired document have to be scanned, which means that the VInts of all positions of those documents have to be read and decoded. 

An improvement is to move the prox pointer lazily to a document only if nextPosition() is called. This will become even more important in the future when the size of the proximity file increases (e. g. by adding payloads to the posting lists).

My patch implements this lazy skipping. All unit tests pass. 


I also attach a new unit test that works as follows:
Using a RamDirectory an index is created and test docs are added. Then the index is optimized to make sure it only has a single segment. This is important, because IndexReader.open() returns an instance of SegmentReader if there is only one segment in the index. The proxStream instance of SegmentReader is package protected, so it is possible to set proxStream to a different object. I am using a class called SeeksCountingStream that extends IndexInput in a way that it is able to count the number of invocations of seek(). 

Then the testcase searches the index using a PhraseQuery ""term1 term2"". It is known how many documents match that query and the testcase can verify that seek() on the proxStream is not called more often than number of search hits.

Example:
Number of docs in the index: 500
Number of docs that match the query ""term1 term2"": 5

Invocations of seek on prox stream (old code): 29
Invocations of seek on prox stream (patched version): 5

- Michael
"
0,"SpellChecker has no ""close"" method. SpellChecker has no close method ... which means there is no way to force it to close the IndexSearcher it maintains when you are done using the SpellChecker.  (a quick skim of IndexSearcher doesn't even suggest there is a finalizer self closing in the event of GC)

http://www.nabble.com/SpellChecker-locks-folder-to23171980.html#a23171980

A hackish work around for people who want to force SpellChecker to close an IndexSearcher opened against a directory they care about doing something with... 
{code}yourSpellChecker.setSpellIndex(new RamDirecotry()){code}"
0,Add settings to IWC to optimize IDV indices for CPU or RAM respectivly. spinnoff from LUCENE-3496 - we are seeing much better performance if required bits for PackedInts are rounded up to a 8/16/32/64. We should add this option to IWC and default to round up ie. more RAM & faster lookups.
0,"Add toString() or getName() method to IndexReader. It would be very useful for debugging if IndexReader either had a getName() method, or a toString() implementation that would get a string identification for the reader.

for SegmentReader, this would return the same as getSegmentName()
for Directory readers, this would return the ""generation id""?
for MultiReader, this could return something like ""multi(sub reader name, sub reader name, sub reader name, ...)

right now, i have to check instanceof for SegmentReader, then call getSegmentName(), and for all other IndexReader types, i would have to do something like get the IndexCommit and get the generation off it (and this may throw UnsupportedOperationException, at which point i have would have to recursively walk sub readers and try again)

I could work up a patch if others like this idea"
0,"Source packaging fails if ${dist.dir} does not exist. package-tgz-src and package-zip-src fail if ${dist.dir} does not exist, since these two targets do not call the package target, which is responsible for making the dir.

I have a fix and will commit shortly."
0,"tests should run checkIndex on indexes they create. I think we should add a boolean checkIndexesOnClose (default=true) to MockDirectoryWrapper.

Only a very few tests need to disable this.
"
0,"CacheClient Javadoc and Constants usage cleanup. CacheClient has some empty public java doc on methods that are not get/set.  These should have some body.  

Also the HeaderConstants Class has some overlap with the existing HTTP class for header values.  These need cleaning up."
0,Better 'invalid format' exception messages for value classes. The valueOf() methods of the Value classes throw an exception without information on the desired type and without the String value that gave the error.
0,PropertyReadMethodsTest should also work on NAME property. Some test cases in PropertyReadMethodsTest require a String property even though a NAME property like jcr:primaryType would be sufficient.
0,"Add JCR-RMI documentation to the Jackrabbit web site. Use the org.apache.jackrabbit.rmi.{client,server} package javadocs as a base for a JCR-RMI document page on the Jackrabbit web site."
0,"Rate-limit IO used by merging. Large merges can mess up searches and increase NRT reopen time (see
http://blog.mikemccandless.com/2011/06/lucenes-near-real-time-search-is-fast.html).

A simple rate limiter improves the spikey NRT reopen times during big
merges, so I think we should somehow make this possible.  Likely this
would reduce impact on searches as well.

Typically apps that do indexing and searching on same box are in no
rush to see the merges complete so this is a good tradeoff.
"
0,"TCK: Transfer of lock token should be tested using open-scoped locks. despite the fact that jsr170 does not limit the usage of Session.removeLockToken(String) and Session.addLockToken(String) to tokens obtained from open-scoped locks, i don't see too much benefit of it. Therefore (and due to the fact that this issue will be addressed within the scope of jsr283), i would  like to suggest to modify those test-cases dealing with transfer of lock tokens and create open-scoped locks.
"
0,Favour QValue.getPath() over getString() where appropriate. To avoid extra conversion round trips QValue.getPath() should be used instead of  QValue.getString() where appropriate.
0,"Revise PagedBytes#fillUsingLengthPrefix* methods names. PagedBytes has 3 different variants of fillUsingLengthPrefix. We need better names for that since CSFBranch already added a 4th one.


here are some suggestions:

{code}
/** Reads length as 1 or 2 byte vInt prefix, starting @ start */
    public BytesRef fillLengthAndOffset(BytesRef b, long start) 
//    was: public BytesRef fillUsingLengthPrefix(BytesRef b, long start) 


 /** @lucene.internal  Reads length as 1 or 2 byte vInt prefix, starting @ start.  Returns the block number of the term. */
    public int getBlockAndFill(BytesRef b, long start) 
//    was: public BytesRef fillUsingLengthPrefix2(BytesRef b, long start) 

/** @lucene.internal  Reads length as 1 or 2 byte vInt prefix, starting @ start. 
     * Returns the start offset of the next part, suitable as start parameter on next call
     * to sequentially read all BytesRefs. */
    public long getNextOffsetAndFill(BytesRef b, long start) 
//    was: public BytesRef fillUsingLengthPrefix3(BytesRef b, long start) 

{code}"
0,"PropertyImpl.getNode() and NamePropertyTest use different exception than documented in the JCR API JavaDoc. The Property.getNode() method's JavaDoc [1] lists 3 types of exceptions: ValueFormatException, ItemNotFoundException, and RepositoryException, and that ItemNotFoundException is to be thrown when the target node could not be found.  However, the NamePropertyTest.testGetProperty() method is checking for a PathNotFoundException rather than the documented ItemNotFoundException (see [2], line 189).  Jackrabbit's implementation in PropertyImpl (see [3] line 539) delegates to Session.getNode(absolutePath) or Property.getParent().getNode(relativePath), and these methods are documented as throwing PathNotFoundException (see [4] and [5]).

Therefore, the unit test and PropertyImpl.getNode() implementation appear to be in disagreement with the JCR 2.0 API JavaDoc.

[1] http://www.day.com/maven/javax.jcr/javadocs/jcr-2.0/javax/jcr/Property.html#getNode()
[2] http://svn.apache.org/viewvc/jackrabbit/trunk/jackrabbit-jcr-tests/src/main/java/org/apache/jackrabbit/test/api/NamePropertyTest.java?revision=772352&view=markup
[3] http://svn.apache.org/viewvc/jackrabbit/trunk/jackrabbit-core/src/main/java/org/apache/jackrabbit/core/PropertyImpl.java?revision=948827&view=markup
[4] http://www.day.com/maven/javax.jcr/javadocs/jcr-2.0/javax/jcr/Session.html#getNode(java.lang.String)
[5] http://www.day.com/maven/javax.jcr/javadocs/jcr-2.0/javax/jcr/Node.html#getNode(java.lang.String)
"
0,"Create jackrabbit-parent. Currently the Jackrabbit components use the top-level multi-module POM as their parent POM for sharing many of the common build settings. However, with the planned mixed component release model for 1.5.x we need to be able to increase the version number of the top-level POM without affecting individual components. Thus it is better if we move the shared settings to an explicit jackrabbit-parent component and keep the top-level POM simply as the multi-module container."
0,"Add NoOpMergePolicy. I'd like to add a simple and useful MP implementation which does .... nothing ! :). I've came across many places where either the following is documented or implemented: ""if you want to prevent merges, set mergeFactor to a high enough value"". I think a NoOpMergePolicy is just as good, and can REALLY allow you disable merges (except for maybe set mergeFactor to Int.MAX_VAL).

As such, NoOpMergePolicy will be introduced as a singleton, and can be used for convenience purposes only. Also, for Parallel Index it's important, because I'd like the slices to never do any merges, unless ParallelWriter decides so. So they should be set w/ that MP.

I have a patch ready. Waiting for LUCENE-2320 to go in, so that I don't need to change it afterwards.

About the name - I like the name, but suggestions are welcome. I thought of a NullMergePolicy, but I don't like 'Null' used for a NoOp."
0,"Build should enable unchecked warnings in javac. Just have to uncomment this:
{code}
        <!-- for generics in Java 1.5: -->
        <!--<compilerarg line=""-Xlint:unchecked""/>-->
{code}
in common-build.xml.  Test & core are clean, but contrib still has many warnings.  Either we fix contrib with this issue, or, conditionalize this (anyone anty who can do this?) so contrib is off until we can fix it."
0,"non mantatory revision property in the Journal configuration. An exception is thrown if property 'revision' is not set. I think it would be great to save the revision file in the repository home dir 
when the property is not set."
0,"org.apache.http.impl.client.cache.memcached.MemcachedHttpCacheStorage uses URL as cache key - shouldn't.. Spy memcached has 250 defined as max key length:
http://dustin.github.com/java-memcached-client/apidocs/constant-values.html#net.spy.memcached.MemcachedClientIF.MAX_KEY_LENGTH

URLs can be (and often are) much longer than 250 characters.

URLs should be hashed before being used as keys."
0,"contrib.benchmark.quality package improvements. Few fixes and improvements for the search quality benchmark package:
- flush report and logger at the end (otherwise long submission reports might miss last lines).
- add run-tag-name to submission report (API change).
- add control over max-#queries to run (useful at debugging a quality evaluation setup).
- move control over max-docs-to-retrieve from benchmark constructor to a setter method (API change).
- add computation of Mean Reciprocal Rank (MRR) in QualityStats.
- QualityStats fixed to not fail if there are no results to average.
- Add a TREC queries reader adequate for the 1MQ track (track started 2007).

All tests pass, will commit this in 1-2 days if there is no objection.
"
0,"Improve BaseTokenStreamTestCase to uses a fake attribute to check if clearAttributes() was called correctly - found bugs in contrib/analyzers. Robert had the idea to use a fake attribute inside BaseTokenStreamTestCase that records if its clear() method was called. If this is not the case after incrementToken(), asserTokenStreamContents fails. It also uses the attribute in TeeSinkTokenFilter, because there a lot of copying, captureState and restoreState() is used. By the attribute, you can track wonderful, if save/restore and clearAttributes is correctly implemented. It also verifies that *before* a captureState() it was also cleared (as the state will also contain the clear call). Because if you consume tokens in a filter, capture the consumed tokens and insert them, the capturedStates must also be cleared before.

In contrib analyzers are some test that fail to pass this additional assertion. They are not fixed in the attached patch."
0,"Unclosed threads in Jackrabbit. The Tomcat integration test added in JCR-2831 shows the following warnings about Jackrabbit threads that remain in place even after the repository has been closed:

08-Dec-2010 12:14:58 org.apache.catalina.loader.WebappClassLoader clearReferencesThreads
SEVERE: The web application [] appears to have started a thread named [Timer-1] but has failed to stop it. This is very likely to create a memory leak.
08-Dec-2010 12:14:58 org.apache.catalina.loader.WebappClassLoader clearReferencesThreads
SEVERE: The web application [] appears to have started a thread named [DynamicPooledExecutor] but has failed to stop it. This is very likely to create a memory leak.
08-Dec-2010 12:14:58 org.apache.catalina.loader.WebappClassLoader clearReferencesThreads
SEVERE: The web application [] appears to have started a thread named [Timer-2] but has failed to stop it. This is very likely to create a memory leak.

It would be best to close all such background threads even if they are singleton daemon threads and thus unlikely to cause trouble when left unattended."
0,"TermsFilter: reuse TermDocs. TermsFilter currently calls termDocs(Term) once per term in the TermsFilter.  If we sort the terms it's filtering on, this can be optimised to call termDocs() once and then skip(Term) once per term, which should significantly speed up this filter.
"
0,"Add subset method to BitVector. Recently I needed the ability to efficiently compute subsets of a BitVector. The method is:
  public BitVector subset(int start, int end)
where ""start"" is the starting index, inclusive and ""end"" is the ending index, exclusive.

Attached is a patch including the subset method as well as relevant unit tests."
0,"Port 3.x FieldCache.getDocsWithField() to trunk. [Spinoff from LUCENE-3390]

I think the approach in 3.x for handling un-valued docs, and making it
possible to specify how such docs are sorted, is better than the
solution we have in trunk.

I like that FC has a dedicated method to get the Bits for docs with field
-- easy for apps to directly use.  And I like that the
bits have their own entry in the FC.

One downside is that it's 2 passes to get values and valid bits, but
I think we can fix this by passing optional bool to FC.getXXX methods
indicating you want the bits, and the populate the FC entry for the
missing bits as well.  (We can do that for 3.x and trunk). Then it's
single pass.
"
0,Add Highlighter test for RegexQuery. 
0,Add init method to CloseableThreadLocal. Java ThreadLocal has an init method that allows subclasses to easily instantiate an initial value.  
0,"masking field of span for cross searching across multiple fields (many-to-one style). This issue is to cover the changes required to do a search across multiple fields with the same name in a fashion similar to a many-to-one database. Below is my post on java-dev on the topic, which details the changes we need:

---

We have an interesting situation where we are effectively indexing two 'entities' in our system, which share a one-to-many relationship (imagine 'User' and 'Delivery Address' for demonstration purposes). At the moment, we index one Lucene Document per 'many' end, duplicating the 'one' end data, like so:

    userid: 1
    userfirstname: fred
    addresscountry: au
    addressphone: 1234

    userid: 1
    userfirstname: fred
    addresscountry: nz
    addressphone: 5678

    userid: 2
    userfirstname: mary
    addresscountry: au
    addressphone: 5678

(note: 2 Documents indexed for user 1). This is somewhat annoying for us, because when we search in Lucene the results we want back (conceptually) are at the 'user' level, so we have to collapse the results by distinct user id, etc. etc (let alone that it blows out the size of our index enormously). So why do we do it? It would make more sense to use multiple fields:
    userid: 1
    userfirstname: fred
    addresscountry: au
    addressphone: 1234
    addresscountry: nz
    addressphone: 5678

    userid: 2
    userfirstname: mary
    addresscountry: au
    addressphone: 5678

But imagine the search ""+addresscountry:au +addressphone:5678"". We'd like this to match ONLY Mary, but of course it matches Fred also because he matches both those terms (just for different addresses).

There are two aspects to the approach we've (more or less) got working but I'd like to run them past the group and see if they're worth trying to get them into Lucene proper (if so, I'll create a JIRA issue for them)

1) Use a modified SpanNearQuery. If we assume that country + phone will always be one token, we can rely on the fact that the positions of 'au' and '5678' in Fred's document will be different.

   SpanQuery q1 = new SpanTermQuery(new Term(""addresscountry"", ""au""));
   SpanQuery q2 = new SpanTermQuery(new Term(""addressphone"", ""5678""));
   SpanQuery snq = new SpanNearQuery(new SpanQuery[]{q1, q2}, 0, false);

the slop of 0 means that we'll only return those where the two terms are in the same position in their respective fields. This works brilliantly, BUT requires a change to SpanNearQuery's constructor (which checks that all the clauses are against the same field). Are people amenable to perhaps adding another constructor to SNQ which doesn't do the check, or subclassing it to do the same (give it a protected non-checking constructor for the subclass to call)?

2) (snipped ... see LUCENE-1626 for second idea)"
0,"broken link to the release notes. On http://jakarta.apache.org/httpcomponents/httpclient-3.x/downloads.html
the link to the release notes is broken"
0,"Jcr-Server: Usage of Cache-Control header. DeltaV-methods (except for Version-Control and Report) require the Cache-Control header to be present in the response.
In turn, RFC 2518 only requires the Cache-Control header to be present in the request  when dealing with the If header.

Currently the Cache-Control header is always present in the response and not specific for DeltaV methods.
Problems may arise with IE  and a couple of mimetypes such as zip-files"
0,"Allow Junit4 tests in our environment.. Now that we're dropping Java 1.4 compatibility for 3.0, we can incorporate Junit4 in testing. Junit3 and junit4 tests can coexist, so no tests should have to be rewritten. We should start this for the 3.1 release so we can get a clean 3.0 out smoothly.

It's probably worthwhile to convert a small set of tests as an exemplar.


"
0,Support for JNDI configuration of BundleDbPersistenceManager. It would be nice to have the option to configure BundleDbPersistenceManager database specifying a JNDI name of a DataSource.
0,"J2EE FORM authentication (also affects pluggable authentication). Add support for J2EE style FORM authentication type. 

Unlike the BASIC and DIGEST types this is not handled by HTTP headers so needs
an adjustment to the way in which the authentication is sent. As far as i can
tell from my testing with one or two J2EE servers the way to successfully login
requires request of a protected page which will respond with the login FORM and
then the submission of that form. The two requests must be associated with one
another using the jsessionid cookie.

It seems to me that this 'bug' must be solved in cooperation with the recent
discussions of pluggable authentication module. i suggestion the following
signature: 

PluggableAuthenticator.authenticate(HttpMethod method, HttpState state). 

This mirrors the existing Authenticator method but also requires change to the
state object to allow access to the connection properties (i dont know how this
affects MultiClient). Alternately we could go for: 

PluggableAuthenticator.authenticate(HttpMethod method, HttpClient client).

In either case Authenticator needs a way to know which plugin to call. I suggest
modification of HttpMethodBase to detect the 'j_security_check' form action in
the response and automatically submit credentials if they are provided using the
new class 

J2EEFormAuthenticator implements PluggableAuthenticator."
0,"SpanQueryFilter addition. Similar to the QueryFilter (or whatever it is called now) the SpanQueryFilter is a regular Lucene Filter, but it also can return Spans-like information.  This is useful if you not only want to filter based on a Query, but you then want to be able to compare how a given match from a new query compared to the positions of the filtered SpanQuery.  Patch to come shortly also contains a caching mechanism for the SpanQueryFilter"
0,"httpMethod.abort needed. This is the problem : I use the httpclient to fire many requests. At some point 
of time, the server has queued up requests. So certain requests are waiting for 
response. Now when I call httpMethod.releaseConnection, the request should stop 
waiting for the response and the connection should be closed. However, this 
does not happen. The request is only given up after it has timed out."
0,SPI: provide batch read functionality. extend RepositoryService interface to allow for BatchRead and modify jcr2spi accordingly.
0,provide a memcached implementation for HttpCache. The feature here would be an implementation of the HttpCache interface that stored cache entries in memcached.
0,"Correct spatial and trie documentation links in JavaDocs and site. After updating myself in the site docs, I have some changes to the site and javadocs of Lucene 2.9:
- Add spatial contrib to javadocs
- Add trie package to the contrib/queries package
Both changes prevent these pacakges from a apearing in core's pacakge list on the javadocs/all homepage. I also adjusted the documentation page to reflect the changes.

I will commit the attached patch, if nobody objects."
0,"contrib/bdb-persistence: update berkeleydb version. berkeleydb dependency should be updated to 2.0.83, already available at ibiblio. At this moment project.xml lists 1.7.1, which is very old.
There are no code changes required, and the PM works correctly with berkeleydb 2.0.83

		<dependency>
			<groupId>berkeleydb</groupId>
			<artifactId>je</artifactId>
			<version>2.0.83</version>
			<type>jar</type>
		</dependency>"
0,"Let the AbstractISMLockingTest tests fail properly. The tests in the AbstractISMLockingTest class call junit.framework.Assert.fail() on threads that are not managed by the JUnit framework. Therefore, such calls to fail are not interpreted as test failures, but are merely logged to the console and the build succeeds. This is easy to see with a stub implementation of the ISMLocking type which returns non-null references from the two acquire methods and the downgrade method: many of the following stacktraces appear, but the build succeeds.

Exception in thread ""Thread-1"" junit.framework.AssertionFailedError: acquireWriteLock must block
	at junit.framework.Assert.fail(Assert.java:47)
	at org.apache.jackrabbit.core.state.AbstractISMLockingTest.checkBlocking(AbstractISMLockingTest.java:214)
	at org.apache.jackrabbit.core.state.AbstractISMLockingTest$1.run(AbstractISMLockingTest.java:88)
	at java.lang.Thread.run(Thread.java:613)


"
0,most tests should use MockRAMDirectory not RAMDirectory. 
0,"misleading contrib/tck-webapp/...RepositoryServlet javadoc. In org.apache.jackrabbit.tck.j2ee.RepositoryServlet it says ""...puts the reference into the application context"" but according to the behavior it should say ""...puts the reference into a class variable"". In a j2ee environment the application context refers to the ServletConext instance bounded to the web application.
"
0,"Remove GData from trunk . GData doesn't seem to be maintained anymore. We're going to remove it before we cut the 2.3 release unless there are negative votes.

In case someones jumps in in the future and starts to maintain it, we can re-add it to the trunk.

If anyone is using GData and needs it to be in 2.3 please let us know soon!"
0,"move SmartChineseAnalyzer into the smartcn package. an offshoot of LUCENE-1862, org.apache.lucene.analysis.cn.SmartChineseAnalyzer should become org.apache.lucene.analysis.cn.smartcn.SmartChineseAnalyzer"
0,"TimeoutHandler visitor should be extracted into a dedicated class. This is a minor problem that I've stumbled upon when looking at a memory leak.
It seems that the TimeoutHandler thread runs each second and it uses a custom ElementVisitor to do its business. By creating a new instance of ElementVisitor each second this creates some garbage that could be avoided by using a predefined class.

Short story: during a unit test it creates 3x instances each second that have 16 bytes each. Having a dedicated visitor class creates just 3 instances for the lifespan of the repository."
0,"spi2davex: clear uri-lookup after removing node identified with uniqueID. some test cases of DocumentViewImportTest fail in the setup (line 325 of AbstractImportXmlTest) since the uri resolved from the specified
nodeID still refers to the node removed during the initial import before. this would equally cause problems whenever a referenceable node was
replaced by another node and can easily be fixed by clearing the uri-cache after the removal as it was already done for move."
0,"Move NamespaceMappings/Index from lucene to namespace registry.. The NamespaceMappings class in the indexer is used for generating small prefixes for namespace uris that are stored in the index. This mechanism of stable prefixes could be used in other places as well, for example in the persistence managers.

Suggest to introduce general methods in the namespace registry:

int getURIIndex(String uri)
String getURI(int index)
"
0,"Simplify NRTManager. NRTManager is hairy now, because the applyDeletes is separately passed
to ctor, passed to maybeReopen, passed to getSearcherManager, etc.

I think, instead, you should pass it only to the ctor, and if you have
some cases needing deletes and others not then you can make two
NRTManagers.  This should be no less efficient than we have today,
just simpler.

I think it will also enable NRTManager to subclass ThingyManager
(LUCENE-3761).
"
0,Avoid String.intern() when indexing. Lucene 3.0 now allows to create Fields with a String name that is already interned. We should use the new constructor in NodeIndexer to avoid unnecessary String.intern() calls. The field names Jackrabbit uses are available in FieldNames and already interned.
0,"URI.parseUriReference treats strings with leading ':' as absolute URIs with zero-length scheme. URI.parseUriReference treats strings with leading ':' as absolute URIs with a
zero-length scheme. If you then try to derelativize such a URI against a base
URI, you just get the same URI with leading ':'. 

IE and Firefox treat URI strings with a leading ':' as relative URIs. For
example, an HREF of "":foo"" in the context of base URI
""http://www.example.com/path/page"" would derelativize as
""http://www.example.com/path/:foo"". (Only if another character comes before the
colon is it interpreted as a URI scheme.)

It'd be desirable for HTTPClient URI to do the same thing.

Example code to demonstrate:

import org.apache.commons.httpclient.URI;
URI base = new URI(""http://www.example.com/path/page"");
URI rel1 = new URI("":foo/boo"");
System.out.println((new URI(base,rel1)).toString()); // displays just "":foo""

A potential fix would be for URI.parseUriReference() to avoid interpreting a ':'
in the zero position as indicating a zero-length scheme:

-       if (atColon < 0 || (atSlash >= 0 && atSlash < atColon)) {
+       if (atColon <= 0 || (atSlash >= 0 && atSlash < atColon)) {

and

-        if (at < length && tmp.charAt(at) == ':') {
+        if (at > 0 && at < length && tmp.charAt(at) == ':') {"
0,"NodeAddMixinTest assumptions on addMixin behaviour. NodeAddMixinTest.testAddMixinReferencable() assumes that mix:referenceable can be added to the test node type. In practice, the node type may already inherit mix:referenceable, but it may not be active until the node is saved. Thus, a ConstraintViolationException upon addMixin should be catched, and the mixin should be checked after save() again.
"
0,"Create org.apache.jackrabbit.core.id. I'd like to create a separate package for the identifier interfaces and classes in jackrabbit-core. Currently all the identifiers are in org.apache.jackrabbit.core, which makes almost all the other packages have dependencies to o.a.j.core and causes trouble for various package-level code quality and dependency analysis tools.

For now the package would contain the ItemId, NodeId, and PropertyId classes."
0,"Cache jcr name to QName mappings. Currently jcr names are always parsed and resolved into QName instances. Introducing a cache would increase performance and also save memory because well known and often used jcr names would always return the same QName instance from cache.

Testing with common read operations shows a performance improvement of about 25%.
The test involved the following methods on Node interface:

- getProperty()
- getProperties()
- getName()
- getPath()
- isLocked()
- isNodeType()
- getPrimaryNodeType()
- hasNodes()
- getNodes()

Attached proposed implementation of a QNameResolver.

Please comment."
0,"Slightly more readable code in Token/TermAttributeImpl. No big deal. 

growTermBuffer(int newSize) was using correct, but slightly hard to follow code. 

the method was returning null as a hint that the current termBuffer has enough space to the upstream code or reallocated buffer.

this patch simplifies logic   making this method to only reallocate buffer, nothing more.  
It reduces number of if(null) checks in a few methods and reduces amount of code. 
all tests pass.

This also adds tests for the new basic attribute impls (copies of the Token tests)."
0,RowIteratorImpl should make use of QueryResultRow.getValues(). Values are currently retrieved from regular nodes. Using QueryResultRow.getValues() uses less calls to the server.
0,"IndexWriter commits unnecessarily on fresh Directory. I've noticed IndexWriter's ctor commits a first commit (empty one) if a fresh Directory is passed, w/ OpenMode.CREATE or CREATE_OR_APPEND. This seems unnecessarily, and kind of brings back an autoCommit mode, in a strange way ... why do we need that commit? Do we really expect people to open an IndexReader on an empty Directory which they just passed to an IW w/ create=true? If they want, they can simply call commit() right away on the IW they created.

I ran into this when writing a test which committed N times, then compared the number of commits (via IndexReader.listCommits) and was surprised to see N+1 commits.

Tried to change doCommit to false in IW ctor, but it got IndexFileDeleter jumping on me .. so the change might not be that simple. But I think it's manageable, so I'll try to attack it (and IFD specifically !) back :)."
0,"Speedup merging of stored fields when field mapping ""matches"". Robert Engels suggested the following idea, here:

  http://www.gossamer-threads.com/lists/lucene/java-dev/54217

When merging in the stored fields from a segment, if the field name ->
number mapping is identical then we can simply bulk copy the entire
entry for the document rather than re-interpreting and then re-writing
the actual stored fields.

I've pulled the code from the above thread and got it working on the
current trunk."
0,Replace IndexReader.getFieldNames with IndexReader.getFieldInfos. 
0,"Node.removeMixin() should not remove valid items. assume you define a mixin like:

[test] mix
- aprop (string)
+ anode (nt:base)

and you add this mixin to a nt:unstructured and add 'anode' and set 'aprop'.
then a subsequent node.removeMixin(""test"") will also remove 'anode' and 'aprop' although they are valid by the definition of nt:unstructured.

imo, the items should only be removed if they become invalid by the definition of the resulting effective node type.
"
0,"Store DocValues per segment instead of per field. currently we are storing docvalues per field which results in at least one file per field that uses docvalues (or at most two per field per segment depending on the impl.). Yet, we should try to by default pack docvalues into a single file if possible. To enable this we need to hold all docvalues in memory during indexing and write them to disk once we flush a segment. "
0,"FSImport.java link on wiki is dead. The link for FSImport.java 

http://svn.apache.org/repos/asf/jackrabbit/trunk/contrib/examples/src/java/org/apache/jackrabbit/examples/FSImport.java

from wiki page


http://wiki.apache.org/jackrabbit/ExamplesPage

is dead, could it be updated please?
"
0,"Typo in log output. jackrabbit/src/java/org/apache/jackrabbit/core/fs/local/LocalFileSystem.java:
133c133
<         log.info(""LocaaFileSystem initialized on "" + root.getPath());
---
>         log.info(""LocalFileSystem initialized on "" + root.getPath());
"
0,"Provide names for constants in QueryConstants. For debugging, logging, and user interaction purposes QueryConstants should include descriptive names for the constants it provides."
0,"JSR 283: Access Nodes and Properties by Array of ""NameGlob"". The proposed final draft contains new variants of Node.getNodes and Node.getProperties:

- Node.getNodes(String[] nameGlobs)
- Node.getProperties(String[] nameGlobs)

see section 5.2.2 Iterating Over Child Items and 5.2.2.2 Name Globs"
0,"Implement QueryObjectModelFactory.fullTextSearch() in QueryManagerImpl. While doing the JCR-1104 upgrade to JCR 2.0, we ran into an issue on how to best handle the QueryObjectModelFactory.fullTextSearch() method that seems to have changed a bit since the spi-commons version was written.

Marcel, can you take a look at this when you have time. The dummy implementation I added now is at line 97 of QueryManagerImpl.java in jackrabbit-core."
0,"Memory codec. This codec stores all terms/postings in RAM.  It uses an
FST<BytesRef>.  This is useful on a primary key field to ensure
lookups don't need to hit disk, to keep NRT reopen time fast even
under IO contention.
"
0,"Mavenize Project. Mavenize project pending mailing list feedback.

I'm attaching an out-dated zip that has some useful ""stuff"" to review or build upon.

Proposed flow:

1) Review the maven specific elements in the zip:
- project.xml, header.txt, license.txt, notice.txt, checkstyle.xml, project.properties, xdocs directory. NOTE: I know the license.txt and maybe header.txt are not correct per earlier discussions.
2) Once new package layout/code is in svn - we can refactor this and move the maven related files into place.

Thoughts,
  a) One of the artifacts generated is the bat file and supporting directory with the jar to run the sample app - could use some ideas on how maven should generate this.
  b) How the local maven repo gets the jcr-api.jar? My current thinking is 'same as any non-redistributable artifact'. The user is responsible for downloading it to a local repository."
0,"NodeTypeRegistry.registerNodetypes(Collection) should not register a partial set. the javadoc says:

     * Note that in the case an exception is thrown, some node types might have
     * been nevertheless successfully registered.

the problem hereby is, that it cannot be determined easily, what nodetypes could be registered, and which couldnt. i would rather prefer a all-or-nothing behaviour."
0,"Add ""tokenize documents only"" task to contrib/benchmark. I've been looking at performance improvements to tokenization by
re-using Tokens, and to help benchmark my changes I've added a new
task called ReadTokens that just steps through all fields in a
document, gets a TokenStream, and reads all the tokens out of it.

EG this alg just reads all Tokens for all docs in Reuters collection:

  doc.maker=org.apache.lucene.benchmark.byTask.feeds.ReutersDocMaker
  doc.maker.forever=false
  {ReadTokens > : *
"
0,"throw exception for fieldcache on a non-atomic reader. In Lucene 4.0, we go through a lot of effort to prevent slow uses of non-atomic readers:

DirectoryReader/MultiReader etc throw exception if you don't try to access postings or docvalues apis per-segment, etc.

But the biggest trap of all is still too easy to fall into, we don't do the same for FieldCache.

I think we should throw exception, forcing the user to either change their code or use a SlowMultiReaderWrapper.
"
0,"Collapse Common module into Lucene core util. It was suggested by Robert in [http://markmail.org/message/wbfuzfamtn2qdvii] that we should try to limit the dependency graph between modules and where there is something 'common' it should probably go into lucene core.  Given that I haven't added anything to this module except the MutableValue classes, I'm going to collapse them into the util package, remove the module, and correct the dependencies."
0,"SPI: Get rid of unused method ItemInfo.getParentId(). Looking at the various SPI impls in the trunk and in the sandbox reveals that ItemInfo.getParentId is not used at all.
I'd like to suggest to get rid of that method.

Any objections/concerns?
angela

"
0,"MultiReader and ParallelReader accidently override doOpenIfChanged(boolean readOnly) with doOpenIfChanged(boolean doClone). I found this during adding deprecations for RW access in LUCENE-3606:

the base class defines doOpenIfChanged(boolean readOnly), but MultiReader and ParallelReader ""override"" this method with a signature doOpenIfChanged(doClone) and missing @Override. This makes consumers calling IR.openIfChanged(boolean readOnly) do the wrong thing. Instead they should get UOE like for the other unimplemented doOpenIfChanged methods in MR and PR.

Easy fix is to rename and hide this internal ""reopen"" method, like DirectoryReader,..."
0,"Compressed fields should be ""externalized"" (from Fields into Document). Right now, as of 2.0 release, Lucene supports compressed stored fields.  However, after discussion on java-dev, the suggestion arose, from Robert Engels, that it would be better if this logic were moved into the Document level.  This way the indexing level just stores opaque binary fields, and then Document handles compress/uncompressing as needed.

This approach would have prevented issues like LUCENE-629 because merging of segments would never need to decompress.

See this thread for the recent discussion:

    http://www.gossamer-threads.com/lists/lucene/java-dev/38836

When we do this we should also work on related issue LUCENE-648."
0,"Spellchecker should take IndexWriterConfig... deprecate old methods?. When looking at LUCENE-3490, i realized there was no way to specify the codec for the spellchecker to use.

It has the following current methods:
* indexDictionary(Dictionary dict): this causes optimize!
* indexDictionary(Dictionary dict, int mergeFactory, int ramMB): this causes optimize!
* indexDictionary(Dictionary dict, int mergeFactor, int ramMB, boolean optimize)

But no way to specify an IndexwriterConfig. Additionally, I don't like that several of these ctors force an optimize in a tricky way,
even though it was like this all along.

So I think we should add indexDictionary(Dictionary dict, IndexWriterConfig config, boolean optimize).

We should either deprecate all the other ctors in 3.x and nuke in trunk, or at least add warnings to the ones that optimize."
0,"Remove recently added getJCRPath()/getQPath() from NamespaceResolver. issue JCR-473 added 2 new methods to NamespaceResolver: 

    public Path getQPath(String jcrPath) throws MalformedPathException;
    public String getJCRPath(Path qPath) throws NoPrefixDeclaredException;

which do not belong here, since the NamespaceResolver has nothing to do with paths. suggest to remove them.

further is the naming of the QName related methods a bit vague. suggest to rename them to:

   QName parseName(String jcrName)
   String formatName(QName qName)

(although they do not belong here either, but helps to leverage evt. caching namespace resolvers).

"
0,"optionally support naist-jdic for kuromoji. This is an alternative dictionary, somewhat larger (~25%).

we can support it in build.xml so if a user wants to build with it, they can (the resulting jar file will be 500KB larger)"
0,"random localization test failures. Some tests fail randomly (hard to reproduce). It appears to me that this is caused by uninitialized date fields. For example Uwe reported a failure today in this test of TestQueryParser:

{code}
 /** for testing legacy DateField support */
  public void testLegacyDateRange() throws Exception {
    String startDate = getLocalizedDate(2002, 1, 1, false);
    String endDate = getLocalizedDate(2002, 1, 4, false);
{code}

if you look at the helper getLocalizedDate, you can see if the 4th argument is false, it does not initialize all date field functions.
{code}
  private String getLocalizedDate(int year, int month, int day, boolean extendLastDate) {
 Calendar calendar = new GregorianCalendar();
 calendar.set(year, month, day);
 if (extendLastDate) {
      calendar.set(Calendar.HOUR_OF_DAY, 23);
      calendar.set(Calendar.MINUTE, 59);
      calendar.set(Calendar.SECOND, 59);
 ...
}
{code}

I think the solution to this is that in all tests, whereever we create new GregorianCalendar(), it should be followed by a call to Calendar.clear().
This will ensure that we always initialize unused calendar fields to zero, rather than being dependent on the local time."
0,"Enhance Ingres persistence bundle to handle unicode. Tiny change to ingres.ddl for persistent bundles to handle unicode strings.

"
0,"Publish source/javadoc jar files to the Maven repository. It would be really nice if HttpComponents (Core and Client) published jar files to the Maven repository for not just the bytecode, but also for the source and javadoc (done by defining a ""classifier"" attribute of ""javadoc"" or ""source"" for the jar when publishing with Maven).

Having these in the Maven repo allows an IDE (like Eclipse) to auto-download and attach the source/javadoc to the HttpComponent jar files - meaning developers will then see the API documentation automatically in their IDE.  This also greatly aids debugging if one needs to step through HttpComponent code, and placing the source in the hands of more developers also means you might see more patches coming back."
0,"Add QueryParser.newFieldQuery. Note: this patch changes no behavior, just makes QP more subclassable.

Currently we have Query getFieldQuery(String field, String queryText, boolean quoted)
This contains very hairy methods for producing a query from QP's analyzer.

I propose we factor this into newFieldQuery(Analyzer analyzer, String field, String queryText, boolean quoted)
Then getFieldQuery just calls newFieldQuery(this.analyzer, field, queryText, quoted);

The reasoning is: it can be quite useful to consider the double quote as more than phrases, but a ""more exact"" search.
In the case the user quoted the terms, you might want to analyze the text with an alternate analyzer that:
doesn't produce synonyms, doesnt decompose compounds, doesn't use WordDelimiterFilter 
(you would need to be using preserveOriginal=true at index time for the WDF one), etc etc.

This is similar to the way google's double quote operator works, its not defined as phrase but ""this exact wording or phrase"".
For example compare results to a query of tests versus ""tests"".

Currently you can do this without heavy code duplication, but really only if you make a separate field (which is wasteful),
and make your custom QP lie about its field... in the examples I listed above you can do this with a single field, yet still
have a more exact phrase search.
"
0,"IndexReader.close should forcefully evict entries from FieldCache. Spinoff of java-user thread ""heap memory issues when sorting by a string field"".

We rely on WeakHashMap to hold our FieldCache, keyed by reader.  But this lacks immediacy on releasing the reference, after a reader is closed.

WeakHashMap can't free the key until the reader is no longer referenced by the app. And, apparently, WeakHashMap has a further impl detail that requires invoking one of its methods for it to notice that a key has just become only weakly reachable.

To fix this, I think on IR.close we should evict entries from the FieldCache, as long as the sub-readers are truly closed (refCount dropped to 0)."
0,"JCR2SPI: improve ItemDefinitionProviderImpl.getMatchingPropdef to better handle multiple residuals. When a new property is set with unknown type (missing PropertyType parameter), ItemDefinitionProviderImpl.getMatchingPropdef() is used to find an applicable property definition.

There may be cases where multiple residual property defs may match, for instance, when the repository allows only a certain set of property types on that node type.

In this case, when the set of allowable types includes STRING, that propdef should be returned. After all, the client did not specify the type, so STRING is most likely the best match.
"
0,"[PATCH] Ant macro for javadocs and javadocs-internal. This removes the duplication introduced in build.xml 
when the javadocs-internal build target was added. 
 
Regards, 
Paul Elschot"
0,"Move listeners from item state to item state managers. Clients interested in item state modifications directly subscribe to the item states, which is a very flexible approach. On the other side, it increases the memory consumption of an item state, because every item state holds a collection of its listeners. It further increases complexity, because item state listeners can potentially have a shorter life and might be garbage collected.

Listeners should therefore be moved to their associated item state manager. At the same time, this enables an item state manager to completely remove an item state from its cache and resurrect it at a later time without losing the listeners interested in notifications."
0,"Issue LUCENE-352 was closed, but the patch there is not applied in the current trunk. See here:
http://issues.apache.org/jira/browse/LUCENE-352

And thanks for making JIRA easier, I noticed the Lucene Java project
was preselected for me.

Regards,
Paul Elschot"
0,Remove synchronization in SegmentReader.isDeleted. Removes SegmentReader.isDeleted synchronization by using a volatile deletedDocs variable on Java 1.5 platforms.  On Java 1.4 platforms synchronization is limited to obtaining the deletedDocs reference.
0,"move route computation from client to director. The computation of routes should be done in the ClientRequestDirector, not in the Client.
The director needs to compute routes for redirects, so it should compute all routes.
"
0,"TestIndexWriterNRTIsCurrent failure. found by jenkins: 

https://builds.apache.org/job/Lucene-Solr-tests-only-trunk/12492/

make your computer busy (e.g. run tests in another checkout) then,

ant test-core -Dtests.iter=100 -Dtestcase=TestIndexWriterNRTIsCurrent -Dtestmethod=testIsCurrentWithThreads -Dtests.seed=-78f6fa16b849cf27:382126da79c1e146:-d2cdec79e86e1b3 -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=ISO8859-1""

takes a few tries till it pops...

{noformat}
junit-sequential:
    [junit] Testsuite: org.apache.lucene.index.TestIndexWriterNRTIsCurrent
    [junit] Tests run: 100, Failures: 1, Errors: 1, Time elapsed: 277.818 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] WARNING: you are using -Dtests.iter=n where n > 1, not all tests support this option.
    [junit] Some may crash or fail: this is not a bug.
    [junit] ------------- ---------------- ---------------
    [junit] ------------- Standard Error -----------------
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterNRTIsCurrent -Dtestmethod=testIsCurrentWithThreads -Dtests.seed=-78f6fa16b849cf27:382126da79c1e146:-d2cdec79e86e1b3 -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=ISO8859-1""
    [junit] The following exceptions were thrown by threads:
    [junit] *** Thread: Lucene Merge Thread #17 ***
    [junit] org.apache.lucene.index.MergePolicy$MergeException: java.lang.AssertionError
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.handleMergeException(ConcurrentMergeScheduler.java:520)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:480)
    [junit] Caused by: java.lang.AssertionError
    [junit] 	at org.apache.lucene.index.IndexWriter$ReadersAndLiveDocs.initWritableLiveDocs(IndexWriter.java:580)
    [junit] 	at org.apache.lucene.index.IndexWriter.commitMergedDeletes(IndexWriter.java:3061)
    [junit] 	at org.apache.lucene.index.IndexWriter.commitMerge(IndexWriter.java:3137)
    [junit] 	at org.apache.lucene.index.IndexWriter.mergeMiddle(IndexWriter.java:3718)
    [junit] 	at org.apache.lucene.index.IndexWriter.merge(IndexWriter.java:3257)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler.doMerge(ConcurrentMergeScheduler.java:382)
    [junit] 	at org.apache.lucene.index.ConcurrentMergeScheduler$MergeThread.run(ConcurrentMergeScheduler.java:451)
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterNRTIsCurrent -Dtestmethod=testIsCurrentWithThreads -Dtests.seed=-78f6fa16b849cf27:382126da79c1e146:-d2cdec79e86e1b3 -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=ISO8859-1""
    [junit] NOTE: reproduce with: ant test -Dtestcase=TestIndexWriterNRTIsCurrent -Dtestmethod=testIsCurrentWithThreads -Dtests.seed=-78f6fa16b849cf27:382126da79c1e146:-d2cdec79e86e1b3 -Dtests.multiplier=3 -Dargs=""-Dfile.encoding=ISO8859-1""
    [junit] NOTE: test params are: codec=Lucene40: {id=MockFixedIntBlock(blockSize=525)}, sim=DefaultSimilarity, locale=es_PY, timezone=Africa/Luanda
    [junit] NOTE: all tests run in this JVM:
    [junit] [TestIndexWriterNRTIsCurrent]
    [junit] NOTE: Linux 3.0.0-14-generic amd64/Sun Microsystems Inc. 1.6.0_24 (64-bit)/cpus=8,threads=1,free=74907448,total=255787008
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testIsCurrentWithThreads(org.apache.lucene.index.TestIndexWriterNRTIsCurrent):	FAILED
    [junit] info=_qx(4.0):C1/1 isn't live
    [junit] junit.framework.AssertionFailedError: info=_qx(4.0):C1/1 isn't live
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit] 	at org.apache.lucene.index.IndexWriter$ReaderPool.infoIsLive(IndexWriter.java:663)
    [junit] 	at org.apache.lucene.index.IndexWriter$ReaderPool.dropAll(IndexWriter.java:717)
    [junit] 	at org.apache.lucene.index.IndexWriter.closeInternal(IndexWriter.java:1136)
    [junit] 	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1069)
    [junit] 	at org.apache.lucene.index.IndexWriter.close(IndexWriter.java:1033)
    [junit] 	at org.apache.lucene.index.TestIndexWriterNRTIsCurrent.testIsCurrentWithThreads(TestIndexWriterNRTIsCurrent.java:68)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$SubclassSetupTeardownRule$1.evaluate(LuceneTestCase.java:707)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$InternalSetupTeardownRule$1.evaluate(LuceneTestCase.java:606)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$TestResultInterceptorRule$1.evaluate(LuceneTestCase.java:511)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$RememberThreadRule$1.evaluate(LuceneTestCase.java:569)
    [junit] 
    [junit] 
    [junit] Testcase: testIsCurrentWithThreads(org.apache.lucene.index.TestIndexWriterNRTIsCurrent):	Caused an ERROR
    [junit] java.lang.AssertionError: Some threads threw uncaught exceptions!
    [junit] java.lang.RuntimeException: java.lang.AssertionError: Some threads threw uncaught exceptions!
    [junit] 	at org.apache.lucene.util.LuceneTestCase.tearDownInternal(LuceneTestCase.java:780)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.access$1000(LuceneTestCase.java:138)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$InternalSetupTeardownRule$1.evaluate(LuceneTestCase.java:607)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$TestResultInterceptorRule$1.evaluate(LuceneTestCase.java:511)
    [junit] 	at org.apache.lucene.util.LuceneTestCase$RememberThreadRule$1.evaluate(LuceneTestCase.java:569)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:165)
    [junit] 	at org.apache.lucene.util.LuceneTestCaseRunner.runChild(LuceneTestCaseRunner.java:57)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.checkUncaughtExceptionsAfter(LuceneTestCase.java:808)
    [junit] 	at org.apache.lucene.util.LuceneTestCase.tearDownInternal(LuceneTestCase.java:752)
    [junit] 
    [junit] 
    [junit] Test org.apache.lucene.index.TestIndexWriterNRTIsCurrent FAILED

BUILD FAILED
{noformat}"
0,"CachingMultiReader has inconsistent name. All other classes that extend IndexReader are also named that way, except CachingMultiReader. It should be renamed to CachingMultiIndexReader."
0,"Factor merge policy out of IndexWriter. If we factor the merge policy out of IndexWriter, we can make it pluggable, making it possible for apps to choose a custom merge policy and for easier experimenting with merge policy variants."
0,"On missing child node, automatically remove the entry (when a session attribute is set). If a node points to a non-existing child node (which is a repository inconsistency), currently this child node is silently ignored for read operations (as far as I can tell). However, when trying to add another child node with the same name, an exception is thrown with a message saying a child node with this name already exists. Also, the parent node can't be removed.

One solution is to remove the bad child node entry, but only if the session attribute ""org.apache.jackrabbit.autoFixCorruptions"" is set (so by default the repository is not changed 'secretly'):

    SimpleCredentials cred = new SimpleCredentials(...);
    cred.setAttribute(""org.apache.jackrabbit.autoFixCorruptions"", ""true"");
    rep.login(cred);

It's not a perfect solution, but it might be better than throwing an exception and basically preventing changes.

Another solution (not implemented) would be to rename the missing child node entry when trying to add a child node with the same name (for example add the current date/time, or a random digit until there is no conflict), and then continue with adding the new child node.

"
0,"JCR2SPI: Avoid individual Item reloading upon Session/Item.refresh(true). with CacheBehaviour.INVALIDATE Item.refresh(true) and Session.refresh(true) results in individual reloading of the existing entries in the hierarchy circumventing all batch read optimization.

Apart from general optimization of the refresh itself, refresh(true) should rather mark existing items and force a reload upon next access (similar to the behaviour implemented with refresh(false)).

"
0,Improve docs for deployment Models 1 and 2 on Tomcat 5.5.x and provide an example webapp.. New users would find a small webapp and associated documentation that walks them through the process of setting up a model 1 or model 2 (or both) deployment scheme.
0,"Missing derby dependency. the derby dependency is missing in the OCM subproject. So, the unit tests cannot be executed. "
0,Remove Author tags from code. Remove all author tags from the code.
0,Hide ugly repository init code for OCM. Hide repository namespace registration and ocm:discriminator node type registration in implementation of OCM
0,"TCK: ImpersonateTest#testImpersonate should allow LoginException. JSR-170 allows Session.impersonate to throw LoginException if the session lacks permission to impersonate another user.  Some repositories may not allow any session to impersonate another user, in which case this test would fail.

Proposal: catch and consume LoginException.

--- ImpersonateTest.java        (revision 422074)
+++ ImpersonateTest.java        (working copy)
@@ -17,11 +17,13 @@
 package org.apache.jackrabbit.test.api;
  
 import org.apache.jackrabbit.test.AbstractJCRTest;
+import org.apache.jackrabbit.test.NotExecutableException;
  
 import javax.jcr.Session;
 import javax.jcr.Credentials;
 import javax.jcr.NodeIterator;
 import javax.jcr.Node;
+import javax.jcr.LoginException;
 import java.security.AccessControlException;
  
 /**
@@ -40,7 +42,14 @@
      */
     public void testImpersonate() throws Exception {
         // impersonate to read-only user
-        Session session = superuser.impersonate(helper.getReadOnlyCredentials());
+        Session session = null;
+
+        try {
+            session = superuser.impersonate(helper.getReadOnlyCredentials());
+        }
+        catch (LoginException e) {
+          throw new NotExecutableException(""impersonate threw LoginException"");
+        }
  
         // get a path to test the permissions on
         String thePath = """";
"
0,HttpClient javadocs need improving. .. patch coming.
0,"Allow to disable referential integrity checking for workspace. Some operations like clone, remove operating on huge subtree of nodes requires a lot of memory. To copy, clone, remove subtree all nodes are loaded into transient spaces. It allows such operations to be transactional, from other side it requires a lot of heap size and this memory size is directly dependent on the size of subtree (number of nodes). In result of this in some cases it is impossible to make such operations in one step. In our environment sometimes 1 GB of java heap is not enough to succesfully clone subtree  from one workspace to another.

You can always clone (copy, remove) tree in chunks, but if you have references between subtrees such approach fails. Possibilty of temporary disabling referential integrity checking for experienced JCR user could be very usefull then.

Another use case is to allow to clone selected subtrees of the whole structure between worskpaces. In our application we need to clone only some selected subtrees from one workspace to another. But we can not do that because of existing references. We need to clone the whol estructure first, then remove all unwanted nodes, which is really time expensive and memory consuming.
"
0,"Replacement for TermAttribute+Impl with extended capabilities (byte[] support, CharSequence, Appendable). For flexible indexing terms can be simple byte[] arrays, while the current TermAttribute only supports char[]. This is fine for plain text, but e.g NumericTokenStream should directly work on the byte[] array.
Also TermAttribute lacks of some interfaces that would make it simplier for users to work with them: Appendable and CharSequence

I propose to create a new interface ""CharTermAttribute"" with a clean new API that concentrates on CharSequence and Appendable.
The implementation class will simply support the old and new interface working on the same term buffer. DEFAULT_ATTRIBUTE_FACTORY will take care of this. So if somebody adds a TermAttribute, he will get an implementation class that can be also used as CharTermAttribute. As both attributes create the same impl instance both calls to addAttribute are equal. So a TokenFilter that adds CharTermAttribute to the source will work with the same instance as the Tokenizer that requested the (deprecated) TermAttribute.

To also support byte[] only terms like Collation or NumericField needs, a separate getter-only interface will be added, that returns a reusable BytesRef, e.g. BytesRefGetterAttribute. The default implementation class will also support this interface. For backwards compatibility with old self-made-TermAttribute implementations, the indexer will check with hasAttribute(), if the BytesRef getter interface is there and if not will wrap a old-style TermAttribute (a deprecated wrapper class will be provided): new BytesRefGetterAttributeWrapper(TermAttribute), that is used by the indexer then."
0,temporary files created by some jUnit test are not automatically removed. 
0,"remove custom encoding support in Greek/Russian Analyzers. The Greek and Russian analyzers support custom encodings such as KOI-8, they define things like Lowercase and tokenization for these.

I think that analyzers should support unicode and that conversion/handling of other charsets belongs somewhere else. 

I would like to deprecate/remove the support for these other encodings.
"
0,"IndexWriter should throw IndexFormatTooOldExc on open, not later during optimize/getReader/close. Spinoff of LUCENE-2618 and also related to the original issue LUCENE-2523...

If you open IW on a too-old index, you don't find out until much later that the index is too old.

This is because IW does not go and open segment readers on all segments.  It only does so when it's time to apply deletes, do merges, open an NRT reader, etc.

This is a serious bug because you can in fact succeed in committing with the new major version of Lucene against your too-old index, which is catastrophic because suddenly the old Lucene version will no longer open the index, and so your index becomes unusable."
0,"Collapse Searcher/Searchable/IndexSearcher; remove contrib/remote; merge PMS into IndexSearcher. We've discussed cleaning up our *Searcher stack for some time... I
think we should try to do this before releasing 4.0.

So I'm attaching an initial patch which:

  * Removes Searcher, Searchable, absorbing all their methods into IndexSearcher

  * Removes contrib/remote

  * Removes MultiSearcher

  * Absorbs ParallelMultiSearcher into IndexSearcher (ie you can now
    pass useThreads=true, or a custom ES to the ctor)

The patch is rough -- I just ripped stuff out, did search/replace to
IndexSearcher, etc.  EG nothing is directly testing using threads with
IndexSearcher, but before committing I think we should add a
newSearcher to LuceneTestCase, which randomly chooses whether the
searcher uses threads, and cutover tests to use this instead of making
their own IndexSearcher.

I think MultiSearcher has a useful purpose, but as it is today it's
too low-level, eg it shouldn't be involved in rewriting queries: the
Query.combine method is scary.  Maybe in its place we make a higher
level class, with limited API, that's able to federate search across
multiple IndexSearchers?  It'd also be able to optionally use thread
per IndexSearcher.
"
0,"Improve automaton's MinimizeOperations.minimizeHopcroft() to not create so many objects. MinimizeOperations.minimizeHopcroft() creates a lot of objects because of strange arrays and useless ArrayLists with fixed length. E.g. it created List<List<List<>>>. This patch minimizes this and makes the whole method much more GC friendler by using simple arrays or avoiding empty LinkedLists at all (inside reverse array). 

minimize() is called very very often, especially in tests (MockAnalyzer).

A test for the method is prepared by Robert, we found a bug somewhere else in automaton, so this is pending until his issue and fix arrives."
0,"Integrate MockBM25Similarity and MockLMSimilarity into the framework. Steps:
1. Decide if {{MockLMSimilarity}} is needed at all (we have {{LMDirichletSimilarity}})
2. Move the classes to the similarities package
3. Move the similarities package to src/
4. Move all sims (inc. Similarity) to similarities
5. Make MockBM25Similarity a subclass of EasySimilarity?"
0,"Implement TokenStream.end() in contrib TokenStreams. See LUCENE-1448. Mike's patch there already contains the necessary fixes.

I'll attach a patch here as soon as LUCENE-1460 is committed."
0,"make the build more friendly to apache harmony. as part of improved testing, i thought it would be a good idea to make the build (ant test) more friendly
to working under apache harmony.

i'm not suggesting we de-optimize code for sun jvms or anything crazy like that, only use it as a tool.

for example:
* bugs in tests/code: for example i found a test that expected ArrayIOOBE 
  when really the javadoc contract for the method is just IOOBE... it just happens to
  pass always on sun jvm because thats the implementation it always throws.
* better reproduction of bugs: for example [2 months out of the year|http://en.wikipedia.org/wiki/Unusual_software_bug#Phase_of_the_Moon_bug]
  it seems TestQueryParser fails with thai locale in a difficult-to-reproduce way.
  but i *always* get similar failures like this with harmony for this test class.
* better stability and portability: we should try (if reasonable) to avoid depending
  upon internal details. the same kinds of things that fail in harmony might suddenly
  fail in a future sun jdk. because its such a different impl, it brings out a lot of interesting stuff.

at the moment there are currently a lot of failures, I think a lot might be caused by this: http://permalink.gmane.org/gmane.comp.java.harmony.devel/39484
"
0,"Log / trace wrapper: upgrade to JCR API 2.0. The JCR Log wrapper (jcrlog) currently only supports the JCR 1.0 API. It should be upgraded to 2.0. Also, the dependency to jackrabbit-core should be removed, and generics should be used."
0,"AbstractResource: Use jcr:createdBy to expose DAV:creator-displayname. as of jcr 2.0 the DeltaV property DAV:creator-displayname could be extracted from the jcr:createdBy property.
the comment in abstract resource: ""// creator-displayname, comment: not value available from jcr"" therefore is outdated. and so is the subsequent line."
0,"QueryParser can produce empty sub BooleanQueries when Analyzer proudces no tokens for input. as triggered by SOLR-261, if you have a query like this...

   +foo:BBB  +(yak:AAA  baz:CCC)

...where the analyzer produces no tokens for the ""yak:AAA"" or ""baz:CCC"" portions of the query (posisbly because they are stop words) the resulting query produced by the QueryParser will be...

  +foo:BBB +()

...that is a BooleanQuery with two required clauses, one of which is an empty BooleanQuery with no clauses.

this does not appear to be ""good"" behavior.

In general, QueryParser should be smarter about what it does when parsing encountering parens whose contents result in an empty BooleanQuery -- but what exactly it should do in the following situations...

 a)  +foo:BBB +()
 b)  +foo:BBB ()
 c)  +foo:BBB -()

...is up for interpretation.  I would think situation (b) clearly lends itself to dropping the sub-BooleanQuery completely.  situation (c) may also lend itself to that solution, since semanticly it means ""don't allow a match on any queries in the empty set of queries"".  .... I have no idea what the ""right"" thing to do for situation (a) is."
0,"Improved Payloads API. We want to make some optimizations to the Payloads API.

See following thread for related discussions:
http://www.gossamer-threads.com/lists/lucene/java-dev/54708"
0,"remove Query.getSimilarity(). Spinoff of LUCENE-2854.

See LUCENE-2828 and LUCENE-2854 for reference.

In general, the SimilarityDelegator was problematic with regards to back-compat, and if queries
want to score differently, trying to runtime subclass Similarity only causes trouble.

The reason we could not fix this in LUCENE-2854 is because:
{noformat}
Michael McCandless added a comment - 08/Jan/11 01:53 PM
bq. Is it possible to remove this method Query.getSimilarity also? I don't understand why we need this method!

I would love to! But I think that's for another day...

I looked into this and got stuck with BoostingQuery, which rewrites to an anon 
subclass of BQ overriding its getSimilarity in turn override its coord method. 
Rather twisted... if we can do this differently I think we could remove Query.getSimilarity.
{noformat}

here is the method in question:

{noformat}
/** Expert: Returns the Similarity implementation to be used for this query.
 * Subclasses may override this method to specify their own Similarity
 * implementation, perhaps one that delegates through that of the Searcher.
 * By default the Searcher's Similarity implementation is returned.*/
public Similarity getSimilarity(IndexSearcher searcher) {
  return searcher.getSimilarity();
}
{noformat}
"
0,Create a sample search page. The web application should have a search page that shows how to use the query features in jackrabbit.
0,"Position based TermVectorMapper. As part of the new TermVectorMapper approach to TermVectors, the ensuing patch loads term vectors and stores the term info by position.  This should let people directly index into a term vector given a position.  Actually, it does it through Maps, b/c the array based bookkeeping is a pain given the way positions are stored.  

The map looks like:
Map<String,   Map<Integer, TVPositionInfo>>

where the String is the field name, the integer is the position, and TVPositionInfo is a storage mechanism for the terms and offsets that occur at a position.  It _should_ handle multiple terms per position (which is always my downfall! )

I have not tested performance of this approach.
"
0,"When we move to java 1.5 in 3.0 we should replace all Interger, Long, etc construction with .valueOf. -128 to 128 are guaranteed to be cached and using valueOf in that case is 3.5 times faster than using contructor"
0,VFS backed file system. File System implementation backed by commons VFS. 
0,"Java 5 port phase II . LUCENE-1257 addresses the public API changes ( generics , mainly ) and other j.u.c. package changes related to the API .  The changes are frozen and closed for 3.0 . This would be a placeholder JIRA for 3.0+ version to address the pending changes ( tests for generics etc.) and any other internal API changes as necessary. "
0,"Hitting disk full during DocumentWriter.ThreadState.init(...) can cause hang. More testing of RC2 ...

I found one case, if you hit disk full during init() in
DocumentsWriter.ThreadState, when we first create the term vectors &
fields writer, such that subsequent calls to
IndexWriter.add/updateDocument will then hang forever.

What's happening in this case is we are incrementing nextDocID even
though we never call finishDocument (because we ""thought"" init did not
succeed).  Then, when we finish the next document, it will never
actually write because missing finishDocument call never happens.
"
0,"RepositoryService.checkin should return information about newly created version. We have a mismatch between Node.checkin(), which returns a new Version object, and RepositoryService.checkin(), which returns void.

Client of SPI, such as JCR2SPI, thus will have to make an additional request for the base version property, with the obvious drawbacks (another call, and a potential cause for a race condition).

Proposal: change the return code to NodeId.
 "
0,"Link javadocs of HttpClient, HttpCore and HttpMime. Presently the javadocs for HttpCore, HttpClient and HttpMime are isolated from each other.  For new users this can create a great deal of confusion and the appearance of limited functionality of HttpClient.  Please set the javadoc creation task to link the javadoc of these three projects together.  "
0,RTFTextExtractor should also support mime type text/rtf. There exist two mime types for RTF documents: application/rtf and text/rtf. The current RTFTextExtractor currently only recognizes the first.
0,Allow use of compact DocIdSet in CachingWrapperFilter. Extends CachingWrapperFilter with a protected method to determine the DocIdSet to be cached.
0,"jcr ext doesn't compile with jdk 1.4. IllegalStateException(String str, Exception e) isn't supported."
0,"TestNLS fails with ja locale. set ANT_ARGS=""-Dargs=-Duser.language=ja -Duser.country=JP""
ant test-core -Dtestcase=TestNLS

The test has 2 sets of message, one fallback, and one ja.
The tests assume if it asks for a non-ja locale, that it will get the fallback message,
but this is not how ResourceBundle.getBundle works:
{noformat}
Otherwise, the following sequence is generated from the attribute values of the specified locale 
(language1, country1, and variant1) and of the default locale (language2, country2, and variant2):

baseName + ""_"" + language1 + ""_"" + country1 + ""_"" + variant1
baseName + ""_"" + language1 + ""_"" + country1
baseName + ""_"" + language1
baseName + ""_"" + language2 + ""_"" + country2 + ""_"" + variant2
baseName + ""_"" + language2 + ""_"" + country2
baseName + ""_"" + language2
baseName
{noformat}

So in the case of ja default locale, you get a japanese message instead from the baseName + ""_"" + language2 match"
0,"Typos in MultiThreadedHttpConnectionManager. I've done a review of the MultiThreadedHttpConnectionManager class in 3.0-beta1,
especially focussing on the documentation. In general, it could use a lot of
improvement, IMHO.

This bug report only deals with some typos I found in the class, and some
minimal style improvements that are compatible with the other classes.

I will attach a proposed patch."
0,"UserImporter should use User.changePassword. the UserImporter lists a limitation that the password value is expected to be hashed already as it writes the
value as it was retrieved from the xml-import.

Instead it could make use of User#changePassword that (in the implementation present with JR) creates a 
pw-hash if the password is found to be plain text."
0,Implementation of a memory file system. I needed a memory file system for my test cases. A patch for a simple implementation the works well in my environment is attached
0,"NamespaceRegistryTest makes assumptions about legal names. org.apache.jackrabbit.test.api.NamespaceRegistryTest.testRegisterNamespace() makes the assumption that once a namespace is registered, it can be used in a node name. In practice, many repositories have their own restrictions on node naming, in particular may not support namespace prefixes in node names at all.

Proposal: change the test case so that it's independant of the repository's ability to create new nodes in that namespace.
"
0,"Restructure the Jackrabbit source tree. Reintroduce some of the changes in JCR-157 as a more general restructuring to simplify the Jackrabbit project structure. See http://thread.gmane.org/gmane.comp.apache.jackrabbit.devel/9170/ for the rationale and discussion. The main parts of this restructuring would be:

1. Create a Jackrabbit ""super-project"" (artifactId: jackrabbit) in trunk/

2. Use the super-project POM as the parent of all Jackrabbit component POMs

3. Move the contents of trunk/jackrabbit/src/site directly to trunk/src/site, and use the super-project to generate the web site

4. Create independent subprojects for the the jackrabbit-api and jackrabbit-commons components, moving the the corresponding parts of the source tree

5. Move the jcr-server subprojects on level up

6. Rename the subproject directories to match their artifactIds

Note that this restructuring depends on JCR-611 and JCR-332, since the best way to implement this by utilizing a snapshot repository for the component dependencies."
0,"Connection is not released back to the pool if a runtime exception is thrown in HttpMethod#releaseConnection method. the default config of leaving the HttpClientParams.CONNECTION_MANAGER_TIMEOUT as zero means 
that the first time the connection manager fails to immediately get a connection you application hangs. 
(at least using MultiThreadedHttpConnectionManager.)

this is because the zero gets passed onto a call to Object.wait(long timeout) and, from the docs, ""If 
timeout is zero, however, then real time is not taken into consideration and the thread simply waits 
until notified."". 

since nothing ever ""notify()""s the thread everything just stops...

the default behaviour of the client more should be more predictable. you don't expect it to hang your 
entire app if it can't get a connection, you expect it to timeout then throw an exception or give some 
other kind of feedback.

it would make sense to give a default of, say, arbitrarily, 10 seconds or so. this would save every single 
user of the classes having to dig around in the code/documentation and explictly set this param. they 
might decide that the default value isn't right and hence change it, but that's tweaking behaviour, not 
correcting it. i certainly thought it was a bug in the code (yours or mine), not my config and have been 
fretting around it for a while.

best,
garry"
0,"Add option to ReverseStringFilter to mark reversed tokens. This patch implements additional functionality in the filter to ""mark"" reversed tokens with a special marker character (Unicode 0001). This is useful when indexing both straight and reversed tokens (e.g. to implement efficient leading wildcards search)."
0,"Change AttributeSource API to use generics. The AttributeSource API will be easier to use with JDK 1.5 generics.

Uwe, if you started working on a patch for this already feel free to assign this to you."
0,"Access to SO_TIMEOUT for open connections. I'm trying to access a set of pages in order, for which I have a maximum delay
permissible.  The complete operation includes following all redirects and
fetching the complete page content.  What I need, which doesn't seem to be
doable right now (according to the common-users list) is to reset the SO_TIMEOUT
property of the socket before each read to the inputstream.  I'd need an access
to the HttpConnection, or a way to set the parameters for that object.

This is a simplified version of what I'm doing:
-----
HttpURL url = new HttpURL(urlString);
method.setURI(url);
method.setFollowRedirects(false);
method.getParams().setSoTimeout(remainingTime);
HostConfiguration hostConfig = new HostConfiguration();
hostConfig.setHost(url);
method.setHostConfiguration(hostConfig);
timeoutChecker.getRemainingTime());

int statusCode = client.executeMethod(hostConfig, method, state);
String pageContent;

if (isRedirect(statusCode)) {
    if (timeoutChecker.isTimeout()) {
        throw new TimeoutException(""Total execution time for fetch exceeded
timeout parameter"");
    } else {
        Header locationHeader = method.getResponseHeader(""location"");
        HttpURL nextLocation = new HttpURL(locationHeader.getValue().toCharArray());
        pageContent = fetchGet(nextLocation.getEscapedURI(), addressHolder,
timeoutChecker, state);
    }
} else if (isSuccess(statusCode)) {
    // at least 4K buffers, might be as big as the webpage
    int responseSize = Math.max(getResponseSize(method), DEFAULT_RESPONSE_SIZE);
    InputStream response = method.getResponseBodyAsStream();
    ByteArrayOutputStream outstream = new ByteArrayOutputStream(responseSize);
    byte[] buffer = new byte[responseSize];
    int len;
    do {
        // ***TODO need to reset the SO_TIMEOUT to the remaining time
        len = response.read(buffer);
        outstream.write(buffer, 0, len);
    while ((len > 0) && !timeoutChecker.isTimeout());
    outstream.close();
    pageContent = EncodingUtil.getString(outstream.toByteArray(),
method.getResponseCharSet());
    response.close();
} else {
    ...
}"
0,"small improvements to DWPTThreadPool. While working on another issue I cleaned up DWTPThreadPool a little, fixed some naming issues and fixed some todos... patch is coming soon..."
0,EnwikiConentSource does not work with parallel tasks. 
0,"Split PrivilegeRegistry in a per-session manager instance and a repository level registry. in order to resolve the privilegeregistry related TODOs within jackrabbit-core, i would like to split off those 
methods from PrivilegeRegistry  that are used on a per-session level (including jcr-names) and add them
to a manager class that was present with each session context. consequently the responsibility of the
registry was then limited to read/build the privilege definitions and would be present on the repositorycontext
deprecating those methods that would be covered by the manager).
in addition the naming was then consistent with what we use to have for nodetypes and namespaces."
0,"IndexWriter should call MP.useCompoundFile and not LogMP.getUseCompoundFile. Spin off from here: http://www.gossamer-threads.com/lists/lucene/java-dev/112311.

I will attach a patch shortly that addresses the issue on trunk."
0,JCR2SPI: remove dependency to state-package within nodetype package. 
0,"LogMergePolicy should use the number of deleted docs when deciding which segments to merge. I found that IndexWriter.optimize(int) method does not pick up large segments with a lot of deletes even when most of the docs are deleted. And the existence of such segments affected the query performance significantly.

I created an index with 1 million docs, then went over all docs and updated a few thousand at a time.  I ran optimize(20) occasionally. What saw were large segments with most of docs deleted. Although these segments did not have valid docs they remained in the directory for a very long time until more segments with comparable or bigger sizes were created.

This is because LogMergePolicy.findMergeForOptimize uses the size of segments but does not take the number of deleted documents into consideration when it decides which segments to merge. So, a simple fix is to use the delete count to calibrate the segment size. I can create a patch for this.

"
0,"IndexReader subclasses must implement flex APIs. To be fixed only on trunk...

I made IndexReader's base flex APIs abstract, fixed all core/contrib/solr places that subclassed IR and didn't already implement flex (including contrib/memory, contrib/instantiated), and remove all the classes for the back-compat layer that emulated flex APIs on top of pre-flex APIs."
0,"Suggestion regarding NodeImpl and PropertyImpl in jackrabbit.core. Both NodeImpl and PropertyImpl contain in their respective setProperty/setValue (respectively) initial validation checks, that is indentical across the various
variants of each and could possibly in either case be place in a separate method.

in NodeImpl.setProperty (also: addMixin, removeMixin, orderBefore):

- sanityCheck
- is-parent-checked-out
- is-not-protected (missing for setProperty ???)
- is-parent-not-locked

in PropertyImpl.setValue:

- sanityCheck
- is-parent-checked-out
- is-not-protected 
- is-value-compatible-with-multivalue-definition (NOTE: check opposite for setting single
  or multiple values)
- is-parent-not-locked


Second, i'm never sure, in which case ChildNode, ChildProperty is prefered 
over Node/Property (as present in the jcr api)...

regards
angela

"
0,"Replace Maven POM templates with full POMs, and change documentation accordingly. The current Maven POM templates only contain dependency information, the bare bones necessary for uploading artifacts to the Maven repository.

The full Maven POMs in the attached patch include the information necessary to run a multi-module Maven build, in addition to serving the same purpose as the current POM templates.

Several dependencies are not available through public maven repositories.  A profile in the top-level POM can be activated to install these dependencies from the various {{lib/}} directories into your local repository.  From the top-level directory:

{code}
mvn -N -Pbootstrap install
{code}

Once these non-Maven dependencies have been installed, to run all Lucene/Solr tests via Maven's surefire plugin, and populate your local repository with all artifacts, from the top level directory, run:

{code}
mvn install
{code}

When one Lucene/Solr module depends on another, the dependency is declared on the *artifact(s)* produced by the other module and deposited in your local repository, rather than on the other module's un-jarred compiler output in the {{build/}} directory, so you must run {{mvn install}} on the other module before its changes are visible to the module that depends on it.

To create all the artifacts without running tests:

{code}
mvn -DskipTests install
{code}

I almost always include the {{clean}} phase when I do a build, e.g.:

{code}
mvn -DskipTests clean install
{code}
"
0,"[API Doc] Document exceptions thrown on execute methods. There should be more detailed documentation on HttpClient::executeMethod and
HttpMethod::execute about exceptions thrown in which cases."
0,Speed up NodeIndexer.isIndexed() check. The isIndexed() method is called for every value in a multi-valued property. This may be quite expensive when there are a lot of values.
0,Replace customized QueryParser.jjt. We should rather use the Lucene default and implement a  Jackrabbit  version that extends from it. This eases maintenance when moving from one Lucene version to another.
0,Support SortedSource in MultiDocValues. MultiDocValues doesn't support Sorted variant ie. SortedSource but throws UnsupportedOperationException. This forces users to work per segment. For consistency we should support sortedsource also if we wrap the DocValues in MDV.
0,"Documentation on SingleClientConnManager(SchemeRegistry schreg) constructor is wrong. Seems that the documentation for single-arg constructor SingleClientConnManager(SchemeRegistry schreg) is wrong.

Documentation says that incoming SchemeRegistry parameter can be null:
    schreg - the scheme registry, or null for the default registry

However, the constructor throws an exception in incoming schreg param is null:

    /**
     * Creates a new simple connection manager.
     *
     * @param params    the parameters for this manager
     * @param schreg    the scheme registry, or
     *                  <code>null</code> for the default registry
     */
    public SingleClientConnManager(HttpParams params,
                                   SchemeRegistry schreg) {
        if (schreg == null) {
            throw new IllegalArgumentException
                (""Scheme registry must not be null."");
        }


So this is likely a documentation bug..."
0,"Contrib/Module-uptodate assume name matches path and jar. With adding a new 'queries' module, I am trying to change the project name of contrib/queries to queries-contrib.  However currently the contrib-uptodate assumes that the name property is used in the path and in the jar name.

By using the name in the path, I must set the value to 'queries' (since the path is contrib/queries).  However because the project name is now queries-contrib, the actual jar file will be lucene-queries-contrib-${version}.jar, not lucene-queries-${version}.jar, as is expected.

Consequently I think we need to separate the path name from the jar name properties.  For simplicity I think adding a new jar-name property will suffice, which can be optional and if omitted, is filled in with the name property."
0,"Implement various ranking models as Similarities. With [LUCENE-3174|https://issues.apache.org/jira/browse/LUCENE-3174] done, we can finally work on implementing the standard ranking models. Currently DFR, BM25 and LM are on the menu.

Done:
 * {{EasyStats}}: contains all statistics that might be relevant for a ranking algorithm
 * {{EasySimilarity}}: the ancestor of all the other similarities. Hides the DocScorers and as much implementation detail as possible
 * _BM25_: the current ""mock"" implementation might be OK
 * _LM_
 * _DFR_
 * The so-called _Information-Based Models_

"
0,"Allow benchmark tasks from alternative packages. Relax current limitation of all tasks in same package - that of PerfTask.
Add a property ""alt.tasks.packages"" - its value are comma separated full package names.
If the task class is not found in the default package, an attempt is made to load it from the alternate packages specified in that property."
0,"Adding Event interface and isLocal(). when a repository cluster is used, it seems that a common problem many people have is to detect if an observation event is send because of changes on the local instance or a remote instance of the cluster.

This is especially important if you want to do post processing of data
based on observation (the post processing should only be done by one instance in the cluster).

A current solution is to cast the jcr event object to the EventImpl of jackrabbit core which is obviously not a nice solution :)

So what about adding an event interface to jackrabbit api which extends the jcr event interface and adds the isLocal() method? "
0,"UserManagement: membership cache default size too small. The membership cache that has been introduced in JCR-2703 is making use of an LRUMap to cache group memberships (authorizable nodeId -> group nodeIds). In environments where users belong to more than 100 groups, the cache quickly becomes ineffective due to the default maximum size of the LRUMap.

Once the cache limit is hit, the rather expensive Node#getWeakReferences API calls resulting in search queries are executed again, leading to quite noticeable performance drops. Thus I'd suggest to either make the membership cache configurable or introduce some logic to let the cache grow dynamically as needed (still having some kind of hard limit to avoid memory issues)."
0,"Cookie guide lists RFC 2965 as unsupported. HttpClient 3.1 added support for RFC 2965 (port-sensitive cookies), but the Cookie guide on the 3.x website still lists that as unsupported.
http://jakarta.apache.org/httpcomponents/httpclient-3.x/cookies.html

cheers,
  Roland
 "
0,"CachingSpanFilter synchronizing on a none final protected object. CachingSpanFilter and CachingWrapperFilter expose their internal cache via a protected member which is lazily instantiated in the getDocSetId method. The current code yields the chance to double instantiate the cache and internally synchronizes on a protected none final member. My first guess is that this member was exposed for testing purposes so it should rather be changed to package private. 

This patch breaks backwards compat while I guess the cleanup is kind of worth breaking it."
0,Query Stats should use the TimeSeries mechanism. Refactor the Query Stats to use TimeSeries for the average query duration.
0,"CLONE -QueryParser is not applicable for the arguments (String, String, Analyzer) error in results.jsp when executing search in the browser (demo from Lucene 2.0). When executing search in the browser (as described in demo3.html Lucene demo) I get error, because the demo uses the method (QueryParser with three arguments) which is deleted (it was deprecated).
I checked the demo from Lucene 1.4-final it with Lucene 1.4-final - it works, because those time the method was there.
But demo from Lucene 2.0 does not work with Lucene 2.0

The error stack is here:
TTP Status 500 -

type Exception report

message

description The server encountered an internal error () that prevented it from fulfilling this request.

exception

org.apache.jasper.JasperException: Unable to compile class for JSP

An error occurred at line: 60 in the jsp file: /results.jsp
Generated servlet error:
The method parse(String) in the type QueryParser is not applicable for the arguments (String, String, Analyzer)


org.apache.jasper.servlet.JspServletWrapper.handleJspException(JspServletWrapper.java:510)
org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:375)
org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:314)
org.apache.jasper.servlet.JspServlet.service(JspServlet.java:264)
javax.servlet.http.HttpServlet.service(HttpServlet.java:802)

root cause

org.apache.jasper.JasperException: Unable to compile class for JSP

An error occurred at line: 60 in the jsp file: /results.jsp
Generated servlet error:
The method parse(String) in the type QueryParser is not applicable for the arguments (String, String, Analyzer)


org.apache.jasper.compiler.DefaultErrorHandler.javacError(DefaultErrorHandler.java:84)
org.apache.jasper.compiler.ErrorDispatcher.javacError(ErrorDispatcher.java:328)
org.apache.jasper.compiler.JDTCompiler.generateClass(JDTCompiler.java:409)
org.apache.jasper.compiler.Compiler.compile(Compiler.java:297)
org.apache.jasper.compiler.Compiler.compile(Compiler.java:276)
org.apache.jasper.compiler.Compiler.compile(Compiler.java:264)
org.apache.jasper.JspCompilationContext.compile(JspCompilationContext.java:563)
org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:303)
org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:314)
org.apache.jasper.servlet.JspServlet.service(JspServlet.java:264)
javax.servlet.http.HttpServlet.service(HttpServlet.java:802)

note The full stack trace of the root cause is available in the Apache Tomcat/5.5.15 logs."
0,"Code cleanups for Java 1.5 and more.. I can't resist giving code a good cleansing when I start hacking.  Here's some simple things:
- Use character constants instead of string contstants
- Use java 1.5 style for loops
- Use StringBuilder where appropriate
- Fix javadocs
- switch somestring.equals("""") to .length() == 0
-  simplify some boolean expressions
- eliminate redundant initializers
- fix some html nits
- remove final keyword from static methods
"
0,"Multiple tests test for locking instead of versioning. Multiple tests claim to check whether versioning is supported, but in reality check for locking.  Patch included."
0,JSR 283 Node Identifier. 
0,"FormBodyPart code does not agree with ContentDescriptor Javadoc wrt nullability of mimeType and transferEncoding. The FormBodyPart does not agree with ContentDescriptor Javadoc wrt nullability of mimeType and transferEncoding:

The code in FormBodyPart explicitly allows mimeType and transferEncoding to be null, in which case the relevant header is not generated.
This is useful behaviour, as the headers are not necessaruly needed.

However the bahaviour disagrees with the Javadoc in the ContentDescriptor interface - null is not allowed.
Also, AbstractContentBody does not allow mime-type to be null."
0,"IndexWriter does not properly account for the RAM consumed by pending deletes. IndexWriter, with autoCommit false, is able to carry buffered deletes for quite some time before materializing them to docIDs (thus freeing up RAM used).

It's only on triggering a merge (or, commit/close) that the deletes are materialized and the RAM is freed.

I expect this in practice is a smallish amount of RAM, but we should still fix it.

I don't have a patch yet so if someone wants to grab this, feel free!!"
0,"don't download/extract 20,000 files when doing the build. When you build lucene, it downloads and extracts some data for contrib/benchmark, especially the 20,000+ files for the reuters corpus.
this is only needed for one test, and these 20,000 files drive IDEs and such crazy.
instead of doing this by default, we should only download/extract data if you specifically ask (like wikipedia, collation do, etc)

for the qualityrun test, instead use a linedoc formatted 587-line text file, similar to reuters.first20.lines.txt already used by benchmark.
"
0,"encoding of GermanAnalyzer.java and GermanStemmer.java isn't utf-8. For PyLucene, the gcj/swig - based python integration of java lucene, it would
be good if java source files didn't use encodings other than utf-8.
On Windows - and systems without iconv support in general - compiling code  
with gcj where the java source text is in another encoding than utf-8 is    
difficult if not impossible.

To change the encoding on these files:

 iconv -f iso-8859-1 -t utf-8 GermanAnalyzer.java > GermanAnalyzer.java.utf-8
 iconv -f iso-8859-1 -t utf-8 GermanStemmer.java > GermanStemmer.java.utf-8"
0,"Add a serializing content handler. Both JCR-1310 and JCR-1343 need XML serialization functionality and we've also previously (JCR-367, JCR-1086) implemented something similar.

It would be good to centralize such code, and so I'd like to use the already referenced code from Cocoon [1] as the basis for a SerializingContentHandler class in jackrabbit-jcr-commons.

[1] https://svn.apache.org/repos/asf/cocoon/trunk/core/cocoon-pipeline/cocoon-pipeline-impl/src/main/java/org/apache/cocoon/serialization/AbstractTextSerializer.java"
0,"DisjunctionMaxScorer allocates 2 arrays per scored doc. It has this:
{noformat}
  @Override
  public float score() throws IOException {
    int doc = subScorers[0].docID();
    float[] sum = { subScorers[0].score() }, max = { sum[0] };
    int size = numScorers;
    scoreAll(1, size, doc, sum, max);
    scoreAll(2, size, doc, sum, max);
    return max[0] + (sum[0] - max[0]) * tieBreakerMultiplier;
  }
{noformat}

They are thread-private arrays so possibly/likely JVM can optimize this case (allocate only on the stack) but still I think instead it should have private instance vars for the score/max."
0,"OracleBundlePersistenceManager needs special blob handling for JDBC drivers prior to oracle 10. the new oracle bundle persistence manager (see JCR-755) needs special blob handling for oracle jdbc drivers prior to version 10. since the pm works for newer versions, i suggest to add a separate pm eg: Oracle9PersistenceManager that contains this special blob handling."
0,"Javadoc in jackrabbit-jcr-rmi is missing an ending "">"" . The javadoc file /jackrabbit-jcr-rmi/src/main/javadoc/apache/rmi/observation/package.html is missing the final "">"" from the ending body tag.
"
0,DisjunctionMaxQuery -  Iterator code to  for ( A  a : container ) construct. For better readability  - converting the Iterable<T> to  for ( A  a : container ) constructs that is more intuitive to read. 
0,"Update overview example code. See http://lucene.apache.org/java/2_4_1/api/core/overview-summary.html - need to update for non-deprecated best-practices/recommended API usage.

Also, double-check that the demo app works as documented."
0,Fixes a handful of misspellings/mistakes in changes.txt. There are a handful of misspellings/mistakes in changes.txt. This patch fixes them. Avoided the one or two British to English conversions <g>
0,"add ability to not count sub-task doLogic increment to contri/benchmark. Sometimes, you want to run a sub-task like CloseIndex, and include the time it takes to run, but not include the count that it returns when reporting rec/s.

We could adopt this approach: if a task is preceded by a ""-"" character, then, do not count the value returned by doLogic.

See discussion leading to this here:

  http://www.gossamer-threads.com/lists/lucene/java-dev/57081"
0,"IndexableBinaryStringTools: convert arbitrary byte sequences into Strings that can be used as index terms, and vice versa. Provides support for converting byte sequences to Strings that can be used as index terms, and back again. The resulting Strings preserve the original byte sequences' sort order (assuming the bytes are interpreted as unsigned).

The Strings are constructed using a Base 8000h encoding of the original binary data - each char of an encoded String represents a 15-bit chunk from the byte sequence.  Base 8000h was chosen because it allows for all lower 15 bits of char to be used without restriction; the surrogate range [U+D800-U+DFFF] does not represent valid chars, and would require complicated handling to avoid them and allow use of char's high bit.

This class is intended to serve as a mechanism to allow CollationKeys to serve as index terms."
0,"BooleanWeight should size the weights Vector correctly. The weights field on BooleanWeight uses a Vector that will always be sized exactly the same as the outer class' clauses Vector, therefore can be sized correctly in the constructor. This is a trivial memory saving enhancement."
0,"Document the problem with MS impl of digest authentication with older JREs and stale connection check. It seems like digest authentication, when used with a username of the format:
domain\username fails in httpclient-3.0-rc2.

I did confirm that digest authentication does work by connecting to a local
Apache HTTP 2.0 server, using just a username and password (no domain\username).
However, it does not support the MD5-sess algorithm, and the server I am getting
the failure from is using MD5-sess. 

It may turn out the username is not causing the problem, but one thing is
consistent--I can connect to the site in the logs below using httpclient-2.0.2.
It fails when I use identical Java code, with the addition of AuthScope, when
using httpclient-3.0-rc2. I will also attach Java code that reproduces the problem.

The following are wire and debug logs from httpclient-2.0.2 and
httpclient-3.0-rc2 respectively. The first one connects and gets an 'HTTP 200'
response. The second one, using 3.0-rc2 fails with an 'HTTP 401'.

LOGS
===============================================================
commons-httpclient-2.0.2 (works):
2005/05/13 11:05:15:185 EDT [DEBUG] HttpClient - Java version: 1.3.1
2005/05/13 11:05:15:185 EDT [DEBUG] HttpClient - Java vendor: IBM Corporation
2005/05/13 11:05:15:185 EDT [DEBUG] HttpClient - Java class path: 
2005/05/13 11:05:15:205 EDT [DEBUG] HttpClient - Operating system name: Windows XP
2005/05/13 11:05:15:205 EDT [DEBUG] HttpClient - Operating system architecture: x86
2005/05/13 11:05:15:205 EDT [DEBUG] HttpClient - Operating system version: 5.1
2005/05/13 11:05:15:205 EDT [DEBUG] HttpClient - SUN 1.2: SUN (DSA key/parameter
generation; DSA signing; SHA-1, MD5 digests; SecureRandom; X.509 certificates;
JKS keystore)
2005/05/13 11:05:15:205 EDT [DEBUG] HttpClient - SunJCE 1.22: SunJCE Provider
(implements DES, Triple DES, Blowfish, PBE, Diffie-Hellman, HMAC-MD5, HMAC-SHA1)
2005/05/13 11:05:15:205 EDT [DEBUG] HttpClient - SunJSSE 1.0303: Sun JSSE
provider(implements RSA Signatures, PKCS12, SunX509 key/trust factories, SSLv3,
TLSv1)
2005/05/13 11:05:20:893 EDT [DEBUG] HttpConnection - HttpConnection.setSoTimeout(0)
2005/05/13 11:05:20:893 EDT [DEBUG] HttpMethodBase - Execute loop try 1
2005/05/13 11:05:20:913 EDT [DEBUG] header - >> ""GET
/CustomerData-30/CustomerDataService.asmx HTTP/1.1[\r][\n]""
2005/05/13 11:05:20:913 EDT [DEBUG] HttpMethodBase - Adding Host request header
2005/05/13 11:05:20:913 EDT [DEBUG] header - >> ""User-Agent: Jakarta
Commons-HttpClient/2.0.2[\r][\n]""
2005/05/13 11:05:20:913 EDT [DEBUG] header - >> ""Host:
mappoint-css.partners.extranet.microsoft.com[\r][\n]""
2005/05/13 11:05:21:173 EDT [DEBUG] header - >> ""[\r][\n]""
2005/05/13 11:05:21:273 EDT [DEBUG] header - << ""HTTP/1.1 401 Unauthorized[\r][\n]""
2005/05/13 11:05:21:273 EDT [DEBUG] header - << ""Content-Length: 1656[\r][\n]""
2005/05/13 11:05:21:283 EDT [DEBUG] header - << ""Content-Type: text/html[\r][\n]""
2005/05/13 11:05:21:283 EDT [DEBUG] header - << ""Server: Microsoft-IIS/6.0[\r][\n]""
2005/05/13 11:05:21:283 EDT [DEBUG] header - << ""WWW-Authenticate: Digest
qop=""auth"",algorithm=MD5-sess,nonce=""b2a83a38cd57c501af3ad2c91f189512060524424ffc2b818c9920db15cd247a9d47cf5a789d63c6"",opaque=""1704373a505e74c4ec692978e5c1a539"",charset=utf-8,realm=""Digest""[\r][\n]""
2005/05/13 11:05:21:283 EDT [DEBUG] header - << ""X-Powered-By: ASP.NET[\r][\n]""
2005/05/13 11:05:21:283 EDT [DEBUG] header - << ""Date: Fri, 13 May 2005 15:05:37
GMT[\r][\n]""
2005/05/13 11:05:21:283 EDT [DEBUG] HttpMethodBase - Authorization required
2005/05/13 11:05:21:283 EDT [DEBUG] HttpAuthenticator - Authenticating with the
'Digest' authentication realm at mappoint-css.partners.extranet.microsoft.com
2005/05/13 11:05:21:283 EDT [DEBUG] DigestScheme - Using qop method auth
2005/05/13 11:05:21:283 EDT [DEBUG] HttpMethodBase - HttpMethodBase.execute():
Server demanded authentication credentials, will try again.
2005/05/13 11:05:21:293 EDT [DEBUG] HttpMethodBase - Resorting to protocol
version default close connection policy
2005/05/13 11:05:21:293 EDT [DEBUG] HttpMethodBase - Should NOT close
connection, using HTTP/1.1.
2005/05/13 11:05:21:293 EDT [DEBUG] HttpMethodBase - Execute loop try 2
2005/05/13 11:05:21:293 EDT [DEBUG] header - >> ""GET
/CustomerData-30/CustomerDataService.asmx HTTP/1.1[\r][\n]""
2005/05/13 11:05:21:293 EDT [DEBUG] HttpMethodBase - Request to add Host header
ignored: header already added
2005/05/13 11:05:21:293 EDT [DEBUG] header - >> ""User-Agent: Jakarta
Commons-HttpClient/2.0.2[\r][\n]""
2005/05/13 11:05:21:293 EDT [DEBUG] header - >> ""Host:
mappoint-css.partners.extranet.microsoft.com[\r][\n]""
2005/05/13 11:05:21:293 EDT [DEBUG] header - >> ""Authorization: Digest
username=""domain\user"", realm=""Digest"",
nonce=""b2a83a38cd57c501af3ad2c91f189512060524424ffc2b818c9920db15cd247a9d47cf5a789d63c6"",
uri=""/CustomerData-30/CustomerDataService.asmx"", qop=""auth"",
algorithm=""MD5-sess"", nc=00000001, cnonce=""393a8abf65cd20f85ffdf46a9273b28b"",
response=""854bf54261112caf2e86652276cb2ce6"",
opaque=""1704373a505e74c4ec692978e5c1a539""[\r][\n]""
2005/05/13 11:05:21:293 EDT [DEBUG] header - >> ""[\r][\n]""
2005/05/13 11:05:21:994 EDT [DEBUG] header - << ""HTTP/1.1 200 OK[\r][\n]""HTTP
result: 200

===========================================================

commons-httpclient-3.0-rc2 (does not work):
2005/05/13 11:16:54:881 EDT [DEBUG] DefaultHttpParams - -Set parameter
http.useragent = Jakarta Commons-HttpClient/3.0-rc2
2005/05/13 11:16:54:881 EDT [DEBUG] DefaultHttpParams - -Set parameter
http.protocol.version = HTTP/1.1
2005/05/13 11:16:54:881 EDT [DEBUG] DefaultHttpParams - -Set parameter
http.connection-manager.class = class
org.apache.commons.httpclient.SimpleHttpConnectionManager
2005/05/13 11:16:54:891 EDT [DEBUG] DefaultHttpParams - -Set parameter
http.protocol.cookie-policy = rfc2109
2005/05/13 11:16:54:891 EDT [DEBUG] DefaultHttpParams - -Set parameter
http.protocol.element-charset = US-ASCII
2005/05/13 11:16:54:891 EDT [DEBUG] DefaultHttpParams - -Set parameter
http.protocol.content-charset = ISO-8859-1
2005/05/13 11:16:54:891 EDT [DEBUG] DefaultHttpParams - -Set parameter
http.method.retry-handler =
org.apache.commons.httpclient.DefaultHttpMethodRetryHandler@5048d78c
2005/05/13 11:16:54:891 EDT [DEBUG] DefaultHttpParams - -Set parameter
http.dateparser.patterns = [EEE, dd MMM yyyy HH:mm:ss zzz, EEEE, dd-MMM-yy
HH:mm:ss zzz, EEE MMM d HH:mm:ss yyyy, EEE, dd-MMM-yyyy HH:mm:ss z, EEE,
dd-MMM-yyyy HH-mm-ss z, EEE, dd MMM yy HH:mm:ss z, EEE dd-MMM-yyyy HH:mm:ss z,
EEE dd MMM yyyy HH:mm:ss z, EEE dd-MMM-yyyy HH-mm-ss z, EEE dd-MMM-yy HH:mm:ss
z, EEE dd MMM yy HH:mm:ss z, EEE,dd-MMM-yy HH:mm:ss z, EEE,dd-MMM-yyyy HH:mm:ss
z, EEE, dd-MM-yyyy HH:mm:ss z]
2005/05/13 11:16:54:911 EDT [DEBUG] HttpClient - -Java version: 1.3.1
2005/05/13 11:16:54:911 EDT [DEBUG] HttpClient - -Java vendor: IBM Corporation
2005/05/13 11:16:54:911 EDT [DEBUG] HttpClient - -Java class path: 
2005/05/13 11:16:54:941 EDT [DEBUG] HttpClient - -Operating system name: Windows XP
2005/05/13 11:16:54:941 EDT [DEBUG] HttpClient - -Operating system architecture: x86
2005/05/13 11:16:54:941 EDT [DEBUG] HttpClient - -Operating system version: 5.1
2005/05/13 11:16:54:941 EDT [DEBUG] HttpClient - -SUN 1.2: SUN (DSA
key/parameter generation; DSA signing; SHA-1, MD5 digests; SecureRandom; X.509
certificates; JKS keystore)
2005/05/13 11:16:54:951 EDT [DEBUG] HttpClient - -SunJCE 1.22: SunJCE Provider
(implements DES, Triple DES, Blowfish, PBE, Diffie-Hellman, HMAC-MD5, HMAC-SHA1)
2005/05/13 11:16:54:951 EDT [DEBUG] HttpClient - -SunJSSE 1.0303: Sun JSSE
provider(implements RSA Signatures, PKCS12, SunX509 key/trust factories, SSLv3,
TLSv1)
2005/05/13 11:16:54:961 EDT [DEBUG] HttpConnection - -Open connection to
mappoint-css.partners.extranet.microsoft.com:443
2005/05/13 11:17:00:629 EDT [DEBUG] header - ->> ""GET
/CustomerData-30/CustomerDataService.asmx HTTP/1.1[\r][\n]""
2005/05/13 11:17:00:629 EDT [DEBUG] HttpMethodBase - -Adding Host request header
2005/05/13 11:17:00:639 EDT [DEBUG] header - ->> ""User-Agent: Jakarta
Commons-HttpClient/3.0-rc2[\r][\n]""
2005/05/13 11:17:00:639 EDT [DEBUG] header - ->> ""Host:
mappoint-css.partners.extranet.microsoft.com[\r][\n]""
2005/05/13 11:17:00:639 EDT [DEBUG] header - ->> ""[\r][\n]""
2005/05/13 11:17:00:989 EDT [DEBUG] header - -<< ""HTTP/1.1 401 Unauthorized[\r][\n]""
2005/05/13 11:17:00:999 EDT [DEBUG] header - -<< ""Content-Length: 1656[\r][\n]""
2005/05/13 11:17:00:999 EDT [DEBUG] header - -<< ""Content-Type: text/html[\r][\n]""
2005/05/13 11:17:00:999 EDT [DEBUG] header - -<< ""Server: Microsoft-IIS/6.0[\r][\n]""
2005/05/13 11:17:00:999 EDT [DEBUG] header - -<< ""WWW-Authenticate: Digest
qop=""auth"",algorithm=MD5-sess,nonce=""c66759cace57c5016cf5645c6dee5b649ed29067f652939d6aaf7239310bb333eeb0153783ae445f"",opaque=""e7e259c137b65766c971d6cfc4115789"",charset=utf-8,realm=""Digest""[\r][\n]""
2005/05/13 11:17:00:999 EDT [DEBUG] header - -<< ""X-Powered-By: ASP.NET[\r][\n]""
2005/05/13 11:17:00:999 EDT [DEBUG] header - -<< ""Date: Fri, 13 May 2005
15:16:51 GMT[\r][\n]""
2005/05/13 11:17:00:999 EDT [DEBUG] HttpMethodDirector - -Authorization required
2005/05/13 11:17:01:009 EDT [DEBUG] AuthChallengeProcessor - -Supported
authentication schemes in the order of preference: [ntlm, digest, basic]
2005/05/13 11:17:01:009 EDT [DEBUG] AuthChallengeProcessor - -Challenge for ntlm
authentication scheme not available
2005/05/13 11:17:01:009 EDT [INFO] AuthChallengeProcessor - -digest
authentication scheme selected
2005/05/13 11:17:01:009 EDT [DEBUG] AuthChallengeProcessor - -Using
authentication scheme: digest
2005/05/13 11:17:01:009 EDT [DEBUG] AuthChallengeProcessor - -Authorization
challenge processed
2005/05/13 11:17:01:009 EDT [DEBUG] HttpMethodDirector - -Authentication scope:
DIGEST 'Digest'@mappoint-css.partners.extranet.microsoft.com:443
2005/05/13 11:17:01:009 EDT [DEBUG] HttpMethodDirector - -Retry authentication
2005/05/13 11:17:01:019 EDT [DEBUG] HttpMethodBase - -Resorting to protocol
version default close connection policy
2005/05/13 11:17:01:019 EDT [DEBUG] HttpMethodBase - -Should NOT close
connection, using HTTP/1.1
2005/05/13 11:17:01:019 EDT [DEBUG] HttpConnection - -Connection is locked. 
Call to releaseConnection() ignored.
2005/05/13 11:17:01:019 EDT [DEBUG] HttpMethodDirector - -Authenticating with
DIGEST 'Digest'@mappoint-css.partners.extranet.microsoft.com:443
2005/05/13 11:17:01:019 EDT [DEBUG] HttpMethodParams - -Credential charset not
configured, using HTTP element charset
2005/05/13 11:17:01:019 EDT [DEBUG] DigestScheme - -Using qop method auth
2005/05/13 11:17:01:019 EDT [DEBUG] HttpConnection - -Connection is stale,
closing...
2005/05/13 11:17:01:019 EDT [DEBUG] HttpConnection - -Open connection to
mappoint-css.partners.extranet.microsoft.com:443
2005/05/13 11:17:01:110 EDT [DEBUG] header - ->> ""GET
/CustomerData-30/CustomerDataService.asmx HTTP/1.1[\r][\n]""
2005/05/13 11:17:01:110 EDT [DEBUG] HttpMethodBase - -Adding Host request header
2005/05/13 11:17:01:110 EDT [DEBUG] header - ->> ""User-Agent: Jakarta
Commons-HttpClient/3.0-rc2[\r][\n]""
2005/05/13 11:17:01:110 EDT [DEBUG] header - ->> ""Authorization: Digest
username=""domain\user"", realm=""Digest"",
nonce=""c66759cace57c5016cf5645c6dee5b649ed29067f652939d6aaf7239310bb333eeb0153783ae445f"",
uri=""/CustomerData-30/CustomerDataService.asmx"",
response=""9cab4fcdb2d09f57523aec80d7b51e95"", qop=""auth"", nc=00000001,
cnonce=""b27507ee79c880b2bb565d363598ce07"", algorithm=""MD5-sess"",
opaque=""e7e259c137b65766c971d6cfc4115789""[\r][\n]""
2005/05/13 11:17:01:120 EDT [DEBUG] header - ->> ""Host:
mappoint-css.partners.extranet.microsoft.com[\r][\n]""
2005/05/13 11:17:01:120 EDT [DEBUG] header - ->> ""[\r][\n]""
2005/05/13 11:17:01:540 EDT [DEBUG] header - -<< ""HTTP/1.1 401 Unauthorized[\r][\n]""
2005/05/13 11:17:01:540 EDT [DEBUG] header - -<< ""Content-Length: 1539[\r][\n]""
2005/05/13 11:17:01:540 EDT [DEBUG] header - -<< ""Content-Type: text/html[\r][\n]""
2005/05/13 11:17:01:550 EDT [DEBUG] header - -<< ""Server: Microsoft-IIS/6.0[\r][\n]""
2005/05/13 11:17:01:550 EDT [DEBUG] header - -<< ""WWW-Authenticate: Digest
qop=""auth"",algorithm=MD5-sess,nonce=""3296b0dace57c5012fe314f8c6f8cafd10abc7a61c09484b2be5c7ef19ecb3c080da1f82c3f5a532"",opaque=""87578a7f0871280654aed868cb9497fb"",charset=utf-8,realm=""Digest""[\r][\n]""
2005/05/13 11:17:01:550 EDT [DEBUG] header - -<< ""X-Powered-By: ASP.NET[\r][\n]""
2005/05/13 11:17:01:550 EDT [DEBUG] header - -<< ""Date: Fri, 13 May 2005
15:17:19 GMT[\r][\n]""
2005/05/13 11:17:01:550 EDT [DEBUG] HttpMethodDirector - -Authorization required
2005/05/13 11:17:01:550 EDT [DEBUG] AuthChallengeProcessor - -Using
authentication scheme: digest
2005/05/13 11:17:01:550 EDT [DEBUG] AuthChallengeProcessor - -Authorization
challenge processed
2005/05/13 11:17:01:550 EDT [DEBUG] HttpMethodDirector - -Authentication scope:
DIGEST 'Digest'@mappoint-css.partners.extranet.microsoft.com:443
2005/05/13 11:17:01:550 EDT [DEBUG] HttpMethodDirector - -Credentials required
HTTP result: 401
2005/05/13 11:17:01:550 EDT [DEBUG] HttpMethodDirector - -Credentials provider
not available
2005/05/13 11:17:01:580 EDT [INFO] HttpMethodDirector - -Failure authenticating
with DIGEST 'Digest'@mappoint-css.partners.extranet.microsoft.com:443
2005/05/13 11:17:01:580 EDT [DEBUG] HttpMethodBase - -Buffering response body
2005/05/13 11:17:01:580 EDT [DEBUG] HttpMethodBase - -Resorting to protocol
version default close connection policy
2005/05/13 11:17:01:580 EDT [DEBUG] HttpMethodBase - -Should NOT close
connection, using HTTP/1.1
2005/05/13 11:17:01:580 EDT [DEBUG] HttpConnection - -Releasing connection back
to connection manager."
0,"Support for single-workspace repositories. There should be a way to configure the test cases in a way such that NodeTest.java can pass although the repository implementation does not support multiple workspaces.

The cleanest approach probably would be to allow javax.jcr.tck.workspacename to stay undefined, and to skip the tests in that case. Alternatives would be a special name indicating lack of support for other workspaces, or an additional config variable.

"
0,"Block tree terms dict & index. Our default terms index today breaks terms into blocks of fixed size
(ie, every 32 terms is a new block), and then we build an index on top
of that (holding the start term for each block).

But, it should be better to instead break terms according to how they
share prefixes.  This results in variable sized blocks, but means
within each block we maximize the shared prefix and minimize the
resulting terms index.  It should also be a speedup for terms dict
intensive queries because the terms index becomes a ""true"" prefix
trie, and can be used to fast-fail on term lookup (ie returning
NOT_FOUND without having to seek/scan a terms block).

Having a true prefix trie should also enable much faster intersection
with automaton (but this will be a new issue).

I've made an initial impl for this (called
BlockTreeTermsWriter/Reader).  It's still a work in progress... lots
of nocommits, and hairy code, but tests pass (at least once!).

I made two new codecs, temporarily called StandardTree, PulsingTree,
that are just like their counterparts but use this new terms dict.

I added a new ""exactOnly"" boolean to TermsEnum.seek.  If that's true
and the term is NOT_FOUND, we will (quickly) return NOT_FOUND and the
enum is unpositioned (ie you should not call next(), docs(), etc.).

In this approach the index and dict are tightly connected, so it does
not support a pluggable index impl like BlockTermsWriter/Reader.
Blocks are stored on certain nodes of the prefix trie, and can contain
both terms and pointers to sub-blocks (ie, if the block is not a leaf
block).  So there are two trees, tied to one another -- the index
trie, and the blocks.  Only certain nodes in the trie map to a block
in the block tree.

I think this algorithm is similar to burst tries
(http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.18.3499),
except it allows terms to be stored on inner blocks (not just leaf
blocks).  This is important for Lucene because an [accidental]
""adversary"" could produce a terms dict with way too many blocks (way
too much RAM used by the terms index).  Still, with my current patch,
an adversary can produce too-big blocks... which we may need to fix,
by letting the terms index not be a true prefix trie on it's leaf
edges.

Exactly how the blocks are picked can be factored out as its own
policy (but I haven't done that yet).  Then, burst trie is one policy,
my current approach is another, etc.  The policy can be tuned to
the terms' expected distribution, eg if it's a primary key field and
you only use base 10 for each character then you want block sizes of
size 10.  This can make a sizable difference on lookup cost.

I modified the FST Builder to allow for a ""plugin"" that freezes the
""tail"" (changed suffix) of each added term, because I use this to find
the blocks.
"
0,"Calls to SegmentInfos.message should be wrapped w/ infoStream != null checks. To avoid the expensive message creation (which involves the '+' operator on strings, calls to message should be wrapped w/ infoStream != null check, rather than inside message(). I'll attach a patch which does that."
0,DescendantSelfAxisQuery creates too many object instances. In DescendantSelfAxisQuery.DescendantSelfAxisScorer.isValid() there is an ArrayList and an Integer instance created on every call. Since this method gets called really often during queries the object creation/gc affects performance.
0,"Improve tests to work easier from IDEs. As reported by Paolo Castagna on the mailing lists, some tests fail when you run them from eclipse.

Some of the failures he reports are actually code problems such as base test classes not being 
abstract when they should be... we should fix things like that."
0,"Fix remaining localization test failures in lucene. see also LUCENE-1836 and LUCENE-1846

all tests should pass under different locales.
the fix is to run 'ant test' under different locales, look and fix problems, and use the LocalizedTestCase from LUCENE-1836 to keep them from coming back.

the same approach as LUCENE-1836 fixes the core queryparser, but I am running ant test under a few locales to look for more problems.
"
0,"JcrUtils.getRepository(...) for simple repository access. As discussed on the mailing list, it would be nice to have a trivially simple way (one line of code) to connect to a repository. The RepositoryFactory interface in JCR 2.0 defines a way for clients to get a repository reference without a direct implementation dependency, but a client still needs extra code to handle the Service Provider lookup and the iteration through all the available repository factories.

To simplify client code I'd like to introduce a JcrUtils.getRepository(Map<String, String>) method that takes care of the tasks mentioned above:

    Map<String, String> parameters = ...; // repository settings
    Repository repository = JcrUtils.getRepository(parameters);

As a further simplification, I'd also like to introduce a JcrUtils.getRepository(String) method that builds the parameter map based on a given ""repository URI"".

    Repository repository = JcrUtils.getRepository(""file:///path/to/repository"");

    Repository repository = JcrUtils.getRepository(""http://localhost:8080/server"");

The set of supported URI types is still to be defined."
0,"TCK: AbstractJCRTest fails if setUp/tearDown cannot remove children of test node. If the test node exists, the setUp and tearDown methods remove all its child nodes.  In some repositories these child nodes may be mandatory or protected, causing test setup/teardown to fail.

Proposal: tolerate exceptions thrown in removing a child node in test setup/teardown.

--- ../AbstractJCRTest.java     (revision 422074)
+++ ../AbstractJCRTest.java     (working copy)
@@ -344,7 +344,11 @@
                 // clean test root
                 testRootNode = root.getNode(testPath);
                 for (NodeIterator children = testRootNode.getNodes(); children.hasNext();) {
-                    children.nextNode().remove();
+                    try {
+                      children.nextNode().remove();
+                    } catch (RepositoryException e) {
+                      // consume
+                    }
                 }
             } else {
                 // create nodes to testPath
@@ -375,7 +379,11 @@
                         // clean test root
                         testRootNode = root.getNode(testPath);
                         for (NodeIterator children = testRootNode.getNodes(); children.hasNext();) {
-                            children.nextNode().remove();
+                            try {
+                              children.nextNode().remove();
+                            } catch (RepositoryException e) {
+                              // consume
+                            }
                         }
                         root.save();
                     }
"
0,"Checksum Wrong for HttpComponent project pom v4.1 on central. As evidenced on the log here: http://vmgump.apache.org/gump/public/httpcomponents/httpcomponents-core/gump_work/build_httpcomponents_httpcomponents-core.html

The checksum in central for httpcomponents-project-4.1.pom is incorrect in maven central.

---------------------------8<--------------------------------

Downloading: http://localhost:8192/maven2/org/apache/httpcomponents/project/4.1/project-4.1.pom

[WARNING] *** CHECKSUM FAILED - Checksum failed on download: local = 'b63ff67e6ffc1940041319e0e06d7c6b1d671fd2'; remote = '8edff11652ca51b9d110ebfb321daac24f031c07' - RETRYING
Downloading: http://localhost:8192/maven2/org/apache/httpcomponents/project/4.1/project-4.1.pom

[WARNING] *** CHECKSUM FAILED - Checksum failed on download: local = 'b63ff67e6ffc1940041319e0e06d7c6b1d671fd2'; remote = '8edff11652ca51b9d110ebfb321daac24f031c07' - IGNORING
This pom appears to be a dependency for httpcomponents 4.0.3

---------------------------8<--------------------------------

This checksum failure causes configurations that reject such artifacts (such as many maven proxy configurations) to result in build failures due to unsatisfied dependencies. 

"
0,"Provide utility for handling large number of child nodes/properties. Jackrabbit does not cope well with 'flat' hierarchies. That is with hierarchies where a node has many child nodes and/or properties. The current recommendation for such situations is to manually add intermediate nodes. 

It would be nice to have an utility which adds/removes intermediate nodes as needed and expose a 'flat' view to users. Such an utility should:

- expose a large number of nodes/properties as sequence
- parametrize the order of how nodes/properties appear in the sequence
- provide methods to lookup/add/remove nodes/properties by key 
- organize the node/properties in the underlying JCR hierarchy in a way which is both efficient for above operations and easily understandable to users looking at the hierarchy. "
0,"comparator API for segment versions. See LUCENE-3012 for an example.

Things get ugly if you want to use SegmentInfo.getVersion()

For example, what if we committed my patch, release 3.2, but later released 3.1.1 (will ""3.1.1"" this be whats written and returned by this function?)
Then suddenly we broke the index format because we are using Strings here without a reasonable comparator API.

In this case one should be able to compute if the version is < 3.2 safely.

If we don't do this, and we rely upon this version information internally in lucene, I think we are going to break something."
0,"dependencies for route planner implementations. The implementations of HttpRoutePlanner that we have depend on the ConnectionManager, but use it only to look up the SchemeRegistry. Consider to depend only on the SchemeRegistry.

"
0,"A number of documentation fixes for the search package summary. Improves readability and clarity, corrects some basic English, makes some example text even more clear, and repairs typos."
0,JSR 283: EventJournal. Implement the event journal as specified in JSR 283.
0,Dont use nt:frozenNode to create nodes. Some test cases may end up trying to create a new node of type nt:frozenNode which fails.
0,"TokenStream/Tokenizer/TokenFilter/Token javadoc improvements. Some of the javadoc for the new TokenStream/Tokenizer/TokenFilter/Token APIs had javadoc errors.  To the best of my knowledge, I corrected these and refined the copy a bit."
0,"Enable IndexWriter to open an arbitrary commit point. With a 2-phase commit involving multiple resources, each resource
first does its prepareCommit and then if all are successful they each
commit.  If an exception or timeout/power loss is hit in any of the
resources during prepareCommit or commit, all of the resources must
then rollback.

But, because IndexWriter always opens the most recent commit, getting
Lucene to rollback after commit() has been called is not easy, unless
you make Lucene the last resource to commit.  A simple workaround is
to simply remove the segments_N files of the newer commits but that's
sort of a hassle.

To fix this, we just need to add a ctor to IndexWriter that takes an
IndexCommit.  We recently added this for IndexReader (LUCENE-1311) as
well.  This ctor is definitely an ""expert"" method, and only makes
sense if you have a custom DeletionPolicy that preserves more than
just the most recent commit.
"
0,"Tiered flushing of DWPTs by RAM with low/high water marks. Now that we have DocumentsWriterPerThreads we need to track total consumed RAM across all DWPTs.

A flushing strategy idea that was discussed in LUCENE-2324 was to use a tiered approach:  
- Flush the first DWPT at a low water mark (e.g. at 90% of allowed RAM)
- Flush all DWPTs at a high water mark (e.g. at 110%)
- Use linear steps in between high and low watermark:  E.g. when 5 DWPTs are used, flush at 90%, 95%, 100%, 105% and 110%.

Should we allow the user to configure the low and high water mark values explicitly using total values (e.g. low water mark at 120MB, high water mark at 140MB)?  Or shall we keep for simplicity the single setRAMBufferSizeMB() config method and use something like 90% and 110% for the water marks?"
0,JSR 283: Locking. 
0,"Avoid ${project.version} in dependencies. Another one for Jackrabbit 1.5, we should avoid using ${project.version} for our dependencies and override the versions of any transitive dependencies that use ${project.version} (notably the Jetty dependencies in jackrabbit-standalone) to avoid problems with Maven < 2.0.9 caused by MNG-2339 [1].

[1] http://jira.codehaus.org/browse/MNG-2339
"
0,"DefaultSkipListReader should not be public. There's no need for org.apache.lucene.index.DefaultSkipListReader to be public.
This class hasn't been released yet, so we should fix this before 2.2."
0,"NodeStateMerger.merge should abort if the primary type of the 2 states to be compare are not the same. the NodeStateMerger#merge currently aborts if the mixin types of the passed state and its overlayed state are not equal.
as of jsr 283 not the only the mixin types but also the primary type of a node can be modified.

for consistency reasons NodeStateMerger#merge should abort and return false if the primary types are not the same."
0,"AbstractRecord does inefficient List.indexOf(). AbstractRecord keeps a list of already used UUIDs and references
them by index when used again in a record. Using a List does not
scale well, when the record grows larger. e.g. a transaction of
10k nodes takes more than a minute on my machine when the journal
is enabled. Most of the time is spent doing List.indexOf() in
AbstractRecord.getOrCreateIndex()."
0,"equals and hashCode implementation in org.apache.lucene.search.* package. I would like to talk about the implementation of equals and hashCode method  in org.apache.lucene.search.* package. 

Example One:

org.apache.lucene.search.spans.SpanTermQuery (Super Class)
	<- org.apache.lucene.search.payloads.BoostingTermQuery (Sub Class)

Observation:

* BoostingTermQuery defines equals but inherits hashCode from SpanTermQuery. Definition of equals is a code clone of SpanTermQuery with a change in class name. 

Intention:

I believe the intention of equals redefinition in BoostingTermQuery is not to make the objects of SpanTermQuery and BoostingTermQuery comparable. ie. spanTermQuery.equals(boostingTermQuery) == false && boostingTermQuery.equals(spanTermQuery) == false.


Problem:

With current implementation, the intention might not be respected as a result of symmetric property violation of equals contract i.e.
spanTermQuery.equals(boostingTermQuery) == true (can be) && boostingTermQuery.equals(spanTermQuery) == false. (always)
(Note: Provided their state variables are equal)

Solution:

Change implementation of equals in SpanTermQuery from:

{code:title=SpanTermQuery.java|borderStyle=solid}
  public boolean equals(Object o) {
    if (!(o instanceof SpanTermQuery))
      return false;
    SpanTermQuery other = (SpanTermQuery)o;
    return (this.getBoost() == other.getBoost())
      && this.term.equals(other.term);
  }
{code}

To:
{code:title=SpanTermQuery.java|borderStyle=solid}
  public boolean equals(Object o) {
  	if(o == this) return true;
  	if(o == null || o.getClass() != this.getClass()) return false;
//    if (!(o instanceof SpanTermQuery))
//      return false;
    SpanTermQuery other = (SpanTermQuery)o;
    return (this.getBoost() == other.getBoost())
      && this.term.equals(other.term);
  }
{code}

Advantage:

* BoostingTermQuery.equals and BoostingTermQuery.hashCode is not needed while still preserving the same intention as before.
 
* Any further subclassing that does not add new state variables in the extended classes of SpanTermQuery, does not have to redefine equals and hashCode. 

* Even if a new state variable is added in a subclass, the symmetric property of equals contract will still be respected irrespective of implementation (i.e. instanceof / getClass) of equals and hashCode in the subclasses.


Example Two:


org.apache.lucene.search.CachingWrapperFilter (Super Class)
	<- org.apache.lucene.search.CachingWrapperFilterHelper (Sub Class)

Observation:
Same as Example One.

Problem:
Same as Example one.

Solution:
Change equals in CachingWrapperFilter from:
{code:title=CachingWrapperFilter.java|borderStyle=solid}
  public boolean equals(Object o) {
    if (!(o instanceof CachingWrapperFilter)) return false;
    return this.filter.equals(((CachingWrapperFilter)o).filter);
  }
{code}

To:
{code:title=CachingWrapperFilter.java|borderStyle=solid}
  public boolean equals(Object o) {
//    if (!(o instanceof CachingWrapperFilter)) return false;
    if(o == this) return true;
    if(o == null || o.getClass() != this.getClass()) return false;
    return this.filter.equals(((CachingWrapperFilter)o).filter);
  }
{code}

Advantage:
Same as Example One. Here, CachingWrapperFilterHelper.equals and CachingWrapperFilterHelper.hashCode is not needed.


Example Three:

org.apache.lucene.search.MultiTermQuery (Abstract Parent)
	<- org.apache.lucene.search.FuzzyQuery (Concrete Sub)
	<- org.apache.lucene.search.WildcardQuery (Concrete Sub)

Observation (Not a problem):

* WildcardQuery defines equals but inherits hashCode from MultiTermQuery.
Definition of equals contains just super.equals invocation. 

* FuzzyQuery has few state variables added that are referenced in its equals and hashCode.
Intention:

I believe the intention here is not to make objects of FuzzyQuery and WildcardQuery comparable. ie. fuzzyQuery.equals(wildCardQuery) == false && wildCardQuery.equals(fuzzyQuery) == false.

Proposed Implementation:
How about changing the implementation of equals in MultiTermQuery from:

{code:title=MultiTermQuery.java|borderStyle=solid}
    public boolean equals(Object o) {
      if (this == o) return true;
      if (!(o instanceof MultiTermQuery)) return false;

      final MultiTermQuery multiTermQuery = (MultiTermQuery) o;

      if (!term.equals(multiTermQuery.term)) return false;

      return getBoost() == multiTermQuery.getBoost();
    }
{code}

To:
{code:title=MultiTermQuery.java|borderStyle=solid}
    public boolean equals(Object o) {
      if (this == o) return true;
//      if (!(o instanceof MultiTermQuery)) return false;
      if(o == null || o.getClass() != this.getClass()) return false;

      final MultiTermQuery multiTermQuery = (MultiTermQuery) o;

      if (!term.equals(multiTermQuery.term)) return false;

      return getBoost() == multiTermQuery.getBoost();
    }
{code}

Advantage:

Same as above. Here, WildcardQuery.equals is not needed as it does not define any new state. (FuzzyQuery.equals is still needed because FuzzyQuery defines new state.) 
"
0,"generate-maven-artifacts target should include all non-Mavenized Lucene & Solr dependencies. Currently, in addition to deploying artifacts for all of the Lucene and Solr modules to a repository (by default local), the {{generate-maven-artifacts}} target also deploys artifacts for the following non-Mavenized Solr dependencies (lucene_solr_3_1 version given here):

# {{solr/lib/commons-csv-1.0-SNAPSHOT-r966014.jar}} as org.apache.solr:solr-commons-csv:3.1
# {{solr/lib/apache-solr-noggit-r944541.jar}} as org.apache.solr:solr-noggit:3.1
\\ \\
The following {{.jar}}'s should be added to the above list (lucene_solr_3_1 version given here):
\\ \\
# {{lucene/contrib/icu/lib/icu4j-4_6.jar}}
# {{lucene/contrib/benchmark/lib/xercesImpl-2.9.1-patched-XERCESJ}}{{-1257.jar}}
# {{solr/contrib/clustering/lib/carrot2-core-3.4.2.jar}}**
# {{solr/contrib/uima/lib/uima-an-alchemy.jar}}
# {{solr/contrib/uima/lib/uima-an-calais.jar}}
# {{solr/contrib/uima/lib/uima-an-tagger.jar}}
# {{solr/contrib/uima/lib/uima-an-wst.jar}}
# {{solr/contrib/uima/lib/uima-core.jar}}
\\ \\
I think it makes sense to follow the same model as the current non-Mavenized dependencies:
\\ \\
* {{groupId}} = {{org.apache.solr/.lucene}}
* {{artifactId}} = {{solr-/lucene-}}<original-name>,
* {{version}} = <lucene-solr-release-version>.

**The carrot2-core jar doesn't need to be included in trunk's release artifacts, since there already is a Mavenized Java6-compiled jar.  branch_3x and lucene_solr_3_1 will need this Solr-specific Java5-compiled maven artifact, though."
0,"Split DocMaker into ContentSource and DocMaker. This issue proposes some refactoring to the benchmark package. Today, DocMaker has two roles: collecting documents from a collection and preparing a Document object. These two should actually be split up to ContentSource and DocMaker, which will use a ContentSource instance.

ContentSource will implement all the methods of DocMaker, like getNextDocData, raw size in bytes tracking etc. This can actually fit well w/ 1591, by having a basic ContentSource that offers input stream services, and wraps a file (for example) with a bzip or gzip streams etc.

DocMaker will implement the makeDocument methods, reusing DocState etc.

The idea is that collecting the Enwiki documents, for example, should be the same whether I create documents using DocState, add payloads or index additional metadata. Same goes for Trec and Reuters collections, as well as LineDocMaker.
In fact, if one inspects EnwikiDocMaker and LineDocMaker closely, they are 99% the same and 99% different. Most of their differences lie in the way they read the data, while most of the similarity lies in the way they create documents (using DocState).
That led to a somehwat bizzare extension of LineDocMaker by EnwikiDocMaker (just the reuse of DocState). Also, other DocMakers do not use that DocState today, something they could have gotten for free with this refactoring proposed.

So by having a EnwikiContentSource, ReutersContentSource and others (TREC, Line, Simple), I can write several DocMakers, such as DocStateMaker, ConfigurableDocMaker (one which accpets all kinds of config options) and custom DocMakers (payload, facets, sorting), passing to them a ContentSource instance and reuse the same DocMaking algorithm with many content sources, as well as the same ContentSource algorithm with many DocMaker implementations.

This will also give us the opportunity to perf test content sources alone (i.e., compare bzip, gzip and regular input streams), w/o the overhead of creating a Document object.

I've already done so in my code environment (I extend the benchmark package for my application's purposes) and I like the flexibility I have. I think this can be a nice contribution to the benchmark package, which can result in some code cleanup as well."
0,"TermVectorAccessor, transparent vector space access . This class visits TermVectorMapper and populates it with information transparent by either passing it down to the default terms cache (documents indexed with Field.TermVector) or by resolving the inverted index."
0,"Subclasses do not have write access to StatusLine. HttpMethodBase provides the readStatusLine method explicitly designed for
subclasses to override. However, any attempt to do so quickly encounters issues
since the subclass does not have access to the statusLine member variable in
HttpMethodBase. The same holds true for several other member variables as well.

Recommend that all access to member variables occur through accessors and that
mutators be provided to set them. See patch below.
----------------------------------------------------------

Index: HttpMethodBase.java
===================================================================
--- HttpMethodBase.java	(revision 390815)
+++ HttpMethodBase.java	(working copy)
@@ -563,7 +563,7 @@
      * @return the status code associated with the latest response.
      */
     public int getStatusCode() {
-        return statusLine.getStatusCode();
+        return getStatusLine().getStatusCode();
     }
 
     /**
@@ -577,6 +577,13 @@
     }
 
     /**
+     * @param statusLine The statusLine to set.
+     */
+    protected final void setStatusLine(StatusLine statusLine) {
+        this.statusLine = statusLine;
+    }
+
+    /**
      * Checks if response data is available.
      * @return <tt>true</tt> if response data is available, <tt>false</tt>
otherwise.
      */
@@ -798,7 +805,7 @@
      * @return The status text.
      */
     public String getStatusText() {
-        return statusLine.getReasonPhrase();
+        return getStatusLine().getReasonPhrase();
     }
 
     /**
@@ -920,16 +927,16 @@
         }
         LOG.debug(""Resorting to protocol version default close connection policy"");
         // missing or invalid connection header, do the default
-        if (this.effectiveVersion.greaterEquals(HttpVersion.HTTP_1_1)) {
+        if (getEffectiveVersion().greaterEquals(HttpVersion.HTTP_1_1)) {
             if (LOG.isDebugEnabled()) {
-                LOG.debug(""Should NOT close connection, using "" +
this.effectiveVersion.toString());
+                LOG.debug(""Should NOT close connection, using "" +
getEffectiveVersion().toString());
             }
         } else {
             if (LOG.isDebugEnabled()) {
-                LOG.debug(""Should close connection, using "" +
this.effectiveVersion.toString());
+                LOG.debug(""Should close connection, using "" +
getEffectiveVersion().toString());
             }
         }
-        return this.effectiveVersion.lessEquals(HttpVersion.HTTP_1_0);
+        return getEffectiveVersion().lessEquals(HttpVersion.HTTP_1_0);
     }
     
     /**
@@ -980,14 +987,14 @@
         this.responseConnection = conn;
 
         checkExecuteConditions(state, conn);
-        this.statusLine = null;
+        setStatusLine(null);
         this.connectionCloseForced = false;
 
         conn.setLastResponseInputStream(null);
 
         // determine the effective protocol version
-        if (this.effectiveVersion == null) {
-            this.effectiveVersion = this.params.getVersion(); 
+        if (getEffectiveVersion() == null) {
+            setEffectiveVersion(this.params.getVersion()); 
         }
 
         writeRequest(state, conn);
@@ -996,7 +1003,7 @@
         // the method has successfully executed
         used = true; 
 
-        return statusLine.getStatusCode();
+        return getStatusCode();
     }
 
     /**
@@ -1048,8 +1055,8 @@
         getRequestHeaderGroup().clear();
         getResponseHeaderGroup().clear();
         getResponseTrailerHeaderGroup().clear();
-        statusLine = null;
-        effectiveVersion = null;
+        setStatusLine(null);
+        setEffectiveVersion(null);
         aborted = false;
         used = false;
         params = new HttpMethodParams();
@@ -1586,18 +1593,18 @@
         ""enter HttpMethodBase.readResponse(HttpState, HttpConnection)"");
         // Status line & line may have already been received
         // if 'expect - continue' handshake has been used
-        while (this.statusLine == null) {
+        while (getStatusLine() == null) {
             readStatusLine(state, conn);
             processStatusLine(state, conn);
             readResponseHeaders(state, conn);
             processResponseHeaders(state, conn);
             
-            int status = this.statusLine.getStatusCode();
+            int status = getStatusCode(); 
             if ((status >= 100) && (status < 200)) {
                 if (LOG.isInfoEnabled()) {
-                    LOG.info(""Discarding unexpected response: "" +
this.statusLine.toString()); 
+                    LOG.info(""Discarding unexpected response: "" +
getStatusLine().toString()); 
                 }
-                this.statusLine = null;
+                setStatusLine(null);
             }
         }
         readResponseBody(state, conn);
@@ -1675,7 +1682,7 @@
         if (Wire.CONTENT_WIRE.enabled()) {
             is = new WireLogInputStream(is, Wire.CONTENT_WIRE);
         }
-        boolean canHaveBody = canResponseHaveBody(statusLine.getStatusCode());
+        boolean canHaveBody = canResponseHaveBody(getStatusCode());
         InputStream result = null;
         Header transferEncodingHeader =
responseHeaders.getFirstHeader(""Transfer-Encoding"");
         // We use Transfer-Encoding if present and ignore Content-Length.
@@ -1714,7 +1721,7 @@
         } else {
             long expectedLength = getResponseContentLength();
             if (expectedLength == -1) {
-                if (canHaveBody &&
this.effectiveVersion.greaterEquals(HttpVersion.HTTP_1_1)) {
+                if (canHaveBody &&
getEffectiveVersion().greaterEquals(HttpVersion.HTTP_1_1)) {
                     Header connectionHeader =
responseHeaders.getFirstHeader(""Connection"");
                     String connectionDirective = null;
                     if (connectionHeader != null) {
@@ -1850,19 +1857,19 @@
         } while(true);
 
         //create the status line from the status string
-        statusLine = new StatusLine(s);
+        setStatusLine(new StatusLine(s));
 
         //check for a valid HTTP-Version
-        String versionStr = statusLine.getHttpVersion();
+        String versionStr = getStatusLine().getHttpVersion();
         if (getParams().isParameterFalse(HttpMethodParams.UNAMBIGUOUS_STATUS_LINE) 
            && versionStr.equals(""HTTP"")) {
             getParams().setVersion(HttpVersion.HTTP_1_0);
             if (LOG.isWarnEnabled()) {
                 LOG.warn(""Ambiguous status line (HTTP protocol version missing):"" +
-                statusLine.toString());
+                getStatusLine().toString());
             }
         } else {
-            this.effectiveVersion = HttpVersion.parse(versionStr);
+            setEffectiveVersion(HttpVersion.parse(versionStr));
         }
 
     }
@@ -1943,9 +1950,9 @@
                     readResponseHeaders(state, conn);
                     processResponseHeaders(state, conn);
 
-                    if (this.statusLine.getStatusCode() ==
HttpStatus.SC_CONTINUE) {
+                    if (getStatusCode() == HttpStatus.SC_CONTINUE) {
                         // Discard status line
-                        this.statusLine = null;
+                        setStatusLine(null);
                         LOG.debug(""OK to continue received"");
                     } else {
                         return;
@@ -2087,7 +2094,7 @@
      */
     private String getRequestLine(HttpConnection conn) {
         return  HttpMethodBase.generateRequestLine(conn, getName(),
-                getPath(), getQueryString(), this.effectiveVersion.toString());
+                getPath(), getQueryString(), getEffectiveVersion().toString());
     }
 
     /**
@@ -2128,6 +2135,13 @@
     }
 
     /**
+     * @param effectiveVersion The effectiveVersion to set.
+     */
+    protected final void setEffectiveVersion(HttpVersion effectiveVersion) {
+        this.effectiveVersion = effectiveVersion;
+    }
+
+    /**
      * Per RFC 2616 section 4.3, some response can never contain a message
      * body.
      *
@@ -2358,7 +2372,7 @@
     ) {
         // set used so that the response can be read
         this.used = true;
-        this.statusLine = statusline;
+        setStatusLine(statusline);
         this.responseHeaders = responseheaders;
         this.responseBody = null;
         this.responseStream = responseStream;"
0,"Review and potentially remove unused/unsupported Contribs. Some of our contribs appear to be lacking for development/support or are missing tests.  We should review whether they are even pertinent these days and potentially deprecate and remove them.

One of the things we did in Mahout when bringing in Colt code was to mark all code that didn't have tests as @deprecated and then we removed the deprecation once tests were added.  Those that didn't get tests added over about a 6 mos. period of time were removed.

I would suggest taking a hard look at:
ant
db
lucli
swing

(spatial should be gutted to some extent and moved to modules)"
0,"Only search the index for the ""jcr:system"" tree if needed. Right now every time a query is executed the index of the current workspace as well as the index of the ""jcr:system"" tree is searched. A lot of queries are not searching in the ""jcr:system"" tree at all therefore it should be checked if the query contains paths that include ""jcr:system"". There are two relevant nodes in the query tree to find that out:

- what's the first location step and does it include the jcr:system tree? I think that's an easy one.
- does the query contain a jcr:deref node? If there is an intermediate result of a query may dereference into the jcr:system tree. 

This should notably speed up query execution if you are working extensively with versioning."
0,"Remove benchmark/lib/xml-apis.jar - JVM 1.5 already contains the required JAXP 1.3 implementation. On [LUCENE-2957|https://issues.apache.org/jira/browse/LUCENE-2957?focusedCommentId=13004991&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13004991], Uwe wrote:
{quote}
xml-apis.jar is not needed with xerces-2.9 and Java 5, as Java 5 already has these interface classes (JAXP 1.3). Xerces 2.11 needs it, because it ships with Java 6's JAXP release (containing STAX & Co. not available in Java 5).
{quote}

On the #lucene IRC channel, Uwe also wrote:
{noformat}
since we are on java 5 since 3.0
we have the javax APIs already available in the JVM
xerces until 2.9.x only needs JAXP 1.3
so the only thing you need is xercesImpl.jar
and serializer.jar
serializer.jar is shared between all apache xml projects, dont know the exact version number
ok you dont need it whan you only parse xml
as soon as you want to serialize a dom tree or result of an xsl transformation you need it
[...]
but if we upgrade to latest xerces we need it [the xml-apis jar] again unless we are on java 6
so the one shipped with xerces 2.11 is the 1.4 one
because xerces 2.11 supports Stax
{noformat}"
0,"DefaultItemStateProvider contains grow-only cache. The DefaultItemStateProvider class contains a private HashMap ""items"" which contains references to ItemState objects. The bad thing about this cache is, that it only grows, but is not being managed to forget about ""unused"" items.

Example: A repository which is filled with 9350 nodes and 52813 properties grows this items map to 1'667'557 (!) entries. In this concrete case, the VM all13ates 213MB to the heap of which 57MB is referenced by the DefaultItemStateProvider.items map."
0,"Factor maxMergeSize into findMergesForOptimize in LogMergePolicy. LogMergePolicy allows you to specify a maxMergeSize in MB, which is taken into consideration in regular merges, yet ignored by findMergesForOptimze. I think it'd be good if we take that into consideration even when optimizing. This will allow the caller to specify two constraints: maxNumSegments and maxMergeMB. Obviously both may not be satisfied, and therefore we will guarantee that if there is any segment above the threshold, the threshold constraint takes precedence and therefore you may end up w/ <maxNumSegments (if it's not 1) after optimize. Otherwise, maxNumSegments is taken into consideration.

As part of this change, I plan to change some methods to protected (from private) and members as well. I realized that if one wishes to implement his own LMP extension, he needs to either put it under o.a.l.index or copy some code over to his impl.

I'll attach a patch shortly."
0,"Source release files missing the *.pom.template files. The source release files should contain the *.pom.template files, otherwise it is not possible to build the maven artifacts using ""ant generate-maven-artifacts"" from official release files."
0,Add benchmark task for FastVectorHighlighter. 
0,"Determination of property state difference should skip binary values. o.a.j.jcr2spi.state.PropertyState.diffPropertyData, PropertyData) should alway consider two binary values to be different. The current implementation compares two binary values with equals(). An implementation will in general have to do a byte by byte comparison of both values. This is most likely always more expensive than considering the values different right from the start. 

"
0,"Document is partially indexed on an unhandled exception. With LUCENE-843, it's now possible for a subset of a document's
fields/terms to be indexed or stored when an exception is hit.  This
was not the case in the past (it was ""all or none"").

I plan to make it ""all or none"" again by immediately marking a
document as deleted if any exception is hit while indexing it.

Discussion leading up to this:

  http://www.gossamer-threads.com/lists/lucene/java-dev/56103
"
0,"Logger (Category) names don't follow common pattern. The Wire class uses two loggers named unexpected. The ""org.apache.commons.""
prefix is missing - so you can't mute all debug level statements with a
one-liner in you log4j.properties for example:

  log4j.logger.org.apache.commons.httpclient INFO

You have to add this, too:

  log4j.logger.httpclient INFO

Please prepend the ""org.apache.commons."" before both names.

Cheers,
Christian

<code>
class Wire {

    public static Wire HEADER_WIRE = new
Wire(LogFactory.getLog(""httpclient.wire.header""));
    
    public static Wire CONTENT_WIRE = new
Wire(LogFactory.getLog(""httpclient.wire.content""));

</code>

http://svn.apache.org/viewcvs.cgi/jakarta/commons/proper/httpclient/trunk/src/java/org/apache/commons/httpclient/Wire.java?rev=155418&view=markup"
0,"apply delete-by-Term and docID immediately to newly flushed segments. Spinoff from LUCENE-2324.

When we flush deletes today, we keep them as buffered Term/Query/docIDs that need to be deleted.  But, for a newly flushed segment (ie fresh out of the DWPT), this is silly, because during flush we visit all terms and we know their docIDs.  So it's more efficient to apply the deletes (for this one segment) at that time.

We still must buffer deletes for all prior segments, but these deletes don't need to map to a docIDUpto anymore; ie we just need a Set.

This issue should wait until LUCENE-1076 is in since that issue cuts over buffered deletes to a transactional stream."
0,IntParser and FloatParser unused by FieldCacheImpl. FieldCacheImpl doesn't use IntParser or FloatParser to parse values
0,"Consolidate Near Real Time and Reopen API semantics. We should consolidate the IndexWriter.getReader and the IndexReader.reopen semantics, since most people are already using the IR.reopen() method, we should simply add::
{code}
IR.reopen(IndexWriter)
{code}

Initially, it could just call the IW.getReader(), but it probably should switch to just using package private methods for sharing the internals"
0,"Improve name resolution. As discussed in JCR-685, the current CachingNamespaceResolver class contains excessive synchronization causing monitor contention that reduces performance.

In JCR-685 there's a proposed patch that replaces synchronization with a read-write lock that would allow concurrent read access to the name cache."
0,"Deprecated AbstractWebdavServlet should be empty and extend new AbstractWebdavServlet. The AbstractWebdavServlet has been copied from the jcr-server to the webdav project. The class at the old location has been marked deprecated. I suggest that in addition to marking it deprecated we should have this class extend the new AbstractWebdavServlet class from the webdav project but not contain any fields and methods. This way, users of the old class will always get the newest and best version but can still use the old class.

Will provide a patch for this proposal"
0,"RepositoryConfig instance can not be reused once it has been passed to RepositoryImpl constructor. RepositoryConfig and other *Config classes maintain state apart from parsed configuration information;
specifically they instantiate FileSystem implementations based on their configurations. this makes it
for the config consumers very hard to control the lifecycle of such FileSystem instances as they need
to close the file systems on repository shutdown.

the following code illustrates the issue:

RepositoryConfig repConf = RepositoryConfig.create(configFile, repHomeDir);
RepositoryImpl rep = RepositoryImpl.create(repConf);
// ...
rep.shutdown();

rep = RepositoryImpl.create(repConf);   
// ==> repConfig (et al) contains references to FileSystem objects 
// that have been closed by previous rep.shutdown() call

"
0,"surround test code is incompatible with *Test pattern in test target.. Attachments to follow:
renamed BooleanQueryTest to BooleanQueryTst,
renamed ExceptionQueryTest to ExceptionQueryTst,
patch for the remainder of the test code to use the renamed classes."
0,"Handle virtual hosts, relative urls, multi-homing. Need to be able to open a socket to one ipaddress (or hostname) and then include
a virtual hostname in the Host header. Use InetAddress class perhaps."
0,"All spatial contrib shape classes implement equals but not hashCode. violates contract - at a min, need to implement return constant."
0,"SmartChineseAnalyzer javadoc improvement. Chinese -> English, and corrections to match reality (removes several javadoc warnings)"
0,"Update idea plugin version. We are using a quite outdated version (2.0). The most recent idea plugin release is 2.2.

Index: jackrabbit-parent/pom.xml
===================================================================
--- jackrabbit-parent/pom.xml	(revision 802755)
+++ jackrabbit-parent/pom.xml	(working copy)
@@ -73,7 +73,7 @@
       <plugin>
         <!-- http://maven.apache.org/plugins/maven-idea-plugin/ -->
         <artifactId>maven-idea-plugin</artifactId>
-        <version>2.0</version>
+        <version>2.2</version>
         <configuration>
           <downloadSources>true</downloadSources>
           <jdkLevel>1.5</jdkLevel>
"
0,"rep:excerpt() should also work on properties. Currently the rep:excerpt() function can only be used to create an excerpt with highlight information from a node, the function should also support highlighting of string properties."
0,"Removing-nodes with unexpected nodetype. tobias adds a logic with JCR-973 that the DefaultHandler checks the nodetype of the jcr:content node
and if it does not match it will be deleted and created with the new one.
i think its dangerous to automacially change a node type.
if the node was added programatically and then saved through webdav it could happen that the nodetype will be changed
from nt:resource to the nt:unstructured.
if some logic depends on the nodetype the failure search will be hard ;-)"
0,Add UserManager#getAuthorizableByPath(String) for symmetry with JCR-3037. JCR-3037 added Authorizable#getPath. I would suggest to also add UserManager#getAuthorizableByPath that was symmetric to Authorizable#getPath
0,"deprecate term and getTerm in MultiTermQuery. This means moving getTerm and term up to sub classes as appropriate and reimplementing equals, hashcode as appropriate in sub classes."
0,"extend ConsistencyChecker API to allow adoption of orphaned nodes to a to-be-specified parent node. The optional ConsistencyChecker API on persistence managers allows analyzing and fixing storage inconsistencies. The current fixup code though does not attempt to ""adopt"" orphaned nodes."
0,"Item retrieval inefficient after refresh. When RepositoryService#getItemInfos() returns a sufficiently large batch for a path, then the second invocation of getItem() below is significantly slower than the first. 

String path = ...
Item i = session.getItem(path);
i.refresh(false); // same for refresh(true)
session.getItem(path);

In my test setup RepositoryService#getItemInfos() returns 3946 elements. The first invocation takes approx. 800ms, the second 3000ms."
0,"package org.apache.xml.utils does not exist (JDK 1.5.0). Executing ""maven jar"" on a freshly checked out source tree fails with the following error messages when using JDK 1.5.0:

/home/jukkaz/src/jackrabbit/src/java/org/apache/jackrabbit/core/xml/DocViewSAXEventGenerator.java:24: package org.apache.xml.utils does not exist
import org.apache.xml.utils.XMLChar;
                            ^
/home/jukkaz/src/jackrabbit/src/java/org/apache/jackrabbit/core/xml/DocViewSAXEventGenerator.java:142: cannot find symbol
symbol  : variable XMLChar
location: class org.apache.jackrabbit.core.xml.DocViewSAXEventGenerator
            if (!XMLChar.isValidName(elemName)) {
                 ^
/home/jukkaz/src/jackrabbit/src/java/org/apache/jackrabbit/core/xml/DocViewSAXEventGenerator.java:162: cannot find symbol
symbol  : variable XMLChar
location: class org.apache.jackrabbit.core.xml.DocViewSAXEventGenerator
                if (!XMLChar.isValidName(attrName)) {

The same build succeeds without problems on JDK 1.4.2_06.

I found some reports about similar problems after upgrading from JDK 1.4 to 1.5. It seems that the org.apache.xml.utils.XMLChar was a part (undocumented?) of the standard JDK classpath, but that it has been dropped from JDK 1.5.

A similar (the same?) XMLChar utility class can be found in the org.apache.xerces.utils package, which is automatically included by the Xerces dependency. The following change fixes the problem on JDK 1.5.0 and seems to work fine also on JDK 1.4.2_06.

Index: src/java/org/apache/jackrabbit/core/xml/DocViewSAXEventGenerator.java
===================================================================
--- src/java/org/apache/jackrabbit/core/xml/DocViewSAXEventGenerator.java      (revision 57540)
+++ src/java/org/apache/jackrabbit/core/xml/DocViewSAXEventGenerator.java      (working copy)
@@ -21,7 +21,7 @@
 import org.apache.jackrabbit.core.state.PropertyState;
 import org.apache.jackrabbit.core.util.Base64;
 import org.apache.log4j.Logger;
-import org.apache.xml.utils.XMLChar;
+import org.apache.xerces.util.XMLChar;
 import org.xml.sax.ContentHandler;
 import org.xml.sax.SAXException;
 import org.xml.sax.helpers.AttributesImpl;

"
0,"[PATCH]character encoding handling is invalid at multipart. Hi,

Commons-Httpclient handle character encoding incorrect at multipart. This is 
significant problem for other than English people like me. Multipart has two 
encoding. First is header encoding which specify header of each part. Second 
is it's body encoding. Body encoding works well but header encoding is fixed 
as 'asc-ii'. This problem user following situation.

* upload file which file name is described by other than ""asc-ii"".
* use parameter which include other than ""asc-ii"" character.

Unfortunately , It seems RFC doesn't define header encoding for multipart but 
a lot of people needs set header encoding for thier own laungage. I attached
the patch. Please fix this problem.

regards,

Takashi Okamoto"
0,"Persian Arabic Analyzer cleanup. While browsing through the code I found some places for minor improvements in the new Arabic / Persian Analyzer code. 

- prevent default stopwords from being loaded each time a default constructor is called
- replace if blocks with a single switch
- marking private members final where needed
- changed protected visibility to final in final class.

"
0,"DefaultLoginModule performs anonymous login in case of unsupported Credentials implementation. If Repository.login in called with an unsupported Credentials implementation the DefaultLoginModule#getCredentials returns null
and thus an anonymous login. The expected behavior from my point of view however was, that login with unsupported credentials 
would not be handled by the LoginModule and - if no other module is able to handle it -  login would consequently fails."
0,"Introduce QValueConstraint and change return type of QPropertyDefinition.getValueConstraints(). public interface QValueConstraint {
+
+    /**
+     * Empty array of <code>QValueConstraint</code>.
+     */
+    public static final QValueConstraint[] EMPTY_ARRAY = new QValueConstraint[0];
+
+    /**
+     * Check if the specified value matches this constraint.
+     *
+     * @param value The value to be tested.
+     * @throws ConstraintViolationException If the specified value is
+     * <code>null</code> or does not matches the constraint.
+     * @throws RepositoryException If another error occurs.
+     */
+    void check(QValue value) throws ConstraintViolationException, RepositoryException;
+
+    /**
+     * For constraints that are not namespace prefix mapping sensitive this
+     * method returns the same defined in
+     * <code>{@link PropertyDefinition#getValueConstraints()}</code>.
+     * <p/>
+     * Those that are namespace prefix mapping sensitive (e.g.
+     * <code>NameConstraint</code>, <code>PathConstraint</code> and
+     * <code>ReferenceConstraint</code>) return an expanded string.
+     *
+     * @return the expanded definition String
+     */
+    String getExpandedDefinition();
+
+}


+++ jackrabbit-spi/src/main/java/org/apache/jackrabbit/spi/QPropertyDefinition.java	(working copy)
@@ -45,7 +45,7 @@
      *
      * @return the array of value constraints.
      */
-    public String[] getValueConstraints();
+    public QValueConstraint[] getValueConstraints();
"
0,"Optimize Memory Use for Short-Lived Indexes (Do not load TermInfoIndex if you know the queries ahead of time). Summary: Provide a way to avoid loading the TermInfoIndex into memory if you know all the terms you are ever going to query.

In our search environment, we have a large number of indexes (many thousands), any of which may be queried by any number of hosts.  These indexes may be very large (~1M document), but since we have a low term/doc ratio, we have 7-11M terms.  With an index interval of 128, that means ~70-90K terms.  On loading the index, it instantiates a Term, a TermInfo, a String, and a char[].  When the document is long lived, this makes some sense because you can quickly search the list of terms using binary search.  However, since we throw away the Indexes very often, a lot of garbage is created per query

Here's an example where we load a large index 10 times.  This corresponds to 7MB of garbage per query.
          percent          live          alloc'ed  stack class
 rank   self  accum     bytes objs     bytes  objs trace name
    1  4.48%  4.48%   4678736 128946  23393680 644730 387749 char[]
    3  3.95% 12.61%   4126272 128946  20631360 644730 387751 org.apache.lucene.index.TermInfo
    6  2.96% 22.71%   3094704 128946  15473520 644730 387748 java.lang.String
    8  1.98% 26.97%   2063136 128946  10315680 644730 387750 org.apache.lucene.index.Term

This adds up after a while.  Since we know exactly which Terms we're going to search for before even opening the index, there's no need to allocate this much memory.  Upon opening the index, we can go through the TII in sequential order and retrieve the entries into the main term dictionary and reduce the storage requirements dramatically.  This reduces the amount of garbage generated by querying by about 60% if you only make 1 query/index with a 77% increase in throughput.

This is accomplished by factoring out the ""index loading"" aspects of TermInfosReader into a new file, SegmentTermInfosReader.  TermInfosReader becomes a base class to allow access to terms.  A new class, PrefetchedTermInfosReader will, upon startup, sort the passed in terms and retrieve the IndexEntries for those terms.  IndexReader and SegmentReader are modified to take new constructor methods that take a Collection of Terms that correspond to the total set of terms that will ever be searched in the life of the index.

In order to support the ""skipping"" behavior, some changes need to be made to SegmentTermEnum: specifically, we need to be able to go back an entry in order to retrieve the previous TermInfo and IndexPointer.  This is because, unlike the normal case, with the index  we want to return the value right before the intended field (so that we can be behind the desired termin the main dictionary).   For example, if we're looking for  ""apple"" in the index,  and the two adjacent values are ""abba"" and ""argon"", we want to return ""abba"" instead of ""argon"".  That way we won't miss any terms in the real index.   This code is confusing; it should probably be moved to an subclass of TermBuffer, but that required more code.  Not wanting to modify TermBuffer to keep it small, also lead to the odd NPE catch in SegmentTermEnum.java.  Stickler for contracts may want to rename SegmentTermEnum.skipTo() to a different name because it implements a different contract: but it would be useful for anyone trying to skip around in the TII, so I figured it was the right thing to do."
0,"Cleanup highlighter test class. cleanup highlighter test class - did some of this in another issue, but there is a bit more to do"
0,Codec is not consistently passed in internal API. While working on SOLR-1942 I ran into a couple of glitches with codec which is not consistently passed to SegmentsInfo and friends. Codecs should really be consistently passed though. I have fixed the pieces which lead to errors in Solr but I  guess there might be others too. Patch is coming up... 
0,"Implement RepositoryFactory in jcr2dav. It's currently a bit cumbersome to set up a spi2dav instance because of the two levels of factories (RepositoryFactory & RepositoryServiceFactory) involved in the process. It would be easier if spi2dav implemented RepositoryFactory directly, so downstream users would only need to provide the server URI parameter instead of specifying also the RepositoryServiceFactory classname.

To do this, spi2dav would need to depend also on jcr2spi. This change would actually simplify downstream projects, that then wouldn't need to depend also to jcr2spi to get JCR -> DAV connectivity."
0,"Occasional IndexingQueueTest failure. Usually the following assertion fails:

junit.framework.AssertionFailedError
	at junit.framework.Assert.fail(Assert.java:47)
	at junit.framework.Assert.assertTrue(Assert.java:20)
	at junit.framework.Assert.assertTrue(Assert.java:27)
	at org.apache.jackrabbit.core.query.lucene.IndexingQueueTest.testQueue(IndexingQueueTest.java:77)"
0,"Consistently refer to ""Apache Jackrabbit"" in docs and site. We need to consistently refer to our project (and product) as Apache Jackrabbit
on our site and docs, except where the context is obvious.
"
0,"additional conditional compliance tests for the caching module for Content-Encoding, Content-Location, Date, Expires, Server, Transfer-Encoding, and Vary headers. Patch is forthcoming."
0,"Add SearcherLifetimeManager, so you can retrieve the same searcher you previously used. The idea is similar to SOLR-2809 (adding searcher leases to Solr).

This utility class sits above whatever your source is for ""the
current"" searcher (eg NRTManager, SearcherManager, etc.), and records
(holds a reference to) each searcher in recent history.

The idea is to ensure that when a user does a follow-on action (clicks
next page, drills down/up), or when two or more searcher invocations
within a single user search need to happen against the same searcher
(eg in distributed search), you can retrieve the same searcher you
used ""last time"".

I think with the new searchAfter API (LUCENE-2215), doing follow-on
searches on the same searcher is more important, since the ""bottom""
(score/docID) held for that API can easily shift when a new searcher
is opened.

When you do a ""new"" search, you record the searcher you used with the
manager, and it returns to you a long token (currently just the
IR.getVersion()), which you can later use to retrieve the same
searcher.

Separately you must periodically call prune(), to prune the old
searchers, ideally from the same thread / at the same time that
you open a new searcher."
0,"Allow storing user data when IndexWriter.commit() is called. Spinoff from here:

    http://www.mail-archive.com/java-user@lucene.apache.org/msg22303.html

The idea is to allow optionally passing an opaque String commitUserData to the IndexWriter.commit method.  This String would be stored in the segments_N file, and would be retrievable by an IndexReader.  Applications could then use this to assign meaning to each commit.

It would be nice to get this done for 2.4, but I don't think we should hold the release for it."
0,"speed up automaton seeking in nextString. While testing, i found there are some queries (e.g. wildcard ?????????) that do quite a lot of backtracking.

nextString doesn't handle this particularly well, when it walks the DFA, if it hits a dead-end and needs to backtrack, it increments the bytes, and starts over completely.

alternatively it could save the path information in an int[], and backtrack() could return a position to restart from, instead of just a boolean.
"
0,"Build file for Highlighter contrib works when run in isolation, but not when core dist is run. Build.xml for Highlighter does not work when compilation is triggered by clean core dist call.

Patch has changes to fix this by updating build.xml to follow xml-query-parser build.xml"
0,CanAddChildNodeCallWithNodeTypeTest.testDefinedAndLegalType() may fail if protected child node definition is picked. If the utility NodeTypeUtil.locateChildNodeDef() picks a protected child node definition the test case will fail because it is not allowed add a protected child node.
0,"HttpMime StringBody constructor throws specification unnecessarily declares UnsupportedEncodingException. The string body constructors that take a charset unnecessarily throw UnsupportedEncodingException - if you have Charset, the encoding is by definition supported:

    public StringBody(
            final String text, 
            final String mimeType, 
            Charset charset) throws UnsupportedEncodingException {
        super(mimeType);
        if (text == null) {
            throw new IllegalArgumentException(""Text may not be null"");
        }
        if (charset == null) {
            charset = Charset.defaultCharset();
        }
        this.content = text.getBytes(charset.name());
        this.charset = charset;
    }
    
    public StringBody(final String text, Charset charset) throws UnsupportedEncodingException {
        this(text, ""text/plain"", charset);
    }
    
I suggest to change this to

    public StringBody(
            final String text, 
            final String mimeType, 
            Charset charset)  {
        super(mimeType);
        if (text == null) {
            throw new IllegalArgumentException(""Text may not be null"");
        }
        if (charset == null) {
            charset = Charset.defaultCharset();
        }
        this.content = text.getBytes(charset);
        this.charset = charset;
    }
    
    public StringBody(final String text, Charset charset) {
        this(text, ""text/plain"", charset);
    }

The important change is to change

        this.content = text.getBytes(charset.name());

to 

        this.content = text.getBytes(charset);

which will not throw and hence the throws specifications can be removed.
"
0,"httpclient build requires jdk 1.4 or jce in classpath. Currently when a 'ant dist'
is performed httpclient is looking for javax.crypt.* which is in jce.jar

The build.xml and build.properties.sample need to be patched
so they allow the jce.jar file to be specified
just like the jsse.jar is specified.

will attach two patch files made from todays cvs"
0,"Change default value of SearchIndex extractorPoolSize. The current default value for the extractorPoolSize is 0, which means it is disabled by default. I think we should change that default because it is a useful feature and people should not have to dig through documentation to make use of it.

The new default should be computed based on the available processors. I suggest we use: 2 * Runtime.availableProcessors()"
0,"cache module should strip 'Content-Encoding: identity' from responses. Per the RFC, the ""identity"" content coding SHOULD NOT be used in the Content-Encoding header:

http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.5

The current implementation will pass 'Content-Encoding: identity' through unchanged, although it would be simple enough to filter this out.
"
0,"[PATCH] Exception not thrown where it appears it should have been. Code creates an expection in an apparent error state, but doesn't throw it when it looks like it should

class: org.apache.jackrabbit.webdav.jcr.search.SearchResultProperty
ctor: public SearchResultProperty(DavProperty property, ValueFactory valueFactory) throws RepositoryException
around line 96

        } else {
            new IllegalArgumentException(""SearchResultProperty requires a list of 'dcr:column' xml elements."");
        }

Patch fixes this"
0,"DisjunctionSumScorer small tweak. Move ScorerDocQueue initialization from next() and skipTo() methods to the Constructor. Makes DisjunctionSumScorer a bit faster (less than 1% on my tests). 

Downside (if this is one, I cannot judge) would be throwing IOException from DisjunctionSumScorer constructors as we touch HardDisk there. I see no problem as this IOException does not propagate too far (the only modification I made is in BooleanScorer2)

if (scorerDocQueue == null) {
      initScorerDocQueue();
}
 

Attached test is just quick & dirty rip of  TestScorerPerf from standard Lucene test package. Not included as patch as I do not like it.


All test pass, patch made on trunk revision 613923
"
0,"implement PerFieldAnalyzerWrapper.getOffsetGap. PerFieldAnalyzerWrapper does not delegates calls to getOffsetGap(Fieldable), instead it returns the default values from the implementation of Analyzer. (Similar to LUCENE-659 ""PerFieldAnalyzerWrapper fails to implement getPositionIncrementGap"")"
0,"[PATCH] Refactoring of SpanScorer. Refactored some common code in next() and skipTo(). 
Removed dependency on score value for next() and skipTo(). 
Passes all current tests at just about the same speed 
as the current version. Added minimal javadoc. 
 
Iirc, there has been some discussion on the dependency of next() 
and skipTo() on the score value, but I don't remember the conclusion. 
In case that dependency should stay in, it can be adapted 
in the refactored code."
0,"[PATCH] usage feedback for IndexFiles demo. Just a small patch that adds ""usage"" output if the demo is called without a 
parameter, makes it a little bit friendlier to beginners."
0,"Support for boost factor in MoreLikeThis. This is a patch I made to be able to boost the terms with a specific factor beside the relevancy returned by MoreLikeThis. This is helpful when having more then 1 MoreLikeThis in the query, so words in the field A (i.e. Title) can be boosted more than words in the field B (i.e. Description)."
0,JSR 283: Evaluate Capabilities . Exposed by Session.hasCapability
0,"contrib/benchmark QueryMaker and Task Refactorings. Introduce an abstract QueryMaker implementation that shares much of the common code between the various QueryMaker implementations.

Add in a new QueryMaker for reading queries from a file that is specified in the properties.

Patch shortly, and if no concerns, will commit tomorrow or Wed."
0,"Lucene benchmark: objective performance test for Lucene. We need an objective way to measure the performance of Lucene, both indexing and querying, on a known corpus. This issue is intended to collect comments and patches implementing a suite of such benchmarking tests.

Regarding the corpus: one of the widely used and freely available corpora is the original Reuters collection, available from http://www-2.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz or http://people.csail.mit.edu/u/j/jrennie/public_html/20Newsgroups/20news-18828.tar.gz. I propose to use this corpus as a base for benchmarks. The benchmarking suite could automatically retrieve it from known locations, and cache it locally."
0,"IndexingAggregateTest#testNtFileAggregate fails occasionally. It may happen that the text extraction from a plain/text resource times out due to the tough extractor time out set on the indexing-test workspace.

The test should check if the indexing queue is empty before it executes a query."
0,NodeImpl.checkin() calls save() three times. The version related properties on a versionable node that is checked in are saved individually. There is no need to save them individually because such a node must not have pending changes and save() can be called safely on the node itself.
0,"disable positions for spellchecker ngram fields. In LUCENE-2391 we optimized spellchecker (re)build time/ram usage by omitting frequencies/positions/norms for single-valued fields,
among other things.

Now that we can disable positions but keep freqs, we should disable them for the n-gram fields, because the spellchecker does
not use positional queries.
"
